% ToDo:
%  * es müssen an allen align, gather und sonstigen Matheumgebungen die
%    Sterne hinzugefügt werden, wenn sie nicht eine Nummer haben sollen
%    align* gather*
%  * einzeilige Gleichungen sollen equation werden
%  * sinnvolle Umgebung für Algorithmen finden, unter Umständen selbst
%    etwas bauen; aber es sollte einheitlich im ganzen Dokument werden
%  * fehlenden Vorlesungen ergängen
%  * Verweise untereinander ergänzen: \thref, \eqref mit \label
%  * fehlende Grafiken ergänzen
%  * in den ersten Vorlesungen fehlen noch die Umgebungen für Sätze und
%    Folgerungen und Definitionen
%  * an sehr vielen Stellen fehlen die Dollars um mathematische Sachen
%    wie n oder k oder i oder \log oder $O(n^{2})$
%  * Überlegen, ob man für KOSTEN auch einen Befehl macht. Kommt es oft
%    genug im Dokument vor?
%
%  * Das Skript IMMER so hinterlassen, daß es mit pdflatex übersetzbar
%	 bleibt!
%
%  * Alle Bilder in center-environment

\documentclass[draft,halfparskip*,german,twoside]{scrreprt}

\usepackage{ifthen}

% Algorithmen:
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{INPUT:}}
\renewcommand{\algorithmicensure}{\textbf{OUTPUT:}}
\renewcommand{\algorithmiccomment}[1]{\textit{// #1}}

% Das sollte einen Fehler bringen, wenn eine for in einer forpar
% geschachtelt ist. Die innere for sollte dann auch ein pardo haben
\newcommand*{\FORPAR}[2][default]%
           {\begingroup\renewcommand{\algorithmicdo}{\textbf{pardo}}%
            \FOR[#1]{#2}}
\newcommand*{\ENDFORPAR}{\ENDFOR\endgroup}

\floatname{algorithm}{Algorithmus}
\renewcommand{\listalgorithmname}{Algorithmenliste}

\newcommand{\help}[1]{\textcolor{green}{help: #1}}
\newcommand{\todo}[1]{\textcolor{blue}{todo: #1}}

\usepackage[final]{graphics}
\usepackage{color}
\usepackage{fancyvrb}
\usepackage{ngerman}
\usepackage[pdftex]{graphicx}
\usepackage{amssymb}
\usepackage[intlimits,leqno]{amsmath}
\usepackage{paralist}
\usepackage[latin1]{inputenc}
\usepackage{xspace}
\usepackage[draft=false,colorlinks,bookmarksnumbered]{hyperref}

% http://user.informatik.uni-goettingen.de/~may/Ntheorem/
\usepackage[amsmath,thmmarks,hyperref]{ntheorem}

\usepackage{svn}         % Zum Auswerten und ordentlichen Darstellen der
                         % SVN-Schlüsselwörter (s. vor \begin{document})
                         % dafür muss in SVN noch das Flag svn:keywords
                         % auf "LastChangedRevision LastChangedDate"
                         % gesetzt werden

\usepackage{times}		% Times
\usepackage{helvet}
\usepackage[T1]{fontenc}

\usepackage{index}

\theoremstyle{break}
\theorembodyfont{\normalfont}
\newtheorem{satz}{Satz}
\newtheorem{bemerk}{Bemerkung}
\newtheorem{lemma}{Lemma}
\newtheorem{bsp}{Beispiel}
\newtheorem{folger}{Folgerung}
\newtheorem{defini}{Definition}
\newtheorem{verein}{Vereinbarung}

\theoremstyle{nonumberbreak}
\theoremsymbol{\ensuremath{_\blacksquare}}
\newtheorem{proof}{\textnormal{\scshape Beweis:}}

% Hier die Definition, wie \autoref die Umgebungen nennen soll, die mit
% \newtheorem definiert wurden
\newcommand*{\satzautorefname}{Satz}
\newcommand*{\bemerkautorefname}{Bemerkung}
\newcommand*{\definiautorefname}{Definition}
\newcommand*{\bspautorefname}{Beispiel}
\newcommand*{\festlautorefname}{Festlegung}
\newcommand*{\proofautorefname}{Beweis}
% Zwischen Unter- und Unterunterabschnitten sollte nicht unterschieden
% werden.
\renewcommand*{\subsectionautorefname}{Abschnitt}
\renewcommand*{\subsubsectionautorefname}{Abschnitt}

\pagestyle{headings}

\graphicspath{{figures/}}

% Zur Korrektur von \thref
% <news:col095$me8$2@n.ruf.uni-freiburg.de>
\makeatletter
% setzt / als Trenner zwischen Abschnitt und Satznummer
\def\@thmcountersep{/}
\makeatother

% Prof. Hecker schwankt in der Vorlesung ständig zwischen Zeit und Time
% bzw. Work und Arbeit. Diese zwei Befehle sollen helfen, die Begriffe
% einheitlich im Skript zu verwenden und ggf. alle Stellen leicht
% änderbar machen
\newcommand{\Time}{TIME\xspace}
\newcommand{\Work}{WORK\xspace}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}

\makeindex

\SVN $LastChangedRevision$
\SVN $LastChangedDate$

\begin{document}

\title{Parallele Algorithmen}
\author{Prof. Dr. Hans-Dietrich Hecker}
\date{SS 2005}
\maketitle

\clearpage
\chapter*{Vorwort}
{\itshape

  Dieses Skript ist im Rahmen des
  \href{http://www.minet.uni-jena.de/~joergs/skripte/}{Projekts
  "`Vorlesungsskripte der Fakultät für Mathematik und Informatik"'}
  entstanden und wird im Rahmen dieses Projekts weiter betreut. Das
  Skript ist nach bestem Wissen und Gewissen entstanden. Denoch
  garantiert weder der auf der Titelseite genannte Dozent, noch die
  Mitglieder des Projekts für dessen Fehlerfreiheit. Für etwaige Fehler
  und dessen Folgen wird von keiner der genannten Personen eine Haftung
  übernommen. Es steht jeder Person frei, dieses Skript zu lesen, zu
  verändern oder auf anderen Medien verfügbar zu machen, solange ein
  Verweis die Internetadresse
  \url{http://www.minet.uni-jena.de/~joergs/skripte/} des Projekts
  enthalten ist.

  Diese Ausgabe trägt die Versionsnummer~\SVNLastChangedRevision\ und ist
  vom \SVNDate. Eine (mögliche) aktuellere Ausgabe ist unter
  \url{http://www.minet.uni-jena.de/~joergs/skripte/pdf/} verfügbar.

  Jeder ist dazu aufgerufen Verbesserungen, Erweiterungen und
  Fehlerkorrekturen für das Skript einzureichen bzw. zu melden oder selbst
  einzupflegen -- einfach eine eMail an die
  \href{mailto:skripte@listserv.uni-jena.de}{Mailingliste
  \texttt{<skripte@listserv.uni-jena.de>}} senden. Weitere Informationen
  sind unter der oben genannten Internetadresse des Projekts verfügbar.

  Hiermit möchten wir allen Personen, die an diesem Skript mitgewirkt
  haben, vielmals danken:
  \begin{itemize}
   \item \href{mailto:joerg@alea.gnuu.de}{Jörg Sommer
    \texttt{<joerg@alea.gnuu.de>}} (2004/05)
   \item Fred Thiele (2005)
   \item Christian Raschka (2005)
  \end{itemize}
}

\clearpage
\pdfbookmark[0]{Inhaltsverzeichnis}{inhaltsverzeichnis}
\tableofcontents

\clearpage
\pdfbookmark[0]{Auflistung der Sätze}{theoremlist}
\chapter*{Auflistung der Theoreme}

\pdfbookmark[1]{Sätze}{satzlist}
\section*{Sätze}
\theoremlisttype{optname}
\listtheorems{satz}

\pdfbookmark[1]{Definitionen und Festlegungen}{definilist}
\section*{Definitionen und Festlegungen}
% \theoremlisttype{all}
\listtheorems{defini,festl}

\pdfbookmark[0]{Literaturverzeichnis}{literaturverzeichnis}
\begin{thebibliography}{Gibbon}
 \bibitem{JaJa} Joseph Ja'Ja: Introduction to parallel algorithms
 \bibitem{AKL} AKL: Introduction to parallel algorithms
 \bibitem{Gibbon} Gibbon and Rytter: parallel algorithms
\end{thebibliography}

\chapter{Einleitung}
Die Paradigmen (\emph{Entwurfsmuster}) für serielle Algorithmen werden nach den
Kriterien Speicher- und Zeitkomplexit"at, sowie ihrer einfachen Formulierbarkeit
bestimmt. Bezüglich dieser Kriterien sollen serielle Algorithmen "`gut"' sein.
Die von Neumann-Architektur ist ein allgemein anerkanntes Modell im Seriellen.
Als konkretes Modell wird die Random access machine (RAM) betrachtet unter der
Annahme, dass Basisoperationen in $O(1)$-Zeit bew"altigt werden.\\

Für parallele Algorithmen hingegen gibt es eine Vielzahl von Modellen. Im
Parallelen ergeben sich wesentlich mehr Abhängigkeiten vom konkreten Rechner.
\begin{itemize}
 \item Geringe Anzahl leistungsf"ahiger Prozessoren
 \item Hohe Anzahl "`einfacher"' Prozessoren
 \item Verteilte Systeme (nicht Gegenstand der Vorlesung, siehe Rechnerarchitektur)
\end{itemize}

\begin{table}
\centering
\begin{tabular}{lcl}
	Faktoren: & CC & Computational Concurrency \\
	& PA & Processor Allocation \\
	& SCH & Scheduling \\
	 & C & Communication \\
	 & S & Syncronization 
\end{tabular}
\caption{Faktoren}
\label{tab:parallele_faktoren}
\end{table}

\section{Algorithmenvorlesung}
	Wir haben also eine Vorlesung, die sich mit den Entwurfsprinzipien für
	parallele Algorithmen beschäftigt. Als allgemeine Prinzipien gelte für ein
	Modell:
	\begin{itemize}
	\item Einfachheit
	\item M"oglichst maschinenunabh"angig
	\item Implementierbarkeit
	\end{itemize}

\begin{defini}[Speed Up]
	$T^*(n)$ - Serielle Komplexität\\
	$T_p(n)$ - Zeit für parallelen Algorithmus mit $p$ Prozessoren\\
	\[
		S_{p}(n):=\frac{T^{\ast}(n)}{T_{p}(n)}
	\]\\
	(Beschleunigung gegenüber einem seriellen Algorithmus)
\end{defini}

\begin{satz}
	\[S_{p}(n)\le p \qquad (T^*(n) \le p T_p(n))\]
	
	\begin{proof}
		Die Simulation der Rechnungen der $p$ Prozessoren schrittweise nacheinander
		ergibt einen neuen seriellen Algorithmus, der nicht schneller als 
		$T^*(n)$ sein kann.
	\end{proof}

\end{satz}

\begin{bemerk}
	Wenn $S_{p}(n)\approx p$, dann
	\begin{gather*}
		  S_{p}(n) = \frac{T^{\ast}(n)}{T_{p}(n)}=p\\
		  \Rightarrow T^{\ast}(n) = p\,T_{p}(n)\\
		  \Rightarrow T^{p}(n) = \frac{T^{\ast}(n)}{p}
	\end{gather*}
\end{bemerk}

Folgerung: Mit polynomial vielen Prozessoren kann man ein NP-schweres
Problem \emph{nicht} parallel in Polynomialzeit lösen!

\begin{defini}[Effizienz]
\begin{equation*}
  E_{p}(n):= \frac{T_{1}(n)}{p\,T_{p}(n)}
\end{equation*}
\end{defini}

\section{DAG (directed acyclic graph)}

Für unsere Bedürfnisse ist das Modell des gerichteten, kreisfreien Graphen zu
allgemein. Hier ein Beispiel für die serielle und parallele Summation einer
Folge von Zahlen:\\

\begin{center}
\input{figures/dag_parallel.pdf_t} 
\end{center}

\begin{defini}
	Gegeben seien $p$ Prozessoren. Jedem inneren Knoten $i$ des DAG ordnen wir ein
	Paar $(j_i,t_i) : 1 \le j_i \le p$ mit $t_i$ - Zeitpunkt zu:
	\begin{enumerate}
		\item wenn $t_i=t_k$ für $i \not= k$, so gilt: $j_i \not= j_k$
		\item wenn $(i,k)$ eine gerichtete Kante ist, so ist $t_k \ge t_i +1$
	\end{enumerate}
\end{defini}

\section{Netzwerkmodell}

Für unsere Bedürfnisse ist das Netzwerkmodell zu speziell. Betrachten wir als
Beispiel das Modell eines Feldrechners:

\begin{center}
\input{figures/network_modell.pdf_t} 
\end{center}

%% in der pdf Grafik sind noch vier Zahlen doppelt,
%% die entfernt werden müssen (siehe pdf)
\begin{center}
\input{figures/netmodell_input.pdf_t}
\end{center}

\section{Synchrones Shared Memory Modell}

\begin{center}
\input{figures/parall_modell.pdf_t} 
\end{center}

\begin{bsp}[Matrixmultiplikation $y = Ax$]
	Annahme: \begin{enumerate}
		\item $ n > p$
		\item $ r = \frac{n}{p}$ (ganze Zahl)
	\end{enumerate}
	\begin{equation*}
		A = \begin{pmatrix}
			a_{11} & & \hdots & & a_{1n} \\
			\vdots & & \ddots & & \vdots \\
			a_{n1} & & \hdots & & a_{nn}
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			\vdots \\
			x_n
		\end{pmatrix}
	\end{equation*}
	Dabei übernimmt jeder Prozessor $P_i$ genau $r$ Zeilen, d.h. $P_1$ rechnet mit
	den Zeilen $1$ bis $r$, $P_2$ mit den Zeilen $r+1$ bis $2r$, usw.

\begin{algorithm}[htb]
\caption{hier muss nen Name hin}
\begin{algorithmic}
	\REQUIRE $A_{(n,n)}, \vec{x}$, Prozessorzahl $i$, $p$ Anzahl der
	Prozessoren ($r:=\frac{n}{p}\in\Z$)\\

	%\BEGIN
	    \STATE globalread( $x , z$ ) \COMMENT{Alle Prozessoren lesen diesen Vektor}\\
	    \STATE globalread( $A((i-1) r + 1 : ir , 1:n ), B$)\\
	    \STATE Berechne $w = B z$\\
	    \STATE globalwrite( $w , y((i-1)r+1:ir)$)\\
	%\END
\end{algorithmic}
\end{algorithm}

	\begin{bemerk}
		\begin{enumerate}
			\item In der ersten Zeile wird von allen Prozessoren
				zur gleichen Zeit gelesen (\emph{CR}, Concurrent Read)
			\item In den anderen Zeilen gibt es einen exklusiven Zugriff
				(\emph{ER}, Exclusive Read)
			\item Es ist keine Syncronisation nötig
			\item Alternative wäre, die Spalten zu Blöcken zusammenzufassen
				und danach die Summe zu bilden, allerdings ist dann eine
				Syncronisation notwendig!
		\end{enumerate}
	\end{bemerk}
	Zur Vereinfachung werden wir folgende Kurzschreibweise verwenden. Dies ist sinnvoll,
	da die Ein- und Ausgabe nur jeweils $O(1)$ Schritte dauert, und deshalb ignoriert
	werden kann.
\begin{algorithm}[htb]
\caption{Vereinfachung}
\begin{algorithmic}
\STATE globalRead($B,x$)
\STATE globalRead($C,y$)
\STATE $z := x + y$
\STATE globalWrite($z,A$) \\
\COMMENT{wird ersetzt durch:}
\STATE $A := B + C$
\end{algorithmic}
\end{algorithm}
\end{bsp}

Wir verwenden in dieser Vorlesung das Modell eines Syncronen Rechners mit Shared Memory und Abstraktion Random Access ($O(1)$.)

\begin{bsp}[Summenbeispiel]


\begin{algorithm}[htb]
\caption{Programm für Prozessor $i$}
\begin{algorithmic}[1]
\STATE $B(i) := A(i)$
\FOR{$h := 1$ to $\log n$}
	\IF{$i < \frac{n}{2^h}$}
		\STATE $B(i) := B(2i-1) + B(2i)$
	\ENDIF
	\IF{$i = 1$}
		\STATE $S := B(1)$
	\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}
\input{figures/add_tree.pdf_t}
Ein Problem tritt auf, bei einer geringen Anzahl Prozessoren (im Verhältnis zur Anzahl der Eingabedaten). D.h. man benötigt eine sinnvolle Allokation der Prozessoren.

\begin{algorithm}[htb]
\caption{Allokation}
\begin{algorithmic}[1]
%Ein Rechner habe $p=2^q \leq n = 2^k$ Prozessoren.
\REQUIRE Sei $l := \frac{n}{p} = 2^{k-q}$ und $A$ das Inputarray, dann wäre ein Programm für Prozessor s:
\FOR{$j := 1$ to $l$}
	\STATE $B(l*(s-1)+j) := A(l(s-1)+j)$
\ENDFOR
\FOR{$h := 1$ to $\log n$}
	\IF{$(k-h-q) \geq 0$}
		\FOR{$j := 2^{k-h-q} * (s-1)+1$ to $2^{k-h-q} *s$}
			\STATE $B(j) := B(2j-1) + B(2j)$
		\ENDFOR
	\ELSE
		\IF{$s \leq 2^{k-h}$}
			\STATE $B(s) := B(2s - 1) + B(2s)$
		\ENDIF
	\ENDIF
\ENDFOR
\IF{$s=1$}
	\STATE $S := B(1)$
\ENDIF
\end{algorithmic}
\end{algorithm}

Für $n=8$ und $p=4$ würde das folgendermaßen aussehen:\\
\input{figures/sum_bsp.pdf_t}
\end{bsp}

Allokation ist technisch aufwendig, ohne dass neue Ideen gefragt sind. Deshalb werden wir uns darauf konzentrieren, die Operationen zu beschreiben, die gleichzeitig möglich sind und die Allokation über das \emph{Theorem von Brent} zu behandeln.

\chapter{Die Güte von parallelen Algorithmen und das Theorem von Brent}

\underline{seriell:} \underline{TIME}, SPACE, Einfachheit des Programmes \\
\underline{parallel:} Zahl der Prozessoren

\begin{defini}[Kosten]
\[
	C(n) := T(n) * P(n)
\]
\end{defini}

\begin{satz}
Kosten können nicht unter der seriellen Komplexität liegen.
\end{satz}

\begin{defini}
Ein paralleler Algorithmus ist \emph{optimal}, wenn seine Kosten der seriellen Komplexität entsprechen.
\end{defini}

\begin{bsp}[Summe]
bei $n$ Prozessoren: $C(n)= \log n * n \qquad$ (nicht optimal) \\
bei $\frac{n}{\log n}$ Prozessoren: $C(n) = O(n)= O(n\frac{\log n}{n}) \qquad$ (optimal)\\
bei 1 Prozessor: $C(n) = O(n) \qquad$ (optimal) \\
$\to$ Definition ist unzureichend
\end{bsp}

\begin{defini}
Ein paralleler Algorithmus heisst WT-optimal (streng optimal), wenn er optimal ist und wenn es keinen schnelleren optimalen Algorithmus gibt.
\end{defini}

\begin{bemerk}
Diese Definition wird auch relativ zu einem speziellen Modell des Maschinentyps gebraucht.
\underline{Modelle:} syncrones shared Memory Modell (PRAM)\\
(EREW, CREW, CRCW(priority, common, arbitrary))
\end{bemerk}

\begin{defini}
WORK eines Algorithmus ist nach Definition die gesamte Anzahl der auszuführenden Einzeloperationen.
\end{defini}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Restbestände vom letzten Jahr
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Sei $P$ ein Problem:
% 
% Serieller Algo $A$, Input size $n$ $\quad\rightarrow\quad T_{A}(n)$
% 
% Def.: $T^{\ast}(n)$ sei das serielle Optimum
% 
% Sei $A$ parall. Algo., $p$ Prozessoren
% 
% %%
% 
% \begin{satz}[Theorem von Brent]
%   Was ist f"ur einen Algorithmus die maximale Anzahl an zur gleichen Zeit
%   ausf"uhrbaren Operationen?
% 
%   Im Normalfall kann man jeden parallelen Algorithmus auf jede Anzahl an
%   Prozessoren anpassen.
% \end{satz}
% 
% synchron\hfill--\hfill asynchron
% \begin{gather}
%   A\vec{x} (a_{11}\ldots a_{1n} \ldots a_{nn}) (x_{1}\ldots x_{n})=(y_{1}\ldots
%      y_{n})
% \end{gather}
% 
% Modell: \ldots
% 
% \begin{enumerate}
%  \item globalread(X,Y) X: Variable des shared memory, Y:local  variable
%   "ubertr"agt den Wert von X nach Y
%  \item globalwite(Y,X) "ubertr"agt Y nach X
% \end{enumerate}
% Beispiel: "`A:=B+C"'
%  globalread(B,x)
%  globalread(C,y)
%  setze z:=x+y (Prozessor-Arbeit)
%  globalwrite(z,A)
% 
% Algo.Mult.:\\ (A in Zeilen zerlegel)
% INPUT: $A_{(n,n)}, \vec{x}$, Prozessorzahl $i$, $p$ Anzahl der
% Prozessoren ($r:=\frac{n}{p}\in\Z$)\\
% OUTPUT: Die Komponenten $[(i-1)\,r+1,\ldots,i\,r] von \vec{y}=A\vec{x}$
% 
% begin
% \begin{enumerate}
%  \item globalread($\vec{x},\vec{z}$) \texttt{Alle Prozessoren lesen diesen Vektor}
%  \item globalread(A((i-1)r+1:ir,1:n), B)
%  \item berechne $\vec{w}=B\vec{z}$
%  \item globalwrite($\vec{w}, y((i-1)r+1:ir)$)
% \end{enumerate}
% end
% 
% \begin{bemerk}
%   Befehlt 1 erfordert CR (concurrent read) im Modell\\
%   Befehlt 4 erfordert nicht CW (concurrent write) sondern EW (exclusive
%   write)\\
%   EREW, CREW (Modell asynchron m"oglich), CRCW
% \end{bemerk}
% 
% A in Spalten zerlegen, $A_{1}x_{1}
% (a_{11},\ldots,a_{1r})(x_{1}\vdots x_{r})=z_{1}=A_{1}x_{1}
% \vec{y}=A_{1}\vec{x}_{1}+\ldots+A_{r}\vec{x}_{r}
% z_{i}=A_{i}\vec{x}_{i}
% (..) (\vdots)=(z_{1}+z_{2}+\ldots) \quad\rightarrow\quad$ erfordert ein
% synchrones Modell
% 
% Beispiel 2:\\
% INPUT: $n=2^{k}$ Zahlen in Array $A$, $n$ Proz, $i:i\le i \le n$\\
% OUTPUT: $\sum_{i=1}^{n}a_{i}\quad\Rightarrow\quad$ gespeichert im shared
% memory, Variable $S$
% 
% begin
% \begin{enumerate}
%  \item globalread(A(i), a)
%  \item globalwrite(a,B(i))
%  \item \texttt{for} $h=1$ \texttt{to} $\log n$ \texttt{do}
%   \begin{enumerate}
%    \item if ($i\le \frac{n}{2^{k}}$) then
%     begin
%     \begin{enumerate}
%      \item globalread(B(2\,i-1),x)
%      \item globalread(B(2\,i), y)
%      \item z:=x+y
%      \item globalwrite(z,B(i))
%     \end{enumerate}
%     end
%   \end{enumerate}
%  \item if (i=1) then globalwrite(z,S)
% \end{enumerate}
% end.
% 
% % Baumdiagramm
% 
% Aus Baum leicht zu erkennen: Berechnung in O($\log n$)
% 
% Kosten $C(n)=\underbrace{P(n)}_{O(n)}\,\underbrace{T(n)}_{O(\log
% n)}=O(n\log n)$
% 
% Serielles Optimum $T^{*}=O(n) C(N)\ge T^{*}(n)$
% 
% \begin{defini}
%   Ein Algorithmus hei"st optimal, wenn $C(n)=T^{*}(n)$
% \end{defini}
% 
% Es existiert ein Variante dieses Algorithmus, die $O(\log n)$ Zeit braucht
% und die Kosten $C(n)=O(n)$ hat.
% 
% % 2. Baumdiagramm
% 
% sei $\log n=r\in\N, \frac{n}{\log n}\in\Z, p_{1},\ldots,p_{\frac{n}{\log
% n}}$
% 
% Algorithmus: $p_{i}$ addiert die Zahlen $a_{(i-1)\log(n)+1}+\ldots+a_{i\log n}$
% (seriell in $O(\log n)$ \Time) weiter mit oberen Algorithmus mit $z_{i}$
% 
% Kosten: $C(n)=O(\log n)\,O(\frac{n}{\log n})=O(n)$
% 
% Ab sofort: Kurzform von globalread() bzw. globalwrite()
% 
% \begin{bemerk}
%   Das synchrone shared memory-Modell hei"st PRAM.
%   EREW-PRAM, CREW-PRAM, CRCW-PRAM (Unterteilung in {\it common}, d.h. das Schreiben ist nur dann
%   zugelassen, wenn alle Prozessoren das Gleiche schreiben,
%   sowie {\it arbitrary}, was z.Bsp. die Summenbildung über allen Werten darstellen kann.
%   Weiterhin: {\it  priority }.
% \end{bemerk}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% Vorgehen: Oberes Level - Unteres Level.
% liefert Zeitpakete(TIMEUNITS) /\ Prozessor-Allocation - schreibt Programm
% f"ur Prozessor $i$\\
\underline{TIME UNITS} (Zeiteinheit), in einer Zeiteinheit k"onnen bel. viele Operationen
zusammengefasst werden, die gleichzeitig augef"uhrbar sind. \\
"`pardo"'\\
\underline{pardo statement:}
\texttt{for} $1\le i\le u$ \texttt{pardo}

% $\quad\rightarrow\quad$ eventuell zerlgt in mehrere Takte, falls nicht
% gen"ugend Proz. da sind % & hier soll ein Spaltenwechsel hin
%  Programm f"ur Proz. $i$\\
% Prozessoren nicht erw"ahnt -- was kann parallel Ausgef"uhrt werden

\underline{WT-Darstellung:} Algorithmus ist Folge solcher TimeUnits

\begin{bsp}

\begin{algorithm}[htb]
\caption{Summenalgorithmus}
\begin{algorithmic}[1]
\REQUIRE $n=2^{k}$-Zahlen in Array $A$
\ENSURE Summe $S$
\FORPAR{$i \le i \le n$}
	\STATE $B(i) := A(i)$
\ENDFORPAR
\FOR{$h=1$ to $\log n$}
	\FORPAR{$1 \le i \le \frac{n}{2^h}$}
		\STATE $B(i) := B(2\,i-1)+B(2\,i)$
	\ENDFORPAR
\ENDFOR
\STATE $S := B(1)$
\end{algorithmic}
\end{algorithm}
$\to$ WT = $O(\log n)$
\end{bsp}

% \begin{defini}
%   \emph{Rechenzeit eines par. Algorithmus} (im upper Level) T(n)=Anzahl der TIME
%   UNITS.
% \end{defini}
% 
% f"ur obiges Beispiel: $T(n) = 2+\log n$
% 
% \begin{defini}
%   \emph{\Work} eines Algorithmus $W(n)$ $:\Leftrightarrow$ Gesamtzahl
%   der auszuf"uhrenden Einzeloperationen
% \end{defini}
% 
% f"ur obiges Beispiel:
% \begin{enumerate}
%  \item W(n) = n+$\sum_{j=1}^{\log n}\frac{n}{2^j}+1$ $\le
%   n+1+n sum_{j=1}^{\log n}\frac{1}{2^j}=O(n)$
% \end{enumerate}
% 
% upper level: \Work-\Time-Darstellung\\
% lower level: Theorem von Brent (\Work-\Time Scheduling-Prinzip)
% Eine Rechnung ist bei $p$ Prozessoren in
% $\lfloor\frac{W(n)}{p}\rfloor+T(n)$ meist m"oglich.
% \begin{proof}
%   geg.: Algorithmus, der 100~T.Units 99 davon nur eine Op. $W_{i}(n)=1$
%   
%   $W_{i}(n) := $ Anzahl der Op. in der i-ten Zeiteinheit
%   $\lceil\frac{W_i(n)}{p}\rceil$ Zeit des Algo mit $p$ Proz.:
%   $\sum_{j=1}{T(n)} \frac{\lceil W_{i}(n)\rceil}{p}\le$
%   $\sum_{j=1}{T(n)} \frac{W_{i}(n)}{p} =$
%   $\lfloor\frac{W(n)}{p}\rfloor+T(n)$
%   
%   Vor.: $W_{i}$ berechnen, Prozessor-Allocation
% \end{proof}
% 
% \begin{bemerk}
%   Wenn die Prozessor-Allocation (Aufteilen des Algos auf mehrere Proz.)
%   m"oglich ist, dann geht es mit obiger Laufzeit.
% \end{bemerk}
% 
% (Algo "`Summe"' im lower level - Programm f"ur den Prozessor $P_{s}$)
% INPUT: n=$2^{k}$-Zahlen, Array $A$, p =$2^{q}\le n$, s
% OUTPUT: S=$\sum_{i=1}^{n}A(i)$
% begin
% \begin{Verbatim}[numbers=left]
% for j=1 to l (=\frac n p) do setze B(l(s-1)+j) := A(l(s-1)+j)
% for h =1 to \log n do
%     if (k-h-q) \ge 0 then
%         for j =$2^{k-h-q}(s-1)$ to $2^{k-h-q}s$ do
% 	    setze B(j) := B(2j-1)+B(2j)
%     else if s \le $2^{k-h}$ then
%         setze B(s) := B(2s-1)+B(2s)
% if (s=1) then S:= B(1)
% \end{Verbatim}
% end
% 
% In Zukunft: upper level
% 
% \begin{tabular}{ccc}
%   \Work& --& KOSTEN\\
%   T(n) $\rightarrow$ p-prozzesor-PRAM: $T_{p}(n)=O(\frac{W(n)}{p}+T(n))$\\
%   Kosten $C_{p}(n)=T_{p}(n)\,p=O(W(n)+T(n)p)\ldots$\\
%   \multicolumn{2}{l}{Ansatz: $P=O(\frac{W(n)}{T(n)})$}\\
%   $\ldots=O(W(n)+T(n)\frac{W(n)}{T(n)})=O(W(n))$
% \end{tabular}
% 
% \begin{satz}
%   $C_{p}(n)\ge W(n)$
%   Die Kosten, die mit $p$ Prozessoren entstehen, sind immer min. \Work, denn wenn nicht
%   w"are dies ein Widerspruch zum Seriellen Optimum.
% \end{satz}
% 
% \begin{folger}
%   bei $p=O(\frac{W(n)}{T(n)})$ Prozessoren sind \Work und KOSTEN gleich.
% \end{folger}
% 
% \begin{defini}[Optimalit"at]
%   \begin{enumerate}
%    \item Ein Algorithmus A hei"st \emph{optimal (zeitoptimal)}, wenn $W(n) =
%     O(T^{\ast}(n))$ ($T^{\ast}$ -- serielles Optimum)
%    \item Ein Algorithmus A hei"st \emph{\Work-\Time-Optimal} oder \emph{streng
%     optimal}, wenn er optimal ist und wenn es keinen schnelleren optimalen
%     Algorithmus gibt.
%   \end{enumerate}
% \end{defini}

\begin{satz}[Satz von Brent]
Sei $W_i(n)$ die Anzahl der Operationen des Algorithmus in der TimeUnit $i$. Simuliere jeden dieser Schritte in $\left\lceil \frac{W_i(n)}{p} \right\rceil$ parallelen Schritten mit $p$ Prozessoren. \\

\underline{Wenn} das erfolgreich, gilt: 
\[
\text{Zahl der Takte} \leq \sum_i \left\lceil \frac{W_i(n)}{p} \right\rceil 
\leq \sum_i \left\lfloor \frac{W_i(n)}{p} + 1 \right\rfloor 
\leq \frac{W(n)}{p} + T(n)
\]
\end{satz}

\chapter{Die sieben Paradigmen beim Entwurf paralleler Algorithmen}
\begin{enumerate}
 \item Summenparadigma
 \item Pointer-Jumping
 \item Teile und Hersche
 \item Zerlungsstrategie (Paritioning)
 \item Accelerated Cascading
 \item Pipelining
 \item Aufbrachen von Symetrien
\end{enumerate}

\section{Präfixsummen}

\begin{algorithm}[htb]
\caption{Päfixsummen}
\begin{algorithmic}[1]
\REQUIRE Array $A, n=2^k$
\ENSURE Array $C(0,j), 1\leq j \leq n$ - Präfixsummen
\FOR{$1 <= j <= n$ {\bf{par}}}
	\STATE $B(0,j) := A(j)$
\ENDFOR
\FOR{$h := 1$ to $\log n$}
	\FOR{$1 <= j <= \frac{n}{2^h}$ {\bf{par}}}
		\STATE $B(h,j) := B(h-1, 2j-1) + B(h-q, 2j)$
	\ENDFOR
\ENDFOR
\FOR{$h := \log n$ to $0$}
	\FOR{$1 <= j <= \frac{n}{2^h}$ {\bf{par}}}
		\IF{j gerade}
			\STATE $C(h,j) := C(h+1, \frac{j}{2})$
		\ELSIF{$j=1$}
			\STATE $C(h,1) := B(h,1)$
		\ELSE
			\STATE $C(h,j) := C(h+1, \frac{j-1}{2} + B(h,j)$
		\ENDIF
	\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{bsp}[n=8]
\begin{eqnarray*}
B(3,1) &=& a_1+\ldots + a_8 = C(3,1) \\
C(2,1) &=& B(2,1) = a_1+ \ldots + a_4 \\
C(2,2) &=& C(3,1) = a_1+ \ldots + a_8 \\
C(1,1) &=& B(1,1) = a_1+ a_2 \\
C(1,2) &=& C(2,1) = a_1+ \ldots + a_4 \\
C(1,3) &=& C(2,1) + B(1,3) = a_1+ \ldots + a_6 \\
C(1,4) &=& a_1+ \ldots + a_8 \\
C(0,\ldots) &\dots& C(0,3)= C(1,1) + B(0,3) = a_1+ \ldots + a_3 \\
\end{eqnarray*}
\end{bsp}
\section{Pointer Jumping}

% TODO: Mehr Text schreiben!

F"ur das Pointer Jumping Paradigma (Pfadverdopplung) betrachten wir einen Teil
eines Disjoint Set Forests. 
\begin{center}
\input{figures/ptr_jumping.pdf_t}
\end{center}

\begin{algorithm}[htb]
\caption{Pointerjumping}
\begin{algorithmic}[1]
\REQUIRE Forrest $(i, p(i)) : 1 \leq i \leq n$
\ENSURE $\forall$ Knoten $i$ : ROOT $(S(i)$
\FOR{$1 \le i \le n$ {\bf par}}
	\STATE $S(i) := p(i)$
	\WHILE{$S(i) \not= S(S(i))$}
		\STATE $S(i) := S(S(i))$ \COMMENT{Pointer Jumping}
	\ENDWHILE
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{bemerk}
Algorithmus liefert in $O(\log n)$-TIME $\forall i$ die Wurzel in $O(h*n)$ Operationen.
\end{bemerk}

\subsection{Parallel-Präfix}

\begin{center}
\input{figures/ptr_jumping_weight.pdf_t}
\end{center}

\begin{algorithm}[htb]
\caption{Pointer Jumping mit Knotengewichten}
\begin{algorithmic}[1]
\REQUIRE Forest $(i, p(i)), \forall i : W(i) =$ Gewicht, für $r$ = ROOT: $W(r)=0$
\ENSURE $\forall i:$ Summe der Gewichte auf dem Weg von $i$ zur Wurzel\\
(Seriell in $O(n)$ \Time)\\
\FORPAR{$1 \le i \le n$}
	\STATE $S(i) := p(i)$
	\WHILE{$S(i) \not= S(S(i))$}
		\STATE $W(i) := W(i) + W(S(i))$
		\STATE $S(i) := S(S(i))$ \COMMENT{Pointer Jumping}
	\ENDWHILE
\ENDFORPAR
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Ende Vorlesung vom 19.04.05
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Beispiel: Liste: $v\in V$ (Knotenmenge), $I\subseteq V$
% % Knotenliste
% 
% Beispiel:
% % Baum
% 
% Anfang: W(8)=W(13)=0, W(rest)=1
% 1. It.: W(1,2,3,4,5,9,10,11)=2 W(6,7,12)=1, W(8,13)=0
% 2. It.: W(1)=W(1)+W(6)=3, W(9)=4, W(2,10)=3, W(6,7,12)=1, W(3,4,5,11)=2
% 
% Modell? CREW; Analyse: h-max. H"ohe im Forest, \Time $O(\log h)$ \Work
% $O(n\log h)$ $\rightarrow$ Algorithmus ist nicht optimal
% 
% \begin{defini}
%   Die Aufgabe \emph{Parallel Prefix} :$\Leftrightarrow$ Spezialfall der
%   lin. Liste
% \end{defini}
% 
% \begin{satz}
%   Parallel Prefix funktioniert in $O(\log n)$ \Time mit $O(n\log n)$ \Work
%   
%   Spezialfall: $W(root)=0, \forall i\not= root: W(i)=1$ liefert der Algorithmus f"ur jeden
%   Knoten den Abstand zum Ende der Liste.
% \end{satz}
% 
% \section{Teile und Herrsche/Divide and Conquer}
% 
% Teile -- trivial, Herrsche -- schwer
% 
% Beispiel: Convex Hull genauer Upper convex hull
% % Grafik: Conv hull
% 
% Ann.: Br"ucke kann in $O(\log n)$ \Time seriell bestimmt werden.
% Algo:
% begin
% \begin{Verbatim}
% if |S|\le 4 then OUTPUT von UCH (Brute force)
% S$_{1}$ = \{$p_{1}, \ldots, p_{\frac{n}{2}}\}$
% S$_{2}$ = \{$p_{n/2+1},\ldots,p_{n}\}$
% Berechne UCH($S_{1}$), UCH($S_{2}$) parallel
% Bestimme die Br"ucke, bearbeite zum OUTPUT (verbinde die beiden h"ochsten
% Punkte von $UCH(S_{1})$ und $UCH(S_{2})$)
% \end{Verbatim}
% end.
% 
% \subsection{Analyse}
% \begin{align*}
%   T(n) &\le T(n/2)+c\,\log n &=O(\log^{2} n)\\
%   W(n) &\le 2W(n/2)+c\,\log n &=O(n\log n)
% \end{align*}
% $\rightarrow$ Algorithmus ist optimal
% 
% \begin{bemerk}
%   zu $O(\log n)$ seriell 
% \end{bemerk}
% 
% \begin{equation*}
%   \begin{vmatrix}
%     1 &p_{x} &p_{y}\\
%     1 &v_{x} &v_{y}\\
%     1 &v'_{x} &v'_{y}
%   \end{vmatrix}\leadsto
%   \begin{cases}
%     > 0 &\Rightarrow\text{left turn}\\
%     < 0 &\Rightarrow\text{right turn}
%   \end{cases}
% \end{equation*}
% 
% Vergleich der $y$-Werte gibt die Unterscheidung
% 
% \begin{defini}
%   1. Supporting
%   2. Concave
%   3. Convex
% \end{defini}
% 
% \begin{tabular}{lccc}
%   konkav& supp& konvex\\
%   \hline
%   konkav\\
%   supp\\
%   konvex
% \end{tabular}

\section{Partitioning (Zerlegunsstrategie)}

Meistens ist das Problem als ganzes sehr kompliziert. Deshalb versucht man, dieses in Teilprobleme zu zerlegen.
Dabei müssen die Teilergebnisse zu einer Gesamtlösung zusammengefügt werden.

\begin{bsp}
Merge (INPUT: 2 sortierte Folgen beide L"ange $n$, OUTPUT: eine sortierte Folge $C$)

Herrsche ist einfach, teile trivial.

$A=(a_1,\ldots,a_n)$, $B=(b_1,\ldots,b_n)$\\
\[
	a_i, b_i \in S, \quad (S,\subseteq) \quad \text{(Halbordnungsrelation)}
\]

(reflexiv, antisymmetrisch, transitiv und total) \\

\begin{defini}
Sei $X = (x_1,\ldots,x_t) \quad x_i \in S,\quad  i=1 \ldots t$
\[
	\text{Rang } (x: X) = \bigl|\left\{ y\, |\, y \leq x,\, y \in \left\{ x_1, \ldots, x_t\right\}\right\}\bigr|
\]
\end{defini}
\begin{bsp}
Rang $(4, (5,7,23,25))=0$
\end{bsp}

\[
\text{Rang } (Y : X) := (r_1, \ldots, r_s) : r_\sigma = \text{Rang } (y_\sigma : X), \quad \sigma = 1,\ldots,s
\]

\subsubsection*{Elementarer Algorithmus}
Mische $(A,B)$
\underline{Ziel:} Bestimmung des Ranges für alle $x \in A \cup B$, also
Rang $(x: A \cup B)$, damit kennen wir den Platz von $x$ in $C$ 

\[
\text{Rang } (x: A \cup B) = \text{Rang }(x:A) + \text{Rang }(x:B) \qquad (\forall x \in A \cup B)
\]

\underline{es genügt:} Rang $(B:A)$ und Rang $(A:B)$

\todo{Bild Merge einfügen}

\subsubsection*{Binäre Suche:} $O(\log n)$ pro Element 
$\to$ parallel: $=(\log n)$ auf CREW\\

$\to$ WORK $O(n\log n)$

\todo{Bild Merge 2 einfügen}

$\underbrace{\text{Mische } (B_i,A_i)}_{\text{"`Setzen der Pfeile"'}}$ untereinander und "`klebe"'.\\

\underline{Programm:} für Partition ($\log m$ und $\frac{m}{\log n}$ sind ganze Zahlen)\\

% Sei $Y\subseteq S$.
% \begin{equation*}
%   Rang(Y:X) := (Rang(y_{1}:S),\ldots,Rang(y_{s}:S))
% \end{equation*}
% 
% $Y=\{y_{1},\ldots,y_{s}\}$ (genauer: $Y=(y_{1},\ldots,y_{s})$)
% 
% $A,B \rightarrow C^{n+m}$: $C$ sortiert
% 
% Annahme (o.B.d.A.): Ele. von $A \cup B$ seien paarw. verschieden.
% 
% Beisp.: A=(3,5,7), B=(2,4,6)
% Rang(2:A) = 0
% Rang(2:B)=1
% Rang(2:A$\cup$B)=1=Rang(2:A)+Rang(2:B)
% 
% Allg.: Rang(x:A$\cup$B)=Rang(x:A)+Rang(x:B)
% 
% wenn Rang(x:A$\cup$B)=i $\Rightarrow$ x=$c_{i}$
% (Sei $x\in A \cup B$)
% 
% \begin{folger}
%   Wenn f"ur allle $x\in A \cup B$ der $Rang(x:A \cup B)$ bekannt ist, ist die
%   gesuchte sortierte Menge $C$ bekannt.
% \end{folger}
% 
% \begin{bemerk}
%   Dazu reicht es aus Rang(A:B) und Rang(B:A) zu berechnen.
% \end{bemerk}
% 
% \begin{folger}
%   Es reicht, den Algo. f"ur die Bestimmung dein rang(B:A) anzugeben.
%   
%   \underline{genauer:} Die \Time-\Work-Komplexit"at hierf"ur ist identisch
%   mit der f"ur MERGE.
% \end{folger}
% 
% Algo.: $\forall b\in B$ suche "uber bin. Suchen $Rang(b:A^{n})$ (pardo)
% $O(\log n) \Time, O(n \log n)$ \Work $\rightarrow$ nicht optimal, da MERGE im
% seriellen nur O(n) ben"otigt.

% \begin{description}
%  \item[Ziel:] Das gleiche optimal erreichen.
%  \item[Strategie:] Partitioning
% \end{description}

\begin{algorithm}[htb]
\caption{Mischen}
\begin{algorithmic}[1]
\REQUIRE $A^{n}, B^{m}$ sortiert
\ENSURE Zerlegung: $k(m) (= \frac{m}{\log m})$ Paare $(A_i,B_i):\quad
|B_i| = \log m, \sum |A_i| = n$\\
Alle Elemente $A_i$ bzw. $B_i$ sind größer oder gleich aller Elemente von $A_{i-1}$ bzw.
$B_{i-1}\quad (1\leq i \leq k-1)$

\STATE $j(0) := 0$
\STATE $j(k(m)) := j(\frac{m}{\log n}) := n$
\FOR{$1 \le i \le k(m)-1$ {\bf par}}
	\STATE $j(i) :=$ Rang$(b_{i \log m} : A)$
\ENDFOR
\FOR{$0 \le i \le k(m)-1$ {\bf par}}
	\STATE $B_i := (b_{i \log (m+1)},\ldots,b_{(i+1) \log m)}$
	\STATE $A_i := (a_{j(i)+1},\ldots,a_{j(i+1)})$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{satz}
  Der Algo. ben"otigt $O(\log n)$ \Time und $O(n+m)$ \Work
  
  \begin{proof}
    Rechenzeit klar
    
%     Sei $m,n>4, m<n \Rightarrow \frac{m}{\log m} < \frac{n}{\log n}
%     (\lim_{n\rightarrow\infty}\frac{n}{\log n}=\infty$ 
%     \begin{align}
%       m<m+n &\Rightarrow \frac{m}{\log m}<\frac{m+n}{\log (m+n)}\\
%       &\Rightarrow \frac{m\log(m+n)}{\log m}<m+n
%     \end{align}
%     \Work: $O(\underbrace{\frac{m}{\log m}}_{=k(m)}\cdot\log m)
%       \leq O(\frac{m\log (m+n)}{\log m}) < O(m+n)$

Schritt 2: 
\[
O(\log n \frac{m}{\log m}
\]
\[
a < b \to \log a < \log b 
\]
\[
\lim \frac{z}{\log z} \to 0 \Rightarrow \frac{m}{\log m} < \frac{n+m}{\log (n+m)} 
\Rightarrow \frac{m \log (n+m)}{\log m} < n+m 
\]
\[
\frac{m \log n}{\log m} < \frac{m \log (n+m)}{\log m} < n+m
\]
\[
\Rightarrow \Work \text{ ist linear } (O(m+n))
\]
  \end{proof}
\end{satz}

\begin{satz}
$A^n$ (sortiert) und $B^n$ (sortiert) lassen sich in $O (\log n)$ Zeit optimal mischen.
\begin{proof}
\begin{description}
\item [1. Fall:] Nicht nur die $B_I$ sondern auch $A_i$ haben $O(\log n)$ Elemente:\\
	Serielles Mischen für alle Paare liefert das Resultat in $O(\log n)$ \Time mit
	$O(n)$ \Work.
\item [2. Fall:] 

\todo{Bild MergeTrick einfügen}

Dann das analoge Vorgehen mit Vertauschung der Rollen für zu lange $A_i$! Rest wie im Fall 1.
\end{description}
$\Rightarrow$ Optimaler paralleler Algorithmus, der aber nicht streng optimal ist.
\end{proof}
\end{satz}

% \begin{bemerk}
%   Der Algorithmus arbeitet korrekt im folgenden Sinn: wenn die Mengen
%   $(A_{i}, B_{i})$ untereinander gemischt werden, so erh"alt man das
%   sortierte $C$ durch aneinanderheften der einzelnen Ergebnisse.
%   
%   Mische $B_{0}$ mit $A_{0}$ $\rightarrow$ $C_{0}$\\
%   Mische $B_{1}$ mit $A_{1}$ $\rightarrow$ $C_{1}$\\
%   Mische $B_{2}$ mit $A_{2}$ $\rightarrow$ $C_{2}$\\
%   $\vdots$
%   
%   $C:C_{0}C_{1}C_{2}C_{3}\ldots$
% \end{bemerk}
% 
% Wie teuer? trivial: \Work$=O(n+m)$ (mit seriellen mischen jeweils),
% \Time? |$B_{i}$|=$\log m$ Sei $m=n\log n$
% Fall |$A_{i}$|=$\log n$ $\Rightarrow$ $O(\log n)$ \Time
% 
% sei nicht erf"ullt $A_{i}=O(\log n)$ f"ur $i$
% 
% jetzt: $A_{i}$ in B einmischen
% 
% % Skizze Algokorrektur
% 
% $O(\log n)$ \Time, $O(n)$ \Work jetzt kann $A_{i}$ mit $B_{i}$ in
% $O(\log n)$ \Time mit $O(n)$ \Work gemischt werden!

\end{bsp}

\section{Teile und Herrsche (Divide and Conquer)}
Convex Hull:
\todo{Bild ConvexHull1 einfügen}

Komplexe Datenstrukturen für "`merge"' (Diss. Goodrich)\\
Andere Möglichkeit:

\todo{Bild ConvexHull2 einfügen}

\begin{algorithm}[htb]
\caption{UCH(S)}
\begin{algorithmic}[1]
\REQUIRE $S \leq \mathbb{R}^2$, nach $x$ sortiert, allgemeine Lage
\ENSURE UCH
\IF{$n \le 4$}
	\STATE Bestimme UCH
	\STATE exit()
\ENDIF
\STATE sei $S_1 = (p_1, \ldots , p_{\frac{n}{2}}), S_2 = (p_{\frac{n}{2} + 1}, \ldots , p_n)$
\STATE berechne die gemeinsame obere Tangente und bilde die konvexe Hülle
\end{algorithmic}
\end{algorithm}

\subsubsection*{Frage:} Modell, Rechenzeit, \Work ? 

%Todo: Bild "ConvexHull3" einfügen
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Ende Vorlesung vom 22.04.05           %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Gesucht ist die Tangente in $O(\log n)$ \Time

\[
	T(n) = T(\frac{n}{2}+\text{ const }\log n=O(\log^2n)
\]
\[
W(n) = 2 W(\frac{n}{2}+\text{ const } n=O(n\log n)
\]

\subsubsection*{Ziel:} optimaler Algorithmus auf der CREW in $O(\log^2n)$ \Time (nicht streng optimal)

\begin{itemize}
\item [1. Fall, Tangente:] \todo{Bild einfügen}
	$(p,v,v'')$ - Linksdrehung \\
	$(p,v,v')$ - Linksdrehung \\
	\begin{bemerk}
	\[
	\text{Linksdrehung} \Leftrightarrow 
\begin{vmatrix}
1 & p_x & p_y  \\ 
1 & v_x & v_y \\ 
1 & v''_x & v''_y
\end{vmatrix}
	\]
	\end{bemerk}
\item[2. Fall Reflex:] \todo{Bild einfügen}
	\[
	\neg \text{ 1. Fall } \and \overline{v''v'} \cap \overline{vp}=\emptyset
	\]
\item[3. Fall Konkav:] \todo{Bild einfügen}
	\[
	\neg \text{ 1. Fall } \and \overline{v''v'} \cap \overline{vp}\not=\emptyset
	\]
\end{itemize}

\todo{Bild Tabelle Konkav Konvex einfügen }\\

Einziges Problem ist der Fall konkav-konkav, da bei diesem die zu suchende Fläche nicht kleiner wird.

\todo{Bild (mit Geraden u,l und Punkt p) einfügen} \\

\begin{itemize}
\item[1. Fall] $p$ liegt rechts von $l$
\item[2. Fall] $p$ liegt links von $l$
\end{itemize}

-----------------------------------------
\section{Pipelining}

2-3-B"aume ("ahnlich Top-Down-2-3-4-B"aumen, ausbalancierter Suchbaum mit Knoten aus zwei oder drei Elementen)\\

Gegeben: $A=(a_1,a_2,\ldots,a_n)\quad : \quad a_{1}<a_{2}<\ldots<a_{n}$ Blattsuchbaum\\
Innere Knoten: Pfad-Infos $(L[v],M[v],R[v])$ - jeweils größter Knoten im linken (bzw. mittleren oder rechten) Teilbaum\\

Wir wollen nun in diesen Baum $B={b_1,\ldots,b_k}\quad : \quad b_{1}<b_{2}<\ldots<b_{k}\quad k<<n$ einfügen:

% \begin{description}
%  \item[B Task] 100 Autos Teilaufgabe 1. Auto ($t_{1}, t_{2},t_{3}, \ldots)$
%   2. Auto ($t_{2}, t_{3},\ldots$)
%  \item[A Task $t$] 1 Auto, zerlege in Folge von Teilaufgaben ($t_{1},
%   t_{2}, t_{3},\ldots)$
% \end{description}
% 
% Nutzen dieser Idee zum Algorithmusentwurf
% 
% Beispiel: INSERT von ($b_{1}<b_{2}<\ldots<b_{k})$ in
% ($a_{1}<a_{2}<\ldots<a_{n}$) $k=O(n)$, sogar $k<<n$
% 
% \begin{description}
%  \item[Variante B] task: INSERT($b_{1}, b_{2},\ldots,b_{k})$
%  \item[Variante A] task: INSERT einen Teil davon
% \end{description}
% 
% Verwaltung der Werte in (2,3)-Suchbaum (-Blattsuchbaum)
% 
% Knoten $v$: L[v], M[v], R[v]
% INSERT : SEARCH+ Einf"ugen
% START: INSERT $b$: gesucht $i$: $a_{i}<b<a_{i+1}$
% 
% % Bild: Einf"ugen in 2-3-Baum

\begin{defini}
$B_i \subseteq B$ - Werte zwischen $a_i$ und $a_{i+1}$\\
$|B_i| := k_i \quad (\to$ Summer der $k_i$ sind ergibt im wesentlichen $k-2$, nach dem ersten Schritt)
\end{defini}

Vorschritt: Füge $b_{1},b_{k}$ ($O(\log n)$ \Time) (Damit alle Elemente
zwischen zwei Elementen im Baum eingef"ugt werden und nicht vor dem
linken oder nach dem rechten)\\

% Dann: Umbenennung: $a_{1}<\ldots<a_{n}$ neu = $a_{1},\ldots,a_{n}$ alt
% erg"anzt um $b_{1},b_{k}$
% Jetzt Def.: $B_{i}:= $ Teilkette der $b_{2},\ldots,b_{k-1}$ zwischen
% $a_{i}$ und $a_{i+1}$
\begin{enumerate}[1.\,{Fall}]
 \item $|B_{i}|\leq 1 \quad\forall i=1,\ldots,n-1$
  
  WORSTCASE
  \todo{ Bild: Worstcase bei Fall1 einfügen}
  
  $\Rightarrow$ bei pardo insert ($b_{2},\ldots,b_{k-1})$ im Fall 1
  entstehen pro Knoten "uber der Blattebene max. 6 S"ohne
 \item ($\neg$ 1. Fall)
  |$B_{i}$|=$k_{i}$ ($\sum k_{i} = k-2$)
  m"oglich f"ur ein $i$: |$B_{i}$|=$k_{i}$=$\Omega(k)$
  
  $b_{i_{1}},\ldots,b_{i_{k}} \rightarrow$ mittleres Element mit Index $z
  := \lceil\frac{1+k_{i}}{2}\rceil$ also $b_{i_{z}}$ das f"ur alle $i$
  
  insert alle $b_{i_{z}}$ = 1. Fall
  
  Damit reduzieren wir die Maximall"ange der Folge der einzuf"ugenden
  Elemente zwischen zwei Elementen auf die H"alfte. Das wird fortgesetzt,
  bis Fall 1 erreicht.
\end{enumerate}

H"ohe des Baumes: $O(\log n)$
Im 1. Fall: parallel (pardo) : $O(\log n)$ \Time, $O(k \log n)$ \Work
2. Fall: ($\log k$)-mal durchzuf"uhren, jedesmal Fall 1 $\Rightarrow$
$O(\log k \log n)$ \Time, $O(k \log n)$ \Work

Aufgabe: INSERT z.B. $b_{i_{1}},\ldots,b_{i_{k_{i}}}$

1. Teilaufgabe INSERT: INSERT jeweils das erste gelbe Element
2. Teilaufgabe: INSERT jeweils die beiden n"achsten gelben Elemente
$\Rightarrow O(\log n + \log k)=O(\log n)$ \Time
\begin{bemerk}
  Das erste Element braucht $\log n$ Schritte um an seiner Stelle zu sein.
  Das letzte Element wird nach $\log k$ Schritten losgeschickt und kommt
  nach weiteren $\log n$ Schritten an, also $\log k+\log n$
\end{bemerk}

\section{Accelarated Cascading}

1. sehr schnell, nicht opt. Algo.
2. langsam Algo. optimal

speziell: seqential subset

Beispiel: Bestimmen des Maximums auf common CRCW in $O(\log\log n)$ \Time
\emph{optimal}.

\begin{satz}
  Das ist auf der CREW prinzipiell unm"oglich: dort gilt $\Omega(\log n)$
  f"ur die Zeit (unabh. von \Work).

  Spezialfall eines Satzes: Def.:
  $n$-stell. Boolsche Fnkt. $f^{n}: \{0,1\}^{n}\rightarrow\{0,1\}
  I=(x_{1},\ldots,x_{n}); I(i)  := (x_{i},\ldots,\bar{x_{i}},\ldots,x_{n})$
  
  $I$ kritisch $:\Leftrightarrow$ $\forall$ $i:1 \leq i \leq n: f(I)\not=
  f(I(i))$ ($\max\{x_{1},\ldots,x_n\}$ -- $(0,\ldots,0)$ kritisch)
\end{satz}

Haupttheorem: $f: \{0,1\}^{n} \rightarrow \{0,1\}$ besitze kritischen
Input. Dann sind auf der CREW $\Omega(\log n)$Schritte zur Berechnung von
$f$ n"otig

Alog: "`Maximum von $n$ Elementen"'
INPUT: Array $A$ von $p$ verschiedenen Elementen
OUTPUT: Boolsches Array $M: M[i] = 1 \Leftrightarrow A[i] = Max$
begin
\begin{Verbatim}
for i \le i,j \le p pardo
    if A[i] \ge A[j]
        then B(i,j) := 1
	else B(i,j) := 0
for 1 $\Leftarrow$ i <= p
    M(i) = B(i,j)$\wedge$\ldots$\wedge$ B(i,p)
\end{Verbatim}
end.

\begin{lemma}
  Auf der common CRCW (schreiben nur dann, wenn alle das gleich schreiben
  wollen) kann das Maximum in O(1) >\Time mit O($p^{2}$) Operationen
  berechnet werden.
\end{lemma}

% 7.5.

1. Ziel: auf CRCW optimal in $O(\log \log n)$ \Time
Schritt A: $O(\log \log n)$ \Time mit $O(n \log \log n)$ \Work
Schritt B: Accellerated cascading $\rightarrow$ optimal

2.) B"aume doppelt logarithmischer Tiefe

% Grafik: Vergleich von \log n und wurzel(n)
% Bild: Baum mit levels

Die Wurzel habe $n=2^{2^{k-1}}$ S"ohne, jeder dieser S"ohne habe
$2^{2^{k-2}}$ S"ohne, $2^{2^{k-i-1}}$ S"ohne f"ur die Knoten im Level $i$
(0$\leq i\leq k-1)$

Jeder Knoten im Level k habe 2 S"ohne. Bsp.: $k=3$ dann $n=2^{2^{3}}=256$
Knoten. Wurzel hat $2^{2^{3-1}}=16$ S"ohne

% Bild: Baum mit 16 S"ohnen

\begin{lemma}
  Jeder Knoten im Level $i$ (0$\leq$ i $\leq$ k-1) hat $2^{2^{k-i-1}}$
  S"ohne.
\end{lemma}

\begin{lemma}
  Die Gesamtzahl der Knoten im Level $i$ f"ur 0$\leq$ i $\leq$ k-1 ist
  $2^{2^{k}-2^{k-i}}$
\end{lemma}

3.) Algorithmus f"ur das Max. auf der common CRCW.

\begin{lemma}
  H"ohe des Baumes doppelt logarithmischer Tiefe f"ur $n$ Bl"atter ist
  $(\log \log n)+1=k+1$.
\end{lemma}

Wir nutzen den Algorithmus in $O(1)$ \Time mit $O(n^{2})$ \Work f"ur $n$
Elemente und arbeiten mit einem Baum doppelt logarithm. Tiefer. Damit
haben brauchen wir in jedem Level $O(1)$ \Time und reduzieren den \Work auf
$O(\log \log n)$

\begin{lemma}
  \Work pro Level ist $O(n)$.
  
  \begin{proof}
    Sei $v$ Knoten im Level $i$. Er hat seine S"ohne in Level $i+1$. Das
    sind $2^{2^{k-i-1}}$ S"ohne. $\Rightarrow$ \Work
    O($(2^{2^{k-i-1}})^{2}$ nach der Eigenschaft des O(1)-Algo.
    $\Rightarrow$ O($2^{2^{k-i}}$ \Work. Nach Lemma~2:
    ($2^{2^{k}-2^{k-i}}$)-mal $\Rightarrow$ O($2^{2^{k}}$)=O(n)
  \end{proof}
\end{lemma}

\begin{folger}
  Das Maximum von $n'$ Elementen kann auf der common CRCW in $O(\log\log
  n')$ \Time mit $O(n' \log \log n')$ \Work berechnet werden.
\end{folger}

I: Bin"arbaum-Algo. $O(\log n)$ \Time optimal
II: Algo (*) (folgerung) $O(\log \log n)$ \Time, $O(n\log\log n)$ \Work

Algo I $\lceil\log\log\log n\rceil$ Level lang ausf"uhren. Schritt 1:
$\frac{n}{2^{1}}$ 2. $\frac{n}{2^{2}}$, 3. $\frac{n}{2^{3}}$
$\frac{n}{2^{\log \log \log n}}=\frac{n}{\log \log n}$ Elemente "als Rest

Diese $\frac{n}{\log \log n}$ Bl"atter f"ur den Baum doppelt log. Tiefe
verarbeiten wir weiter nach Algo II $\Rightarrow$ liefert das Maximum, wa
gesucht ist. \Time $O(\log\log n)$

\Work-Analyse:

\begin{description}
 \item[F"ur I:] $O(n)$ (trivial, da I insgesamt optimal, also imsgesamt
  $O(n)$ \Work ben"otigt!)
 \item[F"ur II:] Folgerung: $n'=\frac{n}{\log \log n}$
  \begin{description}
   \item[\textnormal{\Time:}] $O(\log \log n')=O(\log \log n)$
   \item[\textnormal{\Work:}] $O(\frac{n}{\log \log n} \log \log
    (\frac{n}{\log \log n})= O(n)$
  \end{description}
\end{description}

Acce cas: Man arbeitet mit einem optimalen Algorithmus solange, bis man
noch $\log\log n$ Elemente hat, setzt dann einen nichtoptimalen schnellen
Algo an.

Alternativ:

% alternativer Baum

Zusammenwirken von Acc. casc. und sequential subset

\subsection{Aufbrechen von Symetrien}

% Bild: greichteter Kreis

Einf"arben der Knoten, so dass eine Kante nicht zwei Knoten gleicher
Farbe verbindet. Geht seriell in O(n) \Time

Basisalgortihmus.
INPUT: Array der n Knoten (Kreis), zul"assige Farbung $c_{n}$
OUTPUT: c' neue F"arbung, zul"assig
begin
\begin{Verbatim}
for 1 $\leq$ i $\leq$ n pardo
    setze k gleich der kleinsten singifikanten Position, wo sich c(i)
    und c(s(i)) als Bin"arzahlen sich unterscheiden
    c'(i) := 2k+c(i)
\end{Verbatim}
end

% 11.5.

Basisoperationen:
$i=i_{t-1}i_{t-2}\ldots i_{1}i_{0}$ $k$-t-kleinstes signifikantes Bit :=
$i_{k}$

% Bild: Kreis.gd

-- vierstellige: 0001
Vereinbarung: m"oglichst kurz

\begin{tabular}{{cccccccccccccccc}}
  0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15\\
  *&3& &7& & & &14&& &  &  &  &  &2 &\\
\end{tabular}

\begin{tabular}{cccc}
  v&Basisf"arbung & k& c'\\
  1& 0001 & 1 & 2\\
  3& 0011 & 2 & 4\\
  7& 0111 & 0 & 1\\
  14& 1110& 2&5\\
  2&0010 & 0& 0\\
  15& 1111& 0&1\\
  4&0100&0&0\\
  5& 0101 & 0 & 1\\
  6& 0110 & 0&0
\end{tabular}

\begin{gather}
  c'(1) = 2k+c(1)=2+0=2\\
  c'(2) = 2 \cdot 0+0=0
\end{gather}

\begin{lemma}
  c zul"assig $\rightarrow$ c' ist auch zul"assig, T(n)=O(1), W(n)=O(n)
  
  \begin{proof}
    (indirekt) Ann.: c'(i)=c'(j) f"ur einen Kante $(i,j)\in E$, d.h.
    j=S(i) $\Rightarrow c'(i) =2k+c(i)_{k} =c'(j)=2l+c(j)_{l}$ Wegen
    Faktor 2 und da $c(i)_{k}, c(j)_{l}\in\{0,1\} \Rightarrow k=l
    \Rightarrow c(i)_{k}=c(j)_{l}$ Widerspruch zur Wahl von $k$
    $\Rightarrow c'(i) \not=c'(j)$
    
    Die Aussagen zu \Time und \Work gelten offensichtlich.
  \end{proof}
\end{lemma}

Sei $t>3$. Daf"ur reichen f"ur c' $\lceil\log t\rceil+1$ Bin"arpl"atze

$\Rightarrow c$ $q$ Farben hatte $\Rightarrow$ $2^{t-1}<q\leq2^{t}$, so
braucht c' $2^{\lceil\log t\rceil+1}=O(t)=O(\log g)$ Farben

$t>3 \Rightarrow \lceil\log t\rceil +1 < t$

Iterative Anwendung des Basisalgo. bis $t=3 (\lceil\log t\rceil+1=2+1=3)$

Farben: \{0,1,2,3,4,5\}=6
Eliminiere: 3;4;5 $\rightarrow$ 3 O(1)-Schritte parallel mit O(n) \Work

\begin{satz}
  Wir k"onnen einen Kreis mit n Knoten mit 3 Farben f"arben in
  $O(\log^{\ast} n)$ \Time mit $O(n \log^{\ast} n)$ \Work
  
  \begin{proof}
    Basisalgo. iterativ bis auf $t=3$ anwenden.
  \end{proof}
\end{satz}

\begin{bemerk}
  Dieser Algorithmus ist ohne M"uhe auf der EREW implementierbar.
\end{bemerk}

\begin{bemerk}
  Algo nicht Optimal.
\end{bemerk}

Ziel: optimaler Algrotihmus in $O(\log n)$ Zeit.

\begin{satz}(o.Bew.)
  Man kann ganze Zahlen aus $[0,\log n]$ in $O(\log n)$ \Time mit $O(n)$
  \Work sorierten.
\end{satz}

Optimale F"arbung:
INPUT: Di-Kreis mit n Knoten, S
OUTPUT: 3-F"arbung
begin
\begin{Verbatim}
for i \le i \le n pardo C(i) := i
Wende den Basisalgo genau einmal an
Sortiere die Ecken nach Farbe
for i=3 to \lseil\log n\rseil do f"ur alle Ecken der Farbe i
   pardo F"arbe v mit kleinster farbe aus \{0,1,2\}, die vom Vorg"anger
     und Nachfolger verschieden ist.
\end{Verbatim}
end.

\begin{satz}
  3-F"arben geht in $O(\log n)$ \Time optimal.
\end{satz}

\chapter{Listen, B"aume} % 3. Kapitel
\section{List-Ranking}
verkettete Liste $L$ , Nachfolgerarray $S$ $i\rightarrow S(i)$, $S(i)=0$
Ende der Liste bei Knoten $i$

List-Ranking-Problem: $\forall i$ bestimme den Abstand von $i$ zum Ende der
Liste. (Erinnerung: Pointerjumping: $O(\log n)$ \Time, \Work: $O(n\log n)$)

Idee: sequential subset
$\cdot\rightarrow\cdot\rightarrow\cdot\rightarrow\ldots\rightarrow\cdot$

Aufteilen des Arrays S in Abschnitte der L"ange $\log n$.

Leider funktioniert die Idee nicht, da die Elemente in den Abschnitten
sich auf Elemente au"serhalb des Abschnitts beziehen, was das sp"atere
mischen verkompliziert macht.

\section{optimales Listranking}
\begin{description}
 \item[Ergebnis:] Listranking geht in $O(\log n)$ \Time mit $O(n)$ \Work
 \item[Wir beweisen:] $O(\log n\log\log n)$ geht optimal
\end{description}

\subsection{Pointer Jumping}

$O(\log n)$ \Time, $O(n\log n)$ \Work
  
  Unterteilung des Arrays in Bl"ocke der Gr"o"se $\log n$ funktioniert
  nicht, da die Zeiger in den Bl"ocken auch au"serhalb des Blocks zeigen
  k"onnen -- Block nicht abgeschlossen.
  
  Strategie trotzdem
  \begin{enumerate}
   \item Reduziere die Startliste von $n$ auf $\frac{n}{\log n}$ Knoten.
   \item Pointer Jumping auf red. Liste anwenden
   \item Stelle die Ausgangsliste wieder her.
  \end{enumerate}
  
  Aufgabe f"ur Listranking: INPUT Liste, OUTPUT f"ur jeden Knoten den
  Abstand zum Ende.
  
  Algo. Pointer Jumping
  \begin{tabular}{ccccccccc}
    1&2&3&4&5&6&7&8&9\\
    2 & 6 &1&5&7&8&3&9&0
  \end{tabular}
  % Bild der Liste: Einfacher Strang
  
  OUTPUT: $\forall i: R(i)\ldots$ Abstand vom Ende
  begin
  \begin{Verbatim}[gobble=2]
  for 1 \leq i $\leq$ n pardo
      if S(i) != 0 then R(i) := 1
          else R(i) := 0
  for i=1 to n pardo
      Q(i) := S(i)
      while Q(i) != 0 and Q(Q(i)) != 0 do
          R(i) := R(i)+R(Q(i))
          Q(i) := Q(Q(i))
  \end{Verbatim}
  end.
  
  \begin{defini}
    Eine Teilmenge $I$ aller Knoten hei"se unabh"angig, wenn f"ur alle $i$:
    $[i\in I\rightarrow S(i)\not\in I]$
  \end{defini}
  
  Algo entferne I:
  INPUT: Gesamtliste, Teilliste
  OUTPUT: Gesamtliste ohne die Teilliste
  begin
  \begin{Verbatim}[gobble=2]
  Ordne Reihennummern den Elem. von I zu: N(i) : 1$\leq$ N(i) $\leq$ |I| =: n'
    geht in O(\log n) \Time optimal mithilfe des Prefisummen-Algo "uber
    den Array
  f"ur alle i\in I pardo
      U(N(i)) := (i, S(i), R(i)) -- Info, die sonst verlohren geht!
      R(P(i) (Vorg"anger) ) := R(P(i))+R(i)
      S(P(i)) := S(i)
      P(S(i)) := P(i)
  \end{Verbatim}
  end.
  
  Die Knoten seien k-gef"arbt: Farben \{0,\ldots,k-1\}
  Knoten hei"st lokales Minimum $\Leftrightarrow$ Farbe(i) =
  $\min\{\text{Farbe}(i), \text{Farbe(Vorgänger}(i)), \text{Farbe(Nachfolger}(i))\}$
  
  \begin{lemma}
    Bei gegebener k-F"arbung in einer Liste $L^{n}$ ist die Mende der
    lokalen Minima unabh. Menge von size $\Omega(\frac{n}{k})$ (Finde sie
    in O(1) \Time, O(n) \Work)
    
    \begin{proof}
      u,v lok. Minima, benachbert in dieser Eigenschaft
      max. Anzahl von Knoten zwischen zwei Minima u und v
       = 2k-2-1=2k-3 $\Rightarrow$ $\Omega(\frac{n}{k})$
      
      Bsp.: k=3: 2k-3=3
      
    \end{proof}
  \end{lemma}
  
  $\frac15$ Elemente kommen raus (es bleiben $\frac45$ Rest)  Wir
  reduzieren von n Elementen auf h"ochstens $\frac45n$ Elemente
  
  Optim. Listranking
  INPUT: $S^{n}$
  OUTPUT $\forall$ Knoten Abstand zum Ende
  begin
  \begin{Verbatim}[gobble=2]
  $n_{0}:=n$
  k:=0
  while $n_{k}>\frac{n}{\log n}$
      setze k := k+1
      F"arbe Liste mit 3 Farben, Bestimme I (lok. Minima)
      Entferne die Knoten aus I (wie gehabt)
      $n_{k}$ sei size der Restliste compact die wei"sen Knoten
      aufeinanderfolgend
  Pointer Jumping auf wei"sen Rest - optimal in O(\log n) \Time
  Nutze die Informationen in den Arrays U, um die Gesamtausgabe zu erhalten
  \end{Verbatim}
  end.
  
  Analyse
  $n_{k}\leq\big(\frac{4}{5}\big)^{k}\cdot n\leq\frac{n}{\log n} \Rightarrow
  \big(\frac45\big)^{k}\leq\frac1{\log n} ((\frac45)^{\log\log
  n}=O(\log n)) k=O(\log\log n)$ reicht
  
  while-schleife $O(\log\log n)$ mal.
  $\Rightarrow$ \Time: $O(\log n\log\log n)$, \Work: $O(n)$
  
  \Work des Algorithmus.: $n_{k}$ Elemente bleiben nach Iteration $k$,
  damit
  \begin{gather}
    \Work=O(\sum_{k}n_{k})= O(\sum_{k}\left(\frac45\right)^{k}\cdot n) = O(n)
  \end{gather}
  
\begin{tabular}{l*8c}
  I & 6 & \color{red}{4} & 1 & \color{red}{3} & 7 & \color{red}{2} & 8
     & \color{red}{5}\\
  3-F"arbung & (1) & (0) & (2) & (0) & (2) & (1) & (2) & (0)\\
  R: [1] & [1] & [1] & [1] & [1] & [1] & [1] & [0]\\
  && U(1,N(4)) = (4,1,1)\\
  \hline
  \multicolumn{9}{l}{Nach dem L"oschen der Elemente}\\
  & 6 && \color{red}{1} & & 7 && \color{red}{8}\\
  R: & [2] && [2] && [2] && [1]\\
  3-Farben & (2) && (1) && (2) && (0)\\
  &&& U(2,N(1))=(1,7,2) &&& U(2,N(8)) = (8,0,1)\\
  \hline
  & 6 &&&& 7\\
  & [4] &&&& [3]\\
  $\frac8{\log 8}=\frac83> 2 \rightarrow$ Abbruch\\
  \hline
  einf"ugen der Knoten in umgekehrter Reichenfolge ihres Entnehmens und
     dabei ergibt sich die Entfernung aus dem im U gespeicherten + der
     Entferung des Nachfolgers vom Ende\\
  $R_{\text{Ende}}$& [7]
\end{tabular}

\begin{bemerk}
  Analog dazu ist:
  Wende Parallel-Prefix auf die umkehrte Liste (Nachfolger = Vorg"anger)
  an, um die Entfernung zum Ende zu berechnen.
\end{bemerk}

\begin{bemerk}
  Listranking geht auch in $O(\log n)$, Zeit optimal (Lit.: Ja Ja)
\end{bemerk}

\section{Euler-Tour-Technik}

\begin{defini}
  Eulergraph $:\Leftrightarrow$ Es existiert ein Di-Kreis, der jede Kante
  des Graphen genau einmal durchl"auft ($\Rightarrow$ zusammenh"angend)
\end{defini}

B"aume
% Grafik: Euler-Baum

$T=(V,E) \rightarrow T'=(V,E'), <u,v>\in E \rightarrow$
$\{<u,v>,<v,u>\}\subseteq E'$

\begin{satz}
  Ein zusammenh"angender Graph ist Euler-Graph $\Leftrightarrow$ $\forall
  v\in V: Indegree(v)=Outdegree(v)$
\end{satz}

% Grafik: Eulergrpah

Adjazenzlisten: (Ringlisten) <v,L[v]>
L[1]=<2,3,4>

Beispiel f"ur eulerkreis:
<1,2>$\rightarrow$<2,5>$\rightarrow$<5,2>$\rightarrow$<2,6>$\rightarrow$<6,2>
$\rightarrow$<2,7>\ldots<3,1>$\rightarrow$<1,4>$\rightarrow$<4,1>

Ziel: parallele Berechnung des Eulerkreises mit S(<,>)=<,> (Nachfolger
einer Kante)

L[v]=<$u_{0},\ldots,u_{d-1}$>

\begin{gather}
  s(<u_{i},v>) := <v,u_{(i+1)\mod d}>
\end{gather}

\begin{satz}
  Die Funktion S beschreibt einen Eulerkreis in T'=(V,E').
\end{satz}

\begin{bsp}
  Setzen das Gewicht einer Kante vom Vater zum Sohn $
  w(<p(v),v>) := +1$ und vom Sohn zum Vater $
  w(<v,p(v)>) := -1$
  
  Damit l"asst sich das Level eines jeden Knoten bei Wahl eines Knotens
  als Root in $O(\log n)$ \Time mit $O(n)$ \Work berechnen. (Vor.: Rooting
  geht in diesen Grenzen und Eulertour geht so)
\end{bsp}

\begin{bsp}
  ROOTING: Eulertour-Technik, alle Kanten erhalten Gewicht 1 + Parallel
  Prefix
  
  Der Vorg"anger eines Knoten hat ein kleineres Gewicht
\end{bsp}

\begin{bsp}
  POST-Order, PRE-Order w(<v,p(v)>)=1, w(<p(v),v>)=0
  
  Ergebnis: O(1) \Time mit O(n) \Work kann eine Eulertour berechnet werden
\end{bsp}

"aquvalent: Angabe der Nachfolgerfunktion s:
$s(e)= e' (e,e'\in E') T=(V,E), T'=(V,E')$

Vor.:
% Bild: Stern; Mittelpunktknoten mit Sternf"ormig davonlaufenden S"ohnen;
% gegen den Urzeigersinn nummerieren $u_{i}$

adj(v) = <$u_{0}$, \ldots, $u_{d-1}$> Liste aller Pfade, die von v zu
$u_{i}$ f"uhren.

adj als Ringliste implementieren.

Ringlisten f"ur Standardgraph:
\begin{tabular}{*8l}
  1 &\\
  2 & (1,) (5,) (6,) (7,Zeiger auf (2,) in Liste 7)\\
  7 & (2,) (8,) (9,)\\
  8 & (7,)
\end{tabular}

Beipiel: w"ahle $u_{i}=7$, v=2; bestimme den Index j: L[v] =
<$u_{1},\ldots,u_{j},\ldots>$

Damit l"asst sich der Nachfolger in O(1) berechnen.

\section{Baumkontraktion}

Was ist eine Harke (engl. Rake)? Entfernen des Vaters und eines Sohnes,
der andere Sohn wird mit dem Gro"svater verbunden.

Ziel: Schnelle parallele Auswertung arithm. Ausdr"ucke (+,$\cdot$)

% Bild: arithm. Baum

Von unten nach oben f"uhrt in zweitem Fall zu O(n), was keine
Verbesserung gegen"uber dem Seriellen darstellt.

Algo. Baumkontraktion
INPUT: Baum,
OUTPUT: Reduktion auf Wurzel und linkeste und rechteste Blatt
begin
\begin{Verbatim}
markiere alle Bl"atter von A (Array der Bl"atter) (ohne linkeste und rechteste) wachsend von links
  nach rechts; "uber parallel Prefix mit gewicht w(v) = 0 wenn v innerer
  Knoten, w(v)=1 wenn v Blatt; Eulertour
for \lseil \log n+1\rseil do
  RAKE auf $A_{odd} = \{v_{i}: i odd\}$, die linke S"ohne sind
  RAKE auf Rest von $A_{odd}$
  A := $A_{even}$
\end{Verbatim}
end.

% Hier fehlt was!

% Grafik: "`B"' Kreise mit Zahlen: 8 3 4 5 2 11 15 17 n=$2^{l}$

B = $b_{1},\ldots,b_{n}=b_{2^{l}}$
B = $b_{1},\ldots,b_{i},\ldots,b_{j},\ldots,b_{n}$
\{$b_i,b_{i+1},\ldots,b_{j-1},b_{j}\}$
Sei $v=LCA(b_{i},b_{j})$

LCA: v Teilbaum mit Knoten v enth"alt Bl"atter
\{$b_{r},\ldots,b_{i},\ldots,b_{j},\ldots,b_{s}\}$

Teilung der Folge in linken und rechten Teil:
\{$b_{r},\ldots,b_{i},\ldots,b_{p}\},\{b_{p+1},\ldots,b_{j},\ldots,b_{s}\}$

Suchen: Minimum(Minumum(\{$b_{i},\ldots,b_{p}\})$,
Minimum(\{$b_{p+1},\ldots,b_{j}\}))$

Vorteil dieses Mal: Die Minuma liegen am Rand. Min($b_{i},\ldots,b_{p})$
ist ein Suffix-Minimum, Min($b_{p+1},\ldots,b_{j})$ ist ein Prefix-Minimum

Algo: "`Rang-Min:
Input: $B^{n}, n=2^{l}$
OUTPUT: vollst"andiger Bin"arbaum mit den Arrays P,S
begin
\begin{Verbatim}
for j=1 to n pardo
    P(0,j) := B(j)
    S(0,j) := B(j)
for h=1 to log(n) do
    for j=1 to n/$2^{h}$ pardo
        # h ist das Level von den Bl"attern an (h=0)
        Merge(P(h-1,2j-1) mit P(h-1,2j) zu P(h,j)

Merge: Kopiere erstes Array und f"ur alle Elemente des zweiten Arrays das
Minimum des Elements und des letzten Elements des ersten Array ein
\end{Verbatim}
end.

Analyse: \Time: $O(\log n)$
 \Work: $O(n\log n)$ ($O(n)$ pro Stufe)

\begin{satz}\label{satz:1}
  Das Preprocessing f"ur Rang-Minima-Problem ist $O(\log n)$ \Time mit
  $O(n\log n)$ \Work.
\end{satz}

\begin{bemerk}
  Im Seriellen gilt: optimal in O(n) Zeit sind LCA und Range-Minima
  l"osbar. (im Sinne der Einzelanfrage in $O(1)$ \Time)
\end{bemerk}

\begin{folger}
  \autoref{satz:1} ist nicht optimal.
\end{folger}

Im Parallelen gilt: LCA geht in $O(\log n)$ \Time optimal
Range-Minima geht in $O(\log\log n)$ \Time optimal

Aufgabe: Range-Minima \emph{optimal} in $O(\log n)$ \Time

Standardtechnik:
\begin{enumerate}
 \item Zerlege B in Bl"ocke gleicher L"ange $\log n$
 \item Preprocessing mit seriellem Algorithmus parallel f"ur alle Bl"ocke
 \item berechne f"ur jeden Block das Minimum $x_{i}
  (i=1,\ldots,\frac{n}{\log n})$ und die Prefix- und
  Suffix-Minima innerhalb der Bl"ocke
 \item wende den Algorithmus Range-Minima auf Array
  B'=($x_{1},\ldots,x_{\frac{n}{\log n}})$
\end{enumerate}

Analyse:
\begin{enumerate}
 \item $O(\log n)$ \Time, $O(n)$ \Work
 \item $O(\log n)$ \Time, $O(n)$ \Work
 \item $O(\log n)$ \Time, $O(n)$ \Work
 \item $O(\log n)$ \Time, $O(\frac{n}{\log n} \log n)=O(n)$ \Work
\end{enumerate}

Algo:

MIN($b_{i},\ldots,b_{j}) = MIN(Suffixmin_{B_{s-1}}(b_{i}),
min(x_{s},\ldots,x_{t}), Pr"afixmin_{B_{t+1}}(b_{j})$

Ergebnis: Satz * geht auch mit O(n) \Work.

\begin{folger}
  Die analogen Schranken gelten f"ur LCA.
\end{folger}

\chapter{Suchen, Mischen, Sortieren} % Kapitel 4
\section{Suchen/Search}

Vorraussetzung:
\begin{description}
 \item[INPUT] $x_{1}<x_{2}<\ldots<x_{n}$, zu suchendes Element $s$
 \item[OUTPUT] $i: x_{i}\leq<x_{i+1}$
\end{description}

Kruskal 1984: 

\begin{lemma}
  Mit $k$ Schritten l"asst sich eine sortierte Liste von $n=(p+1)^{k}-1$
  ($p$-Anzahl der Proz.) Elementen durchsuchen.
  
  \begin{proof}
    Sei $k=1$.
    
    Dann ist $n=(p+1)^{1}-1=p$. Damit sagt des Lemma, dass man mit
    $p$~Prozessoren $n$~Elemente in einem Schritt durchsuchen. -- trivial.
    
    $k-1 \rightarrow k$:
    
    $j(p+1)^{k-1} (j=1,\ldots,p)$ -- Pl"atze f"ur die Prozessoren
    
    \begin{enumerate}[1.\,{Fall:}]
     \item $s$ gefunden -- fertig
     \item $s$ ist in einem der Abschnitte, die wir nach dem Entfernen
      der roten Elemente (Elemente, die im ersten Schritt von den
      Prozessoren durchsucht wurden) erhalten. Die Anzahl der
      verbleibenden Elemente ist somit
      \begin{gather}
	\frac{n-p}{p+1}=\frac{(p+1)^{k}-1-p}{p+1}
	   =(p+1)^{k-1}-\frac{p+1}{p+1} =(p-1)^{k-1}-1
      \end{gather}
      Dieser Abschnitt l"asst sich nach Induktionsvorraussetzung in
      $k-1$~Schritten durchsuchen.
      
      Also l"asst sich ein Feld in $1+k-1=k$~Schritten durchsuchen.
    \end{enumerate}
  \end{proof}
\end{lemma}

\begin{satz}[Satz von Kruskel (1984)]
  $T_{s}(n,p)$ sei die Anzahl der Schritte, die notwendig und hinreichend
  sind, um mit $p$ Prozessoren eine Liste von $n$ Elementen zu durchsuchen.
  
  F"ur $n=(p+1)^{k}-1$ gilt: 
  \begin{gather}
    T_{s}(n,p)= \lceil\frac{\log(n+1)}{\log(p+1)}\rceil
  \end{gather}
  
  \begin{proof}
    hinreichend:
    \begin{align}
      (p+1)^{k}-1\geq n &\Leftrightarrow (p+1)^{k}\geq n+1\\
	 &\Leftrightarrow \log(p+1)^{k}\geq \log(n+1)\\
      &\Leftrightarrow k\log(p+1)\geq \log(n+1)\\
      &\Leftrightarrow k\geq \frac{\log(n+1)}{\log(p+1)}
    \end{align}
    
    notwendig: Wenn wir die Prozessoren beliebig positionieren
    $\Rightarrow$ Es bleibt min eine Abschnitt nicht untersuchter
    Elemente, der L"ange $> \frac{n-p}{p+1}$ "ubrig.

    \begin{align}
      \frac{n-p}{p+1} = \frac{n}{p+1}-\frac{p+1}{p+1}+\frac1{p+1} =
	 \frac{n+1}{p+1}-1
    \end{align}
    \begin{align}
      \frac{"ubrig-Zahl der durchsuchten}{Abschnitte}
	 \frac{\frac{n++1}-1-p}{p+1} = \frac{n+1}{(p+1)^{2}}-1
    \end{align}
    
    Mit Induktion kann man zeigen, dass nach $k$~Schritten
    $\frac{n+1}{(p+1)^{k}}-1$ Elemente als Rest bleiben.
    
    Ziel: 
    \begin{gather}
      \frac{n+1}{(p+1)^{k}} - 1 \leq 0
    \end{gather}
  \end{proof}
\end{satz}

\subsection{Konsequenzen des Satzes}
\begin{enumerate}
 \item $X$ sei sortierte Folge von $n$ paarweise verschiedenen Elementen,
  $Y$ sei bel. Folge von $m$~Elementen, $m<<n (m=\Theta(n^{s}), 0<s<1)$
  
  Ranke Y in X! - Kruskal f"ur jeden Element. Wieviele Prozessoren?
  $p=\lfloor\frac{n}{m}\rfloor$
  Einzelnes Element: Zeit $O(\frac{\log(n+1)}{\log(p+1)}) =
  O(\frac{\log(n+1)}{\log n^{1-s}})=O(\frac1{1-s}\frac{\log(n+1)}{\log
  n})=O(1)$ 
  
  parallel f"ur alle Elemente aus Y: $O(1)$ \Time \Work? $O(\frac{n}{m})$
  \Work (Anzahl der Proz*Rechenzeit) f"ur eine Element. Bei m St"uck
  $m \cdot O(\frac{n}{m}) = O(n)$ \Work
  
  \begin{satz}
    Sei Y eine beliebige Folge mit m Elementen, X: $x_{1}<\ldots<x_{n}$,
    sei m=O($n^{s}$): 0<s<1
    
    Dann kann Y in X in O(1) \Time mit O(n) \Work eingeordnet werden.
  \end{satz}

  Merge-Algo:
  INPUT: $A^{n}, B^{m} (\sqrt{m}$ ganzzahlig)
  OUTPUT: Rang(B:A)
  begin
  \begin{Verbatim}[gobble=2]
  if m < 4 -- so B zu kurz $\Rightarrow$ p=n -- exit mO(1)
  Ranke $b_{\sqrt{m}}, b_{2\sqrt{m}},b_{3\sqrt{m}},\ldots,b_{\sqrt{m}\sqrt{m}}$ parallel mit Kruskal in A
  Sei Rang(b_{i\sqrt{m}}:A) =: j(i), j(0) := 0
  $B_{i}:=(b_{i\sqrt{m}+1},b_{i\sqrt{m}+2},\ldots,b_{(i+1)\sqrt{m}-1}$
  $A_{i}:=(a_{j(i)+1},\ldots,a_{j(i+1)})$
  wenn j(i)=j(i+1), so Rang($B_{i}:A_{i})=(0,0,\ldots)$ sonst berechne
  Rang($B_{i}:A_{i})$ rekursiv
  
  Sei i < k $\leq$ m bel. Index, der nicht vielfaches von \sqrt{m} ist.
  Sei i=floor(k/sqrt(m)). Si sei Rang($b_{k}:A) :=j(i)+Rang(b_{k}:A_{i})$
  \end{Verbatim}
  end.

  Einschub:
  Die Klasse P der in polynomeller Zeit l"osbaren Algorithmen zerf"allt
  f"ur parallele Ans"atze in drei Klassen:
  \begin{enumerate}
   \item $O(1)$, z.B. Eulertour-Technik
   \item $O(\log n)$, vielzahl der Probleme
   \item $O(\log\log n)$, z.B. Merge
  \end{enumerate}

  \begin{satz}
    Merge geht optimal in $O(\log\log n)$
  \end{satz}
  
  % Grafik: kruskal-merge
  
  \begin{satz}
    Der $Rang(B^{m}:A^{n})$ kann in $O(\log\log n)$ \Time mit
    $O((n+m)\log\log m)$ \Work berechnet werden.
    
    \begin{proof}
      T(n,m) =: \Time (worst case)
      1.Schritt im Algo: if (m<4) Ranke B in A mit Kruskal mit p=n Proz.
      $\Rightarrow$ O(1) \Time, O(n) \Work
      2. schritt: (=Pfeile $\sqrt{m}$\,St"uck)
       $ p:= \sqrt{n}; Kruskal O(\frac{\log(n+1)}{\log(\sqrt{n}+1)})=O(1)$
        \Work: $\sqrt{m}$ Pfeile parallel $\Rightarrow$ Gesamt O(1) \Time
            \Work = $O(\sqrt{m}\cdot \sqrt{n}) = O(n+m) =$ \Work im Schritt~2
            (mit O(1) \Time)
      \begin{enumerate}[1.\,{Fall}]
       \item $(m\leq) 2\sqrt{m}\cdot\sqrt{n} \leq 2\sqrt{n}\sqrt{n}$
	= n+n = O(2n) = O(n) = O(n+m)
       \item n $\leq$ m
	analog $2\sqrt{m}\sqrt{n} = O(n+m)$
      \end{enumerate}
      
      $|A_{i}| =: n_{i} (0\leq i \leq \sqrt{m}-1);$ Zeit f"ur gr"un
      $(=B_{i}:A_{i}) = T(n_{i}, \sqrt{m})$
      
      $\Rightarrow T(n,m) \leq \max_{i} T(n_{i}, \sqrt{m}), T(n,3) = O(1)$  
      
      oBdA jetzt: n=m
      
      \begin{folger}
	$A^{n}, B^{m}$ sortiert k"onnen in $O(\log\log n)$ \Time mit
	$O(n\log\log n)$ \Work gemischt werden.
      \end{folger}
      
      Ziel: optimal
      
      % Grafik: 11.6./1
      A' := ($p_{1},p_{2},p_{3},\ldots)$
      B' := ($q_{1},q_{2},q_{3},\ldots)$
      $|B'|=\frac{n}{\log\log n}$
      $|A'|=\frac{n}{\log\log n}$
      
      
      Merge(A',B') in $O(\log\log n)$ \Time mit $O(n)$ \Work
      ($O(n) = O(n\log\log n/\log\log n)$)
      
      % Grafik: 11.6./2
      
      Aufgabe: Rang(A':B) - seriell f"ur jedes $p_{i}$ in $O(\log\log n)$
      \Time (Parallel f"ur alle $p_{i}$, \Work: O(n) gesamt)
      
      analog Rang(B':A)
      
      $\Rightarrow$ % Grafik 11.6./3
      
      Wenn auch unten jeweils $\log \log n$ Elemente-- so paarweise mischen
      und kleben -- $O(\log \log n)$ \Time, $O(n)$ \Work

      Wenn nicht: 
      zwichen $q_{i}$ und $q_{i+1}$ sind jeweils $\log\log n$ Element. Die
      Pfeil m"ussen zwischen $p_{i}$ und $p_{i+1}$, was auch nur
      $\log\log n$ Elemente insgesamt sind. Also f"ur jedes $q_{i}\leq
      \log\log n$ Elemente. Diese dann abschnittsweise mischen+kleben
    \end{proof}
  \end{satz}
\end{enumerate}

\section{SORT}

Baumparadigma
Ansatz
% Baum mit sortierten Listen als Bl"atter

- Umwandlung in Bin"arbaum

oBdA
\begin{enumerate}
 \item Bin"arbaum
 \item Innere Knoten haben genau 2 S"ohne
 \item In Bl"attern stehen Einerlisten
\end{enumerate}

Richard Cole:
% Grafik: Cole-Baum

\begin{verein}
  % kleiner baum mit vuw
  v: Vaterknoten
  u: linke Sohn
  w: rechter Sohn
\end{verein}

Trivial gilt: Sortieren geht mit der Idee des Baumparadigmas
("`merge-sort"') in $O(\log n\log\log n)$ \Time (optimal).
\begin{proof}
  Merge in $O(\log\log n)$ \Time optimal
\end{proof}

\subsection{Merge with the help of a cover}
c-Decke $c\in\Z$

\begin{defini}
  sortierte Folge X hei"se c-Decke einer sortierten Folge Y, wenn Y
  h"ochstens c Elemente zwischen zwei aufeinanderfolgenden Elementen der
  Folge $X_{\infty} := (-\infty, X, \infty)$ enth"alt
\end{defini}

\begin{satz}
  Seien $A^{n}, B^{m}$ sortiert, sei X c-Decke von $A^{n}$ und $B^{m}$.
  
  Wenn der Rang(X:A) und Rang(X:B) bekannt sind, so kann Merge(A, B) in
  O(1) \Time und $O(|X|)$ \Work erhalten werden.
  
  \begin{proof}
    X = ($x_{1},\ldots,x_{s}$), Rang(X:A)=($r_{1},\ldots,r_{s}$),
    Rang(X:B)=($t_{1},\ldots,t_{s})$
    
    Da c-Decke: $|B_{i}|,|A_{i}|\leq c$
  \end{proof}
\end{satz}

Sei $T$ unser Baum, $v$ ein Knoten von $T$, $Level(v)$, H"ohe $h(T)$, Altitude
$alt(v) := h(T)-Level(v)$

Algorithmus arbeitet in Schritten $s$.

\begin{defini}
  $L[v]:$ Lister der sortierten Elemente der S"ohne
  
  $L_{s}[v]:$ Liste im Knoten $v$ nach Schritt $s$, Ziel: $L_{s}[v] =
  L[v]$ f"ur $s\geq 3\cdot alt(v)$
  
  v \emph{voll} im Schritt $s$, wenn $L_{s}[v]=L[v]$
\end{defini}

\begin{defini}
  $v$ hei"st \emph{aktiv} im Schritt $s$ :$\Leftrightarrow$ $alt(v) \leq
  s \leq alt(v)$
\end{defini}

\begin{folger}
  Wir bekommen f"ur root:
  \begin{gather}
    alt(root) = h(T) \leq s \leq 3\cdot alt(root) = 3\cdot h(T)
  \end{gather}
  
  Root ist damit voll nach $3\cdot h(T)$ Schritten.
\end{folger}

\begin{defini}
  Sei $L$ sortierte Liste. Das c-Raster $Raster_{c}(L)$ (c-sample) ist die sortierte
  Teilliste aus jedem $c$-ten Element.
\end{defini}

\begin{bsp}
  \begin{gather}
    L = (1,3,5,6,7,8,11,13,14,15,16)\\
    Raster_{4}(L) = (6,13)\\
    Raster_{2}(L) = (3,6,8,13,15)\\
    Raster_{1}(L) = L
  \end{gather}
\end{bsp}

\begin{description}
 \item[Pipeline-Strategie] besteht aus der Bestimmung von $L[v]$ "uber
  Schritte $s$, wobei $L_{s}[v]$ Approximation von $L[v]$ und im Schritt
  $s+1$ verbessert wird. Dabei wird ein \emph{Sample} von $L_{s}[v]$
  benutzt, um Informationen Richtung Root zu senden, die genutzt werden,
  um weiter oben neue Approximationen an die entsprechenden Listen zu
  erhalten.
\end{description}

\begin{defini}
  x Knoten,
  \begin{gather}
    Sample(L_{s}[v]):=
       \begin{cases}
	 Raster_{4}(L_{s}[x])& s\leq3\cdot alt(x)\\
	 Raster_{2}(L_{s}[x])& s=3\cdot alt(x)+1\\
	 Raster_{1}(L_{s}[x])& s=3\cdot alt(x)+2\\
       \end{cases}
  \end{gather}
\end{defini}

Algo: R-Cole
\begin{Verbatim}
Ist v Blatt, so $L_{0}$[v] = Wert im Blatt
 v kein Blatt $L_{0}$[v]=$\emptyset$
\end{Verbatim}
INPUT: $\forall v:L_{s}[v]$ $v$ voll, wenn s$\geq$ 3 alt(v)
OUTPUT: $\forall$ v: $L_{s+1}[v]$, v voll, wenn s+1$\geq$ 3alt(v)
begin
\begin{Verbatim}
f"ur alle aktiven Knoten pardo
L'_{s+1}[w] = Sample($L_{s}[w])$
L'_{s+1}[u] = Sample($L_{s}[u])$
merge($L_{s+1}'[u], L'_{s+1}[w])$ zu $L_{s+1}[v]$
\end{Verbatim}

% Hier fehlen zwei Vorlesungen

\section{Selektion}

Median der Mediane-Technik; R.Cole; Prefixsummenalgo.; Litheratur liefert u.a. 2
Ans"atze: Akl (s. Info 3), R.Cole

Akl: 0<x<1: O($n^{x}$) \Time optimal
INPUT: Folge ($a_{1},\ldots,a_{n}$), k: 1$\leq$k $\leq$n
OUTPUT: El. vom Rang $k$

F"alle k=1,n. $O(\log\log n)$ CRCW, optimal \Work $O(n)$
Median: nicht erreichbar.
Bem. dazu: $par_{n}(\alpha_{1},\ldots,\alpha_{n}) := \alpha_{1}\oplus
\alpha_{2}\oplus\ldots\oplus \alpha_{n}$ ($\alpha_{i}$ boolsche Werte)

Die Berechnung von $par_{n}$ erfordert auf der PRIORITY-CRCW-PRAM
$\omega(\frac{\log n}{\log\log n})$ \Time. (Beweis schwer!) Die gleiche
Schranke gilt f"ur den Median.

Ziel: $O(\log n \log\log n)$ \Time, CREW (EREW) \emph{optimal}. (bekannt ist
sogar $O(\log n\log^{\ast}n))$

Alg. Selektion
INPUT: A=($a_{1},\ldots,a_{n}$), k: 1$\leq$ k $\leq$ n
OUTPUT: \ldots
begin
\begin{Verbatim}
n0 := n; s := 0
while ns > n/log(n) do
    s := s+1
    Zerlege A in Bl"ocke Bi
    for i := 1 to n/log(n) pardo
       mi := median(Bi) (seriell linear) (O(\log $n_{s}$)\time O($n_{s}$)\work)
    a := median(m1,\ldots,mnlogn) (mit Cole) (O(\log $n_{s}$)\time O(\frac{$n_{s}$}{\log $n_{s}$}\log $n_{s}$)\work
    Bestimme s1,s2,s3 - Anzahl der Elemente < a, =a, >a (mit Prefixsummenalgo in O(\log n)\time optimal)
    Case statement s1 < k $\Leftarrow$ s1+s2 -> OUTPUT "a" (}O(\log$n_{s}$)\time)
      k $\Leftarrow$ s1 -> packe in Array, ns := s1        (})
      k > s1 + s2  -> packe in Array, ns := s3             (})
\end{Verbatim}

$ns$\ldots Zahl der Elemente nach Schritt $s$

dazu: 
\begin{equation*}
  \frac{n}4 \leq Rang(a:A) \leq \frac34 n
\end{equation*}

$a\geq$ H"alfe der Mediane $m_{1},\ldots,m_{\frac{n}{\log n}}$, jeder
Median ist $\geq\frac{\log n}{2}$ Elemente
\begin{gather}
  \frac{n_{s}}{2\log n}\frac{\log n}{2}=\frac{n_{s}}{4}\\
  n_{s} \leq \frac34 n
\end{gather}
$\Rightarrow O(\log n\log\log n) \Time, O(n)$ \Work

\chapter{Parallelisierbarkeit}

Theorem von Brent liefert $O(\frac{W(n)}{p}+T(n))$ \Time. Jedoch macht es
in vielen F"allen keinen Sinn, mehr als $\alpha(n)$ Prozessoren zu
w"ahlen, da dies keine Verbesserung bringt. Um z.B. ein Element in 100
Zahlen zu suchen, machen mehr als 100 Prozessoren keinen Sinn.

Was bedeutet parallelisierbarkeit:
\begin{enumerate}[1.\,{Ansatz}]
 \item mehr Prozessoren liefern bessere Zeiten
 \item Das Problem ist schneller l"osbar
\end{enumerate}

\begin{defini}
  Ein Problem $L$ ist in $NC$ (Nick's class -- benannt nach seinem
  Erfinder), wenn die charakteristische Funktion von $L$ in $O( (\log
  n)^{k} )$ \Time mit $O(n^{k})$ Prozessoren f"ur festes $k$ berechnet
  werden kann.
\end{defini}

\begin{gather}
  L\subseteq\sum\limits^{\ast}; w\in\Sigma^{\ast}, w\in L?
     \chi_{L}(w) = \begin{cases} 0& w\in L\\ 1& w\not\in L\end{cases}
\end{gather}

"`polylogarithmische Zeit"' mit polynomial vielen Prozessoren. 

\begin{defini}
  $L$ hochgradig parallelisierbar $:\Leftrightarrow$ $L\in NC$ ist.
  (Vorr. generell: $L\in P$) (wenn nicht in NC, so exist kein schneller
  paralleler Algo.)
\end{defini}

\subsection{Gibt es Probleme in \texorpdfstring{$P$}{P}, die nicht in
  \texorpdfstring{$NC$}{NC} sind?}

\begin{defini}
  $L\subseteq \Sigma^{\ast}$
  
  $L$ ist $P$-vollst"andige $:\Leftrightarrow$ $\forall L_{1}\subseteq
  \Sigma^{\ast}, L_{1}\in P: L_{1}\leq_{NC} L$
  
  $L_{1}\leq_{NC} L :\Leftrightarrow \exists f: \Sigma^{\ast}\rightarrow
  \Sigma^{\ast}, f\in NC: \forall w\in \Sigma^{\ast}: w\in
  L_{1}\Leftrightarrow f(w) \in L$
\end{defini}

NC\ldots Klasse der in polylogarithmischer Zeit und polylogarithmisch vielen
Prozessen

\begin{bemerk}
  $NC \subseteq P$
  
  \begin{proof}
    "uber serielle Modellierung der Rechnung
  \end{proof}
\end{bemerk}

\section{Maximum Flow-f}
Graph $G=(V,E)$

% Grafik: parallel

\begin{enumerate}
 \item Kapazit"at $c(e)$ eines Knotens $e$
  0$\leq f(e) \leq$ c(e) (f: Flu"s durch e)
  Flu"s f: E$\rightarrow \N$
 \item $f^{-}(v) :=\sum_{u}f(u,v)
  (Abfluss), f^{+}(w) :=\sum_{u}f(v,w) (Zufluss); \forall v\in
  V\setminus\{s,t\}: f^{+}(v)=f^{-}(v)$
\end{enumerate}

\begin{defini}
  Ein Flu"s f ist maximal, wenn val(f) := $f^{-}(t)-f^{+}(t)
  (=f^{+}(s)-f^{-}(s)) \geq val(f') \forall$ zul"assigen $f'$ ist (d.h.
  f"ur $f,f'$ gelten (1) (2) % obige Aufz"ahlung
\end{defini}

\subsection{Lineare Programmierung:}
\begin{gather}
  \sum_{i=1}^{n}c_{i}x_{i} \rightarrow \max\\
  A\overrightarrow{x} \leq b, x_{i}\geq0, 1 \leq i \leq n
\end{gather}

\subsection{DFS (geordnete Tiefensuche)}
INPUT: beginnend bei $a$ %garfik dfs
OUTPUT:
\begin{tabular}{l|l}
  Ecken& DFS-Liste\\
  \hline
  a& 1\\
  b & 2\\
  c & 5\\
  d& 3\\
  e& 4\\
  f&6\\
  g&7
\end{tabular}

Formulieren wir ein Problem "`Kommt $e$ vor $d$ in DFS-Liste"' so haben
wir damit ein Entscheidungsproblem -- ein Sprachproblem.

relevante Entscheidungsproblem: -- wenn das Ursprungsproblem l"osbar ist,
dann auch das relevante Entscheidungsproblem. Wenn das relevante
Entscheidungsproblem unl"osbar ist, so erstrecht das Ursprungsproblem.

\begin{defini}
  \begin{gather}
    L_{1} \leq_{NC} L_{2} :\Leftrightarrow \exists f_{L_{1}}
       NC-\text{brechenbar}: \forall w\in \Sigma^\ast: w\in L_{1}
       \Leftrightarrow f(w)\in L_{2}
  \end{gather}
\end{defini}

\begin{defini}
  \begin{gather}
    L,L'\subseteq \Sigma^{\ast}, L P(arallelisierbar)-vollst"andig
       :\Leftrightarrow L\in P \wedge \forall L'\in P: L'\leq_{NC} L
  \end{gather}
\end{defini}

\begin{satz}[Hauptsatz "uber die Reduktion]
  Seien $L_{1}, L_{2}\in \Sigma^{\ast}: L_{1}\leq_{NC} L_{2}$
  
  So gilt: $L_{2}\in NC \Rightarrow L_{1}\in NC$
  
  \begin{proof}
    Gelte: $L_{2}\in NC$
    Beh.: $L_{1}\in NC$ Seien $u_{i}\in \Sigma^{\ast} input f"ur L_{1}:
    u_{1}\in L_{1}?$
    
    Algo:
    \begin{enumerate}
     \item Berechne f($u_{1})$
     \item Pr"ufe: f($u_{1})\in L_{2}$? (in NC)
     \item wegen f($u_{1}) \in L_{2} \Leftrightarrow u_{1}\in L_{1}$ ist
      die Antwort von 2. gleich der Antwort $u_{1}\in L_{1}$ (in NC
      erhalten)
    \end{enumerate}
    $\Rightarrow$ $L_{1}\in NC$
  \end{proof}
\end{satz}

\begin{satz}[Satz "uber Transitivit"at]
  \begin{gather}
    L_{1} \leq_{NC} L_{2}\wedge L_{2} \leq_{NC} L_{3} \Rightarrow
       L_{1}\leq_{NC} L_{3}
  \end{gather}
\end{satz}

\begin{bemerk}
  Zwei Methoden zum Nachweise der P-Vollst"andigkeit:
  \begin{enumerate}
   \item direkte Methode
   \item Indirekte Methode mit Hauptsatz
  \end{enumerate}
\end{bemerk}

CVP (circuit value problem)
$\forall L'\in P: L'\leq_{NC} CVP$

% 6.7.

\begin{defini}
  Ein Schaltkreis (engl. Circuit) $C=<g_{1},\ldots,g_{n}>$
  :$\Leftrightarrow$ $\forall i\in \{1,\ldots,n\}: g_{i} sind INPUT (\in
  \{0,1\}) oder g_{i}=g_{j}\vee g_{k}, g_{i}=g_{j}\wedge g_{k},
  g_{i}=\neg g_{j}: j,k < i$
\end{defini}

\begin{defini}
  CVP-Problem: INPUT: Circuit C. QUESTION: $g_{n}=1$?
\end{defini}

Theorem von Cook: (Beweis von irgendeinem Satz)
z.z. $\forall L\in P: L \leq_{NC} CVP$
Weg: Allgemeines Beweisrezept

$L\in P \Rightarrow \exists TM\in\mathcal{TM} (\text{Turing-Maschine})$
$\Sigma=\{a_{1},\ldots,a_{m}\}, Q=\{q_{1},\ldots,q_{s}\}, T(n)-$Rechenzeit

$L\leq_{NP} CVP$, d.h. es existiert eine Funktion f, die L auf CVP
reduziert, f=$f_{L}() \Rightarrow m,s$ sind konstant, T(n) ist fest
gegeben. L bel., aber fest!

Ziel: Angabe eines NC-Algorithmus, der ein $f$ berechnet: Jedes Beispiel
(engl. Instance) $I$ zu $L$ wird mit $f$ in ein Beispiel $I_{CVP}$ zu
$CVP$ "uberf"uhrt: $I_{L}\in L \Leftrightarrow f_{L}(I_{L})=I_{CVP}\in CVP$.

Wir entwerfen einen Schaltkreis, der die Arbeit der Turning-Maschine
nachbildet und genau dann durchschaltet, wenn die Turing-Maschine die
Sprache erkennt.

Simulation:
\begin{enumerate}
 \item H(i,t): steht der Lese-Schreibkopfes zum Zeitpunkt $t\in
  \{1,\ldots,T(n)\}$ auf dem Band an der Stelle $i$?
 \item C(i,j,t): steht das Zeichen $a_{j}$ an der Position $i$ zum
  Zeitpunkt $t$ auf dem Band?
 \item S(k,t): befindet sich die Turing-Maschine zum Zeitpunkt $t$ im
  Zustand $q_{k}$?
\end{enumerate}

Start:
\begin{align*}
  H(i,0) &= \begin{cases}1&i=1\\0&i>1\end{cases}\\
  S(k,0) &= \begin{cases}1&k=1\\0&k>1\end{cases}\\
  C(i,j,0) &= \begin{cases}1&\text{Zelle} i = a_{j}\\0&\text{sonst}\end{cases}
\end{align*}

Mittels $\delta$ beschreiben wir $t\rightarrow t+1$

% 9.7.

Mit dem letzten Satz haben wir gezeigt, dass NOR-CVP schwer entscheibar ist.
Diese Aussagen wollen wir jetzt nutzen, um zu zeigen, dass auch DFS
schwer ist.

Wir zeigen, dass NOR-CVP leichter ist als DFS. Jedoch ist DFS kein
Entscheidungspoblem, was eine direkte Abbildung der INPUT-Mengen
aufeinander nicht m"oglich macht. Daher konstruieren wir ein
Entscheidungsproblem, das leichter ist als DFS, an dem wir aber zeigen
k"onnen, dass es schwerer ist als NOR-CVP. Damit w"are dann auch gezeigt,
dass DFS schwerer ist, als NOR-CVP und damit auch schwer entscheidbar ist.

DFS:
INPUT: gerichteter Graph, Kantenmarkierung (genormt), Startknoten,
  Traversierung im Graph soll m"oglich sein
OUTPUT: DFS-Liste

zu konstruieren mit NC-Zuordnung.

Jedem INPUT von NOR-CVP: $I_{CVP} \rightarrow f(I_{CVP})=I_{DFS}(v,w):$
Wert($I_{CVP})=1 \Leftrightarrow$ das relevante Entscheidungsproblem von
$I_{DVS}$ wahr ist (=1 ist) (d.h. $u$ wird vor $v$ besucht.

$I_{CVP}: <g_{1},g_{2},\ldots,g_{n}>$
entweder $g_{i}=1 \vee g_{i}=\neg(g_{i}\wedge g_{k}), k,j<i$
$\rightarrow$ Graph, s,u,v
\begin{enumerate}[1.\,{Fall}]
 \item $g_{i}=1$ $\Rightarrow$ Abb.1 $g_{i}$ sei INPUT f"ur
  $g_{j_{1}},\ldots,g_{j_{k}}$, bilde <i,$j_{1}$>,\ldots,<i,$j_{k}$>
  
  Ind.beweise: Wenn wert=1 $\Leftrightarrow$ so wird s(i) vor t(i) besucht
 \item $g_{i} = \neg (g_{j}\vee g_{k}) \Rightarrow $ Abb.2
  
  Ind.beweis: Sei Wert($g_{i}$)=1 $\Rightarrow$ ($g_{j}=0=g_{k})$
  Ind.vor. $\Rightarrow$ t(j) vor s(j) besucht (t(k) vor s(k)) genauer:
  Abb.4-Verlauf: speziell: <j,i>, <k,i> sind nicht besucht $\Rightarrow$
  f"ur $g_{i}$ wird Abb.5 verfolgt $\Rightarrow$ s(i) vor t(i)
  
  sei Wert($g_{i}$)=0 $\Rightarrow$ ($g_{j}$=1 oder $g_{k}$=1)
 
  \begin{enumerate}
   \item $g_{j}=1 \rightarrow G_{j}$ (Graph, der $g_{i}$ zugeordnet ist)
    wird nach Abb.5 durchlaufen $\Rightarrow$ "`Zackenweg"'
    
    $\Rightarrow$ in Abb.3 ist <j,i> bereits besucht $\Rightarrow$ f"ur i
    $\Rightarrow$ Weg wie Abb.4
   \item $g_{j}=0, g_{k}=1$ $\Rightarrow$ im wesentlichen Weg wie in
    Abb.4 (Ausnahme: erst <j,i> dann zur"uck)
  \end{enumerate}
  $\Rightarrow$ t(i) vor s(i)
\end{enumerate}

\begin{defini}
  u:=s(n), v=t(n) $\Rightarrow$ u vor v besucht $\Leftrightarrow$
  wert($g_{n}$)=1
\end{defini}

\clearpage
\appendix
\pdfbookmark[0]{Index}{index}
\printindex

\end{document}
