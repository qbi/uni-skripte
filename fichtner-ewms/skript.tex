% Einige zusätzliche Informationen für rubber
%  rubber erkennt nicht, dass die Datei weg kann, daher sagen wir es ihm
% rubber: clean $base.thm
%  rubber soll nach Änderungen an der Datei nochmal bauen
% rubber: watch $base.thm
% rubber: makeidx.tool      xindy
% rubber: makeidx.language  german-din
% rubber: makeidx.modules   indexstyle.xdy
%
% scrreprt trifft am Besten die Bedürfnisse eines Skripts, das ganze wird
% zweiseitig (twoside), d.h. es wird zwischen linker und rechter Seite
% unterschieden, und wir verwenden zwischen den Absätzen einen Abstand
% von einer halben Zeile (halfparskip) und dafür keinen Absatzeinzug,
% wobei die letzte Zeile eines Absatzes zu min. 1/4 leer ist.

\RequirePackage[l2tabu,orthodox]{nag}  % nag überprüft den Text auf veraltete
                   % Befehle oder solche, die man nicht in LaTeX verwenden
                   % soll -- l2tabu-Checker in LaTeX

\documentclass[halfparskip*,ngerman,draft,twoside]{scrreprt}

\usepackage{ifthen}
\usepackage{makeidx}
% \usepackage[final]{graphicx}  % Für Grafiken
\usepackage{color}
\usepackage[draft=false,colorlinks,bookmarksnumbered,linkcolor=blue,breaklinks]{hyperref}
\usepackage{babel}
\usepackage[latin1]{inputenc}
\usepackage{nicefrac}
% \usepackage{tabularx}

\usepackage{lmodern}		% Latin Modern
% \usepackage{type1ec}           % cm-super
\usepackage[T1]{fontenc}        % T1-Schriften notwendig für PDFs
\usepackage{textcomp}           % wird benötigt, damit der \textbullet
                                % für itemize in lmodern gefunden wird.

\usepackage[intlimits,leqno]{amsmath}
\usepackage[all,warning]{onlyamsmath}  % warnt bei Verwendung von nicht
                                       % amsmath-Umgebungen z.\,B. $$...$$
\usepackage{amssymb}     % wird für \R, \C,... gebraucht
\usepackage{fixmath}     % ISO-konforme griech. Buchstaben

\usepackage[amsmath,thmmarks,hyperref]{ntheorem} % für die Theorem-Umgebungen
                                                 % (satz, defini, bemerk)
\usepackage{xspace}      % wird weiter unten gebraucht
\usepackage{slashbox}    % für schräge Striche links oben in der
                         % Tabelle; s. texdoc slashbox

\usepackage{paralist}    % besseres enumerate und itemize und neue
                         % compactenum/compactitem; s. texdoc paralist

\usepackage{svn}         % Zum Auswerten und ordentlichen Darstellen der
                         % SVN-Schlüsselwörter (s. vor \begin{document})
                         % dafür muss in SVN noch das Flag svn:keywords
                         % auf "LastChangedRevision LastChangedDate"
                         % gesetzt werden
% \usepackage{ifpdf}       % Erkennung, ob PDF generiert wird; nützlich zur
                         % Unterscheidung bei Grafiken \input{XYZ.pdf_t}
\usepackage{ellipsis}    % Korrektur für \dots
\usepackage{fixltx2e}
\usepackage[final]{microtype} % Verbesserung der Typographie
\usepackage{enumerate}   % wird evtl. nicht gebraucht und kann raus.
\usepackage{pst-plot}
\usepackage{wasysym}     % für den Blitz

% Damit auch die Zeichen im Mathemode in Überschriften fett sind
% <news:lzfyyvx3pt.fsf@tfkp12.physik.uni-erlangen.de>
\addtokomafont{sectioning}{\boldmath}

% nach dem Theoremkopf wird ein Zeilenumbruch eingefügt, die Schrift des
% Körpers ist normal und der Kopf wird fett gesetzt
\theoremstyle{break}
\theorembodyfont{\normalfont}
\theoremheaderfont{\normalfont\bfseries}
\theoremnumbering{arabic}

% Die folgenden Umgebungen werden einzeln nummeriert und am Ende jedes
% Kapitels zurückgesetzt
\newtheorem{satz}{Satz}[section]
%\newtheorem*{satz*}{Satz o.\,B.}
%\newtheorem{bemerk}{Bemerkung}[chapter]
%\newtheorem{defini}{Definition}[chapter]
%\newtheorem{bsp}{Beispiel}[chapter]
%\newtheorem{festl}{Festlegung}[chapter]

%\theoremstyle{definition}
\newtheorem*{definition}{Definition}
%\newtheorem{proof}{Beweis}

%\theoremstyle{remark}
\newtheorem*{remark}{Bemerkung}
\newtheorem*{beispiel}{Beispiel}


\theoremheaderfont{\scshape}
\theorembodyfont{\normalfont}
% Das Zeichen am Ende eines Beweises
\theoremsymbol{\ensuremath{_\blacksquare}}
% \theoremsymbol{q.\,e.\,d.}
\newtheorem{proof}{Beweis:}

% Hier die Definition, wie \autoref die Umgebungen nennen soll, die mit
% \newtheorem definiert wurden
\newcommand*{\satzautorefname}{Satz}
\newcommand*{\bemerkautorefname}{Bemerkung}
\newcommand*{\definiautorefname}{Definition}
\newcommand*{\bspautorefname}{Beispiel}
\newcommand*{\proofautorefname}{Beweis}
\newcommand*{\festlautorefname}{Festlegung}
% Zwischen Unter- und Unterunterabschnitten sollte nicht unterschieden
% werden.
\renewcommand*{\subsectionautorefname}{Abschnitt}
\renewcommand*{\subsubsectionautorefname}{Abschnitt}

\pagestyle{headings}

\newcommand*{\R}{\mathbb{R}}      % reelle Zahlen
\newcommand*{\N}{\mathbb{N}}      % natürliche Zahlen
\newcommand*{\Q}{\mathbb{Q}}      % gebrochene Zahlen
\newcommand*{\Z}{\mathbb{Z}}      % ganze Zahlen

\newcommand*{\CF}{\mathcal{F}}
\newcommand*{\CO}{\mathcal{O}}
\newcommand*{\FA}{\mathfrak{A}}
\newcommand*{\FB}{\mathfrak{B}}
\newcommand*{\FE}{\mathfrak{E}}
\newcommand*{\FP}{\mathfrak{P}}
\newcommand*{\FX}{\mathfrak{X}}
\newcommand*{\fmineins}{f^{-1}}
\newcommand*{\hmineins}{h^{-1}}
\newcommand*{\chiai}{\chi_{A_i}}
\newcommand*{\limn}{\lim_{n\rightarrow\infty}}
\newcommand*{\ofmu}{[\Omega,\CF,\mu]}
\newcommand*{\ofp}{[\Omega,\CF,P]}
\newcommand*{\rbel}{[\R,\FB,\ell]}
\newcommand*{\pxi}{P\circ\xi^{-1}}


% Wenn irgendwo Unklarheiten zum Inhalt im Skript auftreten, können sie
% einfach mit \help{Ich verstehe das nicht} hervorgehoben werden. Dies
% macht es leichter sie alle zu finden und auch ganz einfach
% auszublenden, indem man den Befehl einfach leer definiert
\newcommand*{\help}[1]{\textcolor{green}{help: #1}}

% \todo ist das gleiche wie \help nur für offene Aufgaben
\newcommand*{\todo}[1]{\textcolor{red}{todo: #1}}

% Um wichtige Begriffe im Text überall gleich vorzuheben (gleiches
% Markup), sollte dieser Befehl verwendet werden. Das Argument wird
% automatisch als Indexeintrag verwendet. Dieser kann aber auch als
% optionales Argument selbst bestimmt werden.
\newcommand*{\highl}[2][]{\textbf{\boldmath{#2}}%
  \ifthenelse{\equal{#1}{}}{\index{#2}}{\index{#1}}%
}

% Definition für Xindy für die Trennung der einzelnen Abschnitte im
% Index. siehe auch die Datei indexstyle.xdy
\newcommand*{\indexsection}{\minisec}

% Für Leute, die nicht gern o.\,B.\,d.\,A. jedesmal eintippen wollen
%\newcommand*{\obda}{o.\,B.\,d.\,A.\xspace}

% Diese Befehle sind dafür gedacht, dass die Symbole für "genau dann wenn"
% im ganzen Dokument gleich aussehen. Außerdem erlaubt es eine schnelle
% Veränderung aller Stellen, falls der Prof. doch nicht mehr gdw nimmt,
% sondern \Leftrightarrow.
% \newcommand*{\gdw}{\ifthenelse{\boolean{mmode}}%
% 			       {\mspace{8mu}gdw\mspace{8mu}}%
% 			       {$gdw$\xspace}}
% \newcommand*{\gdwdef}{\ifthenelse{\boolean{mmode}}%
% 			       {\mspace{8mu}gdw_{def}\mspace{8mu}}%
% 			       {$gdw_{def}$\xspace}}

% Um sicherzustellen, dass jeder Betrag-/jede Norm links und rechts die
% Striche bekommt, sind diese Befehle da. Damit kann man nicht die
% rechten Striche vergessen und es wird etwas übersichtlicher. (Vorschlag
% ist aus amsldoc) \abs[\big]{\abs{a}-\abs{b}} \leq \abs{a+b}
%\newcommand*{\abs}[2][]{#1\lvert#2#1\rvert}
%\newcommand*{\norm}[2][]{#1\lVert#2#1\rVert}

% Das original Epsilon sieht nicht so toll aus
\renewcommand*{\epsilon}{\varepsilon}
% ... und mancheinem gefällt auch das Phi nicht
\renewcommand*{\phi}{\varphi}

\makeindex

\SVN $LastChangedRevision$
\SVN $LastChangedDate$

% + Brueche auf nicefrac oder frac pruefen


\begin{document}
\title{Elementare Wahrscheinlichkeitstheorie und Mathematische
  Statistik}
\author{Prof.\,Dr.\,Karl-Heinz Fichtner}
\date{WS 2004/05}

\maketitle

\clearpage
\chapter*{Vorwort}

{\itshape
  Dieses Dokument wurde als Skript für die auf der
  Titelseite genannte Vorlesung erstellt und wird jetzt im Rahmen des
  Projekts
  "`\href{http://www.minet.uni-jena.de/~joergs/skripte/}
  {Vorlesungsskripte der Fakultät für Mathematik}
  \href{http://www.minet.uni-jena.de/~joergs/skripte/}{und Informatik}"'
  weiter betreut. Das
  Dokument wurde nach bestem Wissen und Gewissen angefertigt. Denoch
  garantiert weder der auf der Titelseite genannte Dozent, die Personen,
  die an dem Dokument mitgewirkt haben, noch die
  Mitglieder des Projekts für dessen Fehlerfreiheit. Für etwaige Fehler
  und dessen Folgen wird von keiner der genannten Personen eine Haftung
  übernommen. Es steht jeder Person frei, dieses Dokument zu lesen, zu
  verändern oder auf anderen Medien verfügbar zu machen, solange ein
  Verweis auf die Internetadresse des Projekts
  \url{http://www.minet.uni-jena.de/~joergs/skripte/}
  enthalten ist.

  Diese Ausgabe trägt die Versionsnummer~\SVNLastChangedRevision{} und ist vom
  \SVNDate{}. Eine neue Ausgabe könnte auf der Webseite des Projekts verfügbar
  sein.

  Jeder ist dazu aufgerufen Verbesserungen, Erweiterungen und
  Fehlerkorrekturen für das Skript einzureichen bzw. zu melden oder diese
  selbst einzupflegen -- einfach eine E-Mail an die
  \href{mailto:skripte@listserv.uni-jena.de}{Mailingliste
  \texttt{<skripte@listserv.uni-jena.de>}} senden. Weitere Informationen
  sind unter der oben genannten Internetadresse verfügbar.

  Hiermit möchten wir allen Personen, die an diesem Skript mitgewirkt
  haben, vielmals danken:
  \begin{itemize}
   \item \href{mailto:jens@kubieziel.de}{Jens Kubieziel
    \texttt{<jens@kubieziel.de>}} (2004--2007)
  \end{itemize}
}

\clearpage
\pdfbookmark[0]{Inhaltsverzeichnis}{inhaltsverzeichnis}
\tableofcontents

\clearpage
\pdfbookmark[0]{Auflistung der Sätze}{theoremlist}
\chapter*{Auflistung der Theoreme}

\pdfbookmark[1]{Sätze}{satzlist}
\section*{Sätze}
\theoremlisttype{optname}
\listtheorems{satz}

\pdfbookmark[1]{Definitionen und Festlegungen}{definilist}
\section*{Definitionen und Festlegungen}
% \theoremlisttype{all}
\listtheorems{defini,festl}


\chapter{Maß- und allgemeine Integrationstheorie}
Diese Vorlesung gliedert sich im wesentlichen in drei Teile. Dazu gehört
dieses Kapitel und im späteren Verlauf dann die Wahrscheinlichkeitstheorie.
Später wird dann auch auf die Stochastik eingegangen.

Die Maßtheorie stellte früher eine eigenständige 2+4-Vorlesung dar. Für
Wirtschaftsmathematiker ist diese jedoch gekürzt. Hauptsächlich werden
längere Beweise zu Sätzen nicht geführt. Diese können in der Lektüre
nachvollzogen werden.

\section{Einführung in die Maßtheorie}

Die Maßtheorie stellt eine Verallgemeinerung einiger elementargeometrischer
Begriffe dar und ermöglicht es, auch komplizierten Mengen ein Maß
zuzuordnen. Dabei sollte man diese deutlich von der Messtheorie in der
Physik unterscheiden. Wir betrachten hier zunächst allgemeine Maße und
beschränken uns dann im späteren Verlauf auf Wahrscheinlichkeitsmaße.

\subsection{Das Grundmodell der Wahrscheinlichkeitsrechnung}

Die Wurzeln der Wahrscheinlichkeitsrechnung liegen zu Zeiten des
30-jährigen Krieges. In den Gefechtspausen veranstalteten die
Offiziere oft Glücksspiele. Diese haben naturgemäß einen
zufälligen Ausgang und einige Spieler machten sich Gedanken über
den Verlauf und wie der Ausgang beeinflusst werden kann. Hiervon
stammen einige der ersten praktischen Überlegungen zu dem
Themengebiet.

Ein Breifwechsel zwischen Blaise Pascal und Pierre de Fermat aus dem Jahr
1654 wird gemeinhin als Geburtstunde der Wahrscheinlichkeitsrechnung
angesehen. Die ersten Veröffentlichungen zur Wahrscheinlichkeitsrechnung
enthielten noch keine Formeln. Stattdessen wurden die Rechenschritte in
langen Beschreibungen erklärt. Dies hatte den Vorteil, dass man es langsam
lesen musste und es dadurch auch besser verstand.

Der Mathematiker David Hilbert stellte am 08.~August~1900 eine Liste
von 23 bis dahin ungelösten Problemen der Mathematik auf. Dabei formulierte
Hilbert als sechstes Problem die Frage, wie die Wahrscheinlichkeitstheorie
und die Mechanik axiomatisiert werden kann. Mittlerweile hat man diese
Fragestellung auf die gesamte Physik erweitert.

Der russische Mathematiker Andrei Nikolajewitsch Kolmogorow schafft im Jahre
1933 die Axiomatisierung. Seine Axiome werden auch Gegenstand dieser
Vorlesung sein. Unser Anliegen ist es zunächst, ein mathematisches Modell
zur Beschreibung zufälliger Erscheinungen zu Formen. Als Beispiele dienen
uns Münze und Würfel sowie auch Messungen.


\subsection{Mathematisches Modell für  zufällige Vorgänge}

Das mathematische Modell besteht aus den drei Faktoren:
$\ofp$. Diese haben folgende Bedeutung:
\begin{enumerate}[1)]
\item Raum der \highl[Elementarereignis]{Elementarereignisse} - $\Omega$\\
Dies ist eine \emph{nichtleere} Menge ($\Omega\neq\emptyset$), die mögliche
Versuchsausgänge klassifiziert.\\
Beispiel: Würfel $\rightarrow \Omega = \{1,2,3,4,5,6\}$, Messung
$\rightarrow M\subseteq \R$, Münzwurf $\rightarrow \Omega =\{W, Z\}=
\{0, 1\}$
\item System der Ereignisse - $\CF$\\
Die Menge $\CF$ beinhaltet zulässige Aussagen über das
Versuchsergebnis und es gilt $\CF\subseteq 2^\Omega$\footnote{Die
  \highl{Potenzmenge} wird auch mit $\FP$ bezeichnet. Damit gilt hier: $\CF
  \subseteq \FP(\Omega)$}\\
Beispiel: Würfel $\rightarrow A=\{2,4,6\}\subseteq\Omega$ entspricht
der Aussage "`Ich würfle eine gerade Zahl."'
\item \highl{Eintrittswahrscheinlichkeit} - $P$\\
$P$ ist eine Abbildung $P:\CF\rightarrow[0,1]$ und $P(A)$ bezeichnet
die Wahrscheinlichkeit mit der das Ereignis $A\in\CF$ eintritt.\\
Beispiel: Würfel $\rightarrow \Omega=\{1,2,3,4,5,6\}, A=\{2,4,6\},
P(A)=\nicefrac{1}{2}, P(\{k\})=\frac{1}{6}, k=1,\ldots,6$
\end{enumerate}

Welche sinnvollen Forderungen sollte man nun an $\Omega, \CF, P$ stellen?
Daraus ergibt sich der Aufbau der Axiomatik.

\subsection{Eigenschaften von $\CF$}

\begin{itemize}
\item Grundidee: Logische Operatoren werden zu entsprechenden
  mengentheoretischen Operatoren: $A_1\vee A_2 \leftrightarrow A_1\cup
  A_2, \neg A\leftrightarrow A^c$\footnote{\highl{Komplement}}
\item $\CF\neq\emptyset$
\item $\forall A_1,A_2\in\CF (A_1\cup A_2\in\CF\wedge A_1\cap A_2\in\CF)$
\item $\forall A_1,A_2,\ldots\in\CF (\bigcup_{k=1}^\infty A_k\in\CF)$
\item $\forall A \in\CF (A^c\in\CF)$
\end{itemize}


\subsection{Eigenschaften von $P(A)$}

\begin{itemize}
\item Beispiel: $n$-mal Würfeln: $h_n(A)=\frac{\text{Anzahl des
      Eintretens von } A}{n}\xrightarrow{n\rightarrow\infty} =:P(A)$
\item $0\leq h_n(A)\leq 1\Rightarrow 0\leq P(A)\leq 1$
\item $h_n(\Omega)=1=P(\Omega)$
\item $A\cap B=\emptyset\Rightarrow h_n(A\cup B)=h_n(A)+h_n(B)
  \Rightarrow P(A\cup B)=P(A)+P(B)$
\item $A_1, A_2,\ldots\in\CF\wedge A_j\cap A_K=\emptyset (k\neq j)
  \Rightarrow P(\bigcup_{k=1}^\infty A_k)=\sum_{k=1}^\infty P(A_k)$
\end{itemize}

\section{$\sigma$-Algebra}

Im folgenden gilt immer:
\begin{itemize}
\item $\Omega\neq\emptyset$
\item $\FP (\Omega)$ - Potenzmenge
\item $\CF\subseteq\FP(\Omega)$
\end{itemize}

\begin{definition}
  $\CF$ heißt \highl{Algebra}, wenn gilt:
  \begin{enumerate}[(1)]
  \item $\emptyset\in\CF$
  \item $A\in\CF\Rightarrow A^c\in\CF$
  \item $A,B\in\CF\Rightarrow A\cup B\in\CF$
  \end{enumerate}
$\CF$ heißt \highl{$\sigma$-Algebra}, wenn die
Punkte (1) und (2) gelten und zusätzlich\\
 (3') $A_1, A_2, \ldots\in\CF
\Rightarrow \bigcup A_k\in\CF$ gilt.
\end{definition}
 
\begin{satz}
\label{satz:s-alg}
  Jede $\sigma$-Algebra ist eine Algebra.
\end{satz}
\begin{proof}
  Zum Beweis genügt es zu zeigen, dass eine beliebige
  $\sigma$-Algebra $\CF$ den Punkt (3) der Definition erfüllt. Es
  gelten die Eigenschaften (1), (2) und (3'). Man setzt nun $A_1=A,
  A_2=B, A_n=\emptyset (n=3,4,\ldots)$. Dann gilt $A_1, A_2,
  \ldots\in\CF \Rightarrow \bigcup A_k$
\end{proof}

\begin{remark}
  \begin{itemize}
  \item Die Umkehrung von \autoref{satz:s-alg} ist in der Regel
    falsch, d.\,h. es gibt Algebren, die keine $\sigma$-Algebra
    sind. Das klassiche Beispiel hierfür ist: 
    \[\Omega=\N; \CF:=\{A\subseteq\N| A\vee A^c\text{ endlich}\}\]
    Eigenschaften:
    \begin{enumerate}[(1)]
    \item klar
    \item Denn sei $A\in\CF$. Dann ist zu zeigen, dass entweder $A^c\in\CF$
      oder $(A^c)^c$ endlich ist. Falls $A^c$ endlich, ist nichts weiter
      zu zeigen. Wenn $A$ endlich ist, dann ist nichts zu $A^c$
      ausgesagt. Allerdings ist $(A^c)^c=A$. Somit gilt auch Eigenschaft 2.
    \item Für $A,B\in\CF$ ist zu zeigen, dass $(A\cup B)$ endlich
      bzw. $(A\cup B)^c\in\CF$ endlich sind.
      \begin{enumerate}[1. F{a}ll]
      \item $A,B$ endlich $\Rightarrow A\cup B$ endlich
      \item $A^c, B^c$ endlich $\Rightarrow A^c\cap B^c\subseteq A^c$
        endlich $\Rightarrow (A\cap B)^c$ endlich $\Rightarrow A\cup
        B\in\CF$
      \item $A,B^c$ endlich $\Rightarrow A^c\cap B^c\subseteq B^c$
        endlich $\Rightarrow (A\cap B)^c$ endlich $\Rightarrow A\cap B
        \in\CF$
      \item $A^c,B$ endlich $\Rightarrow A^c\cap B^c\subseteq A^c$
        endlich $\Rightarrow (A\cap B)^c$ endlich $\Rightarrow A\cap B
        \in\CF$
      \end{enumerate}
    \end{enumerate}
  \item Jede endliche Algebra ist eine $\sigma$-Algebra.
  \item Jede $\sigma$-Algebra ist bezüglich der (symmetrischen)
    Differenz abgeschlossen.
  \end{itemize}
\end{remark}

\begin{satz}
  Sei $\CF$ eine Algebra. Dann ist $\Omega\in\CF$ und für alle
  Folgen $A_1,\ldots,A_n\in\CF (n\in\N)$ gelten:
  \begin{enumerate}[a)]
  \item $\bigcup_{k=1}^n A_k\in\CF$
  \item $\bigcap_{k=1}^n A_k\in\CF$
  \end{enumerate}
\end{satz}
\begin{proof}
  \begin{itemize}
  \item $\Omega$ ist ein Element von $\CF$. Denn nach (1) gilt
    $\emptyset\in\CF$ und nach (2) folgt $\emptyset^c=\Omega\in\CF$.
  \item[zu a)] Nach Eigenschaft (3) gilt der Punkt a) für $n=2$.\\
    Annahme: $\bigcup_{k=1}^n A_k\in\CF
    (A_1,\ldots,A_n\in\CF)$\\
    Beweis: $A_1,\ldots,A_{n+1}\in\CF (\bigcup_{k=1}^{n+1}
    A_k=\bigcup_{k=1}^n A_k\cup A_{n+1})\Rightarrow
    \bigcup_{k=1}^{n+1} A_{n+1}\in\CF$
  \item[zu b)] Nach Eigenschaft (3) kann folgender Schluss gezogen
    werden $A_1,\ldots,A_n\in\CF\Rightarrow A_1^c,\ldots,A_n^c\in\CF
    \Rightarrow \bigcup_{k=1}^n A_k^c\in\CF$. Nach den deMorganschen
    Regeln bedeutet dies: $\left(\bigcap_{k=1}^n A_k\right)^c
    \Rightarrow \bigcap_{k=1}^n A^c_k\in\CF$
  \end{itemize}
\end{proof}

\begin{satz}
\label{satz:s-alg-abg-123}
  Sei $\CF$ eine $\sigma$-Algebra. Dann gilt $\bigcap_{k=1}^\infty
  A_k\in\CF$ für $A_1,A_2,\ldots\in\CF$ (Beweis wie oben mit $n=\infty$)
\end{satz}

\begin{remark}
  Eine $\sigma$-Algebra ist \emph{nicht} gegenüber
  überabzählbarer Vereinigung abgeschlossen.
\end{remark}

\begin{beispiel}
  \begin{enumerate}
  \item Die "`kleinste"' Algebra/$\sigma$-Algebra ist
    $\CF=\{\emptyset, \Omega\}$.
  \item Die "`größte"' $\sigma$-Algebra ist $\CF=\FP(\Omega)$.
  \item Sei $\emptyset\subset A\subset\Omega, \CF=\{\emptyset, \Omega,
    A, A^c\}$.
  \end{enumerate}
\end{beispiel}

\begin{satz}
\label{satz:s-erzeug}
  Sei $(\CF_i)_{i\in I}$ eine Familie von $\sigma$-Algebren. Dann ist
  $\bigcap_{i\in I} \CF_i$ eine $\sigma$-Algebra.
\end{satz}
\begin{proof}
  Sei $\bigcap_{i\in I}\CF_i\subseteq\FP(\Omega)$.
  \begin{itemize}
  \item[zu (1)] für $\CF_i$ gilt, dass $\emptyset\in\CF_i
    \Rightarrow \emptyset\in\bigcap_{i\in I} \CF_i$
  \item[zu (2)] Sei $A\in\bigcap_{i\in I}\CF_i$. Es ist zu zeigen,
    dass $A^c\in\bigcap_{i\in I}\in\CF_i$:\\
    $A\in\CF_i\Rightarrow A^c\in\CF_i\Rightarrow A^c\in\bigcap_{i\in
      I} \CF_i$
  \item[zu (3')] Sei $A_1,A_2,\ldots\in\bigcap_{i\in I}\CF_i \Rightarrow
    A_k\in\CF_i (k=1,2,\ldots) \Rightarrow \bigcup_{k=1}^\infty A_k
    \in\CF_i \Rightarrow \bigcup_{k=1}^\infty A_k\in\bigcap_{i\in I} \CF_i$
  \end{itemize}
\end{proof}

\begin{definition}
  Sei $\FE\subseteq\FP(\Omega)$ und $\CF$ eine $\sigma$-Algebra.
  \[\sigma(\FE):=\bigcap_{\FE\subseteq\CF} \CF\]
  heisst die von $\FE$ erzeugte
  $\sigma$-Algebra. Gilt
  für eine $\sigma$-Algebra $\FB$ die Beziehung
  \[\FB=\sigma(\FE)\]
  so heißt $\FE$ \highl{Erzeugendensystem} von $\FB$.
\end{definition}

\begin{remark}
  Aus \autoref{satz:s-erzeug} folgt $\sigma(\FE)$ ist eine
  $\sigma$-Algebra. Gleiche Konstruktion ist auch für alle
  Algebren möglich. 
\end{remark}

\begin{beispiel}
  \begin{align*}
    \sigma(\{\emptyset\})&=\{\emptyset,\Omega\} &
    \sigma(\{A\})&=\{\emptyset,\Omega,A,A^c\} & (\emptyset\subset
    A\subset \Omega)
  \end{align*}
\end{beispiel}

\begin{definition}
  Sei $\Omega=M$ ein metrischer Raum. Wir setzen $\FE=\{A\subseteq M\colon
  A \text{ offen}\}$. Dann heißt $\sigma(\FE)$ die $\sigma$-Algebra der
  \highl[Borelmenge]{Borelmengen} aus $M$, $\FB_n$ ist die
  $\sigma$-Algebra der Borelmengen in $\R^n$ und $\FB$ ist die
  $\sigma$-Algebra der Borelmengen in $\R$.
\end{definition}

\begin{satz}
\label{satz:b-erzeugen}
  Jedes der folgenden Mengensysteme ist ein Erzeugendensystem für
  $\FB$ (Es gilt: $\FE_1,\ldots,\FE_8$ sind abzählbar.):
  \begin{align*}
    \FE_1&:=\{(\alpha,\beta)|\alpha,\beta\in\R \text{ und rational}\}\\
    \FE_2&:=\{[\alpha,\beta)|\alpha,\beta\in\R \text{ und rational}\}\\
    \FE_3&:=\{(\alpha,\beta]|\alpha,\beta\in\R \text{ und rational}\}\\
    \FE_4&:=\{[\alpha,\beta]|\alpha,\beta\in\R \text{ und rational}\}\\
    \FE_5&:=\{(-\infty,\beta)|\beta\in\R \text{ und rational}\}\\
    \FE_6&:=\{(-\infty,\beta]|\beta\in\R \text{ und rational}\}\\
    \FE_7&:=\{(\alpha,\infty)|\alpha\in\R \text{ und rational}\}\\
    \FE_8&:=\{[\alpha,\infty)|\alpha\in\R \text{ und rational}\}
  \end{align*}
\end{satz}
\begin{proof}
  \begin{itemize}
  \item[zu $\FE_1$] Sei $\FE=\{A\subseteq\R| A \text{
      offen}\}\Rightarrow \FB=\sigma(\FE)$. Nach der Definition gilt
    $\FE_1\subseteq\FE\Rightarrow\sigma(\FE_1)\subseteq\sigma(\FE)=\FB$. Damit
    ist noch zu zeigen, dass $\FB\subseteq\sigma(\FE_1)$. Sei
    $A\in\FE$. Setzen $K_r(x)=(x-r,x+r)\Rightarrow A=
    \bigcup_{K_r(x)\subseteq A}K_r(x)$\\
    $K_r(x)\in\FE_1\subseteq \sigma(\FE_1)\Rightarrow A\in\sigma
    (\FE_1)\Rightarrow\FE\subseteq\sigma(\FE_1)\Rightarrow \sigma(\FE)
    =\FB\subseteq\sigma(\sigma(\FE_1))=\sigma(\FE_1)$
  \item[zu $\FE_6$] $\forall\beta ((-\infty,\beta]^c=(\beta,\infty)
    \in\FE\subseteq\sigma(\FE))\Rightarrow\FE_6=(-\infty,\beta]
    \subseteq\sigma(\FE)\Rightarrow\sigma(\FE_6)\subseteq \sigma(\FE)$
    Beweis für $\sigma(\FE)\subseteq\sigma(\FE_6)$ fehlt noch.
  \end{itemize}
\end{proof}


\section{Maße}

Im folgenden gilt immer: $\Omega\neq\emptyset$ und $\CF$ ist eine
$\sigma$-Algebra über $\Omega$.

\begin{definition}
  \begin{itemize}
  \item Das Tupel $[\Omega,\CF]$ heißt \highl[Raum!messbarer]{messbarer Raum}.
  \item Eine nichtnegative Funktion $\mu$ auf
    $\CF$\footnote{$\mu:\CF\rightarrow [0,+\infty]$} heißt
    \highl{Maß} auf $[\Omega,\CF]$, wenn gilt 
    \begin{inparaenum}[1.]
    \item $\mu(\emptyset)=0$
    \item Für paarweise disjunkte Folgen $(A_k)_{k=1}^\infty$ aus
      $\CF$ gilt:
      \[\mu\left(\bigcup_{k=1}^\infty A_k\right)=\sum_{k=1}^\infty \mu(A_k)\]
    \end{inparaenum}
  \item Das Tripel $[\Omega,\CF,\mu]$ heißt \highl{Maßraum}.
  \end{itemize}
\end{definition}

\begin{beispiel}
  \begin{enumerate}[(1)]
  \item \highl{Nullmaß} $\CO: \CO(A)=0$ für $a\in\CF$
  \item \highl{Dirac-Maß}: Sei
    $\omega\in\Omega$. Setzen $\delta_\omega(A)=
    \begin{cases}
      1 & \omega\in A\\
      0 & \omega\notin A
    \end{cases}$
  \item Sei $\Omega$ abzählbar und $\CF=\FP(\Omega)$. Dann ist das
    \highl{Zählmaß} $\mu(A):=\#(A)$ die Anzahl der
    Teilmengen $A$ aus $\Omega$.
  \end{enumerate}
\end{beispiel}

\begin{satz}
\label{satz:massfamilie-131}
  Sei $(\mu_i)_{i\in I}$ eine abzählbare Familie von Maßen auf
  $[\Omega,\CF]$ und $(\alpha_i)_{i\in I}$ eine Familie nichtnegativer
  reller Zahlen. Durch
  \[\mu(A):=\sum_{i\in I} \alpha_i \mu_i(A)\quad (A\in\CF)\]
  ist ein Maß auf $[\Omega,\CF]$ definiert.\footnote{Schreibweise:
  $\mu=\sum_{i\in I}\alpha_i\mu_i$}
\end{satz}
\begin{proof}
  Diese Funktion ist nichtnegativ, da alle $\alpha_i$ nichtnegativ
  sind und die $\mu_i(A)$ per Definition ebenfalls positiv sind. Die
  Summenbildung ändert nichts.
  \begin{itemize}
  \item[zu 1)] $\mu(\emptyset)=\sum_{k=1}^\infty \alpha_i\mu_i
    (\emptyset)=0$
  \item[zu 2)] Seien $(A_k)_{k=1}^\infty$ paarweise disjunkt aus
    $\CF$.\\
    $\mu(\bigcup_{k=1}^\infty A_k)=\sum_{i\in I}\alpha_i
    \underbrace{\mu_i \left(\bigcup_{k=1}^\infty A_k\right)}_{\sum_{k=1}^\infty
      \mu_i (A_k)}= \sum_{i\in I} \alpha\sum_{k=1}^\infty \mu_i (A_k)$
    Nach dem großen Umordnungssatz folgt: $\sum_{k=1}^\infty
    \underbrace{\sum_{i\in I} \alpha_i \mu_i(A_k)}_{\mu(A_k)} =
    \sum_{k=1}^\infty \mu(A_k)$
  \end{itemize}
\end{proof}

\begin{satz}
\label{satz:folgenmass}
  Sei $\mu$ ein Maß auf $[\Omega,\CF]$. Für alle Folgen
  $A_1,\ldots, A_n\in\CF$, die paarweise disjunkt sind, gilt:
  \[\mu\left(\bigcup_{k=1}^n A_k\right)=\sum_{k=1}^n \mu(A_k)\]
\end{satz}
\begin{proof}
  Wir setzen $B_k:=
  \begin{cases}
    A_k & k=1,\ldots,n\\
    \emptyset & k>n
  \end{cases}$. Damit ist folgendes bekannt:
  \begin{enumerate}
  \item $\forall k\in \N (B_k\in\CF)$
  \item $\bigcup_{k=1}^\infty B_k=\bigcup_{k=1}^n A_k$
  \item $A_i\cap A_j=A_k\cap B_k=\emptyset \, (i\neq j)$ \todo{stimmen die
  Indizes?}
  \item $\mu(A_k)=\mu(B_k)$ für $k=1,\ldots,n$
  \item $\mu(B_k)=0 \, (k>n)$
  \end{enumerate}
  Damit folgt nun: $\mu(\bigcup_{k=1}^n A_k)\overset{(2)}{=} \mu
  (\bigcup_{k=1}^\infty B_k)\overset{(1),(3)}{=} \sum_{k=1}^\infty
  \mu(B_k) \overset{(5)}{=} \sum_{k=1}^n \mu(B_k) \overset{(4)}{=}
  \sum_{k=1}^n \mu(A_k)$
\end{proof}

\begin{remark}
  \begin{enumerate}
  \item Die Eigenschaften aus \autoref{satz:folgenmass} heißt
    \highl[Additivität!endliche]{endliche Additivität}.
  \item Die zweite Eigenschaft in der Definition des Maßes heißt
    \highl{$\sigma$-Additivität}.
  \item \autoref{satz:folgenmass} impliziert, dass aus der
    $\sigma$-Additivität die endliche Additivität folgt. Die
    Umkehrung ist jedoch falsch.
  \item \highl{Subadditivität}: Für alle
    $A_1,\ldots, A_n\in\CF$ gilt: $\mu(\bigcup_{k=1}^n A_k)\leq
    \sum_{k=1}^n \mu(A_k)$
  \end{enumerate}
\end{remark}

\begin{satz}[elementare Eigenschaften]
\label{satz:elemEigen}
  Sei $[\Omega,\CF,\mu]$ ein Maßraum und $A,B\in\CF$. Dann gilt:
  \begin{enumerate}[(a)]
  \item $\mu(A \cup B)\leq\mu(A)+\mu(B)$
  \item $A\subseteq B\Rightarrow \mu(A)\leq\mu(B)$
  \item $A\subseteq B, \mu$ endlich $\Rightarrow \mu(B\setminus A) =
    \mu(B) -\mu(A)$
  \end{enumerate}
\end{satz}
\begin{proof}
  \begin{itemize}
  \item[zu (b) und (c)] $A\subseteq B\Rightarrow B=A\cup(B\setminus
    A) \Rightarrow\mu(B)=\mu(a\cup(B\setminus
    A)) \overset{\text{\autoref{satz:folgenmass}}}{=} \mu(A)+\mu(B\setminus A)$
    Wir haben $\mu(B\setminus A)\geq 0\Rightarrow\mu(A)\leq\mu(B)$
    Durch Umstellen erhält man $\mu(B\setminus A)=\mu(B) -
    \mu(A)$. Hierzu muss $\mu(A)<+\infty$ sein.
  \item[ zu (a)] $A\cup B=A\cup (B\setminus A)\Rightarrow \mu(A\cup
    B)= \mu(A)\cup\mu(B\setminus A)= \mu(A)+\mu(B\setminus A)$\\
    Es gilt: $B\setminus A\subseteq B\overset{(b)}{\Rightarrow}
    \mu(B\setminus A)\leq\mu(B)\Rightarrow\mu(A\cup B)\leq
    \mu(A) +\mu(B)$
  \end{itemize}
\end{proof}

\begin{satz}
\label{satz:stetigkeit}
  Seien $\mu$ ein endliches Maß ($\mu(\Omega)<+\infty$) und
  $(A_n)_{n=1}^\infty$ eine Folge aus $\CF$. Dann gelten:
  \begin{enumerate}[(a)]
  \item (Stetigkeit von unten): Ist $(A_n)_{n=1}^\infty$ monoton
    wachsend, so gilt:
    \[\mu\left(\bigcup_{n=1}^\infty
      A_n\right)=\lim_{n\rightarrow\infty} \mu(A_n)\]
  \item (Stetigkeit von oben):  Ist $(A_n)_{n=1}^\infty$ monoton
    fallend, so gilt:
    \[\mu\left(\bigcap_{n=1}^\infty
      A_n\right)=\lim_{n\rightarrow\infty} \mu(A_n)\]
  \end{enumerate}
\end{satz}
\begin{proof}
  \begin{itemize}
  \item[zu a)] Setzen $B_1:=A_1, B_{n+1}=A_{n+1}\setminus A_n$. Damit
    gelten folgende Tatsachen:
    \begin{enumerate}
    \item Alle $B_k$ sind paarweise disjunkt.
    \item $B_k\in\CF$
    \item $\bigcup_{k=1}^\infty A_k=\bigcup_{k=1}^\infty B_k$
    \item $\bigcup_{k=1}^\infty B_k=A_n$
    \end{enumerate}
    Somit folgt: $\mu(\bigcup_{k=1}^\infty A_k)\overset{3.}{=}
    \mu(\bigcup_{k=1}^\infty B_k)\overset{1,2}{=} \sum_{k=1}^\infty
    \mu(B_k) =\lim_{n\rightarrow\infty} \sum_{k=1}^n \mu(B_k)
    \overset{1}{=} \lim_{n\rightarrow\infty} \mu(\bigcup_{k=1}^n
    B_k) \overset{4.}{=}\lim_{n\rightarrow\infty} \mu(A_n)$
  \item[zu b)] $A_n\supseteq A_{n+1}\Rightarrow A_n^c\subseteq
    A_{n+1}^c$, d.\,h. die Folge der Komplemente ist monoton
    wachsend. Nach (a) gilt nun: $\mu(\bigcup_{n=1}^\infty A_n^c) =
    \lim_{n\rightarrow\infty} \mu(A_n^c)$\\
    $\mu(\bigcup_{n=1}^\infty A_n^c)=\mu((\bigcap_{n=1}^\infty
    A_n)^c)= \mu(\Omega\setminus(\bigcap_{n=1}^\infty A_n))
     \overset{\text{\autoref{satz:elemEigen} c}}{=} \mu(\Omega)-\mu(\bigcap A_n)$\\
    $\mu(A_n^c)=\mu(\Omega\setminus A_n)=\mu(\Omega)-\mu(A_n)
    \Rightarrow \lim_{n\rightarrow\infty} \mu(A_n)$
  \end{itemize}
\end{proof}

\section{Produkte von Maßräumen}

Im folgenden gilt immer: $[\Omega_1,\CF_1,\mu_1],\ldots,
[\Omega_n,\CF_n,\mu_n]$ eine Folge von Maßräumen. Wir betrachten
direkte Produkte von Mengen: $\times_{i=1}^n A_i:=\{[w_1,\ldots, w_n]|
w_i\in A_i (i=1,\ldots,n)\}$. Das Ziel ist es, einen Maßraum mit
der Grundmenge $\Omega=\times_{i=1}^n \Omega_i$ zu bauen.

\begin{definition}
  \begin{itemize}
  \item Die Menge \[\otimes_{i=1}^n \CF_i:=\sigma(\{\times_{i=1}^n
    A_i|A_i\in \CF_i (i=1,\ldots,n)\})\]
    heißt Produkt der $\sigma$-Algebren $\CF_1,\ldots,\CF_n$.
  \item Die Menge \[\times_{i=1}^n [\Omega_i,\CF_i]:=[\times_{i=1}^n
    \Omega_i, \otimes_{i=1}^n \CF_i]\]
    heißt Produkt der messbaren Räume $[\Omega_1,\CF_1,\ldots,
    \Omega_n,\CF_n]$.
  \end{itemize}
\end{definition}

\begin{remark}
  \begin{enumerate}
  \item $\otimes \CF_i$ ist eine $\sigma$-Algebra auf $\times_{i=1}^n
    \Omega_i$.
  \item $n=2\rightarrow$ Schreibweise: $\Omega_1\times\Omega_2,
    \CF_1\otimes\CF_2$ 
  \end{enumerate}
\end{remark}

\begin{beispiel}
  \begin{enumerate}
  \item $\CF_i=\{\emptyset,\Omega_i\}\Rightarrow\otimes_{i=1}^n
    \CF_i=\{\emptyset, \times_{i=1}^n \Omega_i\}$
  \item Sei $\Omega_1$ bis $\Omega_n$ abzählbar:
    $\CF_i=\FP(\Omega_i)\Rightarrow\otimes_{i=1}^n \CF_i=\FP
    (\times_{i=1}^n \Omega_i)$. Denn wegen der Abzählbarkeit der
    $\Omega_i$ gilt, dass auch $\times_{i=1}^n \Omega_i$ abzählbar
    sind. Diese sind eine Obermenge von $A=\bigcup_{\omega\in A}
    \{\omega\}$ und die Vereinigung ist wieder abzählbar. Nun
    genügt es zu zeigen, dass
    $\{\underbrace{[\omega_1,\ldots,\omega_n]}_{\times_{i=1}^n
      \{\omega_i\}}\} \in
    \otimes_{i=1}^n \CF_i (\omega_i\in\Omega, i=1,\ldots,n)$. Der Rest
    ist klar, denn $\{\omega_i\}\in\CF_i=\FP(\Omega_i)$
  \item Lässt man oben den Zusatz "`abzählbar"' weg, gibt es
    Probleme.
  \end{enumerate}
\end{beispiel}

\begin{definition}
  Ein Maß $\mu$ auf $\times_{i=1}^n [\Omega_i,\CF_i]$ heißt
  \highl{Produkt der Maße} $\mu_1,\ldots,
  \mu_n$\footnote{$\mu=\times_{i=1}^n \mu_i$}, wenn gilt
  $\mu(\times_{i=1}^n A_i) =\prod_{i=1}^n \mu_i(A_i)$.
\end{definition}

\begin{satz*}
  Für endliche Maße $\mu_1,\ldots,\mu_n$ existiert das
  Produktmaß und ist eindeutig bestimmt.
\end{satz*}

\begin{remark}
  \begin{enumerate}
  \item Die Forderung, dass die $\mu_1,\ldots,\mu_n$ endlich sein
    sollen, kann man abschwächen (siehe hierzu auch Lebesguesches
    Maß).
  \item Der Fall $n=+\infty$ ist für Wahrscheinlichkeitsmaße
    interessant. 
  \end{enumerate}
\end{remark}

\begin{definition}
  Sei $\FB_n$ die $\sigma$-Algebra der Borelmengen auf $\R^n$. Ein
  Maß $\ell_n$ auf $[\R^n,\FB_n]$ heißt $n$-dimensionales
  \highl[Maß!Lebeguesches]{Lebesguesches Maß}, wenn gilt:
\[\ell_n\left(\times_{i=1}^n [a_i,b_i)\right)=\prod_{i=1}^n
(b_i-a_i)\quad (a_1<b_i, i=1,\ldots,n)\]
\end{definition}

\begin{satz*}
  Das Lebeguesche Maß $\ell_n$ existiert und ist eindeutig
  bestimmt. Weiterhin gilt:
  \[[\R^n,\FB_n,\ell_n]=\times_{i=1}^n [\R,\FB,\ell]=
  [\R,\FB,\ell]^{nx} =[\R^n,\FB_n,\ell^{nx}]\]
\end{satz*}

\begin{remark}
  Die Bemerkungen sind gleichzeitig Beweisideen für den oben
  angegebenen Satz.
  \begin{itemize}
  \item $\FB_n=\otimes_{i=1}^n \FB=\FB^{n\otimes}$. Weiterhin ist
    klar, dass $\R\supseteq A_1,\ldots,A_n$ offen. Damit ist auch
    $\times_{i=1}^n A_i$ offen.
  \item Wir haben $\ell_n(\times_{i=1}^n A_i)=\ell^{n\otimes}
    (\times_{i=1}^n A_i)$ für Mengen $A_i=[a_i,b_i)$. Ferner wissen
    wir, dass diese Mengen $\FB$ erzeugen (siehe auch 
    \autoref{satz:b-erzeugen}). Die allgemeine Theorie führt zur
    Behauptung des oben stehenden Satzes.
  \end{itemize}
\end{remark}

\begin{satz}
  \label{satz:ell}
  Sei $x\in\R$. Dann gilt: $\ell(\{x\})=0$.
\end{satz}
\begin{proof}
  Wir haben $\ell([a,b))=b-a\,(a<b)$. Nun betrachten wir die
  Mengenfolge $B_k:=[x,x+\frac{1}{n})\Rightarrow B_k\supseteq B_{k+1}
  \Rightarrow \bigcap_{k=1}^\infty B_k=\{x\}$. Wegen der Stetigkeit
  von oben (siehe \autoref{satz:stetigkeit}) folgt nun: $\ell(\{x\})=
  \ell(\bigcap_{k=1}^\infty B_k)=\lim_{k\rightarrow\infty} \ell(B_k) =
  \lim_{k\rightarrow\infty}
  \underbrace{\ell([x,x+\frac{1}{k}))}_{\frac{1}{k}}
  =\lim_{k\rightarrow\infty} \frac{1}{k}=0$
\end{proof}

\begin{satz}[Folgerung aus \autoref{satz:ell}]
  Seien $a<b\rightarrow \ell([a,b])=\ell((a,b)) = \ell((a,b]) =
  \ell([a,b)) = b-a$
\end{satz}
\begin{proof}
  \begin{itemize}
  \item Es gilt $[a,b]=[a,b)\cup\{b\}$. Die Vereinigung beider Mengen ist
    leer ($[a,b)\cap\{b\}=\emptyset$). Damit folgt: $\ell([a,b])=
    \ell([a,b)\cup \{b\})= \ell([a+b))+\ell(\{b\})=b-a+0$
  \item $(a,b]=[a,b]\setminus \{a\}$\\
    Es ist klar, dass $\{a\}$ in $[a,b]$ liegt. Damit folgt nach Punkt
    c von \autoref{satz:elemEigen}: $\ell((a-b]) = \ell([a,b])-
    \ell(\{a\})= (b-a) - 0 = b-a$
  \item $(a-b)=(a,b]\setminus \{b\}$\\
    Wie schon oben ist klar, dass $\{b\}$ in $(a,b]$ liegt. Nach
    gleichem Schluss folgt auch hier: $\ell((a-b)) = \ell((a,b])-
    \ell(\{b\})= (b-a) - 0 = b-a$
  \end{itemize}
\end{proof}

\begin{satz}
  Für alle $A\subseteq\R^n$ mit $A$ abzählbar gilt:
  \[\ell^{(n)} (A)=0\]
\end{satz}
\begin{proof}
  $A=\bigcup_{x\in A} \{x\}=\bigcup_{[x_1,\ldots,x_n]\in A}
  \{[x_1,\ldots, x_n]\}$\\
  Da $A$ abzählbar ist, folgt $\ell^{(n)}(A)=\sum_{x\in A}
  \ell^{(n)} \{[x]\}$. Nunmehr ist noch zu zeigen, dass $\ell^{(n)}
  (\{[x_1,\ldots,x_n]\}) =0$. Dies ist gleich: $\ell^{(n)}
  \times_{i=1}^n \{x_i\}= \prod_{i=1}^n \ell (\{x_i\})=0$
\end{proof}


\section{Wahrscheinlichkeitsmaße und Verteilungsfunktion}

\begin{definition}
  Ein Maß $P$ auf einem meßbaren Raum $[\Omega,\CF]$ heißt
  \highl{Wahrscheinlichkeitsmaß}, wenn gilt: \[P(\Omega)=1\]
  In diesem Fall heißt $\ofp$
  \highl{Wahrscheinlichkeitsraum}.
\end{definition}

\begin{satz}
  Sei $P$ ein Wahrscheinlichkeitsmaß auf $[\Omega,\CF]$. Dann
  gelten:
  \begin{enumerate}[(a)]
  \item $0\leq P(B)\leq 1\,(b\in\CF)$
  \item $P(B^c)=1-P(B)\,(B\in\CF)$
  \end{enumerate}
\end{satz}
\begin{proof}
  \begin{enumerate}[(a)]
  \item klar, da $P(\emptyset)\leq P(B)\leq P(\Omega)\Rightarrow 0\leq
    P(B)\leq 1$
  \item $B^c=\Omega\setminus B, B\subseteq\Omega\Rightarrow P(B^c)=
    P(\Omega)-P(B)\Rightarrow P(B^c)=1-P(B)$ 
  \end{enumerate}
\end{proof}

\begin{definition}
  Sei $P$ ein Wahrscheinlichkeitsmaß auf $[\R^n,\FB_n]\, (n\geq
  1)$. Eine Abbildung $F:\R^n\rightarrow[0,1]$ heißt
  \highl{Verteilungsfunktion} von (zu) $P$,
  wenn gilt:
  \[F(x_1,\ldots,x_n)=P(\times_{i=1}^n (-\infty,x_i))\]
\end{definition}

\begin{satz*}
  Ein Wahrscheinlichkeitsmaß ist durch seine Verteilungsfunktion
  eindeutig bestimmt.
\end{satz*}

\begin{satz}
\label{satz:eigensch-vert-152}
  Eine Verteilungsfunktion $F$ eines Wahrscheinlichkeitsmaßes $P$
  auf $[R,\FB]$ hat folgende Eigenschaften:
  \begin{enumerate}[(F1)]
  \item $F(x)\leq F(y)\quad (x\leq y)$ (monoton nicht fallend bzw. wachsend)
  \item $\lim F(x_n)=F(x)\,(x\in\R, x_n\uparrow x)$ (linksseitig stetig)
  \item $\lim_{x\rightarrow+\infty} F(x)=1$
  \item $\lim_{x\rightarrow-\infty} F(x)=0$
  \end{enumerate}
\end{satz}
\begin{proof}
  \begin{enumerate}[(F1)]
  \item Sei $F(x):=P((-\infty,x))$ mit $x\in\R$. Dann gilt $(-\infty,
    x)\subseteq (-\infty, y)$, da $x\leq y$. Nach 
    \autoref{satz:elemEigen} Punkt b hat man $P((-\infty, x))\leq
    P((-\infty, y))\Rightarrow F(x)\leq F(y)$
  \item Seien $x\in\R$ und $x_n\leq x_{n+1}, \lim x_n=x$. Damit folgt:
    $(-\infty, x_n)\subseteq (-\infty, x_{n+1})$ und $(-\infty, x)=
    \bigcup_{n=1}^\infty (-\infty, x_n)$. Unter Zuhilfenahme von 
    \autoref{satz:stetigkeit} a erhält man $F(x)=P((-\infty,
    x))=P(\bigcup_{n=1}^\infty(-\infty, x_n))=\lim P((-\infty, x_n))=
    \lim F(x_n)$
  \item Sei $x_n \uparrow +\infty$\footnote{d.\,h. $x_n\subseteq
      x_{n+1}, x_n\xrightarrow{n\rightarrow\infty}+\infty$}. Damit
    gilt wieder die schon oben angeführte Teilmengenbeziehung und es
    folgt: $\bigcup_{n=1}^\infty (-\infty, x_n)=\R$. Wie bereits oben
    erhält man nun durch den \autoref{satz:stetigkeit} a, dass
    $1=P(\R)=P(\bigcup_{n=1}^\infty (-\infty, x_n))=\lim P((-\infty,
    x_n))$
  \item Sei nun $x_n\downarrow -\infty\Rightarrow (-\infty,
    x_n)\supseteq (-\infty, x_{n+1}\Rightarrow\bigcap_{n\in\N}
    (-\infty, x_n)=\emptyset$. Damit ist nun $P(\emptyset)=0=\lim
    F(x_n)$
  \end{enumerate}
\end{proof}

\begin{satz*}
  Sei $F\colon\R\rightarrow[0,1]$ mit den Eigenschaften (F1) bis (F4). Dann
  existiert genau ein Wahrscheinlichkeitsmaß $P$ auf $[\R^n, \FB_n]$
  mit $F(x)=P((-\infty, x))$. Dies bedeutet, dass $F$ die
  Verteilungsfunktion von $P$ ist.
\end{satz*}

\begin{remark}
  \begin{enumerate}
  \item Sei $F$ die Verteilungsfunktion zu einem
    Wahrscheinlichkeitsmaß $P$ auf $[\R^n,\FB_n]$. Dann gelten
    folgende Eigenschaften:
    \begin{enumerate}[($\hat{F}$1)]
    \item $F(x_1,\ldots,x_n)\leq F(y_1,\ldots,y_n)\, (x_k\leq y_k,
      k=1,\ldots, n)$
    \item $x_k^{(m)}\uparrow x_k\Rightarrow \lim_{m\rightarrow\infty}
      F(x_1^{(m)}, \ldots, x_n^{(m)})=F(x_1,\ldots,x_m)$
    \item $x_k^{(m)}\uparrow+\infty\Rightarrow
      \lim_{m\rightarrow\infty} F(x_1^{(m)},\ldots,x_n^{(m)})=1$
    \item $x_k^{(m)}\uparrow-\infty\Rightarrow
      \lim_{m\rightarrow-\infty} F(x_1^{(m)},\ldots,x_n^{(m)})=0$
    \end{enumerate}
  \item Sei nun $F:\R^n\rightarrow[0,1]$ mit den Eigenschaften
    ($\hat{F}$1) bis ($\hat{F}$4). Dann muss nicht notwendigerweise
    ein Wahrscheinlichkeitsmaß $P$ auf $[\R^n,\FB_n]$ mit der
    Verteilungsfunktion $P$ existieren. Somit kann der obenstehende
    Satz nicht auf $\R^n$ verallgemeinert werden.
  \end{enumerate}
\end{remark}

\begin{beispiel}
  Sei $\lambda>0$. Wir setzen $F(x):=
  \begin{cases}
    1-e^{-\lambda x} & x>0\\
    0 & x \leq 0
  \end{cases}$. Die Eigenschaften (F1) bis (F4) gelten hier und sind
  auch trivial nachzuweisen. Das Wahrscheinlichkeitsmaß $Ex_\lambda$
  mit der Verteilungsfunktion $F$ heißt
  \highl{Exponentialverteilung} mit dem Parameter $\lambda$.
\end{beispiel}

\begin{definition}
  Sei $A\in\FB_n$ mit $0<\ell^{(n)}(A)\leq +\infty$. Wir setzen:
  \[Gl_A(B):=\frac{\ell^{(n)}(A\cap B)}{\ell^{(n)} (A)}\quad
  (B\in\FB_n)\]
  Dann heißt $Gl_A$ die \highl{Gleichverteilung} zu $A$.
\end{definition}

\begin{remark}
  Die Gleichverteilung ist ein Wahrscheinlichkeitsmaß auf
  $[\R^n,\FB_n]$. Denn es gilt:
  \begin{align*}
    Gl_A(\emptyset)&= \frac{\ell^{(n)}(A\cap \emptyset)}{\ell^{(n)}
      (A)} =0\\
    Gl_A(\R^n)&= \frac{\ell^{(n)}(A\cap \R)}{\ell^{(n)} (A)}=
    \frac{\ell^{(n)}(A)}{\ell^{(n)} (A)} =1\\
    Gl_A\left(\bigcup_k B_K\right)&= \frac{\ell^{(n)}((\bigcup
      B_k)\cap A)}{\ell^{(n)} (A)}= \frac{\ell^{(n)}(\bigcup (B\cap
      A)}{\ell^{(n)} (A)}= \frac{\sum_k\ell^{(n)} (B_k\cap
      A)}{\ell^{(n)} (A)}\\
    &=\qquad \sum_k Gl_A(B_k)
  \end{align*}
\end{remark}

\begin{satz}
  Sei $P$ ein Wahrscheinlichkeitsmaß auf $[\R,\FB]$ für $k=1,\ldots,n$
  mit der Verteilungsfunktion $F_k$ und $P=\times_{k=1}^n P_k$. Dann
  ist $P$ ein Wahrscheinlichkeitsmaß auf $[\R^n,\FB_n]$ und die
  Verteilungsfunktion $F$ zu $P$ hat die Gestalt:
  \[F(x_1,\ldots,x_n)=\prod_{k=1}^n F_k(x_k)\quad (x_1,\ldots, x_n\in\R)\]
\end{satz}
\begin{proof}
  $P(\R^n)=P(\R^{nx})=(\times_{k=1}^n P_k)(\R^{nx})=\prod_{k=1}^n
  \underbrace{P_k(\R)}_{=1}=1$\\
  $F(x_1,\ldots,x_n)=P(\times_{k=1}^n(-\infty, x_k))=\times_{k=1}^n
  P_k (\times_{k=1}^n (\infty,x_k))=\prod_{k=1}^n P_k((-\infty,x_k))=
  \prod_{k=1}^n F_k(x_k)$.
\end{proof}

\begin{satz}
  Sei $F$ eine Verteilungsfunktion eines Wahrscheinlichkeitsmaßes $P$
  auf $[\R,\FB]$. Dann gelten:
  \begin{enumerate}[(1)]
  \item $P([a,b))=F(b)-F(a)$ für $(a<b)$
  \item $P(\{a\})=F(a+0)-F(a)$
  \end{enumerate}
\end{satz}
\begin{proof}
  \begin{enumerate}[zu (1)]
  \item $(-\infty, b)\setminus (-\infty, a)= [a,b)$ und $(-\infty,
    a)\subseteq (-\infty, b)$. Nach \autoref{satz:elemEigen}~c ist
    $P([a,b))= P((-\infty, b))\setminus (-\infty, a))=P((-\infty,
    b))- P((-\infty, a))= F(b)-F(a)$
  \item $\{a\}=\bigcap_{n\geq 1} [a,a+\frac{1}{n})$ und
    $[a,a+\frac{1}{n}) \supseteq [a,a+\frac{1}{n+1})$. Wegen
    \autoref{satz:stetigkeit} ist $P(\{a\})=p(\bigcap_{n\geq 1}
    [a,a+\frac{1}{n})) = \lim
    P([a,a+\frac{1}{n}))=\lim_{n\rightarrow\infty} F(a+\frac{1}{n})
      -F(a) =F(a+0)-F(a)$
  \end{enumerate}
\end{proof}

\begin{satz}
  Seien $I$ abzählbar, $P_i$ ein Wahrscheinlichkeitsmaß auf
  $[\R,\FB]$, $\alpha\geq 0\,(i\in I)$ und $P=\sum_{i\in I} \alpha_i
  P_i$. Dann ist $P$ genau dann ein Wahrscheinlichkeitsmaß, wenn gilt 
  \[\sum_{i\in I} \alpha_i=1\]
  In diesem Fall gilt für die Verteilungsfunktion $F$ von $P$
  \[F(x)=\sum_{i\in I} \alpha_i F_i(x)\quad (x\in\R)\]
  wobei $F_i$ die Verteilungsfunktion zu $P_i$ ist.
\end{satz}
\begin{proof}
  Es ist klar, dass $P$ ein Maß auf $[\R,\FB]$ ist (Folgerung aus
  \autoref{satz:massfamilie-131}). Nun verbleibt zu zeigen, dass
  $P(\R)=1$. Dies ist genau dann der Fall, wenn gilt $\sum \alpha_i
  =1$:
  $P(\R)=\sum_{i\in } \alpha_i P_i(\R)=\sum_{i\in I} \alpha_i=1$. Nach
  der Definition gilt $F(x)=P((-\infty, x))=\sum_{i\in I} \alpha_i
  \underbrace{P_i((\infty, x))}_{=F_i(x)}$
\end{proof}

\begin{beispiel}
  Dies sind im wesentlichen Anwendungen zum oben genannten Satz:
  \begin{itemize}
  \item Für jedes $y\in\R$ ist $\delta_y$ ein Wahrscheinlichkeitsmaß
    auf $[\R,\FB]$.
  \item \highl{Binomialverteilung}: Gegeben
    ist $0\leq p\leq 1, n\in\N$. Hierzu bilden wir 
    \[B_{n,p}:= \sum_{i=0}^n \binom{n}{i} p^i (1-p)^{n-i} \delta_i\]
    Aus dem oben angeführten Satz folgt, dass die Binomialverteilung
    ein Wahrscheinlichkeitsmaß auf $[\R,\FB]$
    ist.\footnote{$(a+b)^n=\sum_{i=0}^n \binom{n}{i} a^i b^{n-i}; a=p,
      b=1-p\Rightarrow (a+b)^n=1)$}
  \item \highl{Poissonverteilung} Gegeben ist
    $\lambda> 0$. Wir definieren:
    \[\pi_\lambda:= \sum_{i=0}^\infty \frac{\lambda^i}{i!}
    e^{-\lambda} \delta_i\]
    Wieder aus dem obigen Satz folgt und aus $\sum_{i=0}^\infty
    \frac{\lambda^i}{i!} =e^\lambda\Rightarrow
    e^\lambda e^{-\lambda}=1$ folgt, dass das ein
    Wahrscheinlichkeitsmaß ist.
  \item \highl[Verteilung!geometrische]{geometrische Verteilung}:
    Gegeben ist $0<q<1$. 
    \[G_q:=\sum_{i=1}^\infty q^{i-1} (1-q) \delta_i\]
    Auch dies ist ein Wahrscheinlichkeitsmaß, da $\lim \sum q^{i-1}=
    \frac{1}{1-q}$.
  \end{itemize}
\end{beispiel}


\chapter{Integrationstheorie}

stetige Funktionen auf einer offenen Menge


\section{Messbare Funktionen}

Im folgenden gilt immer, dass $[\Omega,\CF,\mu]$ ein Maßraum  und
$[X,\FX]$ ein messbarer Raum ist. Wir betrachten $f\colon\Omega\rightarrow
X$ und definieren $\fmineins(A)\colon=\{\omega\in\Omega| f(\omega)\in
A\}\,(A\subseteq X)$. Dies ist nicht zu verwechseln mit inversen
Abbildungen.\footnote{Falls es eine inverse Abbildung gibt, gilt
  $\fmineins (A)=\{\fmineins (x)|x\in A\}$}

\begin{satz}[Elementare Eigenschaften des Urbilds]
  Es gelten
  \begin{enumerate}[a)]
  \item $\fmineins(\emptyset)=\emptyset, \fmineins(X)=\Omega$
  \item $\fmineins(A^c)=(\fmineins(A))^c\quad (\forall A\subseteq X)$
  \item $\fmineins(\bigcup_{i\in I} A_i)= \bigcup_{i\in I} \fmineins
    (A_i)$
  \item $\fmineins(\bigcap_{i\in I} A_i)= \bigcap_{i\in I} \fmineins
    (A_i)$ (Für beide Fälle gilt, dass die $(A_i)_{i\in I}$ Folgen von
    Teilmengen aus $X$ sind.)
  \end{enumerate}
\end{satz}
\begin{proof}
  \begin{enumerate}[zu a)]
  \item $f(\omega)\in\emptyset$ für jedes $\omega\in\Omega\Rightarrow
    \fmineins (\emptyset)=\emptyset$\\
    $f(\omega)\in X$ ist richtig $\forall\omega\in\Omega\Rightarrow
    \fmineins (X)=\Omega$
  \item $\omega\in\fmineins(A^c)\Leftrightarrow f(\omega)\in A^c
    \Leftrightarrow \neg f(\omega)\in A\Leftrightarrow \neg \omega\in
    \fmineins (A)\Leftrightarrow \omega\in (\fmineins(A))^c
    \Rightarrow \fmineins(A^c)=(\fmineins(A))^c$
  \item selbst in Uebung
  \end{enumerate}
\end{proof}

\begin{definition}
  Sei $f:\Omega\rightarrow X$. Die Abbildung $f$ heißt
  \index[messbar]{$(\CF,\FX)$-messbar}, wenn gilt:
  \[\fmineins(A)\in\CF\quad (A\in\FX)\footnote{Schreibweise:
    $f:[\Omega, \CF]\rightarrow [X,\FX]$}\]
  In diesem Fall heißt das Maß $\mu\circ\fmineins$, definiert durch
  $\mu\circ\fmineins(A)=\mu(\fmineins(A)) \,(\forall A\in\FX)$
  \highl{Bildmaß} von $\mu$ bezüglich $f$.
\end{definition}

\begin{remark}
  \begin{enumerate}[1.)]
  \item Es existiert eine formale Ähnlichkeit zwischen der Definition
    der Stetigkeit und der der Messbarkeit.
  \item Im Fall das $\Omega, X$ metrische Räume sind und $\CF, \FX$
    der $\sigma$-Algebra der Borelmengen entsprechen, folgt aus der
    Stetigkeit von $f$, dass $f$ auch messbar ist. Der Beweis hierzu
    ist lang. In beiden Fällen wird die $\sigma$-Algebra der
    Borelmengen durch die Stetigkeit erzeugt.
  \item Sei $\fmineins: \FP(X)\rightarrow\FP(\Omega)$ mit $\mu:
    \CF\subset \FP(\Omega)\rightarrow[0,+\infty]$\\
    Die Definition von $\mu\circ\fmineins$ ist \emph{nur möglich},
    wenn $f$ als $(\CF,\FX)$-messbar vorausgesetzt wird.
  \item Ist $\mu\circ\fmineins$ ein Maß auf $[X,\FX]$. Hierzu ist
    klar, dass $\mu\circ\fmineins:\FX\rightarrow[0,+\infty]$. Es ist
    $\mu\circ\fmineins(\emptyset)= \mu(\fmineins(\emptyset))=
    \mu(\emptyset)= 0$. Im folgenden ist die zweite 
    Maßeigenschaft zu prüfen. Sei dazu $(A_i)_{i=1}^\infty \subseteq
    \FX, A_i\cap A_j=\emptyset, i\neq j\Rightarrow \fmineins (A_i)
    \cap \fmineins(A_j)=\fmineins(\emptyset)=\emptyset\Rightarrow
    \mu\circ\fmineins (\bigcup A_i)=\mu(\fmineins(\bigcup A_i))=
    \mu(\bigcup \fmineins(A_i))=\sum\mu(\fmineins(A_i))=
    \sum\mu\circ\fmineins(A_i)$
  \end{enumerate}
\end{remark}

\begin{beispiel}
  \begin{enumerate}
  \item $f(\omega)=c\in X:\fmineins(A)=
    \begin{cases}
      \emptyset & c\notin A\\
      \Omega & c\in A
    \end{cases}$ Da nun die leere Menge und auch $\CF$ Element von
    $\Omega$ sind, gilt $\fmineins(A)\in\CF$. Damit ist die Funktion
    $(\CF, \FX)$-messbar.
  \item Sei $\CF=\{\emptyset,\Omega\}, [X,\FX]=[\R,\FB]$. Zu zeigen
    ist, ob $f$ $(\R,\FB)$-messbar ist. Dies ist genau dann der Fall,
    wenn ein $c\in\R$ mit $f(\omega)=c$ existiert ...
  \item Sei $\CF=\FP(\Omega)$. Dann ist jede Abbildung
    $f:\Omega\rightarrow X$ messbar.
  \end{enumerate}
\end{beispiel}

\begin{satz}
  \label{satz:verkn-messbar212}
  Seien $f_1:[\Omega_1,\CF_1]\rightarrow[\Omega_2,\CF_2]$ und $f_2:
  [\Omega_2, \CF_2]\rightarrow[\Omega_3,\CF_3]$. Dann gilt $f_2\circ
  f_1: [\Omega_1,\CF_1]\rightarrow[\Omega_3,\CF_3]$.
\end{satz}
\begin{proof}
  Sei $A\in\CF_3$. Es ist nun zu zeigen, dass $(f_2\circ f_1)^{-1} (A)
  \in \CF$. Dazu stellen wir fest, dass $(f_2\circ f_1)^{-1} (A)=
  \{\omega\in\Omega | f_2\circ f_1(\omega)\in A\}$. Für die
  Verknüpfung der Funktionen gilt $f_2(f_1(\omega))\Rightarrow
  f_1(\omega) \in f_2^{-1}(A)\Rightarrow \omega\in f_1^{-1}(f_2^{-1}
  (A))$. Damit ist gezeigt: $(f_2\circ f_1)^{-1}(A)= f_1^{-1}(f_2^{-1}
  (A)) \Rightarrow A\in\CF_3\Rightarrow f_2^{-1} (A)\in\CF_2
  \Rightarrow f_1^{-1}(f_2^{-1}(A))\in\CF_1\Rightarrow (f_2\circ
  f_1)^{-1} (A)\in\CF_1$
\end{proof}

Für die folgenden Aussagen gilt immer $[X,\FX]=[\R,\FB]$ und wir
bezeichnen $(\CF,\FB)$-messbar als messbar.

\begin{satz}
  \label{satz:messb-eigensch213}
  Die Abbildung $f:\Omega\rightarrow\R$ ist genau dann messbar, wenn
  eine der folgenden Aussagen richtig ist ($x, x_1, x_2$ sind dabei
  immer rational):
  \begin{enumerate}[(1)]
  \item $\{\omega\in\Omega|f(\omega)<x\}$
  \item $\{\omega\in\Omega|f(\omega)\leq x\}$
  \item $\{\omega\in\Omega|f(\omega)>x\}$
  \item $\{\omega\in\Omega|f(\omega)\geq x\}$
  \item $\{\omega\in\Omega|x_1\leq f(\omega)<x_2\}$
  \item $\{\omega\in\Omega|x_1< f(\omega)<x_2\}$
  \item $\{\omega\in\Omega|x_1\leq f(\omega)\leq x_2\}$
  \item $\{\omega\in\Omega|x_1< f(\omega)<x_2\}$
  \end{enumerate}
\end{satz}
\begin{proof}
  Die ist eine Beweisidee für (1). Die weiteren Beweise sind analog zu
  führen: Nach dem \autoref{satz:b-erzeugen} wissen wir, dass die
  Menge $\{(-\infty,x)|x\in\Q\}$ ein durchschnittsabgeschlossenes
  Erzeugendensystem für $\FB$ ist. Wenn $\FE$ ein solches
  Systemmit $\fmineins(B)\in\CF$ ist, dass ist $f$ messbar. Es gilt:
  $\{\omega\in\R | f(\omega)<x\}=\fmineins((-\infty,x))$. Aus der
  Kombination beider Aussagen folgt der Satz.
\end{proof}

\begin{satz}
\label{satz:verkn-messbar-fkt-214}
  Seien $f,g$ messbare Funktionen. Dann sind die Funktionen $f+g$ und
  $fg$ ebenfalls messbare Funktionen.
\end{satz}
\begin{proof}
  Seien $f,g:\Omega\rightarrow\R^2$. Diese Funktionen werden wie folgt
  umgewandelt: $h:=[f,g]:\Omega\rightarrow\R^2, [f,g](\omega)=
  [f(\omega), g(\omega)]$. Ist nun $h$ messbar?
  \begin{align*}
    \hmineins (A\times B) &= \{\omega\in\Omega|[f(\omega),g(\omega)]\in
    A\times B\}\\
    &\Rightarrow f(\omega)\in A\wedge g(\omega)\in B\\
    &= \{\omega\in\Omega|f(\omega)\in A\}\cap \{\omega\in\Omega|
    g(\omega)\in B\}\\
    &= \fmineins(A)\cap g^{-1}(B)
  \end{align*}
  Damit ist gezeigt, dass $f,g$ messbar sind und es gilt, dass
  $\fmineins (A)\in\CF\wedge g^{-1}(B)\in\CF$. Wegen der
  Abgeschlossenheit gegenüber Durchschnitten ($\fmineins(A)\cap
  g^{-1}(B)\in\CF$) gilt für alle $A,B\in\FB: \hmineins(A\times B)\in
  \CF$. Und es folgt weiter $\sigma(\{A\times B|A\in\FB, B\in\FB\}) =
  \FB_2$. Damit ist $h$ messbar.\\
  Wir setzen nun $a(x_1,x_2)=x_1+x_2, b(x_1,x_2)=x_1x_2$. Also ist
  $f+g=a\circ h$ und $fg=b\circ h$. Da $a,b$ stetig sind, sind sie
  auch messbar und nach \autoref{satz:verkn-messbar212} ist die
  Verknüpfung beider Abbildungen wieder messbar.
\end{proof}

\begin{satz}
  Sei $(f_i)_{i\in I}$ eine abzählbare Familie messbarer Funktionen,
  so dass die Mengen $\{\omega\in\Omega|\sup f_i(\omega)<+\infty\}=
  \Omega$ bzw. $\{\omega\in\Omega|\inf f_i(\omega)<-\infty\}= \Omega$
  sind. Dann ist $\sup f_i$ bzw. $\inf f_i$
  messbar.\footnote{Erläuterung: $(\sup f_i)(\omega):= \sup
    f_i(\omega)$ mit $\omega\in\Omega$}
\end{satz}
\begin{proof}
  \begin{enumerate}[1. F{a}ll]
  \item Sei $f:\sup f_i$. Dann ist $f$ eine Abbildungen von $\Omega$
    nach $\R$. Wegen des \autoref{satz:messb-eigensch213} genügt es
    zu zeigen, dass $\{\omega\in\Omega|f(\omega)\leq x\}\in\CF$. Dazu
    $f(\omega)\leq x\Leftrightarrow f_i(\omega)\leq x\Rightarrow
    \{\omega\in\Omega| f(\omega)\leq x\}=\bigcap_{i\in I}
    \{\underbrace{\omega\in\Omega|f_i(\omega)\leq x}_{\fmineins_i
      ((-\infty, x))\in\CF}$. Damit folgt, dass $f_i$ messbar
    ist. Nach \autoref{satz:s-alg-abg-123} ist es gegenüber
    Vereinigung abgeschlossen und es folgt somit: $\{\omega\in\Omega\colon
    f(x)\leq x\}=\bigcap_{i\in I} \fmineins_i ((-\infty,x))\in\CF$
  \item $f:=\inf f_1\Rightarrow f:\Omega\rightarrow\R$. Wegen
    \autoref{satz:messb-eigensch213} (4) genügt es zu zeigen, dass
    $\{\omega\in\Omega | f(\omega)\geq x\}\in\CF$. Dazu $f(\omega)\geq
    x\Leftrightarrow f_i(\omega)\geq x (i\in I)\Rightarrow
    \{\omega\in\Omega | f(\omega)\geq x\}=\bigcup_{i\in I}
    \{\underbrace{\omega\in\Omega | f_i(\omega)\geq x}_{\fmineins_i
      ((x,+\infty))\in\CF}\}$
  \end{enumerate}
\end{proof}

\begin{satz}
  Sei $f$ eine messbare Funktion. Dann gelten:
  \begin{enumerate}[(1)]
  \item $af+b$ ist messbar. $(a,b\in\R)$
  \item $f^+:=\max (f,0)$ ist messbar.
  \item $f^-:=\min (f,0)$ ist messbar.
  \item $|f|$ ist messbar.
  \end{enumerate}
\end{satz}
\begin{proof}
  \begin{enumerate}[zu (1)]
  \item Wir wissen, dass "`konstante"' Funktionen immer messbar
    sind. Damit wie auch nach \autoref{satz:verkn-messbar-fkt-214}
    gilt die Behauptung.
  \item Wir setzen $f_1=f, f_2=0$. Beide Funktionen sind messbar und
    wegen des obigen Satzes ist auch $\sup f_i=f^+$ messbar.
  \item Beweis erfolgt analog zu (2).
  \item $|f|=f^+-f^-=f^++(-1)f^-\Rightarrow$ messbar.
  \end{enumerate}
\end{proof}

\begin{satz}
  Sei $(f_n)_{n=1}^\infty$ eine Folge messbarer Funktionen und
  $\limsup f_n$ oder $\liminf f_n$ existieren als reelle
  Funktionen. Dann sind diese messbar. Falls $\lim f_n$ existiert, so
  ist auch er messbar.
\end{satz}
\begin{proof}
  selbst
\end{proof}

\begin{satz}
\label{satz:indimess-218}
  Sei $A\subseteq\Omega$. Die Indikatorfunktion $\chi_A$ ist genau
  dann messbar, wenn $A\in\CF$.
\end{satz}
\begin{proof}
  Es ist $\chi_A^{-1}(B)$ zu prüfen:
  \[\chi_A^{-1}(B)=
  \begin{cases}
    \emptyset & 0,1 \notin B\\
    A & 1\in B \wedge 0 \notin B\\
    A^c & 1\notin B\wedge 0\in B\\
    \Omega & 1\in B\wedge 0\in B
  \end{cases}\]
  Wegen $\emptyset, \Omega\in\CF$ erhalten wir: $\chi_A^{-1}(B)\in\CF
  \Leftrightarrow \chi_A^{-1}(\{0\}),\chi_A^{-1}(\{1\})\in\CF
  \Leftrightarrow A^c,A\in\CF\Leftrightarrow A\in\CF$
\end{proof}

\begin{beispiel}
  $\Omega=\R, A=\{x\in\R|x\text{ rational}\}\Rightarrow A$
  abzählbar. Da alle abzählbaren Mengen in der Borelmenge liegen.
\end{beispiel}

\begin{definition}
  Sei $f:\Omega\rightarrow\R$. Die Funktion $f$ heißt
  \highl[Funktion!einfache]{einfach}, wenn
  existieren: $n\geq 1, a_1,\ldots,a_n\in\R, A_1,\ldots,A_n\in\CF$ mit
  \[f=\sum_{i=1}^n a_1\chi_{A_i}\]
\end{definition}

\begin{remark}
  \begin{enumerate}
  \item Das Wort "`einfach"' bezieht sich auf die gegebenen
    $\sigma$-Algebren $\CF$ und $\FB$.
  \item Einfache Funktionen sind messbar. Denn nach dem vorhergehenden
    Satz folgt, dass $\chiai$ messbar sind und die Linearkombinationen
    messbarer Funktionen sind wieder messbar.
  \end{enumerate}
\end{remark}

\begin{satz}
  Sei $f:\Omega\rightarrow\R$. Dann gelten
  \begin{enumerate}[(1)]
  \item Wenn  $f$ einfach ist, dann existieren $n\geq 1, b_1,\ldots,
    b_n\in\R, B_1,\ldots,B_n\in\CF$ mit $b_i\neq b_j (i\neq j),
    B_i\cap B_j=\emptyset, \bigcup_{i=1}^n B_i=\Omega \footnote{$B_i$ sind
      messbare Zerlegungen von $\Omega$.}, f=\sum_{i=1}^n
    b_i\chi_{B_i}$. Diese Darstellung von $f$ ist bis auf Permutation
    des Indexbereichs eindeutig bestimmt.
  \item $f$ ist genau dann einfach, wenn $f$ messbar ist und die
    Anzahl des Wertebereichs \footnote{$\#\{f(\omega)|\omega\in\Omega\}
    <+\infty$} endlich ist.
  \end{enumerate}
\end{satz}
\begin{proof}
  \begin{enumerate}[zu (1)]
  \item Der Beweis hierzu ist im untenstehenden Beweis enthalten.
  \item
    \begin{itemize}
    \item["`$\Rightarrow$"'] Wegen der Bemerkung und der Definition
      folgt aus einfach auch die Messbarkeit. Auch die zweite
      Bedingung geht sofort aus der Definition hervor.
    \item["`$\Leftarrow$"'] Setzen $\{b_1,\ldots,b_n\}=\{f(\omega)|
      \omega\in\Omega\}$ und $B_i=\fmineins(\{b_i\})$. Damit folgt
      $\{b_i\}\in\FB$. Wegen der Messbarkeit gilt weiter $B_i=
      \fmineins(\{b_i\})\in\CF$. Es ist trivial, dass $B_i\cap B_j=
      \emptyset$ und klar, dass $f=\sum_{i=1}^n b_i\chi_{B_i}$. Daraus
      folgt, dass $f$ einfach ist.
    \end{itemize}
  \end{enumerate}
\end{proof}

\begin{satz}
\label{satz:limfolge-2110}
  Sei $f$ messbar und nichtnegativ\footnote{$f=f^+$}. Dann existiert
  eine Folge nichtnegativer einfacher Abbildungen $(f_n)_{n=1}^\infty$
  mit $f_n\leq f_{n+1}$ und $f=\limn f_n (f_n \uparrow f)$
\end{satz}
\begin{proof}
  Der Beweis erfolgt konstruktiv, d.\,h. man konstruiert eine spezielle
  Folge $(f_n)_{n=1}^\infty$:
  \[f_n:=\sum_{i=0}^{4^n-1} \frac{i}{2^n}
  \chi_{\fmineins([\frac{i}{2^n}, \frac{i+1}{2^n}))} +2^n
  \chi_{\fmineins ([2^n,-\infty))}\]

  \begin{pspicture}(8,6)
    \psaxes[labels=none,tickstyle=bottom,ticks=y](7,5)
    \psbezier(0,0)(1,0.5)(2,1.5)(3,3.7)(5,4)(7,4.5)
    \psline[linestyle=dotted](0,4)(5,4)
    \psline(5,4)(7,4)
    \psline[linestyle=dotted](0,3)(3.1,3)
    \psline[linestyle=dotted](0,2)(2.2,2)
    \psline[linestyle=dotted](0,1)(1.4,1)
    \psline[linestyle=dashed](1.4,0)(1.4,1)
    \psline[linestyle=dashed](2.3,0)(2.3,2)
    \psline[linestyle=dashed](3.1,0)(3.1,3)
    \psline[linestyle=dashed](5,0)(5,4)
    \psline(1.4,1)(2.3,1)
    \psline(2.3,2)(3.1,2)
    \psline(3.1,3)(5,3)
    \rput[B](7.5,4.5){$f(\omega)$}
    \rput[B](7.5,4){$f_n(\omega)$}
    \rput[l](-0.5,4){$\frac{4^n}{2^n}$}
    \rput[l](-0.5,1){$\frac{i}{2^n}$}
    \rput[l](-0.5,2){$\frac{i+1}{2^n}$}
  \end{pspicture}
  
  Damit ergibt sich $0\leq f_n\leq f_{n+1}\leq f$. Wenn $f$ messbar
  ist, folgt, dass $f$ auch einfach ist. Die Differenz $f(\omega)-
  f_n(\omega)\leq\frac{1}{2^n}$\footnote{$\omega\in\Omega$ mit
    $f(\omega) \leq2^n$} und es folgt, dass $\lim f_n(\omega)=
  f(\omega)$
\end{proof}

\section{Einführung des Integrals}

Im folgenden ist immer $\ofmu$ ein Maßraum und wir betrachten $f:
[\Omega, \CF]\rightarrow[\R,\FB]$. Das Integral $\int f(\omega)\mu
(d\omega) \hat{=} \int fd\mu$ wird in drei Stufen eingeführt:

\begin{definition}[1. Stufe]
  Sei $f\geq 0$ und einfach, d.\,h. wir haben eine Darstellung der Form
  $f=\sum_{i=1}^n a_i \chiai$ mit $n=1, a_1,\ldots,a_n\geq 0,
  A_i\in\CF$. Dann setzen wir 
  \[\int f(\omega)\mu(d\omega):=\sum_{i=1}^n
  a_i\mu(A_i)\footnote{Vereinbarung: Es kann sein, dass $\mu=\infty$
    und es kann $0\cdot\infty$ auftauchen. Dann setzen wir
    $0\cdot\infty =0$}\]
\end{definition}

\begin{remark}
  \begin{enumerate}
  \item $\int fd\mu\geq 0$
  \item Die Definition des Integrals ist eindeutig, d.\,h. unabhängig
    von der betrachteten Darstellung als Linearkombination von
    messbaren Indikatorfunktionen:\\ $f=\sum_{i=1}^n
    a_i\chiai=\sum_{k=1}^m b_k\chi_{B_k}\Rightarrow\sum_{i=1}^n
    a_i\mu(A_i) = \sum_{k=1}^m b_k\mu(B_k)$
  \item Das Integral ist linear, d.\,h. $f,g=0$ und einfach,
    $\alpha,\beta \geq 0\Rightarrow\int(\alpha f+\beta g)d\mu=
    \alpha\int fd\mu+\beta\int gd\mu$. Denn sei $f=\sum a_i\chiai$ und
    $g=\sum b_j \chi_{B_j}$. Dann ist $\alpha f + \beta g=\sum \alpha
    a_i \chiai+\sum\beta b_j\chi_{B_j}\Rightarrow\int (\alpha f+\beta
    g) d\mu=\sum\alpha a_i\mu(A_i)+\sum\beta b_j\mu(B_j)=\alpha \int
    fd\mu +\beta\int gd\mu$
  \item Wenn $f,g\geq 0$ einfach und $f\leq g$, dann folgt, dass $\int
    fd\mu\leq\int gd\mu$. Das Integral ist monoton. Denn da $f\leq g$
    ist, ist auch $g-f\geq 0$ und da beide Funktionen einfach sind,
    ist auch die Differenz wieder einfach. Es gilt:
    $g=f+(g-f)\Rightarrow \int gd\mu=\int fd\mu+\underbrace{\int (g-f)
      d\mu}_{\geq 0}\Rightarrow \int gd\mu\geq \int fd\mu$
  \item Sei $(f_n)_{n=1}^\infty$ eine Folge nichtnegativer einfacher
    Funktionen mit $f_n\leq f_{n+1}$. Dann existiert $\limn \int
    f_nd\mu$. Dabei ist auch $+\infty$ möglich. Aus dem vierten Punkt
    folgt $0\leq\int f_n d\mu\leq\int f_{n+1}d\mu$
  \item Seien $(f_n), (g_n)$ Folgen nichtnegativer einfacher
    Funktionen mit $f_n\leq f_{n+1}, g_n\leq g_{n+1}, \lim f_n\leq
    \lim g_n$. Dann gilt $\lim\int f_n d\mu\leq\lim\int g_n d\mu$. Zum
    Beweis muss die zusätzliche Voraussetzung $f_n\leq g_n$ benutzt
    werden. Denn nach dem vierten Punkt folgt: $\int f_nd\mu\leq\int
    g_nd\mu$ und wegen fünftens existieren $\lim\int f_nd\mu, \lim\int
    g_nd\mu$. Damit ist $\lim\int f_nd\mu\leq\lim\int g_nd\mu$.
  \end{enumerate}
\end{remark}

\begin{definition}[2. Stufe]
  Sei $f\geq 0$ und messbar. Nach \autoref{satz:limfolge-2110}
  existieren $(f_n)_{n=1}^\infty, 0\leq f_n\leq f_{n+1}, f=\lim f_n,
  f_n$ einfach. Wir setzen:
  \[\int fd\mu := \limn\int f_nd\mu\]
\end{definition}

\begin{remark}
  \begin{enumerate}
  \item Die Definition ist "`sinnvoll"', weil das Integral nach der
    Definition erster Stufe definiert ist und der Limes nach der
    obigen Bemerkung 5 existiert.
  \item Die Definition ist auch eindeutig, d.\,h. wenn wir haben: $0\leq
    f_n\leq f_{n+1}, 0\leq g_n\leq g_{n+1}, f=\lim f_n=\lim g_n$ und
    beide sind einfach, dann folgt daraus: $\lim\int f_nd\mu=\lim\int
    g_nd\mu$. Dies ist eine direkte Folgerung aus der Bemerkung 6 oben.
  \item Die Definition zweiter Stufe ist eine Erweiterung der
    Definition erster Stufe.
  \item Das so definierte Integral ist linear.\\
    Beweis: Sei $\alpha,\beta\in\R\geq 0, f,g\geq 0$ und
    messbar. Weiter seien $f=\lim f_n, g=\lim g_n, 0\leq f_n\leq
    f_{n+1}, 0\leq g_n\leq g_{n+1}$ alle einfach, d.\,h. $\int fd\mu=
    \limn\int f_nd\mu$ und $\int gd\mu=\limn\int g_nd\mu$.\\
    $\alpha f+\beta g=\limn(\alpha f_n+\beta g_n)\Rightarrow \int
    (\alpha f+\beta g)d\mu=\limn\int(\alpha f_n+\beta g_n) d\mu=
    \limn(\alpha \int f_nd\mu+\beta\int g_nd\mu)=\alpha\int fd\mu
    +\beta\int gd\mu$
  \item Für $0\leq f\leq g$ gilt: $0\leq \int fd\mu\leq\int gd\mu$,
    denn $g=f+(g-f)\Rightarrow \int gd\mu=\int fd\mu+\int(g-f)d\mu
    \Rightarrow\int gd\mu\geq\int fd\mu$
  \end{enumerate}
\end{remark}

\begin{definition}[3. Stufe]
  Sei $f$ messbar und bezüglich $\mu$ integrierbar, d.\,h. $\int
  f^+d\mu$ und $\int(-f^-)d\mu<+\infty$. Dann gilt:
  \[\int fd\mu:=\int f^+d\mu-\int(-f^-)d\mu\]
\end{definition}

\begin{remark}
  \begin{enumerate}
  \item klar: $|f|=f^++(-f^-)\Rightarrow\int|f|d\mu=\int f^+d\mu +\int
    (-f^-)d\mu$
    \[\fbox{$f$ ist genau dann $\mu$-integrierbar, wenn $\int|f|d\mu
      <\infty$}\]
  \item $\int fd\mu$ für $f\geq 0$ und messbar existiert immer. Es ist
    allerdings der Fall $+\infty$ möglich. Das Integral $\int fd\mu$
    im Falle $f$ messbar und nicht notwendigerweise $\geq 0$ ist nach
    der Definition dritter Stufe nur für $\mu$-integrierbare
    Funktionen erklärt.
  \item Die Eigenschaften Monotonie und Linearität des Integrals
    übertragen sich von der zweiten zur dritten Stufe. Für den Beweis
    ist zu zeigen, dass für $f,g$ $\mu$-integrierbar und für
    $a,b\in\R$ gilt, dass $af+bg$ wieder $\mu$-integrierbar sind und
    dass $\int (af+bg)d\mu = a\int fd\mu +b\int gd\mu$ ist. Da $f,g$
    messbar sind, folgt dass auch $af+bg$ messbar sind. Somit ist
    $af+bg$ natürlich auch $\mu$-integrierbar. Es gilt
    $\int(af+bg)d\mu\leq\int(|a||f|+|b||g|)d\mu$ wegen der Monotonie
    zweiter Stufe. Nach der Linearität zweiter Stufe ist das aber
    gleich $|a|\int|f|d\mu +|b|\int |g|d\mu$. Sowohl $|f|$ wie auch
    $|g|$ sind kleiner $+\infty$. Somit ist auch: $|a|\int|f|d\mu
    +|b|\int |g|d\mu < + \infty$.
  \end{enumerate}
\end{remark}

\begin{satz}[Integral bezüglich diskreter Maße]
  Seien $(\alpha_i)_{i\in I}$ eine abzählbare Familie nichtnegativer
  reeller Zahlen und $(\omega_i)_{i\in I}\subseteq\Omega$ und $\mu=
  \sum_{i\in I}\alpha_i \delta_{\omega_i}$\footnote{Maße von diesem
    Typ heißen \highl[Maß!diskretes]{diskrete Maße}}. Eine
  messbare Funktion $f$ ist genau dann $\mu$-integrierbar, wenn gilt
  $\sum_{i\in I} \alpha_i |f(\omega_i)|<\infty$. In diesem Fall gilt:
  \[\int fd\mu=\sum_{i\in I} \alpha_i f(\omega_i)\]
\end{satz}
\begin{proof}
  \begin{enumerate}[1. F{a}ll]
  \item Sei $f=\chi_A (A\in\CF)$\\
    $\int fd\mu=\int\chi_Ad\mu=\mu(A)=\sum_{i\in I} \alpha_i
    \delta_{\omega_i}(A)=\sum_{i\in I}\alpha_i \chi_A(\omega_i)=
    \sum_{i\in I} \alpha_i f(\omega_i)$
  \item Sei $f=\sum_{k=1}^n a_k \chi_{A_k}$ mit $a_k\geq 0,
    A_k\in\CF$. Dann ist $\int fd\mu=\sum_{k=1}^n a_k\mu(A_k)=\int
    \sum_{k=1}^n a_k \chi_{A_k}d\mu=\sum_{k=1}^n a_k
    \int\chi_{A_k}d\mu = \sum_{k=1}^n a_k \sum_{i\in I} \alpha_i
    \chi_{A_k}(\omega_i)=$\\
    $\sum_{i\in I}\alpha_i
    \underbrace{\sum_{k=1}^n a_k \chi_{A_k}(\omega_i)}_{f(\omega_i)}=
    \sum_{i\in I} \alpha_i f(\omega_i)$
  \item Seien $f\geq 0$ und messbar, $0\leq f_m\uparrow f$
    einfach. Nach der Definition zweiter Stufe folgt nun, dass $\int
    fd\mu=\limn\int f_md\mu$. Nach dem vorigen Fall folgt:
    $\limn\sum_{i\in I}\alpha_i f_m(\omega_i)$.Da die Funktion monoton
    wachsend ist, kann der Limes und die Summe vertauscht werden:
    $\sum_{i\in I} \alpha_i \limn f_m(\omega_i)=\sum_{i\in I} \alpha_i
    f(\omega_i)$. Nunmehr muss noch die $\mu$-Integrierbarkeit
    untersucht werden. Nach der Bemerkung 1 der Definition dritter
    Stufe ist $f$ genau dann $\mu$-integrierbar, wenn gilt $\int |f|
    d\mu <\infty\Leftrightarrow \sum_{i\in I} \alpha_i |f(\omega_i)|<
    \infty$
  \item Wenn $f$ $\mu$-integrierbar ist, dann folgt aus der Definition
    dritter Stufe: $\int fd\mu=\int f^+-(-f^-)d\mu=\sum_{i\in I}
    \alpha_i f^+(\omega_i)-\sum_{i\in I} \alpha_i (-f^-)(\omega_i)=
    \sum_{i\in I} \alpha_i (f^+(\omega_i)-(-f^-)(\omega_i))=
    \sum_{i\in I} \alpha_i (f^+(\omega_i)+f^-(\omega_i))=\sum_{i\in I}
    \alpha_i f(\omega_i)$
  \end{enumerate}
\end{proof}

\begin{beispiel}
  \begin{enumerate}
  \item Sei $\mu=\delta_{\omega_0}$ und ein $\omega_0\in\Omega$
    gegeben. Dann ist jede messbare Funktion $f$ $\mu$-integrierbar
    und es gilt: $\int fd\mu=\int fd\delta_{\omega_0}=f(\omega_0)$
  \item Wir betrachten die Reihe $\sum_{k=0}^m a_k$. Dabei ist auch
    der Fall $m=\infty$ möglich. Nun wählen $\Omega=\{0,1,\ldots,n\},
    \CF=\FP(\Omega), \mu=\sum_{i=0}^n \delta_i$ und das Zählmaß
    $\mu(A)= \#A$. Setzen $f:\Omega\rightarrow\R$ mit $f(k):=a_k
    (k\in\Omega)\Rightarrow\int fd\mu=\sum_{k=0}^m a_k$ und $f$ ist
    bezüglich $\mu$ genau dann integrierbar, wenn $\sum_{k=0}^m |a_k|
    <\infty$. Für den Fall $m=\infty$ muss die Reihe absolut
    konvergent sein.
  \end{enumerate}
\end{beispiel}

\begin{satz}[\highl{Riemannintegral} stetiger Funktionen]
\label{satz:riemann-222}
  Seien $f\colon\R\rightarrow\R$ stetig und $a<b$. Dann ist $f\chi_{[a,b]}$
  bezüglich $\ell$ integrierbar und es gilt:
  \[\int f\chi_{[a,b]}d\ell=\int_a^b f(x)dx\]
\end{satz}
\begin{proof}
  Der Fall, dass die Funktion einfach ist, wird nicht betrachtet. Denn
  alle stetigen Funktionen (außer Konstanten) sind nicht einfach.

  Sei nun $f\geq 0$ und stetig. Dann ist $f$ auch messbar und
  $f\chi_{[a,b]}$ ebenfalls. Nach der Definition zweiter Stufe wissen
  wir, dass $\int f\chi_{[a,b]}d\ell=\limn\int f_nd\ell$, falls $0\leq
  f_n \uparrow f$ und $f_n$ einfach. Im folgenden wird $f_n$ als
  untere Darbouxsche Summe konstruiert:

  \begin{pspicture}(6,8)
    \psaxes[tickstyle=bottom,labels=none,ticks=none]{->}(5,7)
    \psplot{1}{4.5}{x x mul 3 div}
    \psline[linestyle=dashed](1,0)(1,0.33)
    \psline[linestyle=dashed](1.5,0)(1.5,0.75)
    \psline[linestyle=dashed](2,0)(2,1.33)
    \psline[linestyle=dashed](2.5,0)(2.5,2.08)
    \psline[linestyle=dashed](3,0)(3,3)
    \psline[linestyle=dashed](3.5,0)(3.5,4.08)
    \psline[linestyle=dashed](4,0)(4,5.33)
    \psline[linestyle=dashed](4.5,0)(4.5,6.75)
    \psline(1,0.33)(1.5,0.33)
    \psline(1.5,0.75)(2,0.75)
    \psline(2,1.33)(2.5,1.33)
    \psline(2.5,2.08)(3,2.08)
    \psline(3,3)(3.5,3)
    \psline(3.5,4.08)(4,4.08)
    \psline(4,5.33)(4.5,5.33)
    \rput[B](5,6.5){$f(x)$}
    \rput[t](1,0){$a$}
    \rput[t](2.5,0){$z_k^{(n)}$}
    \rput[t](4.5,0){$b$}
  \end{pspicture}

  \[z_{k+1}^{(n)}-z_k^{(n)}=\frac{b-a}{2^n} f_n(x)=0\quad (x\notin[a,b])\]
  %Formeln noch weiter ergänzen
  \[\Rightarrow \int
  f_nd\ell=\int\sum_{k=0}^{2^{n-1}}a_k\chi_{[z_k,z_{k+1}]} d\ell=
  \sum_{k=0}^{2^{n-1}} a_k\ell([z_k^{(n)}, z_{k+1}^{(n)}])\]
  \fbox{\parbox{0.4\linewidth}{\[=\frac{b-a}{2^n}
      \sum_{k=0}^{2^{n-1}} a_k\]}}

  Es ist bekannt, dass das Riemannsche Integral der Limes der
  Darbouxschen Summen ist. Somit haben wir $\limn\int f_nd\ell=
  \int_a^b f(x)dx$. Weiter ist klar, dass $f_n\leq f_{n+1}$. Denn die
  Feinheit wird verkleinert, wenn die Intervalle vergrößert
  werden. Damit strebt $f_n$ für $n\rightarrow\infty$ nach
  $f\chi_{[a,b]}$ und nach der Definition zweiter Stufe folgt, $\int
  f_n d\ell\xrightarrow{n\rightarrow\infty}\int f\chi_{[a,b]}d\ell$
\end{proof}

\begin{remark}
  \begin{enumerate}[1)]
  \item Die Behauptung gilt auch dann, wenn nur vorausgesetzt wird,
    dass $f$ auf $[a,b]$ riemannsch integrierbar ist.
  \item Der Satz ist ausdehnbar auf die Fälle $a=-\infty,
    b=\infty$. Dabei ist die Forderung, dass $f$ auf $[a,b]$
    $\ell$-integrierbar ist, notwendig.
  \end{enumerate}
\end{remark}

\begin{satz}[Übertragungssatz]
  \label{satz:uebertrsatz-228}
  Seien $[\Omega_1,\CF_1,\mu]$ ein Maßraum, $[\Omega_2,\CF_2]$ ein
  messbarer Raum, $h:[\Omega_1,\CF_1]\rightarrow[\Omega_2,\CF_2],
  f:[\Omega_2, \CF_2]\rightarrow[\R,\FB]$, wobei $f$ bezüglich $\mu$
  integrierbar ist. Dann ist $f\circ h :\Omega_1\rightarrow\R$
  bezüglich $\mu$ integrierbar und es gilt:
  \begin{gather}
    \label{eq:uesatz}
    \int f\circ h d\mu=\int fd\mu\circ\hmineins
  \end{gather}
\end{satz}
\begin{proof}
  \begin{enumerate}[1. F{a}ll]
    \setcounter{enumi}{-1}
  \item Sei $f=\chi_A (A\in\CF_2)$ und damit messbar. Die Verknüpfung
    $f\circ h(\omega)=\chi_A(h(\omega))$. Es ist klar, dass
    $\chi_A\circ h$ eine Indikatorfunktion auf $\Omega_1$ sein muss,
    d.\,h. $\exists B\subseteq\Omega_1$ mit $\chi_A\circ h=\chi_B$\\
    $\chi_B(\omega)=1\Leftrightarrow \chi_A(h(\omega))=1
    \Leftrightarrow h(\omega)\in A\Leftrightarrow w\in\hmineins(A)
    \Rightarrow B=\hmineins(A)\in\CF_1$
    \[\Rightarrow \chi_A \circ h = \chi_{\hmineins(A)}\]
    $\int f\circ h d\mu=\int\chi_{\hmineins(A)}d\mu=\mu(\hmineins(A))
    =\mu\circ\hmineins(A)=\int\chi_Ad\mu\circ\hmineins=\int fd\mu\hmineins=$
  \item Sei $f\geq 0$ und einfach, d.\,h. $f=\sum_{i=1}^n a_i
    \chiai$ mit $A_i\in\CF_2, a_i\geq 0$. Das Integral $\int fd\mu
    \circ\fmineins$ entspricht wegen der Linearität die Summe
    $\sum_{i=1}^n a_i\int \chiai d\mu\circ\hmineins=\sum_{i=1}^n a_i
    \int \chiai \circ\hmineins d\mu=\int\sum_{i=1}^n a_i \chiai \circ
    \hmineins d\mu=\int f\circ hd\mu$
  \item Wir wählen $(f_n)_{n=1}^\infty$ mit $0\leq f_n\leq f_{n+1},
    f=\lim f_n$ und $f_n$ einfach. Somit folgt $\int f_n
    d\mu\circ\hmineins=\lim \int f_n d\mu\circ\hmineins=\lim\int
    f_n\circ h d\mu$. Es ist klar, dass $g_n:=f_n\circ h$ die
    folgenden Bedingungen erfüllt: $0\leq g_n\leq g_{n+1}, \lim g_n =
    f\circ h, g_n$ einfach.\footnote{Wenn $f_n$ nur endliche viele
      Werte annimmt, nimmt $g_n$ ebenfalls nur endlich viele Werte
      an.} 
    $\Rightarrow \limn \int g_n d\mu=\int f\circ h d\mu\Rightarrow
    \int f\circ h d\mu=\int fd\mu\circ \hmineins$
  \end{enumerate}
  Die Behauptung, dass $f\circ h$ $\mu$-integrierbar ist, folgt sofort
  aus der $\mu\circ\hmineins$-Integrierbarkeit von $f$ und der
  Gleichung (\autoref{eq:uesatz}) im bewiesenen Fall nichtnegativer
  Funktionen.
\end{proof}

\begin{beispiel}
  Sei $\Omega_2=\R, h:\Omega_1\rightarrow\R, f(x)=x,
  f:\R\rightarrow\R$. Dann gilt $\int h(\omega)\mu(d\omega)=\int
  x\mu\circ \hmineins(dx)$.
\end{beispiel}

\begin{definition}
  Seien $f$ und $g$ messbare Funktionen. $f$ und $g$ sind
  \highl[fast überall gleich,gleich!fast
  überall]{$\mu$-fast-überall-gleich}, wenn gilt:
  \[\mu(\{\omega\in\Omega|f(\omega)\neq g(\omega)\})x\]
\end{definition}

\begin{beispiel}
  \begin{enumerate}
  \item Sei $\mu=\delta_{\omega_0}\Rightarrow f=g\Leftrightarrow
    f(\omega_0)=g(\omega_0)$
  \item $\ofmu=\rbel, f=\chi_\Q$ Da die rationalen Zahlen abzählbar
    sind, ist klar, dass $\ell(\Q)=0$. Somit ist auch $\chi_\Q\equiv
    0$, da $\ell(\{\omega\in\Omega|\chi_\Q(\omega)\neq 0\})=\ell(\Q)$
  \end{enumerate}
\end{beispiel}

\begin{remark}
  Die Fast-Überall-Gleichheit ist eine Äquivalenzrelation. Die Klasse
  aller messbaren Funktionen zerfällt in von $\mu$ abhängigen
  Äquivalenzklassen. 
\end{remark}

\begin{satz}
  Seien $f,g$ $\mu$-fast überall gleich ($f=_\mu g$). Weiter seien
  $f,g$ $\mu$-integrierbar. Dann gilt:
  \[\int fd\mu=\int gd\mu\]
\end{satz}
\begin{proof}
  $f=_\mu g\Leftrightarrow f-g=_\mu 0=h$
  \begin{itemize}
  \item Wegen $\int(f-g)d\mu=\int fd\mu-\int gd\mu$ genügt es zu
    zeigen, dass $h=0$ messbar und aus $h=_\mu 0$ folgt $\int hd\mu=0$
  \item Wir wissen: $\int hd\mu=\lim\int f_nd\mu$ für $0\leq f_n\leq
    f_{n+1}, h=\lim f_n$ und $f_n$ einfach. Damit folgt,
    $h=f_n\Rightarrow \int hd\mu\geq \int f_nd\mu$. Nun genügt es zu
    zeigen, dass $0\leq f$ einfach: $f\leq h\Rightarrow \int
    fd\mu=0$. Wir haben: Falls $h\geq f$ und $h=_\mu 0\Rightarrow
    f=_\mu 0$. Damit genügt es zu zeigen, dass $0\leq f$ einfach und
    $f=_\mu 0\Rightarrow\int fd\mu=0$. Sei nun $f=\sum_{i=1}^n a_i
    \chiai$. Alle $a_i\geq )$ und die $A_i$ sind paarweise disjunkt
    und messbar. Klar ist:
    \begin{itemize}
    \item $\int\chiai d\mu=\mu(A_i)=0\Rightarrow \int fd\mu=0$
    \item $f\geq a_i \chiai\Rightarrow$ genügt zu zeigen: $\chi_A
      =_\mu 0\Rightarrow\int\chi_Ad\mu=0$
    \end{itemize}
    Dazu ist $\chi_A=_\mu 0\Leftrightarrow
    \mu(\{\underbrace{\omega\in\Omega|\chi_A(\omega)\neq 0}_{A}\})=0=
    \mu(A)$\\
    $\int\chi_Ad\mu=\mu(A)=0$
  \end{itemize}
\end{proof}


\section{Absolute Stetigkeit von Maßen}

\begin{definition}
  Seien $\mu,\nu$ Maße auf $[\Omega,\CF]$ und $0\leq
  f:[\Omega,\CF]\rightarrow[\R,\FB]$. Dann heißt $\nu$ \highl{absolut
  stetig}\index{stetig!absolut} bezüglich
  $\mu$ mit Dichte $f$\footnote{Schreibweise: $\nu=\mu f$}, wenn gilt:
  \[\nu(A)=\int f\chi_Ad\mu\quad(A\in\CF)\]
\end{definition}

\begin{remark}
  \begin{enumerate}
    \setcounter{enumi}{-1}
  \item Schreibweisen: $\int_A fd\mu:=\int f\chi_Ad\mu, \nu\ll\mu$
  \item $\mu f(A)=\int_A fd\mu$ definiert ein Maß auf $[\Omega,\CF]$\\
    Beweis:
    \begin{itemize}
    \item Fall: $A=\emptyset: \mu f(\emptyset)=\int f\chi_\emptyset
      d\mu=\int 0d\mu=0$
    \item klar: $\mu f(A)=\int f \chi_A d\mu\geq 0$
    \item Die $(A_i)$ sind paarweise disjunkt: $A_i\in\CF\Rightarrow
      \mu f(\bigcup A_i)=\int f \chi_{\bigcup A_i} d\mu=\int\sum
      f\chiai d\mu=\sum\int f\chiai d\mu=\sum \mu f(A_i)$
    \end{itemize}
  \item $\mu f$ ist endlich. Es ist per Definition: $\mu f(\Omega)<+
    \infty=\int f\chi_\Omega d\mu=\int fd\mu\Leftrightarrow f$ ist
    $\mu$-integrierbar. 
  \end{enumerate}
  $\mu f$ ist Wahrscheinlichkeitsmaß. Per Definition ist $\mu
  f(\Omega)=1=\int fd\mu$. $f$ heißt
  \highl{Wahrscheinlichkeitsdichte}
  bezüglich $\mu$.
\end{remark}

\begin{beispiel}
  Sei $\Omega=\N_0, \CF=\FP(\N_0),\mu:=\sum_{i=1}^\infty \delta_i$.\\
  Behauptung: Jedes Maß $\nu$ auf $[\N_0,\FP(\N_0)]$ ist absolut
  stetig bezüglich $\mu$ ($\nu\ll\mu$) mit der Dichte
  $f(i):=\nu(\{i\})$.\\
  Beweis: $\nu(A)=\sum_{i\in A} \nu(\{i\})=\sum_{i\in\N_0} \nu(\{i\})
  \chi_A(i)=\sum_{i\in\N_0} f(i)\chi_A(i)=$\\
  $\int f(i)\chi_A(i)\mu(di)$
\end{beispiel}

\begin{remark}
  Seien $f,g\geq 0$ und messbar. Dann ist $\mu f=\mu g$ genau dann
  wenn, $f=_\mu g$.\\
  Beweis:\footnote{Der Beweis erfolgt einschränkend für Mengen mit
    endlichem Maß.} $\mu f=\mu g\Leftrightarrow \int_A fd\mu=\int_A
  gd\mu\Leftrightarrow \int_A (f-g)d\mu=0=\int (f-g)\chi_Ad\mu$
  \begin{itemize}
  \item["`$\Leftarrow$"'] Sei $f=_\mu g\Rightarrow f-g=_\mu
    0\Rightarrow (f-g)\chi_A=_\mu =0\Rightarrow \int (f-g)\chi_A
    d\mu=0\Rightarrow \mu f=\mu g$
  \item["`$\Rightarrow$"'] Angenommen $\neg f=_\mu g$,
    d.\,h. $\mu(\{\omega\in\Omega|f(\omega)\neq g(\omega)\})=0
    \Rightarrow \mu(\{\omega\in\Omega|f(\omega)>g(\omega)\})>0\vee
    \mu(\{\omega\in\Omega|f(\omega)<g(\omega)\})<0$. o.\,B.\,d.\,A.  gelte
    die erste Gleichung und sei $A_n:=\{\omega\in\Omega|f(\omega)\geq
    g(\omega)+\frac{1}{n}\}\Rightarrow A_n\subseteq A_{n+1}$. Wegen
    der Stetigkeit von unten folgt: $\lim \mu(A_n)=\mu(\bigcup A_n)=
    \mu(\{\omega\in\Omega|f(\omega)>g(\omega)\})>0\Rightarrow \exists
    n_0: \mu(A_{n_0})>0=\mu(\{f(\omega)\geq g(\omega)+\frac{1}{n_0}\})=
    \mu(\{\omega\in\Omega|f(\omega)-g(\omega)\geq \frac{1}{n_0}\})
    \Rightarrow \int (f-g)\chi_{A_{n_0}} d\mu\geq \int \frac{1}{n_0}
    \chi_{A_{n_0}} d\mu=\frac{1}{n_0}\mu(A_{n_0}>0$. Somit folgt, dass
    \emph{nicht} gilt: $\mu f=\mu g$
    \qed
  \end{itemize}
\end{remark}

\begin{satz}
  Sei $P$ ein Wahrscheinlichkeitsmaß auf $[\R,\FB]$ mit stetig
  differenzierbarer Verteilungsfunktion $F(x)=P((-\infty,x))$. Dann
  gilt: 
  \[P=\ell F\]
\end{satz}
\begin{proof}
  \footnote{Für die Gültigkeit der Behauptung reicht die Forderung,
    dass $F'$ stückweise stetig ist. Der Beweis wird hierdurch jedoch
    sehr umfassend, sodass auf die schwächere Bedingung
    zurückgegriffen wird.}
  Wir definieren ein Maß $\nu:=\ell F'$. $F$ ist eine
  Verteilungsfunktion und monoton wachsend. Damit ist die Ableitung
  immer positiv. Zu prüfen ist nun, ob $\nu$ ein
  Wahrscheinlichkeitsmaß ist und ob dies mit $P$ zusammen fällt.
  \begin{enumerate}
  \item Dazu $\nu(\R)=\int_\R F'(x)\ell(dx)$. Da $F$ stetig ist und
    \autoref{satz:riemann-222} gilt: $\int_{-\infty}^\infty F'(x)dx=
    \lim_{a\rightarrow-\infty} \lim_{b\rightarrow\infty} \int_a^b
    F'(x) dx=\lim_{a\rightarrow-\infty} \lim_{b\rightarrow\infty}
    (F(b)- F(a))$. Wegen des \autoref{satz:eigensch-vert-152} Punkt 3
    und 4 gilt $1-0\cdot 1$. Weiterhin ist zu zeigen, dass
    $P=\nu$. Hier genügt es zu zeigen, dass beide die gleiche
    Verteilungsfunktion haben: Die Verteilungsfunktion $F$ von $P$ ist
    gleich der Verteilungsfunktion $G$ von $\nu$. Wir haben $G(x)=\nu
    ((-\infty,x))=\int_{(-\infty,x)} F'(x)\ell(dy)$. Wiederum nach
    \autoref{satz:riemann-222} ist $\int_{-\infty}^x F'(y)dy=
    \lim_{a\rightarrow-\infty} (F(x)-F(a))=F(x)$
  \end{enumerate}
\end{proof}

\begin{beispiel}
  Wir betrachten die \highl{Exponentialverteilung}:
  \begin{align*}
    Ex_\lambda\rightarrow F(x)&=    
    \begin{cases}
      0 & x\leq 0\\
      1-e^{-\lambda x} & x>0
    \end{cases} &\\
    F'(x)&=
    \begin{cases}
      0 & x\leq 0\\
      \lambda e^{-\lambda x} & x>0
    \end{cases} &
    Ex_\lambda &= \ell F'(x)
  \end{align*}
\end{beispiel}

\begin{satz}
  Seien $\mu$ ein Maß auf $[\Omega,\CF],f,g:[\Omega,\CF]\rightarrow
  [\R,\FB], f\geq 0$. Ist $f\cdot g$ bezüglich $\mu$ integrierbar, so
  ist $g$ allein bezüglich $\mu$ mit Dichte $f$ integrierbar und es
  gilt: 
  \[\int gd\mu f=\int fgd\mu\]
\end{satz}
\begin{proof}
  \begin{enumerate}[1. F{a}ll]
    \setcounter{enumi}{-1}
  \item $g=\chi_A\,(A\in\CF)$: Es gilt zu zeigen, dass $\int gd\mu
    f=\int fgd\mu=\int\chi_A d\mu f=\mu f(A)=\int_A fd\mu= \int
    f\chi_A d\mu=\int fgd\mu$
  \item $g=\sum a_i \chiai$\\
    Die weiteren Schritte können wie bereits gehabt mittels der
    Linearität und Limes genzeigt werden.
  \end{enumerate}
\end{proof}

\begin{beispiel}
  Eine vielfache Anwendung stellt folgendes Beispiel dar:\\
  $\mu=\ell,
  [\Omega, \CF]=[\R,\FB], P=\ell f, f$ ist (stückweise) stetig,
  $g:[\R,\FB]\rightarrow[\R,\FB]$ bezüglich $P$ integrierbar und
  stetig. Daraus folgt: $\int gdP=\int_{-\infty}^\infty f(x)g(x)dx$
\end{beispiel}


\section{Der Satz von Fubini}
\begin{definition}
  Sei $\ofmu$ ein Maßraum. Das Maß $\mu$ heißt
  \highl[endlich,Maß!endliches]{$\sigma$-endlich}, wenn
  eine messbare Zerlegung\footnote{d.\,h. $A_k\cap A_j=\emptyset\wedge
    \bigcup A_k=\Omega$} $(A_k)_{k=1}^\infty$ von $\Omega$ existiert
  mit $\mu(A_k)<+\infty\,(k=1,2,\ldots)$.
\end{definition}

\begin{remark}
  Spezialfälle:
  \begin{itemize}
  \item Alle Wahrscheinlichkeitsmaße und alle endlichen Maße
  \item Lebesguesche Maße (jede Dimension); Wähle z.\,B. $A_k=[k,k+1)\,
    (k\in\Z)\Rightarrow \bigcup A_k\R,A_k\cap A_j=\emptyset,
    \ell(A_k)=1$
  \item Zählmaß auf abzählbaren Mengen $\Omega$, Wähle
    $A_\omega=\{\omega\}$. Es ist klar, dass
    $\mu(A_\omega)=\sum_{\omega\in\Omega} \delta_\omega(A_\omega)=
    \sum_{\omega\in\Omega} \delta_\omega(\{\omega\})=1$
  \end{itemize}
\end{remark}

Im folgenden seien $\Omega_1, \CF_1, \mu_1], \ldots, [\Omega_n, \CF_n,
\mu_n]$ Maßräume und $\mu_1,\ldots,\mu_n$ $\sigma$-endlich.

\begin{satz}[Satz von Fubini]
  Sei $f:\times_{i=1}^n [\Omega_i, \CF_i]\rightarrow[\R,\FB]$
  bezüglich $\times_{i=1}^n \mu_i$ integrierbar. Weiter sei $(i_1,
  \ldots, i_n)$ eine Permutation der Zahlen $\{1,\ldots, n\}$. Dann
  gilt:
  \[\int fd\times_{i=1}^n \mu_i=\int\cdots\int f(\omega_1,\ldots,
  \omega_n) \mu_{i_1}(d\omega_{i_1})\ldots \omega_{i_n}(d\omega_{i_n})\]
\end{satz}

\begin{beispiel}
  Wir wissen, $[\R^n, \FB_n,
  \ell^{(n)}]=[\R,\FB,\ell]^{n\times}$. 
  Wenn $f(x_1,\ldots,x_n)$ stetig, ist sie in jeder Variablen stetig
  und $\int_{-\infty}^\infty\cdots\int_{-\infty}^\infty |f(x_1,\ldots,
  x_n)| dx_1\ldots dx_n<+\infty$. Es gilt:
  \[\int fd\ell^{(n)}=\int_{-\infty}^\infty\cdots\int_{-\infty}^\infty
  f(x_1, \ldots, x_n) dx_1\ldots dx_n\]
\end{beispiel}

\begin{satz}
  Seien $[\Omega_1,\CF_1,\mu_1],\ldots, [\Omega_n,\CF_n,\mu_n]$
  Maßräume mit $\sigma$-endlichen Maßen. Weiterhin seien $f_i:
  [\Omega_1, \CF_i]\rightarrow[\R,\FB], f_i\geq 0, \int f_i
  d\mu_i=1$\footnote{d.\,h. $f_i$ ist Wahrscheinlichkeitsdichte
    bezüglich $\mu_i$ bzw. $\mu_i f_i$ ist Wahrscheinlichkeitsmaß.}.
  Setzen $f(\omega_1,\ldots,\omega_n):=\prod_{i=1}^n
  f_i(\omega_i)$. Dann gilt:
  \[\mu f=\times_{i=1}^n \mu_i f_i\quad\text{wobei }
  \mu=\times_{i=1}^n \mu_i\]
\end{satz}
\begin{proof}
  \begin{align*}
    \mu f(\times_{j=1}^n A_j)&=\int\chi_{\times_{j=1}^n A_j}
    d(x\mu_i)f\\
    &= \int\cdots\int \underbrace{\chi_{\times_{j=1}^n
        A_j}}_{\prod_{j=1}^n \chi_{A_j} (\omega_j)} (\omega_1,\ldots,
    \omega_n) \mu_1(d\omega_1)\ldots \mu_n(d\omega_n)\\
    &= \int\cdots\int \prod_{j=1}^n \chi_{A_j} (\omega_j)
    \prod_{k=1}^n f_k(\omega_k) \mu_1(d\omega_1)\ldots
    \mu_n(d\omega_n)\\
    &= \int\cdots\int \prod_{j=1}^n (\chi_{A_j}
    (\omega_j) f_j(\omega_j)) \mu\ldots\mu\\
    &= \prod_{j=1}^n \int \chi_{A_j}(\omega_j) f_j(\omega_j)
    \mu_j(d\omega_j) = \prod_{j=1}^n \mu_j f_j(A_j)
  \end{align*}
\end{proof}


\chapter{Wahrscheinlichkeitsräume}

Das Anliegen dieses Kapitels ist es, die Grundtypen von
Wahrscheinlichkeitsräumen zu vermitteln und die Bildung von Modellen
zu trainieren. Weiterhin werden wichtige Typen von Verteilungen auf
$[\R,\FB]$ abgeleitet.

\section{Der klassische Wahrscheinlichkeitsraum}

Dieser Wahrscheinlichkeitsraum war der erste, der in der Historie
benutzt wurde. Wie schon in der Einleitung geschrieben, haben Spieler
die Wahrscheinlichkeit zuerst gefühlsmäßig wahrgenommen und versucht,
dies genauer zu errechnen. Dies geschah um im Anschluß eventuell eine
Strategie zu entwickeln. Im 18.~Jahrhundert kam dann ein französischer
Mathematiker auf die Idee, den Zufall mit gleichen
Wahrscheinlichkeiten zu modellieren. Dies ist die Wurzel des
klassischen Wahrscheinlichkeitsraumes.

Bei der Betrachtung ergibt sich die folgende Situation: Gegeben sei
ein Experiment oder Versuch mit $n$-möglichen Ergebnissen. Dabei
bezeichnet das Wort "`Experiment"' die prinzipielle Möglichkeit der
Wiederholung. Weiterhin darf kein bestimmtes Ereignis gegenüber einem
anderen bevorzugt werden. Abstrakt hat man dann,
$\Omega=\{\omega_1,\ldots, \omega_n\}$ als die Klasse der möglichen
Ereignisse. Weiterhin ist $\CF=\FP(\Omega)$ das System aller möglichen
Ereignisse und $A\in\CF$ ist eine Aussage über ein Ereignis.

\begin{beispiel}
  \begin{enumerate}
  \item Münzwurf: $\Omega=\{W, Z\}=\{0,1\}$
  \item Würfel: $\Omega=\{1,2,3,4,5,6\}$
  \end{enumerate}
\end{beispiel}

Ein Ansatz für ein Wahrscheinlichkeitsmaß ist:
$P(A)=\frac{\#A}{\#\Omega}$. Dieses Modell nimmt an, dass alle
Ereignisse gleich verteilt sind: $P(\{\omega_i\})=P(\{\omega_j\})$ für
alle $i,j=1,2,\ldots$. Das Wahrscheinlichkeitsmaß hier ist:
$P(\Omega)=1 =P(\bigcup_{i=1}^n \{\omega_i\})\sum_{i=1}^n 1=
P(\{\omega_k\}) n\Rightarrow
P(\{\omega_k\})=\frac{1}{n}=\frac{1}{\#\Omega}$. Somit folgt dann
$P(A)= \sum_{\omega\in A} P(\{\omega\})=\sum_{\omega\in A}
\frac{1}{\#\Omega} =\frac{\# A}{\#\Omega}$.

\begin{definition}
  Ein Wahrscheinlichkeitsraum $\ofp$ heißt
  \highl[Wahrscheinlichkeitsraum!klassischer]{klassisch}, wenn gilt
  \begin{itemize}
  \item $\Omega$ ist endlich und nichtleer
  \item $\CF=\FP(\Omega)$
  \item $P(A)=\frac{\#A}{\#\Omega}\,(A\in\CF\Leftrightarrow A\subseteq
    \Omega)$
  \end{itemize}
\end{definition}

\begin{remark}
  In klassischen Wahrscheinlichkeitsräumen wird $P$ in der Regel wie
  folgt dargestellt: Sei $\mu$ ein Zählmaß auf $\Omega$. Dann kann man
  das \highl[Zählmaß!normiertes]{normierte Zählmaß} als
  $P(A)=\frac{1}{\mu(\Omega)} \mu$ darstellen. Allgemeiner gilt: Sei
  $\mu$ ein endliches Maß auf $[\Omega,\CF]$ mit $\mu(\Omega)=0$. Dann
  kann man $Q:=\frac{1}{\mu(\Omega)}\mu$ setzen.
\end{remark}

\begin{beispiel}
  \begin{enumerate}
  \item Münzwurf: $\Omega=\{0,1\}\Rightarrow
    P(\{\omega\})=\frac{1}{2}$
  \item Würfel: $\Omega=\{1,\ldots,6\}\Rightarrow
    P(\{\omega\})=\frac{1}{6}$
  \item "`kompliziertere"' Modelle:
    \begin{itemize}
    \item $n$-facher Münzwurf: "`Menge der möglichen Protokolle"'
      $\Omega=\{[\omega_1, \ldots, \omega_n]|\omega_i\in\{0,1\}, i=1,
      \ldots, n\}$
    \item $n$-mal Würfeln: $\Omega=\{1,\ldots,6\}^{n\times}$
    \end{itemize}
  \end{enumerate}
\end{beispiel}


\paragraph{Kombinatorische Formeln}

Dies stellt nur einen Einschub dar. In der Regel sollten diese Regeln
dem Student bekannt sein.

Die kombinatorischen Formeln drücken aus, wieviele Möglichkeiten der
Auswahl von $k$ Elementen einer $n$-elementigen Menge
existieren. Dabei kann man sich vorstellen, dass aus eine Urne mit
insgesamt $n$ numerierten Kugeln nach bestimmten Kriterien Kugeln
gezogen werden.
\begin{enumerate}[1. V{a}r{i}{a}nte]
\item Auswahl \emph{mit} Berücksichtigung der Reihenfolge und
  \emph{mit} Zurücklegen\\
  $A_{k,n}^{mWmR}:= \{[\omega_1,\ldots, \omega_k]|
  \omega_i\in \{1,\ldots, n\}, i=1,\ldots, n\}= \{1,\ldots,n\}^k
  \Rightarrow A_{k,n}^{mWmR}=n^k$
\item Auswahl \emph{mit} Berücksichtigung der Reihenfolge und
  \emph{ohne} Zurücklegen $(k\leq n)$\\
  $A_{k,m}^{oWmR}:=\{[\omega_1,\ldots,\omega_k]| \{\omega_1,\ldots,
  \omega_k\}\in \{1,\ldots,n\}\}=n(n-1)(n-2)\ldots (n-k+1)=
  \frac{n!}{(n-k)!}$
\item Auswahl \emph{ohne} Berücksichtigung der Reihenfolge und
  \emph{ohne} Zurücklegen\\
  $A_{k,n}^{oWoR}:=\{\{\omega_1,\ldots,\omega_k\}| \{\omega_1, \ldots,
  \omega_k\} \subseteq \{1,\ldots,n\}\}$\\
  $=\#A_{k,n}^{oWmR}\frac{1}{k!}=
  \frac{n!}{k!(n-k)!} =\binom{n}{k}$
\item Auswahl \emph{ohne} Berücksichtigung der Reihenfolge und
  \emph{mit} Zurücklegen\\
  $A_{k,n}^{oWmR}=\binom{k+n}{n}$
\end{enumerate}

\begin{beispiel}
  $n$-facher Münzwurf: $\Omega=\{0,1\}^n =A_{n,2}^{mWmR}\Rightarrow
  \#\Omega=2^n$. Wir betrachten das Ereignis $k$-mal wird Zahl
  geworfen. Dies entspricht $\{[\omega_1,\ldots,\omega_n]\in\Omega|
  k=\sum_{i=1}^n \omega_i\}=A_{k,n}^{oWoR}\Rightarrow
  P("`A_k"')=\frac{\#A_k}{\#\Omega}=\frac{\binom{n}{k}}{2^n}$.

  Das Ereignis $B_k$ entspricht der Aussage "`beim $k$-ten Wurf wird
  erstmalig Zahl geworfen"':
  \[B_k=\{[\omega_1,\ldots,\omega_n]\in\Omega| \omega_1, \ldots,
  \omega_{k-1}, \omega_k=1\}\]
  \[P("`B_k"')=\frac{\# B_k}{\#\Omega}=\frac{2^{n-k}}{2^n}=2^{-k}\]
\end{beispiel}


\subsubsection{Hypergeometrisches Modell}

Zentrum der Betrachtung ist in der Regel eine Urne. Dabei steht dies
für ein Behältnis, dass von außen nicht einsehbar ist und in dem alle
Kugeln gleiche Oberflächeneigenschaften haben.\footnote{Untersuchungen
während des Zweiten Weltkrieges ergaben, dass Menschen Kugeln anhand
ihrer Oberflächen unbewusst wiedererkennen und dann Präferenzen
ausbilden. Dies sollte bei den Experimenten immer ausgeschlossen werden.}

In der Urne befinden sich $N$ Kugeln. Darunter sind $M$\footnote{Wobei
gilt: $M\leq N$.} rote und $N-M$ weiße Kugeln. Nun werden aus der Urne
"`auf gut Glück"' $n$ Kugeln herausgegriffen. Wir betrachten das
Ereignis $A_k$, welches für "`genau $k$ der $n$ Kugeln sind rot"'
steht. Als Hilfe nehmen wir an, dass die Kugeln nummeriert sind und
zwar von 1 bis $M$ für die roten und $M+1$ bis $N$ für die weißen
Kugeln. Mathematisch wird dies wie folgt ausgedrückt:
\begin{align*}
  \Omega:= \{\{\varepsilon_1,\ldots,\varepsilon_n\}| \{\varepsilon_1,
  \ldots, \varepsilon_n\}\subseteq \{1, \ldots, N\}\}\\
  \CF =\FP(\Omega)\qquad P(A):=\frac{\# A}{\# \Omega}\\
  A_k = \{\{\varepsilon_1, \ldots, \varepsilon_n\}|\# (\{\varepsilon_1,
  \ldots, \varepsilon_n\}\cap \{1, \ldots, M\})=k\wedge \\
  \qquad \# (\{\varepsilon_1, \ldots, \varepsilon_n\} \cap \{M+1,
  \ldots, N\})=n-k\}
\end{align*}
Somit ist $A_k=\emptyset$, wenn $k<0\vee k>M\vee k>n\vee n-k>N-M
\Leftrightarrow k<n+M-N$. Wir betrachten nun, $\max(0, n+M-N)\leq
k\leq \min(M,n)$.
\begin{align*}
  \#A_k &= \# A_{k,M}^{oWoR}\cdot \# A_{n-k, N-M}^{oWoR}-\binom{M}{k}
  \binom{N-M}{n-k}\\
  \#\Omega &= \# A_{n,M}^{oWoR}=\binom{N}{n}\\
  \Rightarrow P(A_k) &=
  \begin{cases}
    \frac{\binom{M}{k}\binom{N-M}{n-k}}{\binom{N}{n}} & k \text{ mit
    }\max(0, n+M-N)\leq k\leq \min(M,n)\\
    0 & \text{sonst}
  \end{cases}
\end{align*}

Aus der obigen Tatsache folgt nun: 
\[\sum_{k=\max(0,n+M-N)}^{\min(M,n)} \binom{M}{k} \binom{N-M}{n-k} =
\binom{N}{n}\] 
Zum Beweis dieser Aussage wissen wir, dass $A_k\cap A_j=\emptyset$ und
$\bigcup_{k} A_k=\Omega$\footnote{Für $k$ gilt $\max(0, n+M-N)\leq
k\leq \min(M,n)$} ist. Somit folgt, dass $P(\Omega)=1=\sum_k P(A_k)=
\sum \frac{\binom{M}{k}\binom{N-M}{n-k}}{\binom{N}{n}}$. 

Weiterhin kann man schlussfolgern:
\[H_{N,M,n}:= \sum_{k=\max(0,n+M-n)}^{\min(M,n)}
\frac{\binom{M}{k}\binom{N-M}{n-k}}{\binom{N}{n}} \delta_k\]
ist ein Wahrscheinlichkeitsmaß auf $[\R,\FB]$.


\section{Das Bernoullischema}

\begin{definition}
  Seien $n\geq 1$ mit $n\in\N$ und $0\leq p\leq 1$. Der
  Wahrscheinlichkeitsraum $\ofp=[\{0,1\},\FP(\{0,1\}),
  p\delta_1 + (1-p)\delta_0]^{n\times}$ heißt (endliches)
  \highl{Bernoullischema} zu $n$ Versuchen mit der
  Erfolgswahrscheinlichkeit $p$.
\end{definition}

\begin{remark}
  \setcounter{enumi}{-1}
  \begin{enumerate}
  \item $\ofp=[\{0,1\}^{n\times},\FP(\{0,1\}^{n\times}),
    (p\delta_1 + (1-p)\delta_0)^{n\times}]$
  \item $\Omega= \{[\omega_1,\ldots, \omega_n]|\omega_i\in \{0,1\},
    i=1,\ldots,n\}$
  \item $\CF=\FP(\Omega)$
  \item $P(\{[\omega_1,\ldots,\omega_n]\})= (p\delta_1+ (1-p)
    \delta_0)^{n\times} (\times_{i=1}^n \{\omega_i\})= \prod_{i=1}^n
    (p\delta_1+ (1-p)\delta_0)(\{\omega_i\})= \prod_{i=1}^n
    p^{\omega_i} (1-p)^{1-\omega_i}= p^{\sum \omega_i} (1-p)^{n-\sum
      \omega_i}$
  \end{enumerate}
\end{remark}

\begin{beispiel}
  Als Beispiel soll der $n$-fache Münzwurf mit einer
  Erfolgswahrscheinlichkeit von $p=\nicefrac{1}{2}$ dienen. Wir
  betrachten die Ereignisse $A_k$ und $B_k$. Dabei entspricht $A_k$
  der Aussage "`$k$-mal Erfolg bei $n$ Versuchen"' und $B_k$ der
  Aussage "`erstmalig beim $k$-ten Versuch Misserfolg"'.
  \begin{align*}
    A_k &= \{[\omega_1,\ldots, \omega_n]| \sum \omega_i=k\}\\
    B_k &= \{\underbrace{[1,\ldots,1,0]}_{\{0,1\}^k}\times
    \{0,1\}^{n-k}\\
    P(A_k) &= \sum_{[\omega_1,\ldots,\omega_n]\in A_k}
    p^{\sum\omega_i} (1-p)^{n-\sum\omega_i}\\
    &= \sum p^k (1-p)^{n-k}= p^k(1-p)^{n-k} \#A_k\\
    &= p^k(1-p)^{n-k}\binom{n}{k}
  \end{align*}
  Damit folgt, dass $\sum_{k=1}^n p^k(1-p)^{n-k}\binom{n}{k}=1$. Denn
  die $A_k$ sind disjunkt und ihre Vereinigung ergibt gerade $\Omega$.
\end{beispiel}

Die \highl{Binomialverteilung} zu $n$ Versuchen mit
der Erfolgswahrscheinlichkeit $p$ ist wie folgt definiert:
\[B_{n.p} := \sum_{k=0}^n \binom{n}{k} p^k (1-p)^{n-k} \delta_k\]
\begin{beispiel}
  Weiterführung des obigen Beispiels:\\
  Es ist klar, dass $P(B_0)=0$. Für $P(B_k)$ gilt:
  \begin{align*}
    P(B_k) &= (p\delta_1 + (1-p)\delta_0)^{n\times} (\{[1, \ldots, 1,
    0]\} \times \{0,1\}^{n-k})\\
    &= (p\delta_1+ (1-p)\delta_0)^{k\times} (\{[1,\ldots, 1,0]\})
    (p\delta_1+
    \underbrace{(1-p)\delta_0)^{n-k}(\{0,1\}^{n-k})}_{=1}\\
    &= p^{k-1} (1-p)
  \end{align*}
  Zur Erinnerung: Die Verteilung
  \[G_p := \sum_{k=1}^\infty p^k (1-p)\delta_k\]
  heißt \highl[Verteilung!geometrische]{geometrische Verteilung}.
\end{beispiel}


\paragraph{Strategien beim wiederholten Glücksspiel}

Im Rahmen derartiger Spiele wird beabsichtigt, einen Gewinn $K$ zu
erzielen. Dabei kann man mit folgender Strategie vorgehen:
\begin{enumerate}[1. Schr{i}tt]
\item Setze $K$. Bei Erfolg wird der Gewinn $K$
  realisiert. Andernfalls gehe zum nächsten Schritt.
\item Setze $(1+1)K$. Bei Erfolg wird Gewinn $K$
  realisiert. Andernfalls gehe zum nächsten Schritt.
\item Setze $(1+2+1)K$. Bei Erfolg wird Gewinn $K$
  realisiert. Andernfalls gehe zum nächsten Schritt.
\end{enumerate}
Im $n$ Schritt wird also immer ein Betrag von $2^{n-1}K$ gesetzt und
im Gewinnfall wird schliesslich $K$ ausgeschüttet.

Im Falle $n=10$ sind $512K$ einzusetzen. Sei nun
$p=\nicefrac{1}{2}$. Dann ergibt sich eine Gewinnwahrscheinlichkeit
von $(1-p)\sum_{k=0}^9 p^k=(1-p)\frac{(1-p^9)}{(1-p)}=1-p^9\approx
0,999$

\begin{beispiel}
  \begin{description}
  \item[radioaktiver Zerfall] Wir betrachten die Zeit $t$. Zu Beginn
      dieser Zeit existieren $n$ Atome des  radioaktiven
      Stoffes. Diese sind mit $1,\ldots, n$ nummeriert. Das Ereignis
      $\omega_k=1$ entspricht der Aussage, "`Atom $k$ ist in der Zeit
      $t$ zerfallen"' und das Ereignis $\omega_k=0$ entspricht der
      Aussage, "`Atom $k$ ist in der Zeit $t$ nicht zerfallen"'. Die
      Anzahl der zerfallenen Atome berechnet sich durch $\sum_{k=1}^n
      \omega_k$. 
    \item[Genetik] Hierbei wird die Teilung eines DNA-Stranges
      beobachtet. Solch ein Strang teilt und "`kopiert"' sich. Dabei
      entsteht keine 1:1-Kopie, sondern an endlich vielen Stellen
      treten Fehler auf. Nach Ansicht der Neodarwinisten erfolgt auf
      diesem Wege die Evolution.

      Insgesamt existieren $n$ Verbindungen\footnote{Beim Mensch
        entspricht das $n$ ungefähr 100~Millionen.}. Das Ereignis
      $\omega_k=1$ entspricht einem Fehler an der $k$-ten Stelle und
      das Ereignis $\omega_k=0$ sagt aus, dass an der $k$-ten Stelle
      kein Fehler ist.
  \end{description}

  Will man nun bei beiden Beispielen versuchen, nach dem bisher
  gelernten die Wahrscheinlichkeit zu errechnen, wird man sehr schnell
  auf Probleme stossen. So ist im Beispiel der Genetik
  $P("`k\text{-mal Erfolg}"')=\binom{n}{k} p^k (1-p)^{n-k}= 
  \binom{100000000}{25} p^{25} (1-p)^{\approx
    100000000}\approx\infty\cdot 0$.
\end{beispiel}


\section{Der Poissonsche Grenzwertsatz}

Wir betrachten die Binomialverteilung $B_{n,p}$, d.\,h. 
\[B_{n,p}(\{k\})=
\begin{cases}
  \binom{n}{k} p^k (1-p)^{n-k} & 0\leq k\leq n\\
  0 & \text{sonst}
\end{cases}\]
oder
\[B_{n,p}=\sum_{k=0}^n \binom{n}{k} p^k (1-p)^{n-k}\delta_k\]
und weiter die Poissonverteilung
\[\Pi_\lambda(\{k\})=\frac{\lambda^k}{k!}e^{-\lambda}\qquad(\lambda>0)\]

\begin{satz}[Poissonsche Grenzwertsatz]
  Sei $(p_n)_{n=1}^\infty$ eine Folge mit $0\leq p\leq 1 \,(\forall
  n=1,2,\ldots)$ und $0<\lambda=\lim_{n\rightarrow\infty} np_n$.
  Dann gilt:
  \[\lim_{n\rightarrow\infty} B_{n,p} (\{k\})=\Pi_\lambda(\{k\})\quad
  (k=0,1,\ldots)\]
\end{satz}
\begin{proof}
  in der Übung
\end{proof}

\begin{beispiel}
  Seien $n=10^{11}, p=c\cdot 10^{-11}\Rightarrow np=c$. Es gilt die
  Annahme, $B_{n,p}(\{k\})\approx \Pi_c(\{k\})$. Wir betrachten
  $k=0,1,2,3,4,5$:
  \[B_{n,p}(\{p\})\approx \Pi_1(\{5\})=\nicefrac{1}{5!}e^{-1}=
  \nicefrac{1}{120\cdot e}\leq \nicefrac{1}{250}\]
\end{beispiel}


\section{Ein kontinuierliches Analogon des klassischen Wahrscheinlichkeitsraumes}

Im kontinuierlichen Modeel ist $\Omega\subseteq \R^n, 0\leq \ell^{(n)}
(\Omega)< +\infty$:
\[\Rightarrow Gl_\Omega :=\frac{\ell^{(n)}}{\ell^{(n)}(\Omega)}\]


\chapter{Bedingte Wahrscheinlichkeit und Unabhängigkeit}

\section{Bedingte Wahrscheinlichkeit}

Wenn man einer Lottoziehung zuschaut und feststellt, dass man
nacheinander die gezogenen Zahlen auch getippt hat, verändern sich die
Wahrscheinlichkeiten kontinuierlich. Im folgenden soll dies erklärt
werden. 

Im folgenden gilt immer, dass $\ofp$ ein
Wahrscheinlichkeitsraum ist.

\begin{definition}
  Seien $A,B\in\CF$ mit $P(B)>0$. Dann heißt
  \[P(A|B):=\frac{P(A\cap B)}{P(B)}\]
  \highl[Wahrscheinlichkeit!bedingte]{bedingte Wahrscheinlichkeit} von
  $A$ unter der Bedingung $B$.
\end{definition}

\begin{beispiel}
  Als Beispiel dient hier wieder der Würfel:\\
  \[\Omega=\{1,2,3,4,5,6\}, P(\{k\})=\nicefrac{1}{6}, k\in\Omega,
  A=\{6\}, B=\{4,5,6\}\]
  \[P(A|B)=\frac{P(\{6\}\cap\{4,5,6\})}{P(\{4,5,6\})}=
  \frac{P(\{6\})}{P(\{4,5,6\})}=
  \frac{\nicefrac{1}{6}}{\nicefrac{3}{6}}= \frac{1}{3}\]
\end{beispiel}

\begin{satz}[Elementare Eigenschaften]
  Sei $B\in\CF, P(B)>0$. Dann gelten:
  \begin{enumerate}[(a)]
  \item $0\leq P(A|B)\leq 1$
  \item $P(B|B)=P(\Omega|B)=1$
  \item $P(\bigcup_{i=1}^\infty A_i|B)=\sum_{i=1}^\infty P(A_i|B)$ für
    alle $(A_i)_{i=1}^\infty\in\CF$ mit $A_i\cap A_j\neq\emptyset$
    ($i\neq j$)
  \end{enumerate}
\end{satz}
\begin{proof}
  \begin{enumerate}[(a)]
  \item Nach der Definition ist klar, dass $P(A|B)\geq 0$
    gilt. Weiterhin ist $A\cap B\subseteq B$. Nach der Monotonie von
    $P$ folgt damit, dass $P(A\cap B)\leq P(B)$. Durch Umstellen
    erhält man nun $\frac{P(A\cap B)}{P(B)}\leq 1$.
  \item $P(\Omega|B)=\frac{P(B)}{P(B)}=1=P(B|B)$
  \item Seien die $(A_i)$ aus $\CF$ paarweise disjunkt. Dann sind auch
    $(A_i\cap B)$ paarweise disjunkt und es folgt,
    $P(\bigcup_{i=1}^\infty (A_i\cap B))=\sum_{i=1}^\infty P(A_i\cap
    B)$. Somit hat man damit:
    \[\frac{P(\bigcup_{i=1}^\infty A_i \cap B)}{P(B)}=
    \frac{\sum_{i=1}^\infty P(A_i\cap B)}{P(B)}\]
  \end{enumerate}
\end{proof}

\begin{satz}[Formel bzw. Satz über die totale Wahrscheinlichkeit]
\label{satz:totalWahr}
  Sei $(B_i)_{i\in I}$ eine abzählbar messbare Zerlegung von
  $\Omega$. Dann gilt:
\[P(A)=\sum_{i\in I} P(A|B_i)P(B_i)\qquad A\in\CF\footnote{zusätzlich
  muss $P(B_i)>0$ gefordert werden.}\]
\end{satz}
\begin{proof}
  \begin{align*}
    \sum_{i\in I} P(A|B_i)P(B_i)&= \sum_{i\in I} \frac{P(A\cap
      B_i)}{P(B_i)} P(B_i)&= \sum_{i\in I} P(A\cap B_i)\\
    P\left(\bigcup_{i\in I} (A\cap B_i)\right)&= P\left(A\cap
      \bigcup_{i\in I} B_i\right) &= P(A\cap \Omega)=P(A)
  \end{align*}
\end{proof}

\begin{remark}
  Die zusätzliche Forderung $P(B_i)>0$ ist nicht notwendig, da
  $P(A|B_i) \cdot P(B_i)=0$. In dem Zusammenhang ist die Forderung
  jedoch  sinnvoll.
\end{remark}

\begin{satz}[Satz von Bayes]
  Sei $(B_i)_{i\in I}$ eine messbare Zerlegung von $\Omega$ und
  $A\in\CF$ mit $P(A)>0$. Dann gilt:
  \[P(B_i|A)=\frac{P(A|B_i)\cdot P(B_i)}{\sum_{j\in I} P(A|B_j)\cdot
    P(B_j)} \qquad i\in I\]
  Für den Spezialfall $A,B\in\CF$ und $P(A)>0$ gilt:
  \[P(B|A)= \frac{P(A|B)\cdot P(B)}{P(A|B)P(B)+P(A|B^c)P(B^c)}\]
\end{satz}
\begin{proof}
  Aus \autoref{satz:totalWahr} folgt $P(A)= \sum_{j\in I} P(A|B_j)
  P(B_j)$. Weiter folgt aus der Definition der bedingten
  Wahrscheinlichkeit: $P(A|B_i)P(B_i)= \frac{P(A\cap B_i)}{P(B_i)}
  \cdot P(B_i)=P(A\cap B_i)$. Aus beiden schließlich ergibt sich:
  \[\frac{P(A|B_i)\cdot P(B_i)}{\sum_{j\in I} P(A|B_j)P(B_j)}=
  \frac{P(A\cap B_i)}{P(A)}= P(B_i|A)\]
\end{proof}

\begin{beispiel}
  In einem BSE-Schnelltest werden 100~Millionen Kühe getestet. Davon
  sind 100 Kühe an BSE erkrankt und bei nahezu allen ist der Test auch
  positiv. Aber bei 0,001\,\% der gesunden Kühe schlägt der Test auch
  an. Wir betrachten folgende Ereignisse:
  \begin{itemize}
  \item $A=$ "`Test positiv"' $\Rightarrow A^c=$ "`Test negativ"'
  \item $B=$ "`Kuh hat BSE"' $\Rightarrow B^c=$ "`Kuh hat kein BSE"'
  \item "`Testzuverlässigkeit"': $P(A|B)\approx 1-10^{-3}=99,9$\,\%
    Dies entspricht der Wahrscheinlichkeit, dass der Test bei einer
    BSE-Kuh auch BSE anzeigt.
  \end{itemize}
\end{beispiel}

% Informationsübertragungsmodell ausgelassen

\section{Lebensdauerverteilungen}

Im folgenden gilt immer: $\Omega=\R_+ =(0, +\infty), \CF=\FB\cap\R_+ =
\{A\subseteq \R_+| A\in\FB\}$

\begin{definition}
  Ein Wahrscheinlichkeitsmaß auf $[\Omega, \CF]$ heißt
  \highl{Lebensdauerverteilung}.
  \begin{itemize}
  \item Sei $Q=\sum_{i=1}^\infty q_i \delta_i$ ($Q$ ist bezüglich des
    Zähömaßes absolut stetig.). Dann heißt
    \[r_Q(i) := \frac{q_i}{\sum_{j=i}^\infty} \qquad i=1,2, \ldots\]
    \highl{Sterberate} von $Q$.
  \item Sei $Q=\ell f$ mit $F\geq 0, \int fd\ell =1$ ($Q$ ist absolut
    stetig mit Dichte $f$.). Dann heißt
    \[r_Q(x) := \frac{f(x)}{\int_{[x,a)} f(y)\ell (dy)} \qquad
      x\in\R_+\]
    \highl{Sterberate} vom $Q$.
  \end{itemize}
\end{definition}

\begin{satz}
\label{satz:lebensd}
  \begin{enumerate}[(a)]
  \item Sei $Q= \sum_{i=1}^\infty q_i\delta_i$. Dann gilt
    \[r_Q(i)=Q(\{i\}|\{i,i+1,\ldots\}) \qquad i=1,2,\ldots\]
  \item Sei $Q=\ell f$ und $f$ stetig. Dann gilt
    \[r_Q(x)= \lim_{\Delta\rightarrow 0} \nicefrac{1}{\Delta} Q([x,
    x+\Delta)|[x, +\infty)) \qquad x\in\R_+\]
  \end{enumerate}
\end{satz}
\begin{proof}
  \begin{enumerate}[(a)]
  \item $Q(\{i\}|\{j\geq i\})= \frac{Q(\{i\}\cap \{j\geq
      i\})}{Q(\{j\geq i\})}= \frac{Q(\{i\})}{\sum_{j\geq i} Q(\{j\})}=
    \frac{q_i}{\sum_{j\geq i} q_j}= r_Q (i)$
  \item $Q([x,x+\Delta)|[x,+\infty))= \frac{Q([x,x+\Delta)\cap [x,
      +\infty))}{Q([x, +\infty))}= \frac{Q([x, x+\Delta))}{Q(x,
      +\infty)}= \frac{\int_{x, x+\Delta} f(y) \ell (dy)}{\int_{x,
        +\infty} f(y)\ell (dy)}$ Nunmehr werden die Zähler separat
    betrachtet und mit der Verwendung, dass $f$ stetig ist, folgt:
    $\nicefrac{1}{\Delta} \int_{[x,x+\Delta)} f(y)\ell(dy)\cdot
    \nicefrac{1}{\Delta} \int_x^{x+\Delta} f(y)(dy)=
    \nicefrac{1}{\Delta} \left(\int_0^{x+\Delta} f(y)dy\cdot \int_0^x
      f(y)dy\right) \xrightarrow{\Delta\rightarrow 0} f(x)$. Somit
    folgt die Behauptung.
  \end{enumerate}
\end{proof}

\begin{beispiel}[Interpretation zu \autoref{satz:lebensd}]
  \begin{enumerate}[F{a}ll a)]
  \item Der Wert $\{i\}$ bedeutet, dass das Sterbealter $i$ ist und
    $\{i, i+1, \ldots\}$ heißt, dass mindestens das Alter $i$ erreicht
    wird. $\Rightarrow r_Q(\{i\})=P(\text{"`Sterben im Alter i"'}|
    \text{"`Alter i wurde erreicht"'})$
  \item $[x, x+\Delta)$ heißt, Sterben in einem Alter zwischen $x$ und
    $x+\Delta$ und $[x, +\infty)$ bedeutet, dass mindestens das Alter
    $x$ erreicht wird.
  \end{enumerate}
\end{beispiel}

%%Zwei Beispiele weggelassen.

\paragraph{Typisches Verhalten der Sterberate $r_Q$}

\begin{enumerate}
\item technische/mechanische Systeme (Reibung, allg. Abnutzung)
\item Elektronische Bauteile
\item Biologische Systeme (höher organisiert, insbesondere Mensch)
\end{enumerate}


\section{Unabhängigkeit von Ereignissen}

Im folgenden gilt immer, dass $\ofp$ ein
Wahrscheinlichkeitsraum ist.

\begin{definition}
  Seien $A,B\in\CF$. $A$ und $B$ heißen
  \highl{unabhängig}, wenn gilt
  \[P(A\cup B)=P(A)\cdot P(B)\]
\end{definition}

\begin{remark}
  \begin{enumerate}
  \item Sei $P(B)>0\Rightarrow A,B$ unabhängig $\Leftrightarrow P(A|B)
    =P(A)$. Denn $P(A)=P(A|B)=\frac{P(A\cap B)}{P(B)}\Leftrightarrow
    P(A)P(B)=P(A\cap B)$
  \item Sei $P(B)=0\Rightarrow A,B$ unabhängig für beliebige
    $A\in\CF$. Denn $P(B)=0\Rightarrow P(A)P(B)=0; A\cap B\subseteq
    B\Rightarrow 0=P(A\cap B)\leq P(B)=0$
  \item $P(B)=1\Rightarrow A,B$ unabhängig. Zum Beweis hierfür siehe
    auch den folgenden Satz: $P(B)=1\Leftrightarrow P(B^c)=0
    \Rightarrow A,B^c$ unabhängig $\Rightarrow A, (B^c)^c$ unabhängig.
  \end{enumerate}
\end{remark}

\begin{satz}
\label{satz:b-unabh}
  $A,B$ sind genau dann unabhängig, wenn $A,B^c$ unabhängig.
\end{satz}
\begin{proof}
  Rein mengentheoretisch ist klar, dass gilt $A= (A\cap B)\cup (A\cap
  B^c)$ und $(A\cap B)\cap (A\cap B^c)=\emptyset$. 
  \begin{align*}
    P(A) &= P(A\cap B)+P(A\cap B^c)\\
    &= P(A)P(B)+P(A\cap B^c)\\
    P(A\cap B^c) &= P(A)-P(A)P(B)= P(A)(\underbrace{1-P(B)}_{P(B^c)})=
    P(A)P(B^c)
  \end{align*}
\end{proof}

\begin{definition}
  Sei $\FA\subseteq\CF$. Dann heißt $\FA$ \highl{vollständig
    unabhängig}, wenn $\forall \{A_1, A_2, \ldots, A_n\}\subseteq\FA$
    gilt
  \[P\left(\bigcap_{k=1}^n A_k\right)=\prod_{k=1}^n P(A_k)\]
  Weiter heißt $\FA$ \highl{paarweise unabhängig}, wenn $\forall \{A,B\}
  \subseteq\FA$ gilt
  \[P(A\cap B)=P(A)P(B)\]
\end{definition}

\begin{remark}
  \begin{enumerate}
  \item Aus der Tatsache, dass $\FA$ (vollständig) unabhängig ist,
    folgt, dass $\FA$ auch paarweise unabhängig ist. Die Umkehrung
    hingegen ist falsch.
  \item Wenn $\FA=\{A,B\}\Rightarrow\FA$ vollständig unabhängig
    $\Leftrightarrow\FA$ paarweise unabhängig $\Leftrightarrow A,B$
    unabhängig. 
  \item Sei $\FA=\{A_1,A_2,A_3\}$. Aus $P(A_1\cap A_2\cap A_3)= P(A_1)
    P(A_2) P(A_3)$ folgt im allgemeinen nicht, dass $\FA$ vollständig
    unabhängig ist.
  \item $\FA$ ist genau dann vollständig unabhängig, wenn $\forall
    \{A_1, \ldots, A_n\}\subseteq A$ gilt: $A_1, \bigcap_{k=2}^n A_k$
    unabhängig. Beweisidee: Die betrachtete Unabhängigkeit heißt:
    $P(A_1\cap \bigcap_{k=2}^n A_k)=P(A_1)P(\bigcup_{k=2}^n A_k)$.
  \item $\FA$ ist genau dann vollständig unabhängig, wenn $\forall
    \{A_1, \ldots, A_n\}\subseteq A$ gilt, dass $\{A_1, \ldots, A_n\}$
    vollständig unabhängig. Dabei werden keine beliebigen Systeme
    betrachtet, sondern wir beschränken uns auf abzählbare Mengen
  \end{enumerate}
\end{remark}

\begin{beispiel}
  Sei $A\in\CF, P(A)\notin\{0,1\}$. Setzen $A_1=A_2=A, A_3=\emptyset
  \Rightarrow P(A_1\cap A_2\cap A_3)= P(A_1)P(A_2)P(A_3)=0$. Aber
  $P(A_1\cap A_2)=P(A)\neq (P(A))^2=P(A_1)\cdot P(A_2)$
\end{beispiel}

\begin{satz}
\label{satz:vollun-432}
  Wenn $\FA$ vollständig unabhängig ist, dann ist $\tilde{\FA}:=
  \{A^c| A\in\FA\}$ vollständig  unabhängig.
\end{satz}
\begin{proof}
  Nach der obigen Bemerkung 5 genügt es,  für den Fall $\FA= \{A_1,
  \ldots, A_n\}$ die Unabhängigkeit zu zeigen. Dies ist jedoch nach
  der Bemerkung 4 und \autoref{satz:b-unabh} klar.
\end{proof}

\begin{satz}
\label{satz:unendlEreignisse}
  Sei $(A_i)_{i=1}^\infty$ vollständig unabhängig und
  $\sum_{i=1}^\infty P(A_i)=+\infty$. Dann gilt:
  \[P\left(\bigcap_{n=1}^\infty \bigcup_{i\geq n} A_i\right)=1\]
\end{satz}
\begin{proof}
  Es genügt zu zeigen, dass $P((\bigcap_{n=1}^\infty \bigcup_{i\geq n}
  A_i)^c) =0$. Es ist klar, dass $P((\bigcap_{n=1}^\infty \bigcup_{i\geq n}
  A_i)^c) = P(\bigcup_{n\geq 1} \bigcap_{i\geq n} A_i^c)\leq
  \sum_{n\geq 1} P(\bigcup_{i\geq n} A_i^c)$. Damit genügt zu zeigen,
  dass $P(\bigcup_{i\geq n} A_i^c)=0= \bigcup_{k=n}^\infty
  \bigcup_{i=n}^k A_i^c\Rightarrow P(\bigcup_{i\geq n} A_i^c)=
  P(\bigcup_{k=n}^\infty \bigcup_{i=n}^k A_i^c)$.  Wegen der
  Stetigkeit von oben ist $\lim_{k\rightarrow\infty} P(\bigcup_{i=n}^k
  A_i^c)$. Da $A_i$ unabhängig ist, ist auch $A_i^c$ unabhängig. Daraus
  folgt, dass $\lim_{k\rightarrow\infty} \prod_{i=n}^k P(A_i^c)=
  \lim_{k\rightarrow\infty} \prod_{i=n}^k (1-P(A_i))$. Somit genügt es
  noch zeigen, dass $\sum_{i=n}^\infty
  -\ln(1-P(A_i))=+\infty$. Weiterhin wissen wir, dass $\sum
  P(A_i)=+\infty$. Somit muss nur noch gezeigt werden, dass
  $-\ln(1-P(A_i))\geq P(A_i)$. Wir haben allgemein somit die
  Ungleichung $-\ln (1-x)\geq x$ oder $e^{-\ln(1-x)}\geq e^x$. Durch
  die Ableitung erhält man: $\frac{1}{1-x}\geq 1$ für $0\leq x\leq 1$.
\end{proof}

\begin{beispiel}
  Man stelle sich ein unendliches Lottospiel vor. Dann entspricht
  $A_i$ der Aussage, dass beim $i$-ten Spiel 6 Richtige gezogen
  werden und $\bigcap_{n=1}^\infty \bigcup_{i\geq n} A_i$ entspricht
  unendlich oft 6 Richtigen. Es ist klar, dass die $A_i$ unabhängig
  sind und dass $P(A_i)=c>0$.
\end{beispiel}

\begin{remark}
  Ohne die Forderung nach Unabhängigkeit gilt die Aussage in
  \autoref{satz:unendlEreignisse} im Allgemeinen nicht. Denn wähle
  beispielsweise $A_i=A$ und sei $P(A)\notin\{0,1\}$. Dann ist die
  Summe aller $A_i=+\infty$, aber $P(\bigcup_{n=1}^\infty
  \bigcap_{i\geq n})=P(A)$ ist ungleich eins.
\end{remark}

\begin{satz}
  Sei $(A_i)_{i=1}^\infty$ eine Folge aus $\CF$ mit $\sum_{i\geq 1}
  P(A_i)< +\infty$. Dann gilt
  \[P\left(\bigcup_{n\geq 1} \bigcap_{i\geq n} A_i\right) =0\]
\end{satz}
\begin{proof}
  Trivialerweise ist: $\bigcup_{n\geq 1} \bigcap_{i\geq n}
  A_i\subseteq \bigcap_{i\geq k} A_i$.
  \begin{align*}
    &\Rightarrow 0\leq P\left(\bigcup\bigcap A_i\right)\leq
    P\left(\bigcap_{i\geq k} A_i\right)\\
    &\Rightarrow 0\leq P\left(\bigcup\bigcap A_i\right)\leq
    \lim_{k\rightarrow\infty} P\left(\bigcup_{i\geq k} A_i\right)\\
    P\left(\bigcup_{i\geq k} A_i\right) &\leq \sum_{i\geq k} P(A_i)
    \xrightarrow{k\rightarrow\infty} 0
  \end{align*}
  Beim letzten Schritt wurde die Subadditivität ausgenutzt.
\end{proof}

\begin{remark}
  Aus den obigen beiden Sätzen kann folgende Folgerung gemacht
  werden: Sei $(A_i)_{i=1}^\infty$ vollständig unabhängig. Dann gelten:
  \begin{inparaenum}[a)]
  \item $P(\bigcap_{n\geq 1} \bigcup_{i\geq n} A_i)\in \{0,1\}$ ($0,1$-Gesetz)
  \item $P(\bigcap_{n\geq 1} \bigcup_{i\geq n} A_i)\Leftrightarrow
    P(A_i) =+\infty$ (Lemma von Borel-Cantelli)
  \end{inparaenum}
\end{remark}


\chapter{Zufallsgrößen}

\section{Zufallsvariablen}

\begin{definition}
  Seien $\ofp$ ein Wahrscheinlichkeitsraum und $[X,\FX]$ ein
  messbarer Raum.
  \begin{itemize}
  \item Eine messbare Abbildung $\xi: [\Omega,\CF]\rightarrow[X,\FX]$
    heißt \highl{Zufallsvariable} über $[\Omega,
    \CF, P]$ mit Werten in $[X,\FX]$.
  \item $P_\xi:=\pxi$ heißt
    \highl{Verteilungsgesetz} von $\xi$.
  \item Ist $[X,\FX]=[\R,\FB]$, so heißt $\xi$
    \highl{Zufallsgröße}.
  \end{itemize}
\end{definition}

Um sich dies etwas genauer vorzustellen, kann man annehmen, dass man
ein zufälliges System hat. Dies ist ein Vorgang, der bei mehreren
Versuchen wechselwirkungsfrei andere Ergebnisse bringt. Als
Realisierung erhält man $\omega\in\Omega$. Daraus folgt die Abbildung
$\xi(\omega)$. Das $P$ muss die Verteilung $P_\xi$ haben. Das Anliegen
der Statistik ist die Bestimmung von $P_\xi$.

\begin{beispiel}
  \begin{align*}
    \ofp &= [\{0,1\}, \FP(\{0,1\}), p\delta_1+ (1-p)
    \delta_0]^{n\times}\\
    \Omega &= \{[\varepsilon_1, \ldots, \varepsilon_n]| \varepsilon_i
    \in \{0,1\}, i=1,\ldots,n\}
  \end{align*}
  Nun fixieren wir ein $i$ und betrachten das Tupel
  $\xi_i([\varepsilon_1, \ldots, \varepsilon_n])=\xi_i$. Es ist klar,
  dass $\xi_i$ eine Zufallsgröße ist.
  \begin{align*}
    P_{\xi_i}(A) &= P(\xi^{-1}_i(A))\\
    \xi^{-1}_i &= \{[\varepsilon_1, \ldots, \varepsilon_n]|
    \xi_i([\varepsilon_1, \ldots, \varepsilon_n])\in A\}=
    \{0,1\}^{(i-1)\times} \times A \times \{0,1\}^{(n-1)\times}\\
    P(\xi^{-1}_i (A)) &= P(\{0,1\}^{(i-1)\times} \times A \times
    \{0,1\}^{(n-1)\times})\\
    &= (p\delta_1+(1-p)\delta_0)^{n\times} (\{0,1\}^{(i-1)\times}
    \times A\times \{0,1\}^{(n-1)\times})\\
    &= \underbrace{(p\delta_1+(1-p)\delta_0)^{(i-1)\times}
      (\{0,1\})^{(i-1)\times})}_{=1} (p\delta_1+(1-p)\delta_0)A\\
    &\qquad\underbrace{(p\delta_1+(1-p) \delta_0)
      (\{0,1\})^{(n-1)\times})}_{=1} \\
    &= (p\delta_1+(1-p)\delta_0)(A)=B_{1,p}(A)\\
    &\Rightarrow P_{\xi_i}=B_{1,p}=p\delta_1+(1-p)\delta_0
  \end{align*}
  Im allgemeinen Fall gilt: Wenn $P=\times_{k=1}^n P_k$  und $\xi_i$ die
  Projektion in $\times_{k=1}^n P_k$ ist, dann ist $P_{\xi_i}=P_i$
\end{beispiel}

\begin{satz}
  Sei $Q$ ein Wahrscheinlichkeitsmaß auf $[\R, \FB]$ mit stetiger und
  streng monoton wachsender Verteilungsfunktion $F_Q$. Dann ist
  $\xi_i=F_Q^{-1}$ eine \highl{Zufallsgröße} über dem
  Wahrscheinlichkeitsraum $[(0,1),\FB\cap (0,1), Gl_{(0,1)}]$ mit
  $P_\xi=Q$.
\end{satz}
\begin{proof}
  Die strenge Monotonie und Stetigkeit bedingen, dass die
  Umkehrfunktion wieder stetig und damit messbar ist. Weiter ist
  bekannt, dass das Wahrscheinlichkeitsmaß auf $[\R,\FB]$ durch seine
  Verteilungsfunktion eindeutig bestimmt ist. Damit genügt es zu
  zeigen, dass $F_{P_\xi}=F_Q$.

  Dazu ist  nach Definition $F_{P_\xi}=P_\xi((-\infty, x))$. Weiterhin
  ist:
  \begin{align*}
    P_\xi((-\infty, x)) &=  P(\underbrace{\xi^{-1}(-\infty,
      x)}_{\{y\in (0,1)|\xi y\in(-\infty, x)\}=\{y\in(0,1)|
      \xi(y)<x\}}) = P(\{y\in (0,1)|\xi(y)<x\})\\
    &= P(\{y\in(0,1)|F_Q^{-1}(y)<x\})= P(\{y\in(0,1)|y<F_Q(x)\})\\
    &= P((0, F_Q(x)))
  \end{align*}
  Wegen $P=Gl_{(0,1)}$ gilt $Gl_{(0,1)}((0,F_Q(x)))=
  \frac{\ell((0,F_Q(x)))}{\ell((0,1))}= \frac{F_Q(x)}{1}=F_Q(x)
  \Rightarrow F_{P_\xi}(x)=F_Q(x)$
\end{proof}

\begin{remark}
  Schreibweise: $F_\xi:= F_{P_\xi}$ und $P(\xi^{-1}(A))=: P("`\xi\in
  A"') := P(\{\omega\in\Omega| \xi(\omega)\in A\}) \Rightarrow
  F_\xi(x) = P("`\xi\in(-\infty,x)"')=P("`\xi< x"')$ 
\end{remark}


\section{Unabhängige Zufallsgrößen}

Im folgenden Abschnitt gilt immer, dass $\ofp$ ein
Wahrscheinlichkeitsraum und eine Familie $(\xi_i)_{i\in I}$ mit Werten
in $[X_i, \FX_i]$ ist.

\begin{definition}
  $(\xi_i)_{i\in I}$ heißt \highl{unabhängig}, wenn für alle
  Folgen $B_i\in\FX_i\,(i\in I)$ die Familie $\{\xi_i^{-1}(B)|i\in
  I\}\subseteq \CF$ (vollständig) unabhängig ist.
\end{definition}

\begin{satz}
\label{satz:chiiunab-521}
  Wegen der Bemerkung h nach Definition von unabhängigen
  Ereignissystemen ist klar: $(\xi_i)_{i\in I}$ ist genau dann
  unabhängig, wenn für alle $n\geq 2$ gilt, dass $\{\xi_1, \ldots,
  \xi_n\}\subseteq \{\xi_i|i\in I\}$ unabhängig ist.
\end{satz}

\begin{satz}
  Sei $(A_i)_{i\in I}\subseteq\CF$. Die Aussage $(A_i)_{i\in I}$ ist
  genau dann (vollständig)  unabhängig, wenn die Familie der
  Zufallsgrößen $(\chi_{A_i})_{i\in I}$ unabhängig ist.
\end{satz}
\begin{proof}
  Da die $A_i$ in $\CF$ liegen, folgt wegen des
 \autoref{satz:indimess-218}, dass auch die $\chiai$ messbar 
  und somit Zufallsgrößen sind.
  \begin{itemize}
  \item["`$\Leftarrow$"'] Es sei vorausgesetzt, dass die
    $(\chiai)_{i\in I}$ unabhängig sind. Daraus folgt nach Definition,
    dass die $(\chiai^{-1}(B_i))_{i\in I}$ für alle $B_i \in \FB$
    vollständig unabhängig sind. Wir wählen speziell $B_i=\{1\}$ und
    es folgt, dass $\chiai(\{1\})=A_i$. Dies liefert, dass
    $(A_i)_{i\in I}$ (vollständig) unabhängig sind.
  \item["`$\Rightarrow$"'] Wir setzen voraus, dass die $(A_i)_{i\in
      I}$ (vollständig) unabhängig sind und wissen, dass
    $\chiai^{-1}(B_i) \in \{\emptyset, \Omega, A_i, A_i^c\}$. Die
    leere und die gesamte Menge müssen nicht betrachtet werden, da
    beide unabhängig sind. Somit genügt es zu zeigen, dass die
    $(c_i)_{i\in I}$ in den Fällen $c_i=A_i$ oder $c_i=A_i^c$
    unabhängig sind. Dem \autoref{satz:vollun-432} folgend gilt
    hierfür die Behauptung.
  \end{itemize}
\end{proof}

\begin{remark}
  Wegen dem \autoref{satz:chiiunab-521} werden folgend nur endliche
  Folgen $\xi_1,\ldots, \xi_n$ von Zufallsvariablen betrachtet.
  \begin{itemize}
  \item Schreibweise: $\overline{\xi}(\omega):= [\xi_1(\omega),
    \ldots, \xi_n(\omega)]$\\
    $\overline{\xi}$ heißt $n$-dimensionaler 
      \highl[Vektor!zufälliger]{zufälliger Vektor}.
  \item Frage: Ist $P_{\overline{\xi}}$ durch $P_{\xi_1}, \ldots,
    P_{\xi_n}$ eindeutig bestimmt? Im allgemeinen nicht. Dies gilt nur
    im Spezialfall, dass $\xi_1, \ldots, \xi_n$ unabhängig sind.
  \end{itemize}
\end{remark}

\begin{satz}
  \label{satz:523}
  $(\xi_i)_{i=1}^n$ ist genau dann unabhängig, wenn
  $P_{\overline{\xi}} = \times_{i=1}^n P_{\xi_i}$
\end{satz}
\begin{proof}
  Der Beweis zeigt, dass der Satz auch im allgemeinen Fall von
  Zufallsvariablen $\xi_1, \ldots, \xi_n$ gilt.
  \begin{itemize}
  \item["`$\Rightarrow$"'] Wir haben die Unabhängigkeit von $\xi_1,
    \ldots, \xi_n$ gegeben und es ist zu zeigen, dass
    $P_{\overline{\xi}} (\times_{i=1}^n B_i)= \prod_{i=1}^n
    P_{\xi_i}(B_i)$. Dazu wissen wir:
    \begin{align*}
      P_{\overline{\xi}}(\times_{i=1}^n B_i) &=
      P(\overline{\xi_1^{-1}}(\times_{i=1}^n B_i))= P("`[\xi_1,
      \ldots, \xi_n]\in\times_{i=1}^n B_i"')= P("`\xi_i\in B_i "')\\
      &=P\left(\bigcap_{i=1}^n \xi_i^{-1}(B_i)\right)= \prod_{i=1}^n
      (\xi_i^{-1}(B_i))\\
      &= \prod_{i=1}^n P_{\xi_i}(B_i)
    \end{align*}
  \item["`$\Leftarrow$"'] Wir haben die Aussage,
    $P_{\overline{\xi}}(\times_{i=1}^n B_i)=\prod_{i=1}^n
    P_{\xi_i}(B_i)$ und es ist zu zeigen, dass
    $(\xi_i^{-1}(B_i))_{i=1}^n$ vollständig unabhängig sind. Das
    heißt, für $\{i_1, \ldots, i_k\}\subseteq\{i_1, \ldots, i_n\}$
    muss $P(\bigcap_{j=1}^k \xi_{ij}^{-1}(B_{ij}))= \prod_{j=1}^k
    P(\xi_{ij}^{-1}(B_{ij}))$ gelten. Wir setzen, $A_j:= \R$ mit
    $j\notin\{i_1, \ldots, i_k\}$ und $A_{ij}:=B_{ij}$ mit $j=1,
    \ldots, k$.
    \begin{align}
      \label{eq:5231}
      \Rightarrow P_{\overline{\xi}}\left(\times_{i=1}^n A_i\right) &=
      \prod_{i=1}^n P_{\xi_i}(A_i)
    \end{align}
    Wegen der obigen Feststellungen ist klar:
    \begin{align}
      \label{eq:5232}
      P_{\overline{\xi}} (\times_{i=1}^n A_i)&= P\left(\bigcap_{i=1}^n
        \xi_i^{-1} (A_i)\right)=
      P\left(\bigcap_{j=1}^k \xi_i^{-1}(B_{ij})\right)
    \end{align}
    Falls $A_i=\R$, dann ist $\xi_i^{-1}(A_i)=\Omega$ und kann
    weggelassen werden.
    \begin{align*}
      P_{\xi_i}(A_i)&= P(\xi_i^{-1}(A_i))=
      \begin{cases}
        1 & i\notin\{i_1, \ldots, i_k\}\\
        P(\xi_i^{-1}(B_i)) & i\in \{i_1, \ldots, i_k\}
      \end{cases}
    \end{align*}
    \begin{align}
      \label{eq:5233}
      \Rightarrow\prod_{i=1}^k P_{\xi_i}(A_i)&= \prod_{j=1}^k
      P(\xi_{ij}^{-1}(B_{ij}))= \prod_{j=1}^k P_{\xi_{ij}}(B_{ij})
    \end{align}
    Aus den Gleichungen~(\autoref{eq:5231}), (\autoref{eq:5232}) und
    (\autoref{eq:5233}) folgt die Behauptung.
  \end{itemize}
\end{proof}

\begin{remark}
  Als Folgerung lässt sich festhalten, dass die $\xi_1, \ldots, \xi_n$
  genau dann unabhängig sind, wenn gilt $P(\bigcap_{i=1}^n
  \xi_i^{-1}(B_i)) = \prod_{i=1}^n P(\xi_i^{-1}(B_i))$ mit $B_1,
  \ldots, B_n\in\FB$.
\end{remark}

\begin{beispiel}
  Sei $\ofp= [\{0,1\}, \FP(\{0,1\}),
  B_{1,p}]^{n\times}$. Wir betrachten $\xi_k([\varepsilon_1, \ldots,
  \varepsilon_n])= \varepsilon_k$ mit $\varepsilon_1, \ldots,
  \varepsilon_n\in \{0,1\}$. Hieraus folgt, dass $P_{\xi_k}=
  B_{1,p}$ und es ist klar, $\overline{\xi}([\varepsilon_1, \ldots,
  \varepsilon_n])= [\varepsilon_1, \ldots, \varepsilon_n]$ (identische
  Abbildung). Somit folgt, $P_{\overline{\xi}}=P=\times_{i=1}^n
  B_{1,p}= \times_{i=1}^n P_{\xi_i}$ und in Verbindung mit
 \autoref{satz:523} ergibt sich, dass die $\xi_1, \ldots, \xi_n$
  unabhängig sind.
\end{beispiel}


\section{Summen unabhängiger Zufallsgrößen}

\begin{definition}
  Seien $P_1, P_2$ Wahrscheinlichkeitsmaße auf $[\R, \FB]$.
  \[P_1*P_2 := \int P_1(\underbrace{B-x}_{\{y-x|y\in B\}})
  P_2(dx)\qquad (B\in\FB)\]
  heißt \highl{Faltung} von $P_1$ und $P_2$.
\end{definition}

\begin{remark}
  Die Faltung $P_1*P_2$ ist ein Wahrscheinlichkeitsmaß auf
  $[\R,\FB]$. Denn die Eigenschaften lassen sich wie folgt nachweisen:
  \begin{enumerate}[1)]
  \item Da das Integral nie negativ werden kann, ist $P_1*P_2(B)\geq 0$
  \item $P_1*P_2(\R)=\int P_1(\R-x)P_2(dx)=\int P_1(\R)P_2(dx)= \int 1
    P_2(dx)= P_2(\R)=1$\\
    $P_1*P_2(\emptyset)=\int P_1(\emptyset-x)P_2(dx)=\int
    P_1(\emptyset)P_2(dx)= \int 0 P_2(dx)=0$
  \item Seien die $(B_i)_{i=1}^\infty$ paarweise disjunkt und
    $B_i\in\FB$. Dann sind auch die $(B_i-x)_{i=1}^\infty$ paarweise
    disjunkt und $\bigcup_{i=1}^\infty(B_i-x)=(\bigcup_{i=1}^\infty
    B_i)-x$. Somit folgt, $P_1*P_2(\bigcup_{i=1}^\infty B_i)= \int P_1
    (\bigcup_{i=1}^\infty (B_1-x))P_2(dx)= \int\sum_{i=1}^\infty
    P_1(B_i-x) P_2(dx)=\sum_{i=1}^\infty\int P_1(B_1-x)P_2(dx)=
    \sum_{i=1}^\infty P_1*P_2(B_i)$.
  \end{enumerate}
\end{remark}

\begin{satz}
  \label{satz:pchiadd-531}
  Seien $\xi_1, \xi_2$ unabhängige Zufallsgrößen. Dann  gilt
\[P_{\xi_1+\xi_2} = P_{\xi_1}+P_{x_2}\]
\end{satz}
\begin{proof}
  Wir setzen $h(x_1, x_2)=x_1+x_2$. Die $x_1, x_2$ sind aus $\R$ und
  somit ist die Abbildung $h$ eine Abbildung von $\R^2$ nach $\R$
  ($h:\R^2\rightarrow\R$), die stetig und damit messbar ist.
  \begin{align*}
    \xi_1+\xi_2&= h\circ \overline{\xi}\Rightarrow
    (\xi_1+\xi_2)^{-1}(B)= \overline{\xi^{-1}} (\hmineins(B))\\
    &\Rightarrow P_{\xi_1+\xi_2}(B)= P((\xi_1+\xi_2)^{-1}(B))=
    P(\overline{\xi^{-1}} (\hmineins(B)))\\
    &= P_{\overline{\xi}}(\hmineins(B))=P_{\overline{\xi}} (\{[x_1,
    x_2]|x_1+x_2\in B\})\\
    &= \int \chi_{\{[x_1, x_2]|x_1+x_2\in B\}}(y_1, y_2)
    P_{\overline{\xi}}(d[y_1, y_2])= \int \chi_{B-y_2} (y_1)
    P_{\overline{\xi}} ([y_1, y_2])\\
    &= \iint \chi_{B-y_2} (y_1) P_{\xi_1}(dy_1) P_{\xi_2}(dy_2)= \int
    P_{\xi_1}(B-y_2) P_{\xi_2}(dy_2)\\
    &= P_{\xi_1}*P_{\xi_2}
  \end{align*}
\end{proof}

\begin{remark}
  $*$ ist kommutativ und assoziativ (siehe auch
 \autoref{satz:pchiadd-531}). Somit ist für $P_1, \ldots, P_n$:
  \begin{align}
    \label{eq:5311}
    *_{i=1}^n P_i= (((P_1*P_2)*P_3)*\cdots*P_n)
  \end{align}
\end{remark}

\begin{satz}[Verallgemeinerung von \autoref{satz:pchiadd-531}]
\label{satz:verallg-533}
  Seien $\xi_1, \ldots, \xi_n$ unabhängige Zufallsgrößen:
\[\Rightarrow P_{\sum_{i=1}^n \xi_i}=*_{i=1}^n P_i\]
\end{satz}
\begin{proof}
  Der Beweis erfolgt durch vollständige Induktion. Für $n=2$ ist die
  Behauptung richtig und wir nehmen an, dass die
  Gleichung~(\autoref{eq:5311}) für gewisse $n$ gilt. Dann ist zu zeigen,
  dass, wenn $\xi_1, \ldots, \xi_i$ unabhängige Zufallsgrößen sind,
  folgt, $*_{i=1}^n P_i= (((P_1*P_2)*P_3)*\cdots*P_n)$. Seien $\xi_1,
  \ldots, \xi_n$ unabhängig und $[\xi_1, \ldots, \xi_n], \xi_{n+1}$
  unabhängig sowie $\sum_{i=1}^n \xi_i, \xi_{n+1}$ ebenfalls
  unabhängig\footnote{Der Beweis hierfür findet sich in der gängigen
    Lektüre zum Thema.}. Dann folgt, $P_{\sum_{i=1}^n \xi_i}=
  *_{i=1}^n P_{\xi_i}$. Unter Anwendung von
  \autoref{satz:pchiadd-531} ergibt sich, $P_{\sum_{i=1}^n
    \xi_i+\xi_{i+1}}= *_{i=1}^n P_{\xi_i}*P_{\xi_{n+1}}\Rightarrow
  P_{\sum_{i=1}^{n+1}} = *_{i=1}^{n+1} P_{\xi_i}$
\end{proof}

\begin{satz}
  Seien $\xi_1, \xi_2$ unabhängige Zufallsgrößen. Dann gilt:
  \[F_{\xi_1+\xi_2} (y)=\int F_{\xi_1}(x-y) P_{\xi_2}(dx)\]
\end{satz}
\begin{proof}
  Wir wenden den \autoref{satz:pchiadd-531} mit $B=(-\infty, y)$ an
  und haben:
  \begin{align*}
    F_{\xi_1+\xi_2} &= P_{\xi_1+x_2}((-\infty, y))=P_{\xi_1}*P_{\xi_2}
    ((-\infty, y))\\
    &= \int P_{\xi_1}((-\infty, y)-x)P_{\xi_2}(dx)= \int P_{\xi_1}
    ((-\infty, y-x)) P_{\xi_2}(dx)\\
    &= \int F_{\xi_1} (y-x)P_{\xi_2}(dx)
  \end{align*}
\end{proof}

\begin{satz}
  Seien $\xi_1, \xi_2$ unabhängige Zufallsgrößen mit $P_\xi=\ell f_i$
  ($i=1,2$). Dann gilt:
  \[P_{\xi_1+x_2}=\ell f_1*f_2\]
  wobei $f_1*f_2(x):= \int f_1(x-y)f_2(y)\ell(dy)$.
\end{satz}
\begin{proof}
  Der Beweis erfolgt weniger mathematisch streng. Da der Aufwand
  hierfür unangemessen hoch ist. Für streng mathematische Rechnungen
  sei der Leser an weitere Lektüre verwiesen.

  Wegen \autoref{satz:verallg-533} ist $F_{\xi_1+\xi_2}(x)= \int
  F_1(x-y) P_{\xi_2}(dy)$ und wegen $P_{\xi_2}=\ell f_2$ ergibt sich
  $\int F_1(x-y)f_2(y)\ell(dy)$\footnote{Der Sachverhalt wurde früher
    bewiesen.}. Man geht nun davon aus, dass die Funktion
  differenzierbar ist. Alle Fälle, wo die Funktion nicht überall
  differenzierbar ist, werden nicht mit abgedeckt.
  
  Somit folgt, dass $F_{\xi_1+\xi_2}'(x)=\int F_1'(x-y)f_2(y)\ell(dy)$
  die Dichte von $P_{\xi_1+\xi_2}$ ist (nach Definition). Dies ist
  aber $\int f_1(x-y)f_2(y)\ell(dy)=f_1*f_2(y)$
\end{proof}

\begin{satz}
\label{satz:535}
  Seien $\xi_1, \xi_2$ unabhängige Zufallsgrößen mit $P_{\xi_i}(\Z)=1$
  ($i=1,2$), d.\,h. $P_{\xi_1}=\sum_{k\in\Z} a_k\delta_k, P_{\xi_2}=
  \sum_{k\in\Z} b_k\delta_k, a_k, b_k \geq 0, \sum a_k=\sum
  b_k=1$. Dann gilt:
  \begin{align*}
    P_{\xi_1+\xi_2}(\Z) &= 1\\
    P_{\xi_1+\xi_2}(\{n\}) &= \sum_{k\in\Z} P_{\xi_1}(\{n-k\})
    P_{\xi_2} (\{k\})
  \end{align*}
  d.\,h. $P_{\xi_1+\xi_2}=\sum_{k\in\Z} c_k\delta_k$ mit $c_k=\sum
  a_{n-k} b_k$.
\end{satz}
\begin{proof}
  Nach \autoref{satz:pchiadd-531} wissen wir, dass $P_{\xi_1+\xi_2}
  (B)= \int P_{\xi_1}(B-x)P_{\xi_2}(dx)$ ist und wählen
  $B=\{n\}$. Somit erhalten wir nun, $P_{\xi_1+\xi_2} (\{n\})=\int
  P_{\xi_1} (\{n-x\})P_{\xi_2}(dx)=\sum_{k\in\Z} P_{\xi_1}(\{n-k\})
  P_{\xi_2}(\{k\})$.
\end{proof}

\begin{beispiel}
  Seien $\xi_1, \xi_2$ unabhängig und $P_{\xi_i}=B_{n_i,p}$ mit
  $i=1,2$. Zur Bestimmung von $P_{\xi_1+x_2}$ wäre es jetzt möglich,
  \autoref{satz:535} anzuwenden:
  \begin{align*}
    P_{\xi_1+\xi_2}(\{n\}) &= \sum_{k\in\Z} P_{\xi_1}(\{n-k\})
    P_{\xi_2}(\{k\})= \sum_{k\in\Z} B_{n_1,p}(\{n-k\})
    B_{n_2,p}(\{k\})\\
    &= \sum_{0\leq k\leq n_2} \binom{n_1}{n-k}
    p^{n-k}(1-p)^{n_1-(n-k)} \binom{n_2}{k}p^k (1-p)^{n_2-k}\\
    &=
    \begin{cases}
      \binom{n_1+n_2}{n}p^n (1-p)^{n_1+n_2-n} & 0\leq n\leq n_1+n_2\\
      0 & \text{sonst}
    \end{cases}\\
    = B_{n_1+n_2,p}(\{n\})
  \end{align*}
  Diese Rechnung ist relativ kompliziert und es existieren weitere
  Möglichkeiten zur Bestimmung. Wir betrachten das Bernoullischema zu
  $n_1+n_2,p$ und haben durch $[\{0,1\},\FP(\{0,1\}),
  B_{1,p}]^{n_1+n_2}$ ein Bernoullischema gegeben. Dabei stellen
  $\hat{\xi_1}:= \sum_{i=1}^{n_1} \pi_i$ und $\hat{\xi_2}:=
  \sum_{i=n_i+1}^{n_1+n_2} \pi_i$ die Anzahl der Erfolge im
  Bernoullischema dar. Wir wissen, dass $P_{\hat{\xi_1}}=B_{n_1,p},
  P_{\hat{\xi_2}}=B_{n_2,p}, \hat{\xi_1}, \hat{\xi_2}$ unabhängig
  sind. Daraus folgt: $P_{\xi_1+\xi_2}=P_{\hat{\xi_1}+\hat{\xi_2}}$
  und es ist klar, dass $\hat{\xi_1}+\hat{\xi_2}= \sum_{i=1}^{n_1+n_2}
  \pi_i \Rightarrow P_{\hat{\xi_1}+\hat{\xi_2}}=B_{n_1+n_2,p}$
\end{beispiel}



\section{Der Erwartungswert}

Im folgenden sind immer $\ofp$ ein Wahrscheinlichkeitsraum
und wir betrachten die Zufallsgrößen $\xi, \eta, \zeta$.

\begin{definition}
  Sei $\xi$ eine Zufallsgröße, so dass $E\xi:=
  \int\xi(\omega)P(d\omega)$ existiert. Dann heißt $E\xi$
  \highl{Erwartungswert} von $\xi$.
\end{definition}

\begin{remark}
  \begin{itemize}
  \item In älteren Büchern wird $E\xi$ auch manchmal als $M\xi$ bezeichnet.
  \item $E\xi$ existiert, wenn $\xi\geq 0$ oder $\xi$ $P$-integrierbar
    ist. Der letztere stellt einen wichtigen Fall dar.
  \item $E\xi$ ist ein fundamentales, aber grobes Kennzeichen der
    Verteilung $P_\xi$. Die Definition lässt sich allein mit $P_\xi$
    aufschreiben.
  \end{itemize}
\end{remark}

\begin{satz}
\label{satz:zgreew-41}
  Sei $\xi$ eine Zufallsgröße mit endlichen
  Erwartungswert\footnote{Die Aussage gilt auch für unendlichen
    Erwartungswert. Doch der hierzu zu führende Beweis ist deutlich
    aufwendiger.}. Dann gilt:
  \[E\xi=\int x P_\xi(dx)=\left(\int \iota dP_\xi\right)\]
\end{satz}
\begin{proof}
  Zum Beweis kommt der Übertragungssatz
  (\autoref{satz:uebertrsatz-228}) zur Anwendung. Wir haben
  $[\Omega_1, \CF_1, \mu]:= \ofp, [\Omega_2,
  \CF_2]=[\R,\FB], h:[\Omega_1,\CF_1]\rightarrow [\Omega_2,\CF_2],
  f:[\Omega_2,\CF_2]\rightarrow [\R,\FB]$. Speziell sind $h=\xi$ und
  $f=\iota$. Somit folgt, $\int f\circ hd\mu= \int f d\mu\circ \hmineins$
  und in unserem Fall: $\int\xi dP=\int\iota dP_\xi$
\end{proof}

\begin{satz}
  \label{satz:542}
  Sei $P_\xi(\Z)=1$\footnote{diskrete Verteilung, $\xi$ nimmt nur
    ganze Zahlen an.}. Falls $E\xi$ konvergiert, gilt:
  \[E\xi= \sum_{k\in\Z} k P\xi(\{k\}) (=\sum_{k\in\Z} ka_k\]
\end{satz}
\begin{proof}
  Aus \autoref{satz:zgreew-41} folgt, $E\xi=\int xP_\xi(dx)=
  \sum_{k\in\Z} ka_k$.
\end{proof}

\begin{satz}
  Sei $P\xi=\ell f$ mit $f$ stetig. Falls der Erwartungswert $E\xi$
  existiert, gilt:
  \[E\xi=\int_{-\infty}^\infty x f(x) dx\]
\end{satz}
\begin{proof}
  Aus \autoref{satz:zgreew-41} folgt, $E\xi=\int x P_\xi(dx)= \int x
  f(x) \ell dx= \int_{-\infty}^\infty xf(x)dx$.
\end{proof}

\begin{satz}
  Sei $\xi$ eine nichtnegative Zufallsgröße, d.\,h. $P("`\xi\geq ="') =1 
  \Leftrightarrow P_\xi([0, +\infty))=1$. Dann gilt:
  \[E\xi = \int_{[0, +\infty)} (1-f_\xi(x)) \ell dx\]
\end{satz}
\begin{proof}
  ergänzen
\end{proof}

\begin{definition}
  Sei $\xi$ eine Zufallsgröße, so dass $E\xi^k$ existiert. Dann heißt
  $m_k(\xi) = E\xi^k$ $k$-tes \highl{Moment} der Zufallsgröße $\xi$.
\end{definition}

\paragraph{Momentenproblem}
Ist $P_\xi$ durch die Folge $(m_k(\xi))_{k=1}^n$ eindeutig bestimmt?
Diese Frage ist negativ zu beantworten. Nach dem Übertragungssatz ist
klar, dass $E\xi^k= \int x^k P_\xi(dx)$ gilt. Aus der Kenntnis von
$P_\xi$ folgt somit die Kenntnis des Moments. Aber im allgemeinen ist
$E\xi^k \not\Rightarrow P_\xi$. Eine posistive Antwort auf die obige
Frage ist nur unter gewissen Beschränktheitsforderungen möglich.

\begin{satz}
  \begin{enumerate}[(E1)]
  \item Seien $E\xi_1, E\xi_2$ endlich. Dann folgt: $E(a_1\xi_1 +
    a_2\xi_2) = a_1E\xi_1 + a_2E\xi_2$ mit $a_1, a_2\in\R$.
  \item Sei $\xi=\chi_A, A\in\CF\Rightarrow E\xi=P(A)$
  \item Sei $P("`\xi=c"')=1\Rightarrow E\xi=c$ 
  \item Sei $P("`a\leq \xi \leq b"')=1 \Rightarrow a\leq E\xi\leq b$ 
  \end{enumerate}
\end{satz}
\begin{proof}
  \begin{enumerate}[(E1)]
  \item $E(a_1\xi_1+a_2\xi_2) = \int (a_1\xi_1+ a_2\xi_2) dP =
    a_1\int \xi_1dP+ a_2\int \xi_2dP= a_1E\xi_1+a_2E\xi_2$
  \item $E\chi_A= \int\chi_A dP=P(A)$
  \item $P("`\xi=c"')=1\Rightarrow P_\xi=\delta_c$\\ 
    Nach \autoref{satz:zgreew-41} folgt nun $E\xi= \int x P_\xi(dx)
    =\int x \delta_c(dx)$ und letzteres ist ein Integral bei diskreten
    Maßen. Damit ergibt sich nach \autoref{satz:542}, dass die letzte
    Aussage gleich $\sum_{k\in\R} 1\cdot\delta_c(\{k\})=c$ ist. Im
    allgemeinen Fall gilt $\int f(x)\delta_c (dx)= f(x)$. %muss das
                                %nicht eigentlich f(_c_) sein?
    Noch allgemeiner formuliert: $\int f(x) \sum_{k}
    a_k\delta_{\omega_k} (dx)= \sum_k a_k f(\omega_k)$.
  \item $P("`a\leq \xi\leq b"')=1 \Rightarrow P_\xi([a,b])=1 
    \Rightarrow E\xi=\int x P_\xi(dx)= \int_{[a,b]} xP_\xi(dx) \geq
    \int_{[a,b]} aP_\xi(dx) = aP([a,b])=a$
  \end{enumerate}
\end{proof}

\begin{beispiel}
  \begin{enumerate}
  \item Bernoulli-Verteilung: $P_\xi= B_{1,p} = p\delta_1+ (1-p)
    \delta_0 \Rightarrow E\xi= 1p+0(1-p)=p$
  \item $P_\xi= B_{n,p} = \sum_{k=0}^n \binom{n}{k} p^k (1-p)^{n-k}
    \delta_k \Rightarrow E\xi = \sum_{l=0}^n k\binom{n}{k} p^k
    (1-p)^{n-k} =np$\\
    Ein anderer Weg, um dies zu ermitteln: Sei $\ofp =
    [\{0,1\}, \FP(\{0,1\}), B_{1,p}]^{n\times}$ und $\xi_k$ die
    Projektion auf die $k$-te Ebene in $\Omega$. Es ist bekannt, dass
    $P_{\xi_k} = B_{1,p}$ und $P_{\sum\xi_k}= B_{n,p}=P_\xi$. Damit
    folgt, $E\xi= E\sum_{k=1}^n \xi_k= \sum_{k=1}^n E\xi_k =
    \sum_{k=1}^n p= np$.
  \item Cauchy-Verteilung: $P_\xi=\ell f$ mit $f(x)= \frac{1}{\pi
      (1+x^2)}$ und $x\in\R$. Es ist klar, dass $\int_0^{+\infty}
    xf(x) dx=\infty$ gilt.

    Ein anderer Weg ist folgender: Sei $\ofp = [\{0,1\}, \FP(\{0,1\}),
    B_{1,p}]^{n\times}$ und $\xi_k$ die Projektion auf die $k$-te
    Komponente in $\Omega$. Es ist bekannt, dass $P_{\xi_k}=B_{1,p}$
    und $P_{\sum\xi_k}= B_{n,p}=P_\xi$. Somit folgt, $E\xi=
    E\sum_{k=1}^n \xi_k= \sum_{k=1}^n E\xi_k= \sum_{k=1}^n p=np$.
  \item Sei $P\xi=\ell f$ mit $f(x)=\frac{1}{\pi(1+x)^2}$ für
    $x\in\R$. Es ist klar, dass $\int_0^{+\infty} xf(x)dx=+\infty$
    gilt. Formal wäre $E\xi=+\infty$\lightning. Damit folgt, dass
    $E\xi$ nicht existiert.
  \end{enumerate}
\end{beispiel}

\begin{satz}
  Seien $\xi, \eta$ unabhängige Zufallsgrößen mit endlichem
  Erwartungswert. Dann hat die Zufallsgröße $\xi\cdot\eta$ einen
  endlichen Erwartungswert und es gilt:
  \[E(\xi\cdot\eta)=E\xi\cdot E\eta\]
\end{satz}
\begin{proof}
  \begin{align*}
    [\Omega_1, \CF_1, \mu] &:= \ofp & [\Omega_2, \CF_2] &:= [\R^2,
    \FB_2]\\
    h &: \Omega_1\rightarrow \Omega_2 & h &= [\xi, \eta]\\
    f &: \Omega_2\rightarrow \R & f(x,y) &= xy
  \end{align*}
  \begin{align*}
    \Rightarrow \int f\circ h d\mu &= \int f d\mu\circ \hmineins\\
    \int \xi\cdot \eta dP &= \int xy dP_{[\xi,\eta]}d([x,y])\\
    \Rightarrow E(\xi\cdot\eta) &= \int xy dP_{[\xi,\eta]}d([x,y])
    =\int xy (P_\xi\times P_\eta)(d[x,y])\\
    &=\iint xy P_\xi(dx) P_\eta(dy)= \int x P_\xi(dx) \cdot \int y
    P_\eta(dy)\\
    &= E\xi\cdot E\eta
  \end{align*}
\end{proof}

\begin{remark}
  Ohne die Forderung der Unabhängigkeit ist die Behauptung im obigen
  Satz im allgemeinen nicht richtig.
\end{remark}

\section{Die Varianz}

Sei $\ofp$ ein Wahrscheinlichkeitsraum. Wir betrachten die
Zufallsgrößen $\xi, \eta$.

\begin{definition}
  Sei $\xi$ eine Zufallsgröße mit \emph{endlichem} Erwartungswert.
  \[D\xi := E(\xi-E\xi)^2\]
  heißt \highl{Varianz} von $\xi$. Weiterhin heißt
  $\sqrt{D\xi}$ \highl{Standardabweichung}.
\end{definition}

\begin{satz}
  \begin{enumerate}[a)]
  \item $D\xi\geq 0$
  \item $D\xi=0\Leftrightarrow
    P_\xi=\delta_{E\xi}$. d.\,h. $P("`\xi=c"') =1$, 
    also Determinismus.
  \item $D\xi$ endlich $\Leftrightarrow E\xi^2$ endlich. In diesem
    Fall gilt $D\xi=E\xi^2-(E\xi)^2$
  \item $D(c_1\xi+c_2)=c_1^2D\xi$ mit $c_1, c_2\in\R$
  \item $D\xi=\int (x-E\xi)^2 P_\xi (dx)$
  \end{enumerate}
\end{satz}
\begin{proof}
  \begin{enumerate}[a)]
  \item klar, weil $(\xi-E\xi)^2\geq 0\Rightarrow \int (\xi-E\xi)^2
    dP\geq 0$
  \item
    \begin{itemize}
    \item $P_\xi= \delta_{E\xi}\Rightarrow D\xi= \int(x-E\xi)^2 P_\xi
      (dx)= (E\xi-E\xi)^2=0$
    \item $E(\xi-E\xi)^2=0 \Rightarrow P("`(\xi-E\xi)^2=0"')=1= 
      P("`\xi- E\xi"')$ 
    \end{itemize}
  \item $D\xi= E(\xi-E\xi)^2= E(\xi)^2-e\xi E\xi+(E\xi)^2)= E\xi^2 -
    2E\xi E\xi + (E\xi)^2= E\xi^2- 2(E\xi)^2+(E\xi)^2= E\xi^2-(E\xi)^2$
  \item $D(c_1\xi+c_2)= E(c_1\xi+c_2-E(c_1\xi+c_2))^2=
    E(c_1\xi-c_1E\xi)^2 =E(c_1(\xi-E\xi))^2= c_1^2 E(\xi-E\xi)^2= c_1 D\xi$
  \item $D\xi= E(\xi-E\xi)^2 = \int (\xi-E\xi)^2dP$. Nach dem
    Übertragungssatz folgt mit $h=\xi, f(x)=(x-E\xi)^2, \mu=P$
    sowohl $f\circ h= (\xi-E\xi)^2$ als auch $\mu\circ
    \hmineins=P_\xi$. Weiter folgt somit $E(\xi-E\xi)^2=\int f(x)P_\xi
    (dx)= \int (x-E\xi)^2P_\xi(dx)$. Daraus ergibt sich die Behauptung.
  \end{enumerate}
\end{proof}

\begin{remark}[Folgerungen]
  \begin{enumerate}
  \item Aus den Punkten a) und c) ergibt sich: $E\xi^2\geq (E\xi)^2$
  \item Aus c) ergibt sich: $D\xi=0\Leftrightarrow E\xi^2=(E\xi)^2
    \Leftrightarrow P_\xi=\delta_{E\xi}$
  \end{enumerate}
\end{remark}

\begin{beispiel}[für $E\xi\cdot\eta\neq E\xi\cdot E\eta$]
  Sei $\xi$ eine Zufallsgröße mit $D\xi>0$, d.\,h. $\xi$ ist keine
  Konstante, und $\eta := \xi$. Dann folgt, $E\xi\cdot\eta=E\xi^2$. Da
  weiter gilt, $D\xi=E\xi^2-(E\xi)^2$ und $D\xi\neq 0$, folgt, dass
  $E\xi^2\neq E\xi\cdot E\xi$. Angenommen, es gelte, $E\xi\cdot\eta=
  E\xi\cdot E\cdot\eta$. Dann wäre $E\xi^2=(E\xi)^2\Rightarrow D\xi=
  E\xi^2- (E\xi)^2=0$ \lightning
\end{beispiel}

\begin{satz}
  Seien $\xi, \eta$ unabhängige Zufallsgrößen mit endlicher
  Varianz. Dann gilt:
  \[D(\xi+\eta)=D\xi+D\eta\]
\end{satz}
\begin{proof}
  \begin{align*}
    D(\xi+\eta) &= E(\xi+\eta-E(\xi+\eta))^2=
    E(\xi-E\xi+\eta-E\eta)^2\\
    &= E\left((\xi-E\xi)^2+2(\xi-E\xi)(\eta-E\eta)+
      (\eta-E\eta)^2\right)\\
    &= E(\xi-E\xi)^2+E(\eta-E\eta)^2+2\underbrace{E(\xi-E\xi)}_{0=}
    \cdot \underbrace{E(\eta-E\eta)}_{=0}=D\xi+D\eta
  \end{align*}
\end{proof}

\begin{remark}
  Ohne die Forderung der Unabhängigkeit ist die Behauptung im
  allgemeinen falsch.

  Weiterhin kann man per vollständiger Induktion zeigen, dass für
  $\xi_1, \ldots, \xi_n$ unabhängige Zufallsgrößen mit endlichen
  Erwartungswerten folgt,
  \[D\left(\sum_{k=1}^n \xi_k\right) = \sum_{k=1}^n D\xi_k\]
\end{remark}

\begin{satz}
  \begin{enumerate}[a)]
  \item Sei $P_\xi= \sum_{k\in\Z} a_k \delta_k$ und $E\xi$
    endlich. Dann gilt:
    \[D\xi= \sum_{k\in\Z} (k-e\xi)^2 a_k\]
  \item Sei $P_\xi=\ell f$ mit $f$ stetig und $E\xi$ endlich. Dann
    ist:
    \[D\xi= \int_{-\infty}^{+\infty} (x-E\xi)^2 f(x)dx\]
  \end{enumerate}
\end{satz}

\begin{beispiel}
  \begin{enumerate}
  \item $P_\xi=B_{n,p} =p\delta_1+(n-p)\delta_0\Rightarrow D\xi=
    (0-p)^2 (1-p)+(1-p)^2 p= p^2(1-p)+p-2p^2+p^3= p-p^2=p(1-p)$
  \item $P_\xi=B_{n,p}$\\
    Seien $\xi_1,\ldots, \xi_n$ unabhängig und $P_{\xi_k}=B_{1,p}$ für
    $k=1,\ldots,n$. Dann gilt $P_{\sum\xi_k}=B_{n,p}\Rightarrow P_\xi=
    P_{\sum \xi_k}\Rightarrow D\xi=D\sum\xi_k= \sum_{k=1}^n D\xi_k=
    np(1-p)$. Andererseits gilt auch: $D\xi=\sum_{k=0}^n (k-np)^2p^k
    (1-p)^{n-k} = np(1-p)$
  \end{enumerate}
\end{beispiel}

\paragraph{Interpretation der Varianz}

Sei $(H,\lVert\cdot\rVert)$ ein \highl{Hilbertraum}. Dabei
ist $H$ ein linearer Raum und $\lVert\cdot\rVert$ die Norm. Weiter
seien $h\in H, G\subseteq H$.

Wir betrachten den Spezialfall:
\begin{gather*}
H\colon=\left\{f[\Omega,\CF]\rightarrow [\R,\FB]| \lVert f\rVert= \sqrt{\int
  f^2dP} <+\infty\right\}
\end{gather*}

Dabei ist $f$ eine Zufallsgröße und $Ef$ ist endlich, denn es gilt,
$\infty> Ef^2\geq (Ef)^2$. Damit ist auch $Df$ endlich und
existiert. Weiter sei:
\begin{gather*}
  G\colon=\{a\colon\Omega\rightarrow \R|\text{konstant}\}
\end{gather*}
und $\xi$ eine Zufallsgröße mit $E\xi^2<+\infty$. Es ist klar, dass
$\lVert \xi-a\rVert=\sqrt{E(\xi-a)^2}$

\begin{satz}
  Sei $\xi$ eine Zufallsgröße mit $E\xi^2<\infty$. Dann gilt:
\[D\xi=\inf_a E(\xi-a)^2=\min_aE(\xi-a)^2\]
Dabei ist $\sqrt{D\xi}$ der Abstand einer Zufallsgröße zur Menge der
Konstanten mit $\sqrt{D\xi}=\inf_a \sqrt{E(\xi-a)^2}=\inf_a \lVert
\xi-a\rVert$. Wenn der Abstand 0 ist, ist somit $\xi$ konstant. Für
einen kleinen Abstand bedeutet dies, dass der Zufall gering ist.
\end{satz}
\begin{proof}
  \begin{align*}
    E(+x-a)^2 &= E((\xi-E\xi)+(E\xi-a))^2\\
    &= E((\xi-E\xi)^2 + Z(\xi-E\xi)(E\xi-a) + (E\xi-a)^2)\\
    %%Woher kommt das Z?
    &= D\xi+ Z(E\xi-a)\underbrace{E(\xi-E\xi)}_{=0} +(E\xi-a)^2\\
    &= D\xi+ (E\xi-a)^2\\
    \Rightarrow \min_a E(\xi-a)^2 &= \min_a D\xi+(E\xi-a)^2
    \Leftrightarrow a=E\xi\\
    \Rightarrow D\xi &= \min_a E(\xi-a)^2
  \end{align*}
\end{proof}

\begin{remark}
  Sei $\xi$ eine Zufallsgröße mit $E\xi, E\xi^2<+\infty$ und endlich.
  \begin{itemize}
  \item $\xi-E\xi$ heißt \highl{Zentrierung} von
    $\xi$. Die Zufallsgröße $\eta$ heißt zentriert, wenn $E\eta=0$.
  \item Sei $D\xi>0$. Dann heißt $\frac{\xi}{\sqrt{D\xi}}$
    \highl{Normierung} von $\xi$. Die Zufallsgröße
    $\eta$ heißt normiert, wenn $D\eta=1$.
  \item Wir betrachten Normierung und Zentrierung: $\eta:=
    \frac{\xi-E\xi}{\sqrt{D\xi}}$. Dann ist:
    \begin{align*}
      E\eta &= \frac{E(\xi-E\xi)}{\sqrt{D\xi}}=0\\
      D\eta &= D\frac{\xi-E\xi}{\sqrt{D\xi}}=\frac{1}{D\xi}
      D(\xi-E\xi) = \frac{1}{D\xi}D\xi= 1
    \end{align*}
  \end{itemize}
\end{remark}

\section{Die Tschebyscheffsche Ungleichung}

Sei $\ofp$ ein Wahrscheinlichkeitsraum. Wir betrachten Zufallsgrößen
über diesen Wahrscheinlichkeitsraum.

\begin{satz}[Die Markoffsche Ungleichung]
  \label{satz:markoff}
  Sei $\xi\geq 0$ eine Zufallsgröße\footnote{Damit existiert $\int\xi
    d\mu$ nach der Definition zweiter Stufe immer.} und $a>0$ eine
  reelle Zahl. Dann gilt:
  \[E\xi\geq aP("`\xi\geq a"')= aP_\xi([a, +\infty])=a(1-F_\xi(a))\] 
\end{satz}
\begin{proof}
  Sei $\xi\geq 0$. Dann ist $\xi(\omega)\geq
  \xi(\omega)\chi_{\{\hat{\omega} | \xi(\hat{\omega})\geq a\}}(\omega)
  \geq a\chi_{\{\hat{\omega} | \xi(\hat{\omega})\geq a\}}(\omega)$. Da
  das Integral monoton ist, folgt $E\xi\geq E(a\chi_{\{\hat{\omega} |
    \xi(\hat{\omega})\geq a\}}) = aE\chi_{\{\hat{\omega} |
    \xi(\hat{\omega})\geq a\}}=
  aP(\{\hat{\omega}|\xi(\hat{\omega})\geq a)= aP("`\xi\geq a"')$. 
\end{proof}

\begin{satz}[Tschebyscheffsche Ungleichung]
  \label{satz:tscheby}
  Sei $\eta$ eine Zufallsgröße mit endlichem Erwartungswert. Dann
  existiert $D\eta$ und es gilt:
  \[P("`\lvert\eta-E\eta\rvert\geq\varepsilon"')\leq\frac{D\eta}{\varepsilon^2} 
  \qquad \varepsilon> 0\]
\end{satz}
\begin{proof}
  Zum Beweis wird der \autoref{satz:markoff} für folgenden
  Spezialfall angewendet. Wir setzen $\xi:=(\eta-E\eta)^2\geq 0$ und
  $a:=\varepsilon^2$. Dann ist klar, dass $E\xi=D\eta$ gilt. Dann
  ergibt sich: $D\eta=E\xi\geq aP("`\xi\geq a"')= \varepsilon^2
  P("`(\eta-E\eta)^2 \geq\varepsilon^2"')= \varepsilon^2
  P("`\lvert\eta-E\eta\rvert \geq\varepsilon"')$. 
\end{proof}

\begin{satz}
\label{satz:563}
  Seien $\xi_1,\ldots,\xi_n$ eine Folge unabhängiger Zufallsgrößen mit
  $E\xi_i=a, D\xi_i=\delta^2$ für $i=1,\ldots, n$. Dann gilt:
  \[P\left("`\lvert\frac{1}{n}\sum_{i=1}^n \xi_i-a\rvert
    \geq\varepsilon"'\right)\leq \frac{\delta^2}{n\varepsilon^2}\qquad 
    \varepsilon > 0\]
\end{satz}
\begin{proof}
  Für den Beweis wird \autoref{satz:tscheby} angewendet und wir
  setzen: $\eta:=\frac{1}{n} \sum_{i=1}^n \xi_i$
  \begin{gather*}
    \Rightarrow E\eta = E\left(\nicefrac{1}{n}\sum_i \xi_i\right)=
    \nicefrac{1}{n} \sum_i E\xi_i = a\nicefrac{1}{n}n\\
    D\eta= D\left(\nicefrac{1}{n}\sum_i \xi_i\right) =
    \nicefrac{1}{n^2} \sum_i D\xi_i = \nicefrac{1}{n^2} n\sigma^2
    = \frac{\sigma^2}{n}\\
    \Rightarrow P\left("`\lvert\nicefrac{1}{n}\sum\xi_i-a\rvert\geq
      \varepsilon"'\right) =P("`\lvert\eta-E\eta\rvert\geq
    \varepsilon"') 
    \leq=\frac{D\eta}{\varepsilon^2}=\frac{\sigma^2}{n\varepsilon^2}
  \end{gather*}
\end{proof}

\paragraph{Anwendung von \autoref{satz:563} auf Wette}

\begin{itemize}
\item $n=250$ Münzwürfe mt $\xi_1,\ldots,\xi_{250}$
\item $\xi_1=1$ entspricht der Aussage, dass der $i$-te Wurf Wappen zeigt.
\item Damit sind die $\xi,\ldots,\xi_n$ unabhängig
\item $P_{\xi_i}=B_{1,p}=\nicefrac{1}{2}(\delta_1+\delta_0)$ mit
  $p=\nicefrac{1}{2}\Rightarrow E\xi_i=\nicefrac{1}{2},
  D\xi_i=\nicefrac{1}{2} (1-\nicefrac{1}{2})=\nicefrac{1}{4}$
\item $\lvert\sum_{i=1}^{250} \xi_i-125\rvert \geq 2=:A$ entspricht der
  Aussage, dass ich gewinne.
\end{itemize}
Aus \autoref{satz:563} folgt nun: $P(A)=P("`\lvert \nicefrac{1}{n}
\sum \xi_i-a\rvert\geq\varepsilon"')\leq 
\frac{\sigma^2}{n\varepsilon^2}=
\frac{\nicefrac{1}{4}}{250\cdot \nicefrac{1}{10}^2}= \frac{1}{10}$.

\subsubsection*{(reale) Anwendung --- Meßtheorie}

Viele Messgeräte arbeiten nicht exakt. Die Ursachen liegen in
systematischen Fehlern und durch zufällige Einflüsse bedingte
Fehler. Die systematischen Fehler können durch Eichung minimiert oder
abgeschafft werden.

Die ideale Messapparatur liefert der Wert $a$ und der reale Vorgang
die zufällige Größe $\xi$. Da das Gerät zwar korrekt misst, aber auch
anfällig für äußere Einflüsse ist, ergibt sich ein nicht
systematischer Fehler, d.\,h. $E\xi=a$. Vom Hersteller erhält man
$D\xi=\sigma^2$. Die übliche Verfahrensweise ist nun, dass wiederholte
Messungen durchgeführt werden und man so viele $\xi,\ldots,\xi_n$
bekommt. Für diese wird angenommen, dass sie unabhängig und identisch
verteilt (iid\footnote{vom englischen independent and identical
  distributed}) sind und es gilt:
\begin{align*}
  D\xi_i &= \sigma^2 & E\xi_i &= a
\end{align*}
Als Verfahren wird das arithmetische Mittel als "`Schätzwert"' für $a$
gebildet:
\begin{align*}
  \frac{1}{n} \sum_{i=1}^n \xi_i=\eta_n
\end{align*}
Folgende Ansprüche werden formuliert:
\begin{enumerate}
\item Die Schätzung soll "`hinreichend"' exakt sein, d.\,h. für ein
  vorgegebenes $\varepsilon>0$ wird gefordert: $\lvert \eta_n -a\rvert
  < \varepsilon$.
\item Vorgabe eines Sicherheitsniveaus $s<1$ mit
  \begin{align}
    \label{eq:1}
    P("`\lvert\eta_n-a\rvert<\varepsilon"')\geq s 
  \end{align}
\end{enumerate}

Damit stellt sich das Problem, wie $n$ gewählt werden muss, so dass
die Gleichung (\autoref{eq:1}) gilt. Eine mögliche Antwort isT:
\begin{align}
  \label{eq:2}
  n\geq\frac{\sigma^2}{(1-s)\varepsilon^2}
\end{align}

\begin{remark}
  \begin{enumerate}
  \item Wählen "`kleinstes"' zulässiges $n$
  \item Kompromiss bei der Wahl von $s, \varepsilon$ gewöhnlich
    erforderlich. Denn aus großem $s$ und kleinem $\varepsilon$ folgt
    ein großes $n$.
  \item Bei Informationen über den Typ $P_\xi$ kann man bessere
    Grundlagen für die Ermittlung des $n$ als \autoref{satz:563}
    erhalten. (siehe Vorlesung Stochastik).
  \end{enumerate}
\end{remark}

\begin{proof}
  Es folgt ein Beweis dafür, dass aus Gleichung~(\autoref{eq:2}) die
  Gleichung~(\autoref{eq:1}) folgt. Aus dem \autoref{satz:563} folgt,
  $P("`\lvert \eta_n-a\rvert <\varepsilon"')=1- P("`\lvert
  \underbrace{\eta_n}_{\nicefrac{1}{n} \sum\xi_i} -a\rvert\geq
  \varepsilon"')$. 
  Nunmehr sich hier wieder \autoref{satz:563}
  anwenden und die obige Gleichung ist größergleich
  $1-\frac{\sigma^2}{n\varepsilon^2}\geq
  s$. d.\,h. Gleichung~(\autoref{eq:1}) wird realisiert, wenn $s\leq 1-
  \frac{\sigma^2}{n\varepsilon^2}\Leftrightarrow n\geq
  \frac{\sigma^2}{(1-s)\varepsilon^2}$. Letzteres entspricht gerade
  Gleichung~(\autoref{eq:2}).
\end{proof}

\section{Konvergenzarten und das Gesetz der großen Zahlen}

Sei $\ofp$ ein Wahrscheinlichkeitsraum und wir betrachten
Zufallsgrößen oder -variablen über diesem Wahrscheinlichkeitsraum.

\begin{definition}
  Seien $\xi$ eine Zufallsgröße und $(\xi_n)_{n\geq 1}$ eine Folge von
  Zufallsgrößen. Die Folge $(\xi_n)$ konvergiert mit
  Wahrscheinlichkeit gegen $\xi$, wenn für alle $\varepsilon>0$ gilt:
  \[P("`\lvert\xi_n-\xi\rvert\geq\varepsilon"')\xrightarrow{n\rightarrow
    \infty} 0\Leftrightarrow P("`\lvert
  \xi_n-\xi\rvert<\varepsilon"') \xrightarrow{n\rightarrow\infty} 1\]
\end{definition}

\begin{satz}[Schwaches Gesetz der großen Zahlen nach Tschebyscheff]
  \label{satz:schwachges-571}
  Sei $(\xi_i)_{i\geq 1}$ eine Folge unabhängiger identisch verteilter
  Zufallsgrößen mit $E\xi_i=a, D\xi_i=\sigma^2$. Dann gilt:
  \[\frac{1}{n} \sum_{i=1}^n \xi_i\xrightarrow{P} a\]
\end{satz}
\begin{proof}
  Aus \autoref{satz:563} folgt: %%Beweis unklar
\end{proof}

\paragraph{Bezug zu relativen Häufigkeiten (Spezialfall)}

Wir betrachten die Folgen von Zufallsvariablen $(\eta_i)_{i\geq 1}$
mit Werten in $[X, \FX]$, wählen ein $A\in\FX$ und betrachten
$P_{n_1}(A) =P_{n_i}(A)$. Dann wird gefordert, dass die
$(\eta_i)_{i\geq 1}$ iid sind. Nun betrachten wir die $\xi_i:=\xi_i^A
:=\chi_A(\eta_i)=\chi_A\circ\eta_i$ für $i=1,2,\ldots$. Diese sind
messbar, da $A\in\FX$ und die Summe über alle $\xi_i$ entspricht der
Anzahl, wie oft $A$ bei $1,\ldots,n$ realisiert wird,
d.\,h. $\nicefrac{1}{n} \sum_{i=1}^n \xi_i$ ist die relative Häufigkeit
des Eintretens von $A$. Es ist klar, dass $(\xi_i)$ iid sind und aus
\autoref{satz:schwachges-571} ergibt sich $\nicefrac{1}{n} \sum \xi_i
\xrightarrow{P} P_{\eta_1}(A)$. Dies ist eine rein qualitative Aussage
und macht keine Angabe über die Konvergenzgeschwindigkeit. Durch die
Anwendung von \autoref{satz:563} kann nun folgender Schluss gezogen
werden: $P("`lvert \nicefrac{1}{n}\sum\xi_i-P_{\eta_1}(A)\rvert\geq
\varepsilon"')\leq
\frac{P_{\eta_1}(A)(1-P_{\eta_1}(A))}{n\varepsilon^2} \leq
\frac{1}{n\varepsilon^2} \Rightarrow \sup_A P("`\lvert \nicefrac{1}{n}
\sum\xi_i -P_{\eta_1}(A)\rvert \geq\varepsilon"')\xrightarrow{n
  \rightarrow \infty} 0$ 

\paragraph{andere Konvergenzarten}

Wir betrachten die schwache Konvergenz. Dazu sei $(Q_n)_{n=1}^\infty$
und Q ein Wahrscheinlichkeitsmaß auf $[\R,\FB]$. Man sagt, $(Q_n)$
konvergiert schwach gegen $Q$, wenn gilt:
\[\int fdQ_n \xrightarrow{n\rightarrow\infty} \int fdQ\]
Dabei ist $f$ eine Abbildung von $\R$ nach $\R$, die stetig und
beschränkt ist. Die Schreibweise dafür, dass $(Q_n)$ schwach gegen $Q$
konvergiert, ist $Q_n\Rightarrow Q$.

\begin{satz*}
  \[Q_n\Rightarrow Q\Leftrightarrow F_{Q_n}(x)\rightarrow F_Q(x)\]
für alle Stetigkeitspunkte von $F_Q$.
\end{satz*}

\begin{satz*}
  \[\eta_n\xrightarrow{P} \Rightarrow P_{\eta_n}\Rightarrow P_\eta\]
\end{satz*}

\begin{remark}
  Die Umkehrung ist im Allgemeinen falsch. Denn sei $(\eta_n)$ iid,
  $P_\eta = P_{\eta_1}, \eta, \eta_n$ unabhängig. Dann folgt, dass
  $P_{\eta_n} =P_\eta\Rightarrow P_{\eta_1}\Rightarrow P_n$. Aber
  $P("`\lvert \eta_n-\eta\rvert\geq\varepsilon"')$
  ist konstant und ungleich 0. Außer fpr den Fall, dass $P_\eta,
  P_{\eta_n}$ Diracmaße sind.
\end{remark}

\begin{satz*}
  Sei $P_\eta=\delta_a$. Dann folgt:
  \[\eta_n\xrightarrow{P} \eta \Leftrightarrow P_{\eta_n}\Rightarrow P_\eta\]
\end{satz*}

\begin{definition}
  Sei $(\eta)_{n=1}^\infty, \eta$ eine zufällige Größe. Dann
  konvergiert $(\eta_n)_{n=1}^\infty$ mit Wahrscheinlichkeit 1 gegen
  $\eta$, wenn gilt\footnote{Schreibweise: $\eta_n\xrightarrow{P=1} \eta$}:
  \[P("`\lim \eta_n =\eta"')=1\]
\end{definition}

\begin{satz*}
  \[\eta_n\xrightarrow{P=1} \eta \Rightarrow \eta_n\xrightarrow{P} \eta\]
\end{satz*}

\begin{satz}[Starkes Gesetz der großen Zahlen]
  Sei $(\xi_k)_{1}^n$ eine Folge unabhängiger identisch verteilter
  Zufallsgrößen mit endlichem Erwartungswert. Dann gilt:
  \[\frac{1}{n} \sum_{k=1}^n \xi_k \xrightarrow{P=1} E\xi_1\]
\end{satz}
%Beweis muss ergänzt werden

\begin{satz}[Satz von Weierstraß]
  Sei $f:[0,1]\rightarrow \R$ stetig. Weiter sei $b_n^f(x):=
  \sum_{k=0}^n f(\frac{n}{k})\binom{n}{k}x^k(1-x)^{n-k}$ ein
  Bernoullipolynom mit $x\in[0,1]$. Dann gilt:
  \[\limn b_n^f(x)=f(x)\]
\end{satz}

\printindex
\end{document}
