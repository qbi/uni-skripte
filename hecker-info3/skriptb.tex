%\documentclass[draft,12pt]{scrreprt}
%
%\usepackage{listings}
%\usepackage{ngerman}
%\usepackage[draft=false]{hyperref}
%\usepackage{fancyvrb}
%\usepackage{amssymb}
%\usepackage{amsmath}
%\usepackage{color} 
%\usepackage{graphicx}
%\usepackage{theorem}
%\usepackage{float}
% \usepackage{ae}  %probably interesting if font-collision
                   % always remember to use pdflatex instead of
                   % dvipdf, fonts are so much better the first way
%
%\fvset{fontsize=\small,frame=single,numbers=left}
%
%\theoremstyle{break}
%\theorembodyfont{\upshape}
%\newtheorem{beweis}{Beweis}
%\theorembodyfont{\itshape}
%\newtheorem{definition}{Definition}
%\newtheorem{satz}{Satz}
%\newfloat{Algorithmus}{h}{alg}[chapter]
%
%\begin{document}
%
\chapter{Sortieren und Selektion}

\( \textup{O}(n\log n) \) ist die obere Schranke f"ur \textsc{MergeSort} (O(n$^2$) f"ur \textsc{Insertionsort}).

Frage: Geht es besser? 
\begin{description}
	\item[- Ja]
		\begin{enumerate}
			\item Mit Parallelrechnern, aber das ist nicht Thema dieser Vorlesung.
			\item Unter bestimmten Bedingungen.
		\end{enumerate}
	\item[- Nein]
		bei allgemeinen Sortierverfahren auf der Basis von Schl"usselvergleichen. Unser Ziel ist der Beweis, da"s f"ur  
		allgemeine Sortierverfahren auf der Basis von Schl"usselvergleichen \( \Omega(n\log n)\) eine untere Schranke ist.
\end{description} 

\begin{beweis}
\textbf{Modellieren des Ansatzes: "`Auf der Basis von Schl"usselvergleichen"'.}
  \begin{itemize}
  \item INPUT ist ein Array mit (o.~B.~d.~A.) paarweise verschiedenen Werten 
  \( (a_1, \dots , a_{\textup{n}})\), \(a_i \in S\), \(i=(1, \dots, n)\) auf das nur
  mit der Vergleichsfunktion  \begin{displaymath} V(i,j) : = \left\lbrace
  \begin{array}{ll} 1, & a_i < a_j \\ 0, & a_i > a_j \end{array} \right.\end{displaymath}
  zugegriffen werden kann.
  \item OUTPUT ist eine Permutation \( \pi \) f"ur die \( a_{\pi (1)} < a_{\pi (2)} < \dots < a_{\pi (n)} \) ist 
\end{itemize}

Sei A ein beliebiges o.~B.~d.~A. deterministisches Sortierverfahren dieser Art.
Die erste Anfrage ist dann nicht \( (a_i < a_j) \) sondern \( (i < j) \)

  \begin{definition}
  \(a(i<j) := \lbrace (a_1, \dots , a_n \vert\, a_i \in S \wedge a_i < a_j \rbrace \)
  \end{definition}

%
% Sebastian Oerding
Der erste Test $V(i,j)$ der Vergleichsfunktion wird immer f"ur dasselbe Indexpaar $(i,j)$ der Eingabe \((a_1, \dots , a_{\textup{n}})\)
ausgef"uhrt, "uber die der Algorithmus A zu diesem Zeitpunkt noch keinerlei Informationen besitzt.

Falls nun $V(i,j)=1$ ist, d. h. f"ur alle Eingaben, die der Menge a(i$<$j)\(=\{(a_1,\ldots,a_{\textup{n}})\in \mathbb{R}^n :
a_{\textup{i}}<a_{\textup{j}}\}\) angeh"oren, wird der zweite Funktionsaufruf immer dasselbe Indexpaar (k,l) als Parameter
enthalten, da A deterministisch ist und zu diesem Zeitpunkt nur wei"s, da"s \(a_{\textup{i}}<a_{\textup{j}}\) ist. Analog wird f"ur alle
Folgen a(j$<$i) derselbe Funktionsaufrauf als zweiter ausgef"uhrt. Die Fortf"uhrung dieser "Uberlegung f"uhrt zu dem vergleichsbasierten
Entscheidungsbaum von Algorithmus A, einem bin"aren Baum, dessen Knoten mit Vergleichen "`\(a_{\textup{i}}<a_{\textup{j}}\)"' beschriftet
sind. An den Kanten steht entweder "`j"' oder "`n"'. Ein kleines Beispiel ist in Abbildung~\ref{271003c} zu sehen.

Genau die Eingabetupel aus der Menge \(\textup{a}(3<4)\cap \textup{a}(3<2)=\{(a_1,\ldots,a_{\textup{n}})\in \mathbb{R}^n : a_3<a_4 \wedge
a_3<a_2\}\) f"uhren zum Knoten $\mathcal{V}$. 

Weil A das Sortierproblem l"ost, gibt es zu jedem Blatt des Entscheidungsbaumes eine Permutation $\pi$, so das nur die Eingaben mit 
\(\textup{a}_{\pi(1)}<\textup{a}_{\pi(2)}<\ldots<\textup{a}_{\pi(\textup{n})}\) zu diesem Blatt f"uhren. Der Entscheidungsbaum hat 
daher mindestens n! Bl"atter. Der Beweis daf"ur stammt fast unver"andert aus \cite{klein}.
 
%
%Annette Eisenbraun
  \begin{figure}
    \begin{center}\input{271003c.latex}\end{center}
  \caption{Entscheidungsbaum}
  \label{271003c}
  \end{figure}

Im Regelfall hat ein Entscheidungsbaum allerdings mehr 
als $n!$ Bl"atter. Es gibt auch Knoten, die die leere Menge enthalten, oder Knoten, die nie erreicht werden k"onnen
(z.~B. Knoten $\mathcal{W}$).
\bigskip

\noindent	
Beispiele f"ur den Baum aus Abbildung~\ref{271003c}:

\noindent
Bei der Eingabe (3, 4, 17, 25) w"are man nach Abarbeitung der Vergleiche 17$<$25, 4$<$17 und 3$<$4 im linkesten Knoten.

\noindent
Bei der Eingabe (17, 4, 3, 25) w"are man nach Abarbeitung der Vergleiche 3$<$25 und 4$<$3 im Knoten $\mathcal{V}$.

\noindent
Wir gehen "uber $\mathcal{V}$, wenn $a_3 < a_4$ und $a_3 < a_2 $, also f"ur alle Tupel aus \( a(3<4) \cap a(3<2) \).

  \begin{satz}
  Ein Bin"arbaum der H"ohe $h$ hat h"ochstens \( 2^h \) Bl"atter.
  \end{satz} 
 
Die H"ohe eines Entscheidungsbaumes ist die maximale Wegl"ange von der Wurzel zu einem Blatt, sie entspricht der Rechenzeit. Wie bereits vorhin angedeutet, mu"s ein
solcher Baum mindestens n! Bl"atter haben, wenn er alle Permuationen der Eingabe ber"ucksichtigen k"onnen soll (z.B. f"ur das
Sortieren), damit mu"s gelten

  \begin{displaymath}
   2^h \geq n! \leftrightarrow h \geq \log n! \geq \log \left(\frac{n}{2}\right)^{\frac{n}{2}}  
  \end{displaymath}

  \begin{displaymath}
  \textup{Der Beweis ist trivial da}\
  n! = 1 \cdot 2 \cdot 3 \ldots \cdot n = 1 \cdot 2 \cdot 3 \ldots \cdot \underbrace{\left(\frac{n}{2}+1\right) \cdot \ldots 
  \cdot n}_{\frac{n}{2}} \geq \left(\frac{n}{2}\right)^{\frac{n}{2}} 
  \end{displaymath}

\(\log \left(\frac{n}{2}\right)^{\frac{n}{2}} = \frac{n}{2} \cdot \log \frac{n}{2} = \frac{n}{2} \log n - \frac{n}{2}
\underbrace{\log 2}_{1} = \frac{n}{3} \log n + \lbrack \frac{n}{6} \log n - \frac{n}{2} \rbrack \geq \frac {n}{3} \log n \) 

f"ur \( n \geq 8\ \textup{ist}\ \log n \geq 3 \leftrightarrow \frac{n}{6} \log n \geq \frac {n}{2} 
\leftrightarrow \frac{n}{6} \log n - \frac{n}{2} \geq 0\), also \(h \geq n \log n\).

Worst-case-Fall im Sortieren hier ist ein Ablaufen des Baumes von der Wurzel bis zu einem Blatt und dies geht bei einem Baum der H"ohe \(n
\log n\) in \(\textup{O}(n \log n)\) Zeit.
\end{beweis}
\hfill q.~e.~d. 

\section{Versch"arfung des Hauptsatzes 1. "`Lineares Modell"'}

\( a_i < a_j \leftrightarrow a_j - a_i > 0
\leftrightarrow \exists \,d > 0 : a_j - a_i = d
\leftrightarrow \exists \, d > 0 : a_j - a_i - d = 0 \) 
\bigskip

Von Interesse ist, ob \( g(x_1 , \dots , x_n) < 0 \), wobei \( g(x_1 , \dots , x_n) = c_1 x_1 + \dots + c_n x_n + d \) mit
\( c_1, \dots , c_n, d \) als Konstanten und \( x_1, \dots, x_n  \) als Variablen.
Da Variablen nur in linearer Form vorkommen, nennt man dies "`Lineares Modell"'.

\begin{satz}
Im linearen Modell gilt f"ur das Sortieren eine untere Zeitschranke von \( \Omega (n \log n) \).
\end{satz}
		
Der Beweis erfolgt dabei "uber die Aufgabenstellung "`$\varepsilon$-closeness"'. Denn, wenn die Komplexit"at der $\varepsilon$-closeness
in einer unsortierten Folge \( \Omega (n \log n) \) und in einer sortierten Folge \( \textup{O}(n)\) ist, dann mu"s die Komplexit"at des
Sortierens \( \Omega (n \log n)\) sein.
		
Beim Elementtest ist eine Menge \( \mathbb{M} \), \( \mathbb{M} \subseteq \mathbb{R}^n\) gegeben, sowie ein variabler Vektor \(
(x_1, \dots , x_n) \). Es wird getestet, ob \( (x_1, \dots , x_n) \in \mathbb{M} \), wobei \( \mathbb{M} \) nat"urlich fest ist. 
	
Bei der $\varepsilon$-closeness sieht die Fragestellung etwas anders aus. Als Eingabe sind wieder n reelle Zahlen \(a_1, \dots , a_n \)
und \( \varepsilon > 0 \) gegeben. Von Interesse ist aber, ob es zwei Zahlen in der Folge gibt, deren Abstand kleiner oder gleich
$\varepsilon$ ist.

Viel k"urzer ist die mathematische Schreibweise: \( \exists \, i,  j \ (1 \leq i, j \leq n):\vert a_i - a_j \vert < \varepsilon \)?

Trivalerweise ist bei einer bereits sortierten Eingabe $\varepsilon$-closeness in O($n$) entscheidbar. Dazu wird einfach nacheinander
gepr"uft, ob \( \vert a_1 - a_2 \vert < \varepsilon \) oder \( \vert a_2 - a_3 \vert < \varepsilon \) oder \dots \( \vert a_{n-1} -
a_n \vert < \varepsilon \).

\begin{satz} 
Wenn $\varepsilon$-closeness die Komplexit"at $\Omega (n\, log\, n)$ hat, so auch das Sortieren.
\end{satz}		
		
\begin{satz}
Die Menge  \( \mathbb{M}  \) habe $m$ Zusammenhangskomponenten. Dann gilt, dass jeder Algorithmus im linearen Modell im worst case
mindestens \( \log m \) Schritte braucht, wenn er den Elementtest l"ost.
\end{satz}
		 	
\begin{beweis}
Jeder Algorithmus A im linearen Modell liefert einen Entscheidungsbaum mit Knoten, in denen f"ur alle m"oglichen Rechenabl"aufe
gefragt wird, ob \( g( x_1, \dots , x_n) < 0 \)  ist. Jetzt gen"ugt es zu zeigen, da"s der Entscheidungsbaum mindestens soviele
Bl"atter hat, wie die Menge \( \mathbb{M}  \) Zusammenhangskomponenten. Mit dem eben bewiesenen folgt, da"s dies "aqivalent zu einer H"ohe
des Entscheidungsbaumes von \(\log (card(\mathbb{M}))=\log m\) ist. Nun sei $b$ ein Blatt des Baumes.

  \begin{definition}
  \(E(b) := \lbrace \vec x \in \mathbb{R}^n : \) Alg. A landet bei der Eingabe von \( \vec x = (x_1, \dots ,x_n) \) in Blatt \( b \rbrace \)
  \end{definition}
		
  \begin{tabular}{rcl}
  \( g(x_1\), & \dots & , \( x_n) < 0 \) ? \\
  ja / & & \( \backslash \) nein\\
  \( g(x_1, \dots, x_n) < 0 \) & & \( g(x_1, \dots x_n) \geq 0 \)
  \end{tabular}
\bigskip
  
Nach Definition des linearen Modells sind diese Mengen $E(b)$ jeweils Durchschnitt von offenen und abgeschlossenen affinen Teilr"aumen
\begin{displaymath}
  \begin{array}{c}
  \{(x_1,\ldots,x_{\textup{n}}) \in \mathbb{R}^n : g(x_1,\ldots,x_{\textup{n}})<0\}\\
  \{(x_1,\ldots,x_{\textup{n}}) \in \mathbb{R}^n : g(x_1,\ldots,x_{\textup{n}})\geq0\}
  \end{array}
\end{displaymath}
Die Mengen $E(b)$ sind konvex und insbesondere zusammenh"angend. F"ur alle Punkte a in $E(b)$ trifft der Algorithmus dieselbe
Entscheidung; folglich gilt entweder \(E(b) \subset \mathbb{M}\) oder \(E(b) \cap \mathbb{M}=\emptyset\). 

Sei $\mathcal{V}$ ein beliebiger Knoten.
Genau die Inputs einer konvexen und zusammenh"angenden Menge f"uhren dorthin (= der Durchschnitt von Halbr"aumen).	
  
  \begin{definition}	
  \( \mathbb{K} \) ist \textit{konvex}
  \(\leftrightarrow  \forall \, p, \, q: p\in \mathbb{K} \wedge q \in \mathbb{K}
  \rightarrow  \overline{pq} \subseteq \mathbb{K}\)
  \end{definition}
  
Nun  gilt \(\mathbb{R}^n = \bigcup_{b \ ist\ Blatt} E(b)\) also
\( \mathbb{M} = \mathbb{R}^n \cap \mathbb{M} = \bigcup_{b \, Blatt} E(b) \cap \mathbb{M} = \dots = \bigcup_{b \in \mathbb{B}} E(b) \)
f"ur eine bestimmte Teilmenge $\mathbb{B}$ aller Bl"atter des Entscheidungsbaumes. Weil jede Menge $E(b)$ zusammenh"angend ist, kann
diese Vereinigung h"ochstens $\vert\mathbb{B}\vert$ viele Zusammenhangskomponenten haben. Die Anzahl aller Bl"atter des
Entscheidungsbaumes kann nicht kleiner sein als $\vert\mathbb{B}\vert$, sie betr"agt also mindestens $m$.

\hfill q.~e.~d. 

Die Komplexit"at des Elementtests ist also \( \Omega (\log m) \)	
  \begin{satz}
  Die Komplexit"at der $\varepsilon$-closeness ist \( \Omega (n \log n) \).
  \end{satz}

  \begin{beweis}
  $\varepsilon$-closeness ist ein Elementtest-Problem!

  \( \mathbb{M}_{\varepsilon^i} := \lbrace ( a_1, \dots , a_n ´) \in \mathbb{R}^n \vert \forall \, i \ne j : \vert a_i - a_j 
  \vert \geq \varepsilon \rbrace\) ist ein spezielles Elementtestproblem.
				
  $\pi$ sei eine Permutation \( \pi(1, \dots ,n) \), dann ist
  \( \mathbb{M}(\pi ) := \lbrace (a_1, \dots , a_n) \in \mathbb{M} \vert a_{\pi (1)} < a_{\pi (2)} < \dots a_{\pi (n)}  \rbrace \)
	
	
  \begin{satz} 
  Die Zusammenhangskomponenten von \( \mathcal{M}_\varepsilon \) sind die Mengen \( \mathcal{M}_\pi  \) (\textup{Beweis "Ubung})
  \end{satz}		
		
  \textit{Folgerung: Jeder Entscheidungsbaum im linearen Modell erfordert \( \log (n!) \) Schritte im worst case. 
  (\( \Rightarrow \Omega (n \log n \)))}
		
  \end{beweis}	
\textit{Folgerung: Sortieren hat im linearen Modell die Komplexit"at \( \Omega (n \log n) \)}
\bigskip
	
Angenommen das stimmt nicht, dann existiert ein schnelleres Sortierverfahren. 
Dann benutze das f"ur den Input von $\varepsilon$-closeness und l"ose $\varepsilon$-closeness schneller als in \( \Omega (n \log n)\)
\( \rightarrow \) dann exisitiert f"ur $\varepsilon$-closeness ein Verfahren von geringerer Komplexit"at als \( \Omega (n \log n) \)
\( \rightarrow \) Widerspruch zur Voraussetzung \( \rightarrow \) Behauptung
		
(relativer Komplexit"atsbeweis)

\hfill q.~e.~d.
\end{beweis}


% 
% Janine Roy
Hilfssatz zur Berechnung der Pfadl"angen (Ungleichung von Kraft):

\noindent
Sei $t_{i}$ die Pfadl"ange f"ur alle m Pfade eines Bin"arbaumes von der Wurzel zu den m Bl"attern, dann gilt:
\begin{displaymath}
\sum_{i=1}^{m} 2^{-t_{i}}\leq 1
\end{displaymath} 


Beweis induktiv "uber $m$

\noindent
$m = 1:$ trivial

\noindent
$m \geq 2:$ Dann spaltet sich der Baum in maximal zwei B"aume auf, deren Pfadl"angen m$_1$ und m$_2$ um eins geringer sind, als die
von m. Die neuen L"angen werden mit t$_{1,1}$ \ldots t$_{1,m_1}$ und t$_{2,1}$ \ldots t$_{2,m_2}$ bezeichnet.
\bigskip

Nach Voraussetzung gilt:
\begin{displaymath}
\sum_{i=1}^{m_{1}} 2^{-t_{1i}}\leq 1\ \textup{und}  \sum_{i=1}^{m_{1}} 2^{-t_{2i}}\leq 1
\end{displaymath}

Jetzt werden Pfadl"angen um 1 gr"o"ser, dann gilt f"ur $t_{1i}$ (und analog f"ur $t_{2i}$):
\begin{displaymath}
2^{(-t_{1i}+1)}=2^{-t_{1i}-1}=2^{-1}\,2^{-t_{1i}}
\end{displaymath}

F"ur T folgt also:
\begin{displaymath}
\sum_{i=1}^{m_1,\, m_2} 2^{-t_{j}}=2^{-1}\left(\sum_{i=1}^{m_{1}} 2^{-t_{1i}}+\sum_{i=1}^{m_{2}} 2^{-t_{2i}}\right)
\leq 2^{-1}(1+1)=1
\end{displaymath}
\hfill  q.e.d

Hilfssatz:
\begin{displaymath}
\frac{1}{m}\, \sum_{i=1}^{m}t_{i}\geq \log m
\end{displaymath}

Dabei gelten die selben Bezeichnungen wie oben.

Beweis:
\[
\frac{1}{m}\,  \sum_{i=1}^{m}2^{-t_{i}} \geq \sqrt[m]{\pi_{i=1}^{m}\ 2^{-t_{i}}}=\sqrt[m]{2^{-t_i}\, 2^{-t_2}\cdot \ldots \cdot 2^{-t_m}}
=\sqrt[m]{2^{-t_1-\, \ldots \, -t_m}}= 2^{-\frac{1}{m}\sum_{i=1}^{m}t_i}
\]
\[
\Rightarrow m \leq 2^{\frac{1}{m}\, \sum_{i=1}^{m}t_i}
\]
\[
\textup{Also gilt f"ur die Pfadl"ange:}\ \log m \leq\frac{1}{m}\, \sum_{i=1}^{m}t_i
\]
\hfill q.e.d.

\begin{satz}
Hauptsatz "uber das Sortieren

Das Sortieren auf der Basis von Schl"usselvergleichen kostet bei Gleichwahrscheinlichkeit aller Permutationen der Eingabe
$\theta (n\ log\ n)$ (mit den schnellstm"oglichen Algorithmen).
\end{satz}

\begin{beweis}
Annahme $m>n!$

\begin{displaymath}
\Omega(n\, \log n) \ni \log n!\leq\frac{1}{n!}\, \sum_{i=1}^{m}t_i \leq\frac{1}{m}\, \sum_{i=1}^{m}t_i
\end{displaymath}

Da wir bereits die untere Schranke bewiesen haben, mu"s \(\frac{1}{m}\, \sum_{i=1}^{m}t_i \geq \frac{1}{n!}\, \sum_{i=1}^{m}t_i\) gelten,
also \(\frac{1}{m} \geq \frac{1}{n!}\) und damit \(m \geq n!\) sein.

Falls \(m>n!\), dann ist aber \(\frac{1}{m} < \frac{1}{n!}\).

Widerspruch zur Voraussetzung (untere Schranke) \(\Rightarrow (m \geq n! \wedge \lnot (m>n!)) \rightarrow m=n!\)
\end{beweis}
\hfill q.e.d.

\section{\textsc{Quicksort}}
Bei \textsc{Quicksort} handelt es sich ebenfalls um ein Teile-und-Hersche-Verfahren.

Eingabe ist wieder ein Feld A=(\(a_0, \ldots, a_{\textup{n}}\)), die Werte stehen dabei in den Pl"atzen \(a_1, \ldots, a_{\textup{n}}\).
Dabei dient $a_0:=-\infty$ als Markenschl"ussel, als sogenanntes Sentinel-Element (siehe auch \textsc{MergeSort}) und
v ist das Pivotelement.

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Quicksort}}
%\lstset{emph={Quicksort}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, Quicksort}, emphstyle=\textsc, escapeinside=~~}  
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Quicksort}, gobble=2]{Quicksort}
  1)Quicksort(l,r)
    if r>l then
      i:=Partition(l,r)
      Quicksort(l,i-1)
      Quicksort(i+1,r) 

  2)Quicksort(l,r)
    if r>l then
    v:= a[r] 
    i:= l-1 
    j:= r
    repeat
      repeat 
        i:=i+1 
      until a[i]>=v
      repeat 
        j:=j-1 
      until a[j]<=v
      t:=a[i]  
      a[i]:=a[j] 
      a[j]:=t
    until j<=i
  Quicksort(l,i-1)
  Quicksort(i+1,r) 
\end{lstlisting}
\end{Algorithmus}

Was passiert bei 2)?
\begin{enumerate}
\item Es wird von links nach rechts gesucht, bis ein Element gr"o"ser v ist
\item Es wird von rechts nach links gesucht, bis ein Element kleiner v ist
\item Dann werden die beiden Elemente vertauscht, falls sie sich treffen, so kommt v an diese Stelle
\end{enumerate}

Beispiel (getauscht werden die \textbf{Fetten}):

\begin{tabular}{lllllllllll}
2 & \textbf{7} & 8 & 9 & 0 & 1 & 5 & \textbf{3} & 6 & 4 & \hspace{15pt} 4 =: Pivotelement\\
2 & 3 & \textbf{8} & 9 & 0 & \textbf{1} & 5 & 7 & 6 & 4 &\\ 
2 & 3 & 1 & \textbf{9} & \textbf{0} & 8 & 5 & 7 & 6 & 4 &\\
2 & 3 & 1 & 0 & \textbf{9} & 8 & 5 & 7 & 6 & \textbf{4} &\\
%2 & 3 & 1 & 0 & 4 & 8 & 5 & 7 & 6 & 9 & \hspace{15pt} $i = 5$, $j = 4$\\
\end{tabular}

%
\begin{figure}[H]
  \begin{center}\input{031103a.latex}\end{center}
  \label{031103a}
\end{figure}

Am Ende sind alle Zahlen, die kleiner bzw. gr"o"ser sind als 4, davor bzw. dahinter angeordnet. 
Jetzt wird der Algorithmus rekursiv f"ur die jeweiligen Teilfolgen aufgerufen.

Nach einer anderen Methode von Sch"oning (nachzulesen in \cite{sedgewick}) sieht die Eingabe 2 7 8 9 0 1 5 7 6 4 nach dem ersten Durchlauf so aus: 
\underline{2 0 1} 4 \underline{7 8 9 5 7 6}

\subsection{Komplexit"at des \textsc{Quicksort}-Algorithmus'}

T(n) sei die Anzahl der Vergleiche, im besten Fall "`zerlegt"' das Pivotelement die Sequenz in zwei gleich gro"se Teile und es gilt die
bekannte Rekurrenz
\begin{displaymath}
T(n)=2T\left(\frac{n}{2}\right)+n \Rightarrow O(n \log n)
\end{displaymath}

Im schlechtesten Fall, n"amlich bei bereits sortierten Folgen, ist aber \(T(n) \in \Omega(n^2)\) 

\begin{satz}
\textsc{Quicksort} ben"otigt bei gleichwahrscheinlichen Eingabefolgen im Mittel etwa $1,38 n \log n$ Vergleiche.
\end{satz}

\begin{beweis}
$n=1$:
\begin{displaymath}
T(1)=T(0)=0
\end{displaymath}
$n \geq 2:$

\begin{displaymath}
T(n)=(n+1)+ \frac{1}{n}\sum_{1\leq k\leq n}[T(k-1)+ T(n-k)]= (n+1)+ \frac{2}{n} \sum_{1\leq k\leq n}T(k-1)
\end{displaymath}

Zur L"osung dieser Rekurrenz wird zuerst die Summe beseitigt, dies sollte jeder selbst nachvollziehen.
Die ausf"uhrliche Rechnung steht im Anhang \ref{quicksort}. Es ergibt sich \(\frac{T(n)}{n+1}= \frac{T(n-1)}{n}+ \frac{2}{n+1}\) und
Einsetzen der Rekurrenz f"uhrt zu:

%\begin{eqnarray*}
%n\, T(n)& = & (n-1)\ T(n-1)-n(n+1)-(n-1)n+2T(n-1)\\
%& = & (n-1)\ T(n-1)+2T(n-1)+n((n+1)-(n-1))\\
%& = & (n+1)\ T(n-1)+2n\\
%\end{eqnarray*}

\begin{displaymath}
\frac{T(n)}{n+1}= \frac{T(n-1)}{n}+ \frac{2}{n+1}=\frac{T(n-2)}{n-1}+\frac{2}{n}+\frac{2}{n+1}=\ldots
\end{displaymath}

\begin{displaymath}
\ldots= \frac{T(2)}{3}+ \sum_{3\leq k \leq n}\frac{2}{k+1} \approx 2\sum_{k=1}^{n}\frac{1}{k}\approx 2
\int_{1}^{n}\frac{1}{x} dx =2 \ln n
\end{displaymath}

\begin{displaymath}
T(n)=2n\, \ln n \approx 1,38 n \, \log n
\end{displaymath}
\end{beweis}
\hfill q.e.d.
%
% Susanne Gernandt
 \section{Ausw"ahlen (Sortieren)}
 Wie in vorangegangenen Vorlesungen besprochen wurde, braucht \textsc{QUICKSORT} bestenfalls $O(n \log n)$ Zeit und im
 worst case, wenn beim "`Teile und Herrsche"' - Verfahren die L"ange der beiden Teilfolgen sehr unterschiedlich ist, $O(n^{2})$ Zeit.

 Um diese ben"otigte Zeit zu verringern, versuchen wir nun, einen Algorithmus zu finden, mit dem wir den worst case ausschlie"sen
 k"onnen.

 Die Idee hierbei ist, ein Element zu finden, da"s in der sortierten Folge ungef"ahr in der Mitte stehen wird und diesen sogenannten
 "`Median"' als Pivot-Element f"ur das "`Teile und Herrsche"' - Verfahren bei \textsc{QUICKSORT} zu verwenden.

 Wie kompliziert ist es nun, diesen Meridian zu ermitteln? Dazu ist zuerst zu sagen, da"s bei einer geraden Anzahl von Elementen zwei
 Elemente als Meridian in Frage kommen. Hierbei ist es allerdings egal, ob man sich f"ur das kleinere oder f"ur das gr"o"sere Element
 entscheidet.

\begin{definition}
 Sei eine Folge $A=(a_{1}, ... , a_{n})$ gegeben, wobei alle $a_{i}$ die Elemente einer linear geordneten Menge sind. 
 Dann ist der Rang eines Elements $a_i$ im Feld $A$ definiert als $Rang(a_{i}:A):=\left|\left\{x|x \in A:x\leq a_{i}\right\}\right|$.
\end{definition}

 Sei $A=(9,-5,2,-7,6,0,1)$, dann ist $Rang(1:A):=4$ (4 Elemente von $A$ sind $\leq 1$)
\bigskip

Sei nun $A$ sortiert, also $A_{sortiert}=(a_{\pi(1)}, ... , a_{\pi(n)})$, dann gilt f"ur das Element $c$ mit $Rang(c:A)=k$ f"ur $1\leq k
\leq n$, da"s $c=a_{\pi(k)}$, das hei"st:
$a_{\pi(1)}\leq ... \leq a_{\pi(k)}\leq ... \leq a_{\pi(n)}$.
Die urspr"ungliche Reihenfolge paarweise gleicher Elemente wird hierbei beibehalten. Im weiteren wird
eine Kurzschreibweise f"ur den Rang verwendet, $a_{(k)}$ ist das Element mit dem Rang k. Ein Feld $A$ mit $n$ Elementen wird kurz mit $A^n$
bezeichnet.

\begin{definition}
Der Median von $A$ ist demzufolge:

$a_{\left(\left\lfloor \frac{n}{2}\right\rfloor\right)}$ (Element vom Rang $\left\lfloor \frac{n}{2}\right\rfloor$ bei n Elementen)
\end{definition}

Hierbei kann allerdings, wie oben schon erw"ahnt, auch $a_{\left(\left\lceil \frac{n}{2}\right\rceil\right)}$, also die n"achstgr"o"sere
ganze Zahl, als Rang festgelegt werden.

Unter Selektion verstehen wir eine Vorbehandlung, die als Eingabe 
$A:=(a_{1}, ... , a_{n})$ erh"alt und unter der Bedingung $1\leq k\leq n$ als Ausgabe das Pivot-Element $a_{(k)}$ liefert.

 Dieser Algorithmus zur Selektion (brute force) besteht nun aus folgenden zwei Schritten:
 \begin{enumerate}
	\item SORT $A$: $a_{\pi(1)}, a_{\pi(2)}, ... , a_{\pi(n)}$ (braucht $O(n \log n)$ Zeit)
	\item Ausgabe des k-ten Elementes
 \end{enumerate}
 F"ur die Selektion wird die "`Median-der-Mediane-Technik"' verwendet. 

 \subsection{Algorithmus $SELECT(A^{n},k)$:}
 \addcontentsline{alg}{Algorithmus}{\textsc{Selektion}($A^n,\, k$)}
 W"ahle beliebige feste Ganzzahl $Q$ (z.B. 5)
 \begin{description}
  \item[Schritt 1:] 
  
  If $\left|A\right|\leq Q$ Then sortiere $A$ \hfill(z.B. mit Bubblesort)

  \hspace{2cm}Ausgabe des k-ten Elementes \hfill(da Anzahl konstant: $O(1)$)

  \hspace{1cm}Else Zerlege $A$ in $\frac{\left|A\right|}{Q}$ Teilfolgen der maximalen L"ange Q
  \item[Schritt 2:] Sortiere jede Teilfolge und bestimme den Median $m_{i}$ \hfill(dauert $O(n)$ Zeit)
  \item[Schritt 3:] $SELECT(\{m_{1}, m_{2}, ... , m_{\frac{\left|A\right|}{Q}}\},\frac{\left|A\right|}{2Q})$, Ausgabe m
  \item[Schritt 4:] Definition von drei Mengen:

  $A_{1}:=\left\{x\in A|x<m\right\}$

  $A_{2}:=\left\{x\in A|x=m\right\}$

  $A_{3}:=\left\{x\in A|x>m\right\}$
  \item[Schritt 5:] If $\left|A_{1}\right|\geq k$ Then $SELECT (A_{1},k)$

  \hspace{1cm}Elseif $\left|A_{1}\right|+\left|A_{2}\right|\geq k$ Then Output m

  \hspace{1cm}Else $SELECT (A_{3},k-(\left|A_{1}\right|+\left|A_{2}\right|))$
 \end{description}
 
  \subsubsection{Zeitanalyse:}
 \hspace{4mm}zu Schritt 1: $max\{O(1),O(n)\}$

 zu Schritt 2: $O(1)$ f"ur jedes Sortieren der $O(n)$ Teilfolgen

 zu Schritt 3: $T(\frac{n}{Q})$

 zu Schritt 4: $O(n)$

 zu Schritt 5: $T(\frac{n}{Q})$
 \bigskip
 
Seien die Mediane $m_{j}$ aller Teilfolgen sortiert. Dann ist $m$, der Median der Mediane, der Median dieser Folge. 
Wieviele Elemente aller Teilfolgen sind gr"o"ser oder gleich $m$?

 $\frac{\left|A\right|}{2Q}$ Mediane der Teilfolgen sind gr"o"ser oder gleich $m$ und f"ur jeden dieser $\frac{\left|A\right|}{2Q}$
 Mediane sind $\frac{Q}{2}$ Elemente "`seiner"' Teilfolge gr"o"ser oder gleich $m$. Damit sind mindestens
 $\frac{\left|A\right|}{2Q}\cdot \frac{Q}{2}=\frac{\left|A\right|}{4}$ Elemente gr"o"sergleich $m$.
	
 $\Rightarrow\left|A_{1}\right|\leq\frac{3}{4}\left|A\right|\Rightarrow T(n)=O(n)+T(\frac{n}{Q})+T(\frac{3}{4}n)$
und $T(n)=O(n)\Longleftrightarrow\frac{n}{Q}+\frac{3}{4}n<n$

 Dies trifft f"ur $Q\geq 5$ zu. Damit hat \textsc{Selection}($A^n,k$) die Komplexit"at O(n).

 \subsection{Algorithmus \textsc{S-Quicksort(A)}}

\begin{Algorithmus}[h]
\addcontentsline{alg}{Algorithmus}{\textsc{S-Quicksort}(A)}
%\lstset{emph={S-Quicksort, Select}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, S-Quicksort, Select}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{S-Quicksort}, gobble=1]{S-Quicksort}
 If $\left|A\right|=2$ und $a_{1}<a_{2}$ Then Tausche $a_{1}$ und $a_{2}$
 Elseif $\left|A\right|>2$ Then
   Select $(A,\frac{\left|A\right|}{2})\longrightarrow m$
   $A_{1}:=\left\{x\leq m, x\in A,\ \textup{soda"s} \left|A_{1}\right|=\left\lceil\frac{\left|A\right|}{2}\right\rceil\right\}$
   $A_{2}:=\left\{x\geq m, x\in A,\ \textup{soda"s} \left|A_{2}\right|=\left\lfloor \frac{\left|A\right|}{2}\right\rfloor\right\}$
   S-Quicksort$(A_{1})$
   S-Quicksort$(A_{2})$
 End
 \end{lstlisting}
  \end{Algorithmus}

Der worst case wird durch die Bestimmung des Medians ausgeschlossen, die die Komplexit"at O(n) hat. Damit gilt die Rekurrenz
$T(n)=2T(\frac{n}{2})+O(n)$ und der Algorithmus funktioniert immer in $O(n \log n)$ Zeit.
 
\section{\textsc{Heapsort}}
Abstrakte Datentypen wurden bereits auf Seite \pageref{ADT} definiert, ebenso der ADT \textbf{Dictionary}. Nun wird ein weiterer
Datentyp, der \textbf{Heap} vorgestellt. Der \textbf{Heap} wird in der Literatur oft auch mit \textbf{Priority Queue} bezeichnet.
Er findet z.B. bei der Verwaltung von Druckauftr"agen Anwendung.

\begin{definition}[Heap]
Der abstrakte Datentyp, der das Quintupel der Operationen
\textsc{MakeHeap}, \textsc{Insert}, \textsc{Max} und \textsc{ExtractMax} unterst"utzt hei"st \textbf{Heap} (oft auch \textbf{Priority Queue} genannt).
 \end{definition}

Hierbei handelt es sich um einen sogenannten Max-Heap. Analog kann ein Min-Heap gebildet werden, indem statt des Maximums immer das
Minimum genommen wird (bzw. statt $\geq$ immer $\leq$ im Algorithmus). 
% 
% 10.11.03 Reana Sommerkorn 
%
\begin{definition}[In place]
Ein Verfahren hei"st \textbf{In place}, wenn es zur Bearbeitung der Eingabe unabh"angig von der Eingabegr"o"se nur zus"atzlichen
Speicherplatz konstanter Gr"o"se ben"otigt.
\end{definition}

Auf der Basis der genannten und sp"ater erl"auterten Operationen mit Heaps kann ein effizientes Sortierverfahren namens
\textsc{Heapsort} definiert werden, das in place funktioniert.

Das wird so erreicht, da"s ein direkter Zugriff auf das Feld besteht und das Sortieren an Ort und Stelle und ohne Verwendung weiteren
Speicherplatzes vorgenommen werden kann. Des weiteren wird garantiert, dass n Elemente in O($n \log n$) Schritten sortiert werden,
unabh"angig von den Eingabedaten.

Die Vorteile von \textsc{MergeSort} (O($n \log n$)) und \textsc{InsertionSort} (In place) werden also vereint.

\begin{definition}[Bin"arer Heap]
Ein \textit{Bin"arer Max-Heap} ist ein spezieller Bin"arbaum (wird im Folgenden nochmals definiert) mit den Eigenschaften, da"s der
Wert jedes Knotens jeweils gr"o"sergleich den Werten seiner S"ohne ist und da"s au"ser dem Level mit der H"ohe 0 alle Level voll besetzt
sein m"ussen. Jeder Level wird von links beginnend aufgef"ullt. Hat also ein Blatt eines Heaps die H"ohe 1 im gesamten Heap, so haben
auch alle rechts davon stehenden Bl"atter genau dieselbe H"ohe.

Dies ist "aquivalent dazu, da"s f"ur eine Folge $F=k_{1},\,k_{2},\,...,\,k_{n}$ von Schl"usseln 
f"ur alle i mit $2\leq i\leq n$ die Beziehung $k_{i}\leq k_{\left\lfloor \frac{i}{2}\right\rfloor}$ gilt (Heap-Eigenschaft), wobei kein
Eintrag im Feld undefiniert sein darf.
\end{definition}

Wegen der Speicherung in einem Array werden die S"ohne auch als Nachfolger bezeichnet.
Wird ein bin"arer Heap in einem Array gespeichert, so wird die Wurzel des Baums an Position 1 im Array gespeichert. Die beiden
S"ohne eines Knotens an der Arrayposition i werden an den Positionen 2i und 2i+1 gespeichert.
Und mit der Heap-Eigenschaft gilt $k_{i}\geq k_{2i}\,\textup{und}\, k_{i}\geq k_{2i+1}$ f"ur alle $i$ mit $2i<n$  

Anschaulicher ist vielleicht die Vorstellung als Bin"arbaum
\begin{figure}[H]
	\begin{center}\input{101103a.latex}\end{center}
	\caption{Bin"arer Max-Heap}
	\label{101103a}
\end{figure}
Das korrespondierende Array w"are A=(8,7,5,3,1).
\bigskip

Um auf den Vater, den linken oder den rechten Sohn eines Knotens i zuzugreifen, gen"ugen die folgenden einfachen Berechnungen:

\begin{tabular}{l|l}
Ziel & Berechnung\\
\hline
Vater(i) & $\left\lfloor\frac{i}{2}\right\rfloor$\\
LSohn(i) & $2i$\\
RSohn(i) & $2i+1$\\
\end{tabular}

F"ur die folgenden "Uberlegungen sind noch weitere Definitionen n"utzlich.

\begin{definition}[Graphentheoretische Definition eines Bin"arbaumes]
Ein \textbf{Bin"arbaum} ist ein Baum, bei dem jeder Knoten vom Grad h"ochstens 3 ist. Ein Knoten mit
h"ochstens Grad 2 kann dabei als Wurzel fungieren. Ein solcher Knoten existiert immer (im Extremfall ist er ein Blatt, hat also den Grad
1). 
\end{definition}

\begin{definition}[Ein vollst"andiger Bin"arbaum]
Ein \textbf{vollst"andiger Bin"arbaum} hat zus"atzlich die Eigenschaften, da"s genau ein Knoten den Grad 2 besitzt und alle Bl"atter die
gleiche H"ohe haben. In diesem Fall wird immer der Knoten vom Grad 2 als Wurzel bezeichnet.
\end{definition}

\begin{satz}
In einem vollst"andigen (im strengen Sinne) Bin"arbaum der H"ohe h gibt es $2^{h}$ Bl"atter und $2^h-1$ innere Knoten.
\end{satz}
Der Beweis ist mittels vollst"andiger Induktion "uber h m"oglich.

\begin{satz}
Der linke Teilbaum eines Bin"arheaps mit n Knoten hat maximal $\frac{2\textup{n}}{3}$ Knoten.
\end{satz}

Beweisidee: Berechne erst wieviele Knoten der rechte Teilbaum hat. Dann benutze dies um die Knotenanzahl des linken Teilbaumes zu
berechnen. Rechne dann das Verh"altnis der Knotenanzahlen zueinander aus.
\begin{beweis}
Da der Grenzfall von Interesse ist, wird von einem m"oglichst asymmetrischen Heap ausgegangen. Sei also der linke Teilbaum voll besetzt
und habe der rechte genau einen kompletten H"ohenlevel weniger. Noch mehr Disbalance ist aufgrund der Heap-Eigenschaft nicht m"oglich.
Dann ist der rechte Teilbaum ebenfalls voll besetzt, hat allerdings einen Level weniger als der linke.

Sei also $i$ die Wurzel eines solchen Baumes mit der H"ohe l und $j$ der linke Sohn von $i$. Dann ist $j$ Wurzel des linken Teilbaumes. 
Nun bezeichne $K(v$) die Anzahl der Knoten im Baum mit der Wurzel $v$. Dann soll also gelten 
\(\frac{K(j)}{K(i)} \leq \frac{2}{3}\).
Nach Voraussetzung gilt $K(j)=2^l-1$ und $K(i)=2^l-1+2^{l-1}-1+1=3 \cdot 2^{l-1}-1$, es folgt
\[\frac{K(j)}{K(i)}= \frac{2^l-1}{3\cdot 2^{l-1}-1} \leq \frac{2^l}{3 \cdot 2^{l-1}}= \frac{2}{3}\]

\hfill q.e.d.
\end{beweis}

\begin{satz}
In einem n-elementigen Bin"arheap gibt es h"ochstens $\left\lceil \frac{n}{2^{h+1}}\right\rceil$ Knoten der H"ohe h.
\end{satz}

\begin{definition}[H"ohe eines Baumes]
Die H"ohe eines Knoten $\vartheta$ ist die maximale L"ange des Abw"artsweges von $\vartheta$ zu einem beliebigen Blatt (also die Anzahl
der Kanten auf dem Weg).
\end{definition}

Beweisidee:
Die Knoten der H"ohe 0 sind die Bl"atter. Dann wird von unten beginnend zur Wurzel hochgelaufen und dabei der
Exponent des Nenners wird immer um eins erh"oht
\begin{figure}[H]
	\begin{center}\input{101103b.latex}\end{center}
	\caption{Bin"arer Min-Heap}
	\label{101103b}
\end{figure}

Aus der Heap-Eigenschaft folgt, da"s das Maximum in der Wurzel steht. Sei nun also F=(8, 6, 7, 3, 4, 5, 2, 1) gegeben. Handelt es sich
dabei um einen Heap?

Ja, da $F_{i}\geq F_{2i}$ und $F_{i}\geq F_{2i+1}$, f"ur alle $i$ mit $5\geq i \geq 1$, da $8\geq 6 \wedge 8\geq 7 \wedge6\geq 3 \wedge 
6\geq 4 \wedge 7\geq 5 \wedge 7\geq 2 \wedge 3\geq 1$.

Dieser Max-Heap sieht dann grafisch wie folgt aus:

\begin{figure}[H]
	\begin{center}\input{101103c.latex}\end{center}
%	\caption{Bin"arer Max-Heap}
	\label{101103c}
\end{figure}
\vspace{3mm}

Sei nun eine Folge von Schl"usseln als Max-Heap gegeben und die Ausgabe sortiert in absteigender Reihenfolge gew"unscht; f"ur einen
Min-Heap m"ussen die Relative "`kleiner"' und "`gr"o"ser"' ausgetauscht werden. Um die Erl"auterung einfacher zu halten, wird von dem
gr"o"seren Sohn gesprochen, nicht von dem Knoten mit dem gr"o"seren gespeicherten Wert. Genauso werden Knoten und nicht Werte vertauscht.
Dies ist allerdings formal falsch!

F"ur den ersten Wert ist dies einfach, da das Maximum bereits in der Wurzel steht.
Dies l"a"st sich ausnutzen, indem der erste Wert in die Ausgabe aufgenommen wird und aus dem Heap entfernt wird, danach wird aus den
verbleibenden Elementen wieder ein Heap erzeugt. Dies wird solange wiederholt, bis der Heap leer ist. Da in der Wurzel immer das Maximum
des aktuellen Heaps gespeichert ist, tauchen dort die Werte der Gr"o"se nach geordnet auf.

Der neue Heap wird durch Pfl"ucken und Versickern des Elements mit dem gr"o"sten Index erreicht. Dazu wird die Wurzel des anf"anglichen Heaps geleert. Das Element mit dem gr"o"sten Index wird aus dem Heap gel"oscht
(gepfl"uckt) und in die Wurzel eingesetzt.
Nun werden immer die beiden S"ohne des Knotens verglichen, in dem der gepfl"uckte Wert steht. Der gr"o"sere der beiden S"ohne wird mit
dem Knoten, in dem der gepfl"uckte Wert steht, vertauscht.

Der gepfl"uckte Wert sickert allm"ahlich durch alle Level des Heaps, bis die
Heap-Eigenschaft wieder hergestellt ist und wir einen Heap mit n-1 Elementen erhalten haben.

Im obigen Beispiel hei"st das also:
Wenn man nun die Wurzel (hier: 8) wieder entfernt, wandert die 1 nach oben und in diesem Moment ist es kein Bin"arheap. D.h. es mu"s
ein neuer Heap erzeugt werden und dies geschieht unter zu Zuhilfenahme von Heapify (Versickern).
In der grafischen Darstellung wurde die Position im Array rechts neben die Knoten geschrieben:
\begin{figure}[H]
	\begin{center}\input{101103d.latex} \hspace{2cm}\input{101103e.latex}\end{center}
%	\caption{Bin"arer Max-Heap}
	\label{101103de}
\end{figure}
\vspace{3mm}
\noindent
$\rightarrow$ entnehme die Wurzel \hspace{5cm}
$\rightarrow$ setze 1 an die Wurzel

\begin{figure}[H]
	\begin{center}\input{101103f.latex}\hspace{6mm}\input{101103g.latex}\hspace{6mm}\input{101103h.latex}\end{center}
%	\caption{Bin"arer Min-Heap}
	\label{101103f}
\end{figure}
$\rightarrow$ Heapify (Versickern) der "`1"' 

Als Algorithmus:
  \begin{Algorithmus}[h]
  \addcontentsline{alg}{Algorithmus}{\textsc{Heapify}(A, i)}
%  \lstset{emph={Heapify}}
  \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
  \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Heapify\textnormal{(A)}}, gobble=4]{Heapify}
    l:= LSohn(i)
    r:= RSohn(i)
    if l <= Heapsize[A] und A[l]=Succ(A[i])
      then Max:= l
      else Max:= i
    if r <= Heapsize[A] und A[r]=Succ(A[Max])
      then Max:= r
    if Max != i
      then tausche A[i] und A[Max]
        Heapify(A,Max) 
    \end{lstlisting}
  \end{Algorithmus}

\textbf{INPUT:} F bzw. A in einen Heap "uberf"uhren
\begin{Algorithmus}[h]
\addcontentsline{alg}{Algorithmus}{\textsc{Build-Heap\textnormal{(A)}}}
%\lstset{emph={Heapify}}
  \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Build-Heap\textnormal{(A)}}, gobble=2]{Build-Heap}  
   Heapsize[A]:= L~\ttfamily"a~nge[A]
   for i:= $\left\lfloor \texttt{L\"ange(}\frac{A}{2}\texttt{)} \right\rfloor$ down to 1
      Heapify(A,i)
    \end{lstlisting}
  \end{Algorithmus}

  \begin{Algorithmus}[h]
  \addcontentsline{alg}{Algorithmus}{\textsc{Heapsort}(A)}
%  \lstset{emph={Build-Heap, Heapify}}
  \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, Heapify,
  Build-Heap}, emphstyle=\textsc, escapeinside=~~}  
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Heapsort\textnormal{(A)}}, gobble=4]{Heapsort}
    Build-Heap(A)
    for i:= Laenge[A] down to 2
      do tausche A[1] und A[i]
        Heapsize[A]:= Heapsize[A]-1
        Heapify(A,1)
    \end{lstlisting}
  \end{Algorithmus}
  
Komplexit"at der einzelnen Algorithmen

F"ur \textsc{Heapify} gilt die Rekurrenz T(n)=T($2/3$n)+O(1), damit gilt T(n) $\in$ O($\log n$)

F"ur \textsc{Build-Heap} ist dies etwas komplizierter Sei $h$ die H"ohe eines Knotens und $c$ eine Konstante gr"o"ser 0, dann gilt:
 
\[\sum_{h=0}^{\left\lfloor(\log n)\right\rfloor}\left\lceil\frac{n}{2^{h+1}}\right\rceil \cdot ch \in O\left(n\cdot\sum_{h=0}^{\lfloor(\log
n)\rfloor}
\frac{n}{2^{h+1}}\right) \in O\left(n\cdot\underbrace{\sum_{h=0}^{\infty}
\frac{h}{2^{h+1}}}_{=2}\right)=O(n)\] 
Damit kostet \textsc{Build-Heap} nur O($n$).

Damit hat \textsc{Heapsort} die Komplexit"at O($n \log n$). In jedem der O($n$) Aufrufe von \textsc{Build-Heap}  braucht
\textsc{Heapify} O($\log n$) Zeit.
%
% 12.11.03 Kay Schieck
\subsection{Priority Queues}
In der Literatur wird die Begriffe Heap und Priority Queue (Priorit"atswarteschlange) oftmals synonym benutzt. Hier wird begrifflich
etwas unterschieden und Heaps werden f"ur die Implementierung von solchen Warteschlangen benutzt. Auch die Bezeichnungen f"ur
\textsc{BUILDHEAP} ist nicht einheitlich, in einigen B"uchern wird stattdessen \textsc{MAKEHEAP} oder \textsc{MAKE} verwendet.

\begin{definition}
Der abstrakte Datentyp, der die Operationen \textsc{Make-Heap, Insert, Max, ExtractMax} und \textsc{Increase Key} unterst"utzt wird
\textbf{Priority
Queue} genannt.
\end{definition}

Wie bei einem Heap kann nat"urlich auch hier immer mit dem Minimum gearbeitet werden, die Operationen w"aren dann \textsc{Buildheap,
Insert, Min, ExtractMin} und \textsc{Decrease Key}

Behauptung: Binary Heaps unterst"utzen Warteschlangen.

\begin{Algorithmus}[h]
\addcontentsline{alg}{Algorithmus}{\textsc{ExtractMax}(A)}
%\lstset{emph={Heapify}}
  \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{ExtractMax\textnormal{(A)}}, gobble=4]{ExtractMax(A)}
    if heap-size[A] < 1
         then Fehler
    Max := A[1]
    A[1] := A[heap-size[A]]
    heap-size[A] := heap-size[A]-1
    Heapify(A, 1)
    Ausgabe Max
\end{lstlisting}

Der Heap wird mittels A an Extractmax "ubergeben. Diese Funktion merkt sich die Wurzel (das Element mit dem gr"o"sten Schl"ussel).
Dann nimmt es das letzte im Heap gespeicherte Blatt und setzt es als die Wurzel ein. Mit dem Aufruf von Heapify() wird die
Heap-Eigenschaft wieder hergestellt. Das gemerkte Wurzel wird nun ausgegeben.
Falls die Anzahl der Elemente in A (heap-size) kleiner als 1 ist, wird eine Fehlermeldung ausgel"ost.
\end{Algorithmus}

Increasekey sorgt daf"ur, das nach "Anderungen die Heapbedingung wieder gilt. Es vertauscht solange ein Element mit dem Vater, bis
das Element kleiner ist.

\begin{Algorithmus}[h]
\addcontentsline{alg}{Algorithmus}{\textsc{IncreaseKey}(A, x, k)}
%\lstset{emph={Add}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Add, MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{IncreaseKey\textnormal{(A, x, k)}}, gobble=4]{IncreaseKey(A, x, k)}
     if key[x] > k
         then Fehler "Alter Schl~\ttfamily{"u}~sselwert ist gr~\ttfamily{"o"s}~er"    
     key[x] := k
     y := x
     z := p[y]
     while z $\not=$ NIL und key[y] > key[z]
         do tausche key[y] mit key[z]
         y := z
         z := p[y]
\end{lstlisting}

Die "ubergebene Variable x ist der Index im Array und k ist der neue Schl"usselwert. Der Vater des Knotens x in
der Baumstruktur wird mit p[x] bezeichnet.
\end{Algorithmus}
\textsc{IncreaseKey} kostet $O(\log n)$, dazu siehe auch Skizze~\ref{121103a}.
Damit wird \textsc{IncreaseKey} unterst"utzt und wir wenden uns der Operation \textsc{Insert} zu.
Dabei wandert key solange nach oben, bis die Heap-Eigenschaft wieder gilt. Damit kostet auch \textsc{Insert} $O(\log n)$.

\begin{figure}[H]
  \begin{center}\input{121103a.latex}\end{center}
  \caption{\textsc{Heapify}}
  \label{121103a}
\end{figure}
Ist der im Vaterknoten gespeicherte Wert gr"o"ser als der im Sohn gespeicherte, vertauscht  \textsc{Increasekey} die beiden Werte.

\begin{figure}[H]
  \begin{center}\input{121103b.latex}\end{center}
  \caption{Insert}
  \label{121103b}
\end{figure}

Damit unterst"utzen bin"are Heaps:
\begin{itemize}
\item \textsc{BuildHeap} $O(1)$ - (im Sinne von \textsc{MakeHeap} = Schaffen der leeren Struktur)
\item \textsc{Insert} $O(log\ n)$
\item \textsc{Max} $O(1)$
\item \textsc{ExtractMax} $O(log\ n)$
\item \textsc{IncreaseKey} $O(log\ n)$
\end{itemize}
Somit ist die Behauptung erf"ullt.

Ein interesantes Anwendungsbeispiel ist, alle $\leq$ durch $\geq$ zu ersetzen. Also \textsc{Max} durch \textsc{Min},
\textsc{ExtractMax} durch \textsc{ExtractMin} und \textsc{IncreaseKey} durch \textsc{Decreasekey} zu ersetzen und nur \textsc{Insert}
zu belassen.

\section{\textsc{Dijkstra}}

 Problem:
\begin{figure}[H]
  \begin{center}\input{121103c.latex}\end{center}
  \caption{K"urzesten Weg finden}
  \label{121103c}
\end{figure}

Sehr wichtig f"ur die Informatik sind Graphen und darauf basierende Algorithmen. Viele anscheinend einfache Fragestellungen erfordern
recht komplexe Algorithmen.

So sei z.B. ein unwegsames Gel"ande mit Hindernissen gegeben und der k"urzeste Weg dadurch herauszufinden. F"ur die algorithmische
Fragestellung ist es v"ollig egal, ob es sich um ein Gel"ande oder eine andere Fl"ache handelt. Deswegen wird soweit abstrahiert, da"s
aus der Fl"ache und den Hindernissen Polygone werden. Doch unver"andert lautet die Fragestellung, wie man hier den k"urzesten Weg
finden kann. Es ist zu erkennen, das dies nur "uber die Eckpunkte (von Eckpunkt zu Eckpunkt)
zu bewerkstelligen ist. Dabei darf nat"urlich nicht der zugrunde liegende Graph verlassen werden. Ist eine aus Strecken zusammengesetzte
Linie der k"urzeste Weg?

\begin{definition}[Visibility Graph]
M = Menge der Ecken = Menge der Polygonecken.
$a, b \in M \rightarrow \overline{ab}$ ist Kante des Graphen $\leftrightarrow \overline{ab}$ ganz innerhalb des Polygons liegt.
\end{definition}

\begin{satz}
Der k"urzeste Pfad verl"auft entlang der Kanten des Sichtbarkeitsgraphen (Lorenzo-Parez).
\end{satz}
Problem \textsc{all-to-one shortest paths}

One (In Abbildung~\ref{121103d} ist das der Punkt a) ist der Startpunkt und von allen anderen
Punkten wird der k"urzeste Weg dahin berechnet.
Die Gewichte an den Kanten sind dabei immmer $\geq 0$.

\begin{figure}[h]
  \begin{center}\input{121103d.latex}\end{center}
  \caption{Graph mit gerichteten Kanten}
  \label{121103d}
\end{figure}

Zur L"osung des Problems verwenden wir den Algorithmus von Dijkstra (1959).

\begin{description}
\item[Paradigma: ] Es ist ein Greedy (gieriger) Algorithmus.
(einfach formuliert: ich denk nicht gro"s nach, ich nehm einfach den k"urzesten Weg um von Punkt a weg zu kommen)
\item[Start: ] $V$ ist die Menge der Knoten, $W$ ist die Menge der erreichten Knoten.
\end{description}

\begin{figure}[H]
  \begin{center}\input{121103e.latex}\end{center}
  \caption{Menge W wird aufgeblasen}
  \label{121103e}
\end{figure}
Nun zum eigentlichen Algorithmus:

\begin{Algorithmus}[h]
\addcontentsline{alg}{Algorithmus}{\textsc{Dijkstra}}
%\lstset{emph={ExtractMin, DecreaseKey}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={ExtractMin,
DecreaseKey, MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Dijkstra}, gobble=1]{Dijkstra}
 for all v in V do d(v) := + $\infty$
 d(s) := 0; W := $\emptyset$
 Initalisiere Struktur V (mit d(v), v in V)
 while V \ W $\not= \emptyset$ do
   v := Min(V \ W); ExtractMin(V \ W);
   W := W$\cup${v}
   for all w in Succ(v) \ W do
     if d(r) + l(vw) < d(w)
       then DecreaseKey(w, d(v) + l(vw))
\end{lstlisting}

Der Nachweis der Korrektheit dieses Algorithmus ist sehr schwer und soll nicht Gegenstand
dieses Skriptes sein.
\end{Algorithmus}   % noch nicht fertig

Ein einfaches Beispiel soll seine Funktion veranschaulichen. Die verwendeten Knoten und Kanten entsprechen denen
aus Abbildung~\ref{121103d}. Jede Zeile der Tabelle entspricht einer Ausf"uhrung des Rumpfes der while Schleife (ab Zeile 5)
und zeigt die Werte, welche die verschiedenen Variablen annehmen. Aus Gr"unden der "Ubersicht werden nur 3 Spalten
augef"uhrt, der / tritt deshalb nochmal als Trennhilfe auf:

\begin{center}
\begin{tabular}{|l|l|l|}\hline
$(v,d(v)):v \in W$ /		& $v = min(V \setminus W)$ /	& $v=min(V \setminus W),$ \\
$ (v,d(v)): v \in V\cup W$	& $SUCC(v)$		& $w \in SUCC(v)\setminus (W\cup {v}),$ \\
				&			& $(w,l(\vec{vw})$ / \\
				&			& $min(d(w),d(v)+l(\vec{vw}))$ \\ \hline

$\emptyset$ /			& $a$ /			& $v=a:(c,13),(b,7)$ / \\
$(a,0),(b,\infty),(c,\infty),$	& $c,b$			& $(c,13),(b,7)$ \\
$(d,\infty),(e,\infty)$		& 			& \\ \hline

$\{(a,0)\}$ /			& $b$ /			& $v=b:w\in \{c,d,e\}\setminus \{a,b\},$ \\
$(b,7),(c,13),(d,\infty),(e,\infty)$ & $c,d,e$		& $(c,5),(d,12),(e,4)$ / \\
				&			& $(c,12),(d,19),(e,11)$ \\ \hline

$(a,0),(b,7)$ /			& $e$ /			& $v=e, w\in\{a,d\}\setminus \{a,b,e\}$ \\
$(e,11),(c,12),(d,19)$		&			& $=\{d\}, (d,7)$ / \\
				& $a,d$			& $min(19,18):(d,18)$ \\ \hline

$(a,0),(b,7),(e,11)$ /		& $c$ /			& \\
$(c,12),(d,18)$			& $b,d$			& \\
n"achste Zeile $(d,13)$		&			& \\ \hline
\end{tabular}
\end{center}

Nun interessiert uns nat"urlich die Komplexit"at des Algorithmus. Dazu wird die jeweilige Rechenzeit
der einzelnen Zeilen betrachtet, wobei $|V|=n$ und $|{Knoten}| = m$.
\begin{enumerate}
\item $O(|V|) = O(n)$
\item $O(n)$
\item meist in $O(1)$
\item nicht zu beantworten
\item nicht zu beantworten
\item $O(1)$ im Regelfall (es kann auch komplexer sein, da es darauf ankommt, wie $W$ verwaltet wird)
\item
\item $O(1)$
\item die Komplexit"at von DECREASEKEY ist auch nicht zu beantworten
\end{enumerate}

Die Laufzeit h"angt wesentlich davon ab, wie die Priority Queue verwaltet wird.
\begin{enumerate}
\item M"oglichkeit: $V$ wird in einem Array organisiert \hfill $\rightarrow O(n^{2})$
\item M"oglichkeit: Bin"arer Heap f"ur V$\backslash$W, dann geht EXTRACTMIN in $O(n\ log\ n)$ und DECREASEKEY in $O(m\ log\ n)$ \hfill $\rightarrow O((m+n)\ log\ n)$
\item M"oglichkeit: V in einen Fibonacci-Heap \hfill $\rightarrow O((n \log n)+m)$
\end{enumerate} 
%
% 17.11.03 Alexander Hofmeister
%
  \section{\textsc{Counting Sort}}
  Einen ganz anderen Weg zum Sortieren von Zahlen beschreitet \textsc{Counting Sort}.
  Es funktioniert, unter den richtigen Bedingungen angewendet, schneller als in O($n \log n$) und basiert nicht auf
  Schl"usselvergleichen.

  \begin{Algorithmus}[H]
  \addcontentsline{alg}{Algorithmus}{\textsc{Counting Sort}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={ExtractMin,
DecreaseKey, MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}  
    \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Counting Sort}, gobble=4]{Counting Sort}
    for i := 0 to k do
     C[i]:= 0;
     for j := 1 to Laenge[A] do
       C[A[j]] := C[A[j]]+1;
     for i := 1 to k do
       C[i] := C[i] + C[i-1];
     for j := Laenge[A] downto 1 do
       B[C[A[j]]] := A[j];
       C[A[j]] := C[A[j]]-1
    \end{lstlisting}

  Das Sortierverfahren \textsc{Counting Sort}  belegt eventuell auf Grund der Verwendung von zwei zus"atzlichen Feldern 
  B und C sehr viel Speicherplatz.
  Es funktioniert ohne die Verwendung von Schl"usselvergleichen. Es wird zu Begin ein Z"ahler (Feld C) erzeugt 
  dessen Gr"o"se abh"angig ist von der Anzahl der m"oglichen in A 
  enthaltenen Zahlen (Zeile 1 und 2). F"ur jeden m"oglichen Wert in A, also f"ur jeden Wert des Zahlenraumes, wird eine Zelle des Feldes
  reserviert. Seien die Werte in A z.B. vom Typ Integer mit 16 Bit. Dann gibt es 2$^{16}$ m"ogliche Werte und das Feld C w"urde f"ur
  jeden der 65536 m"oglichen Werte eine Zelle erhalten; dabei wird C[0] dem ersten Wert des Zahlenraums zugeordnet, C[1] dem zweiten
  Wert des Zahlenraumes usw.
  Anschlie"send wird die Anzahl der in 
  A enthaltenen Elemente schrittweise in C geschrieben. Bei mehrfach 
  vorhandenen Elementen wird der entsprechende Wert erh"oht, daher kommt auch 
  der Name Counting Sort (Zeile 3 und 4). Nun werden die Adressen im Feld
  berechnet. Dazu wird die Anzahl eines Elemente mit der Anzahl eines 
  Vorg"angerelements addiert um die entsprechende Anzahl im Ausgabefeld frei 
  zu halten (Zeile 5 und 6). Zum Schluss wird die richtige Reihenfolge durch 
  zur"ucklaufen des Arrays A und der Bestimmung der richtigen Stelle, mit Hilfe 
  von C, in B geschrieben. Bei \textsc{Counting Sort} handelt es sich um ein stabiles Sortierverfahren.
  \end{Algorithmus}
  
  \begin{definition}[Stabile Sortierverfahren]
  Ein Sortierverfahren hei"st \textbf{stabil}, falls mehrfach vorhandene Elemente in der Ausgabe in der Reihenfolge auftauchen, in der sie auch
  in der Eingabe stehen.
  \end{definition}
  
  \subsection{\textsc{Counting Sort} an einem Beispiel}

  \begin{description}
    \item[Input:] $A\,=\,(\,1_a\,,\,3\,,\,2_a\,,\,1_b\,,\,2_b\,,\,2_c\,,\,1_c\,)$ und $k\,=\,3$
    \item[Output:] $B\,=\,(\,1_a\,,\,1_b\,,\,1_c\,,\,2_a\,,\,2_b\,,\,2_c\,,\,3\,)$, $C\,=\,(\,0,\,0,\,3,\,6\,)$
    \item[Ablauf:]
    \begin{tabular}[t]{*{3}{c}}
      Zeile & Feld $C$ & Erl"auterung\\
      \hline
      nach 1 und 2 & $<\,0,\,0,\,0,\,0\,>$ & Z"ahler wird erzeugt und 0 gesetzt\\
      nach 3 und 4 & $<\,0,\,3,\,3,\,1>$   & Anzahl der Elemente wird ``gez"ahlt''\\
      nach 5 und 6 & $<\,0,\,3,\,6,\,7>$   & Enth"alt Elementzahl kleiner gleich i\\
      nach 9       & $<\,0,\,0,\,3,\,6>$   & $A[\,i\,]$ werden in B richtig positioniert\\
    \end{tabular}
    
    
  \end{description}

  \subsection{Komplexit"at von \textsc{Counting Sort}}
   Die Zeitkomplexit"at von \textsc{Counting Sort} f"ur einen Input von $A^n$ mit 
   \(k \in \textup{O}(\textup{n})\) ist 
   \\ $T(\,n\,)\, \in \,O(\,n\,)\,\cup\,O(\,k\,)$.
  \begin{satz}
   Falls $k\,\in \,O(\,n\,)$, so funktioniert \textsc{Counting Sort} in O(n).
  \end{satz}

Die St"arke von \textsc{Counting Sort} ist gleichzeitig auch Schw"ache. So ist aus dem obigen bereits ersichtlich, da"s dieses Verfahren
z.B. zum Sortieren von Flie"skommazahlen sehr ungeeignet ist, da dann im Regelfall riesige Z"ahlfelder erzeugt werden, die mit vielen
Nullen besetzt sind, aber trotzdem Bearbeitungszeit (und Speicherplatz!) verschlingen.

\section{Weitere Sortieralgorithmen}
  Au"ser den hier aufgef"uhrten Sortieralgorithmen sind f"ur uns noch \textsc{Bucket Sort} und
  \textsc{Radix Sort} von Interesse.

  \chapter{Einfache Datenstrukturen: Stapel, B"aume, Suchb"aume}
  Bevor mit den einfachen Datenstrukturen begonnen wird,
  noch eine Bemerkung zum Begriff des abtrakten Datentyps (siehe Seite \pageref{ADT}).
  Auch dieser Begriff wird leider nicht immer einheitlich verwendet. Mal wird er wie eingangs definiert oder als Menge von Operationen,
  dann wieder wie in der
  folgenden Definition oder noch abstrakter, wie z.B. in \cite{guting}, wo ein ADT als Signatur vereinigt mit Axiomen f"ur die
  Operationen definiert wird.

  \begin{definition}[ADT]
    Ein Abstrakter Datentyp ist eine (oder mehrere) Menge(n) von Objekten und
    darauf definierten Operationen
  \end{definition}

  \subsubsection{Einige Operationen auf ADT's}
  \begin{description}
  \item Q sei eine dynamische Menge
   
   \begin{tabular}[t]{@{}ll@{}} %{*{2}{l}}
      Operation & Erl"auterung \\
      \hline
      \textsc{Init}(Q) & Erzeugt eine leere Menge Q \\
      \textsc{Stack-Empty} & Pr"uft ob der Stack leer ist \\
      \textsc{Push}(Q,x),\textsc{Insert}(Q,x) & F"ugt Element x in Q ein ( am Ende ) \\
      \textsc{Pop}(Q,x),\textsc{Delete}(Q,x) & Entfernt Element x aus Q (das Erste x was auftritt)\\
      \textsc{Pop} & Entfernt letztes Element aus Q\\
      \textsc{Top} & Zeigt oberstes Element an \\
      \textsc{Search}(Q,x) & Sucht El. x in Q (gibt erstes Vorkommende aus)\\
      \textsc{Min}(Q) & Gibt Zeiger auf den kleinsten Schl"usselwert zur"uck \\
      \textsc{Max}Q) & Gibt Zeiger auf den gr"o"sten Schl"usselwert zur"uck \\
      \textsc{Succ}(Q,x) & Gibt Zeiger zur"uck auf das n"achst gr. El. nach x\\
      \textsc{Pred}(Q,x) & Gibt Zeiger zur"uck auf das n"achst kl. El. nach x 
   \end{tabular}
    
  \end{description}

  \section{Bin"are Suchb"aume}

   \begin{definition}[Bin"arer Suchb"aume]
    Ein bin"arer Suchbaum ist ein Bin"arbaum folgender Suchbaumeigenschaft:
    Sei x Knoten des bin"aren Suchbaumes und Ahn vom Knoten y. Falls der Weg von x nach y "uber den linken Sohn von x erfolgt, ist
    $key[\,y\,]\leq key[\,x\,]$. Andernfalls istt $key[\,y\,]> key[\,x\,]$.
   \end{definition}

   \begin{definition}
   Ein Suchbaum hei"st \textbf{Blattsuchbaum}, falls die Elemente der dynamischen Menge im Gegensatz zum normalen Suchbaum nur in den
   Bl"attern gespeichert werden.
   \end{definition}

  \subsection{Beispiel f"ur einen bin"aren Suchbaum}
  \begin{figure}[h]
  \begin{center} \input{171103a.latex} \end{center}
   \caption{Bin"arer Suchbaum}
   \label{171103a}
  \end{figure}

  \begin{itemize}
   \item Rotes Blatt wird erst durch INSERT hinzugef"ugt
   \item $Q\,=\,\{\,5,\,2,\,1,\,3,\,8,\,7,\,9\,\}$
   \item \textsc{Search}(Q,4): \\
      Beim Suchen wird der Baum von der Wurzel an durchlaufen und der zu 
      suchende Wert mit dem Wert des Knoten verglichen. Ist der zu suchende 
      Wert kleiner als der Knotenwert wird im linken Teilbaum weitergesucht. 
      Ist er gr"o"ser wie der Knotenwert wird im rechten Teilbaum 
      weitergesucht. Zur"uckgegeben wird der zuerst gefundene Wert. Ist das 
      Element nicht im Suchbaum enthalten, wird NIL bei Erreichen eines 
      Blattes zur"uckgegeben.\\
      Im Beispiel wird der Suchbaum in der Reihenfole 5, 2, 3 durchlaufen 
      und dann auf Grund des Fehlens weiterer Knoten mit der R"uckgabe von 
      NIL verlassen.
   \item INSERT(Q,4): \\
      Beim Einf"ugen wird das einzuf"ugende Element mit dem jeweiligen 
      Element des aktuellen Knotens verglichen. Begonnen wird dabei in der 
      Wurzel. Ist das einzuf"ugende Element gr"o"ser, wird im Baum nach 
      rechts gegangen, ist es kleiner, nach links. Ist in ein 
      Blatt erreicht, wird dann, die Suchbaumeigenschaft erhaltend, entweder rechts oder 
      links vom Blatt aus eingef"ugt.\\
      Im Beispiel wird der Baum in der Reihenfolge 5, 2, 3 
      durchlaufen und die 4 dann rechts von der 3 als neues Blatt mit dem Wert 4
      eingef"ugt.
   \item Nach Einf"ugen: $Q\,=\,\{\,5,\,2,\,1,\,3,\,4,\,9,\,7,\,8\,\}$
  \end{itemize}

  \subsection{Operationen in bin"aren Suchb"aumen}

  \begin{Algorithmus}[H]
    \addcontentsline{alg}{Algorithmus}{\textsc{Tree-Search}}
    %\lstset{emph={Tree-Search}}
    \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={ExtractMin,
    Tree-Search, DecreaseKey, MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
    \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Tree-Search}, gobble=4]{Tree-Search}
    if x=NIL or k=key[x]
       then Ausgabe x
    if k<key[x]
       then Ausgabe Tree-Search(li[x],k)
       else Ausgabe Tree-Search(re[x],k)
    \end{lstlisting}

    Bei \textsc{TREE-SEARCH} wird der Baum von der Wurzel aus durchlaufen. 
    Gesucht wird dabei nach dem Wert k. Dabei ist x der Zeiger, der auf den 
    Wert des aktuellen Knotens zeigt. In den ersten beiden Zeilen 
    wird der Zeiger zur"uckgegeben wenn ein Blatt erreicht oder der 
    zu suchende Wert gefunden ist (Abbruchbedingung f"ur Rekursion). In 
    Zeile 3 wird der zu suchende Wert mit dem aktuellen Knotenwert 
    verglichen und anschl"ie"send in den Zeilen 4 und 5 entsprechend im 
    Baum weitergegangen. Es erfolgt jeweils ein rekursiver Aufruf.\\
    Die Funktion wird beendet wenn der Algorithmus in einem Blatt 
    angekommen ist oder der Suchwert gefunden wurde.
    \end{Algorithmus}
 
 \subsubsection{Traversierung von B"aumen}
   \begin{Algorithmus}[H]
   \addcontentsline{alg}{Algorithmus}{\textsc{Treepostorder}}
   %\lstset{emph={Treepostorder}}
   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search}, emphstyle=\textsc, escapeinside=~~}
   \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Treepostorder\textnormal{(x)}}, gobble=4]{Treepostorder(x)}
     if x $\not=$ NIL
       then Treepostorder(li[x])
            Treepostorder(re[x])
	    Print key[x]
    \end{lstlisting}

   Bei \textsc{Treepostorder} handelt es sich um einen rekursiven Algorithmus.
   Es wird zuerst der linke, dann der rechte Teilbaum und erst zum Schlu"s die Wurzel durchlaufen.
   \end{Algorithmus}

  \begin{figure}[H]
  \begin{center} \input{171103b.latex} \end{center}
   \caption{\textsc{Treepostorder}(x)}
   \label{171103b}
  \end{figure}
  \begin{itemize}
   \item x ist der Zeiger auf dem Knoten
   \item Die Ausgabereihenfolge ist \{1,4,3,2,7,9,8,5\}
  \end{itemize}
 
 \subsubsection{\textsc{Treepreorder}(x)}
   \begin{Algorithmus}[H]   
   \addcontentsline{alg}{Algorithmus}{\textsc{Treepreorder}}
   %\lstset{emph={Treepreorder}}
   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search, Treepreorder}, emphstyle=\textsc, escapeinside=~~}
    \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Treepreorder\textnormal{(x)}}, gobble=4]{Treepreorder(x)}
     if x $\not=$ NIL
        then Print key[x]
             Treepreorder(li[x])
             Treepreorder(re[x])
    \end{lstlisting}

   Beim ebenfalls rekursiven \textsc{Treepreorder} wird bei der Wurzel begonnen, dann wird der 
   linke Teilbaum und anschlie"send der rechte Teilbaum durchlaufen.
   \end{Algorithmus}
Beispiel f"ur \textsc{Treepreorder}
   
  \begin{figure}[H]
  \begin{center} \input{171103c.latex} \end{center}
   \caption{\textsc{Treepreorder }}
   \label{171103c}
  \end{figure}
 \begin{itemize}
  \item x ist der Zeiger auf dem Knoten
  \item Die Ausgabenreihenfolge ist \{5,2,1,3,4,8,7,9\}
 \end{itemize}
 
 \subsubsection{\textsc{Treeinorder}(x)}
   \begin{Algorithmus}[H]	
   \addcontentsline{alg}{Algorithmus}{\textsc{Treeinorder}}
   %\lstset{emph={Treeinorder}}
   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search, Treepreorder, Treeinorder}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Treeinorder\textnormal{(x)}}, gobble=4]{Treeinorder(x)}
     if x $\not=$ NIL
        then Treeinorder(li[x])
        	Print key[x]
             Treeinorder(re[x])
\end{lstlisting}

    Bei \textsc{Treeinorder} wird zuerst der linke Teilbaum, dann die 
    Wurzel und anschlie"send der rechte Teilbaum durchlaufen.
\end{Algorithmus}

  Beispiel f"ur \textsc{Treeinorder}
  \begin{figure}[H]
  \begin{center} \input{171103d.latex} \end{center}
   \caption{\textsc{Treeinorder}}
   \label{171103d}
  \end{figure}
 \begin{itemize}
  \item x ist der Zeiger auf dem Knoten
  \item Die Ausgabenreihenfolge ist \{1,2,3,4,5,7,8,9\}
 \end{itemize}
 
  \begin{satz}
 Bei gegebenem bin"aren Suchbaum ist die Ausgabe mit allen drei Verfahren (\textsc{Inorder}, 
 \textsc{Preorder} und \textsc{Postorder}) in $\Theta(\,n\,)$ m"oglich.
 \end{satz}
 \begin{description}
  \item[Folgerung:] Der Aufbau eines bin"aren Suchbaumes kostet $\Omega(\,n\,log\,n\,)$ Zeit.
 \end{description}
 
 \subsubsection{\textsc{Tree-Successor}(x)}
 
   \begin{Algorithmus}[H]
   \addcontentsline{alg}{Algorithmus}{\textsc{Min}(x)}
   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search, Treepreorder, Treeinorder}, emphstyle=\textsc, escapeinside=~~}
    \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Min\textnormal{(x)}}, gobble=4]{Min(x)}
     while li[x] $\not=$ NIL do
       x:= li[x]
     return x
    \end{lstlisting}

   \textsc{Min}(x) liefert das Minimum des Teilbaumes, dessen Wurzel x ist.
   \end{Algorithmus}
   
   \begin{Algorithmus}[H]
    \addcontentsline{alg}{Algorithmus}{\textsc{Tree-Successor}(x)}
    %\lstset{emph={Min}}
    \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search, Treepreorder, Treeinorder, Min}, emphstyle=\textsc, escapeinside=~~}
    \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Tree-Successor\textnormal{(x)}}, gobble=4]{Tree-Successor(x)}
     if re[x] $\not=$ NIL
        then return Min(re[x])
     y:=p[x]
     while y $\not=$ NIL and x=re[y]
        do x:=y
           y:=p[y]
     return y
    \end{lstlisting}

   Beim \textsc{Tree-Successor} werden zwei F"alle unterschieden. Falls x 
   einen rechten Teilbaum besitzt, dann ist der Nachfolger das Blatt, das im 
   rechten Teilbaum am weitesten links liegt (\textsc{MIN}(x)). Besitzt x keinen 
   rechten Teilbaum, so ist der successor y der Knoten dessen linker Sohn 
   am n"achsten mit x verwandt ist. Zu beachten ist dabei, da"s sich der Begriff Nachfolger auf einen Knoten bezieht, der Algorithmus
   aber den Knoten liefert, dessen gespeicherter Wert im Baum Nachfolger des im ersten Knoten gespeicherten Wertes ist. 
   \end{Algorithmus}
   
	% Vorlesungsskript vom 19.11.03
	% Christian Lütz
	% M.-Nr.: 62311
	% e-mail: Krisy0910@gmx.de
Die Operationen zum L"oschen und Einf"ugen von Knoten sind etwas komplizierter, da sie die Baumstruktur stark ver"andern k"onnen und
erhalten deswegen jeweils einen eigenen Abschnitt.
\subsection{Das Einf"ugen}

  	\begin{description}
			\item{Beim \textsc{Tree-Insert} werden zwei Parameter "ubergeben, wobei}
			\begin{itemize}
				\item T der Baum ist, in dem eingef"ugt werden soll und
		 		\item z der Knoten, so da"s	
		 		\begin{itemize}
					\item key[z]   = v (einzuf"ugender Schl"ussel),
					\item left[z]  = NIL und
					\item right[z] = NIL
				\end{itemize}
				ist.
			\end{itemize}
		\end{description}
 	Erkl"arung:
	Bei diesem Einf"ugealgorithmus werden die neuen Knoten immer als 
	Bl"atter in den bin"aren Suchbaum T eingef"ugt. Der einzuf"ugende 
	Knoten z hat keine S"ohne. Die genaue Position des Blattes wird 
	durch den Schl"ussel des neuen Knotens bestimmt. Wenn ein neuer 
	Baum aufgebaut wird, dann ergibt der erste eingef"ugte Knoten die 
	Wurzel. Der zweite Knoten wird linker Nachfolger der Wurzel, wenn 
	sein Schl"ussel kleiner ist als der Schl"ussel der Wurzel und rechter 
	Nachfolger, wenn sein Schl"ussel gr"o"ser ist als der Schl"ussel der 
	Wurzel. Dieses Verfahren wird fortgesetzt, bis die Einf"ugeposition bestimmt ist.

	Anmerkungen dazu:
	Dieser Algorithmus zum Einf"ugen ist sehr einfach. Es finden keine 
	Ausgleichs- oder Reorganisationsoperationen statt, so da"s die 
	Reihenfolge des Einf"ugens das Aussehen des Baumes bestimmt, deswegen 
	entartet der bin"are Suchbaum beim Einf"ugen einer bereits sortierten Eingabe
	zu einer linearen Liste. 

\begin{Algorithmus}
\addcontentsline{alg}{Algorithmus}{\textsc{Tree-Insert}}
   %\lstset{emph={Treeinorder}}
   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search, Treepreorder, Treeinorder}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Tree-Insert}, gobble=1]{Tree-Insert}
 y := NIL
 x := root[T]
 while ( x $\not=$ NIL ) do
   y := x
   if ( key[z] = key[x] ) 
     then x := left[x]
     else x := right[x]
 p[z] := y
 if ( y = NIL )
   then root[T] := z
   elseif ( key[z] < key[y] )
     then left[y]  := z
     else right[y] := z    
	    \end{lstlisting}

Die Laufzeit liegt in O(h), wobei h die H"ohe von T ist.
\end{Algorithmus}    
    Da der Knoten immer in einem Blatt eingef"ugt wird, ist damit zu rechnen,
    da"s im worst case das Blatt mit der gr"o"sten Entfernung von der Wurzel
    genommen wird. Da dieses die H"ohe h hat sind folglich auch h Schritte
    notwendig, um zu diesem Blatt zu gelangen.
        
\subsection{Das L"oschen eines Knotens}
	Beim L"oschen eine Knotens z in eiem bin"aren Suchbaum m"ussen drei F"alle
	unterschieden werden:\
	\begin{itemize}
	 	\item \textbf{1. Fall:} z hat keine S"ohne \\
	 	Der Knoten kann gefahrlos gel"oscht werden und es sind
	 	keine weiteren Operationen notwendig.
	 	\begin{center}
	 		\input{191103a.latex}
%	 		\vspace{3em}
	 	\end{center} 
	 	\item \textbf{2. Fall:} z hat genau einen linken Sohn \\
	 	Der zu l"oschende Knoten wird entfernt und durch den Wurzelknoten
	 	des linken Teilbaums ersetzt.
	 	\begin{center}
	 		\input{191103b.latex}
%	 		\vspace{3em}
	 	\end{center}
	 	\item \textbf{3. Fall:} z hat genau einen rechten Sohn \\
		  Analog dem 2. Fall.
%		  \vspace{3em}
		\item \textbf{4. Fall:} z hat zwei S"ohne \\
		\textit{Problem:} Wo werden die beiden Unterb"aume nach dem L"oschen von z
		angeh"angt?
		\textit{L"osung:} Wir suchen den Knoten mit dem kleinsten Schl"ussel im rechten
		Teilbaum von z. Dieser hat keinen linken Sohn, denn sonst g"abe es einen
		Knoten mit einem kleineren Schl"ussel. Der gefunden Knoten wird mit dem
		zu l"oschenden Knoten z vertauscht und der aktuelle Knoten entfernt.
	 	\begin{center}
	 		\input{191103c.latex}
	 	\end{center}
		\end{itemize}		 		

		Auch beim L"oschen (\textsc{Tree-Delete}) werden wieder zwei Parameter "ubergeben, dabei ist
		\begin{itemize}
			\item T der Baum und
		 	\item z der zu l"oschende Knoten
		\end{itemize}
		R"uckgabewert ist der (tats"achlich) aus dem Baum entfernte Knoten, dies muss nicht z sein, (siehe 4. Fall)
		\begin{Algorithmus}
		\addcontentsline{alg}{Algorithmus}{\textsc{Tree-Delete}}
		   %\lstset{emph={Tree-Successor}}
		   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
                   Tree-Search, Treepreorder, Treeinorder, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
                        \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Tree-Delete}, gobble=4]{Tree-Delete}
	if ( left[z] = NIL or right[z] = NIL )
		then y := z
		else y := Tree-Successor(z)
	if ( left[y] $\not=$ NIL )
		then x := left[y]
		else x := right[y]
	if ( x $\not=$ NIL )
		then root[T] := x
		else if ( y = left[p[y]] )
			then left[p[y]] := x
			else right[p[y]] := x
	if ( y $\not=$ z )
		then key[z] := key[y]
	return y
			\end{lstlisting}

		Laufzeit liegt wieder in O(h), wobei  h wieder die H"ohe von T bezeichnet.
		\end{Algorithmus}		

		Im worst case wird \textsc{Tree-Successor} mit einer Laufzeit von O(h) einmal aufgerufen, 
		andere Funktionsaufrufe oder Schleifen gibt es nicht.
\subsubsection{Bin"are Suchb"aume als Implementierung des ADT W"orterbuch}
 		\begin{center}\input{191103d.latex}\end{center}
%	  	\vspace{1em}
	  	Wie bereits bei der Funktion \textsc{Tree-Insert} beschrieben, kann eine ung"unstige 
	  	Einf"ugereihenfolge den Suchbaum zu einer linearen Liste entarten lassen. Deswegen sind allgemeine 
	  	bin"are Suchb"aume nicht geeignet, den ADT W"orterbuch zu implementieren.  	
\section{2-3-4-B"aume}
\begin{definition}[2-3-4-B"aume]
\textbf{2-3-4-B"aume} sind B"aume mit folgenden speziellen Eigenschaften:
\begin{itemize}
				\item Jeder Knoten im Baum enth"alt einen, zwei oder drei Schl"ussel, 
							die von links nach rechts aufsteigend sortiert sind.		
				\item Ein Knoten mit k Schl"usseln hat k+1 S"ohne (oder er hat "uberhaupt keine:``Blatt'') und wird als
							 (k+1)-Knoten bezeichnet.
				\item F"ur Schl"ussel im Baum gilt die verallgemeinerte Suchbaumeigenschaft.
				\item Alle Bl"atter haben den gleichen Abstand zur Wurzel.	
			\end{itemize}
\end{definition}
Zur Veranschaulichung dienen die folgenden Abbildungen (2-, 3- und 4-Knoten, ein Blatt und ein skizzierter m"oglicher Baum).
			 	\begin{center}\input{191103e.latex}\end{center}		
				\begin{center}\input{191103f.latex}\end{center}
	 			\begin{center}\input{191103g.latex}\end{center}
		Bei einem 2-3-4-Baum ist die Anzahl der Knoten deutlich geringer als bei
		einem vergleichbaren bin"aren Suchbaum. Damit ist die Zahl der besuchten 
		Knoten bei einer Suche geringer. Daraus folgt, da"s das Suchen nach einem
		Schl"ussel in einem 2-3-4-Baum effizenter ist, als in einem vergleichbaren bin"aren Suchbaum.
		Allerdings ist der Aufwand beim Einf"ugen und beim L"oschen von Schl"usseln h"oher.
\subsubsection{Beispiel f"ur einen 2-3-4-Baum}
		\begin{center}\input{191103h.latex}\end{center}
		\begin{itemize}
			\item Erfolgreiche Suche nach 35
			\item Erfolglose Suche nach 69
		\end{itemize}
		Die Laufzeit f"ur das Suchen liegt wieder in O(h), mit h als H"ohe des Baumes.
% 24.11.03 Marcel Konstanz
\subsection{Top Down 2-3-4-B"aume f"ur den ADT Dictionary}

Wenn die H"ohe des Baumes logarithmisch ($h \in O(\log n)$) ist, eignet er sich gut f"ur den Datentyp
W"orterbuch, da dann alle Operationen in O($\log n$) gehen. Insbesondere das in einem W"orterbuch zu erwartende h"aufige Suchen hat
die Komplexit"at O($\log n$).	

\begin{figure}[H]
    \begin{center}\input{241103a.latex}\end{center}
    \caption{$h \in O(\log n)$}
    \label{241103a}
 \end{figure}	

Bereits bei den Bin"arb"aumen mu"s die Baumstruktur nach dem L"oschen eines Knotens manchmal repariert werden.
Nun ist klar ersichtlich, da"s ein Baum, der immer logarithmische H"ohe haben soll, nicht zu einer linearen Liste entarten darf. 	
Falls also das Einf"ugen eines Elemente in einen Baum die Baumstruktur so "andert, da"s die Eigenschaften 
verletzt sind, mu"s der Baum repariert werden.

Wie in Abbildung \ref{241103b} zu sehen ist, kann das Einf"ugen eines einzigen Knotens dazu f"uhren, da"s eine neue Ebene
einf"ugt werden mu"s. Falls dies in der untersten Ebene geschieht (worst-case), kann sich das bis zur Wurzel fortsetzen.	
Top down 2-3-4 B"aume sollen diese Situation verhindern! 
 
 \begin{figure}[H]
    \begin{center}\input{241103b.latex}\end{center}
    \caption{$h \in O(\log n)$}
    \label{241103b}
 \end{figure}

Der worst case wird dadurch verhindert, da"s zwischendurch etwas mehr Aufwand betrieben wird.
So wird beim dem Einf"ugen vorausgehenden Suchen jeder erreichte 4-Knoten sofort aufgesplittet.
So ist gew"ahrleistet, da"s immer Platz f"ur einen neuen Knoten ist.

Allerdings haben Top down 2-3-4-B"aume auch den Nachteil, da"s sie schwerer implementierbar sind als die 2-3-4 B"aume. 
    Daf"ur sind \textbf{Rot-Schwarz-B"aume} besser geeignet. %\textcolor{red}{Wof"ur besser geeignet? Implementierung?}      

\section{Rot-Schwarz-B"aume}
 
\begin{definition}[Rot-Schwarz-B"aume]
   \textbf{Rot-Schwarz-B"aume} sind bin"are Suchb"aume mit folgenden zus"atzlichen Eigenschaften:
      \begin{enumerate}
	          \item Jeder Knoten ist rot oder schwarz.
	          \item Die Wurzel ist schwarz.
	          \item Jedes Blatt ist schwarz.
	          \item Ein roter Vater darf keinen roten Sohn haben.
	          \item Die Schwarzh"ohe $(Bh(j))$ ist die Anzahl der schwarzen Knoten auf einem Weg von einem Knoten $j$ zu einem
		  Blatt. Sie ist f"ur einen Knoten auf allen Wegen gleich.
      \end{enumerate}
\end{definition}

\begin{definition}[Die Schwarzh"ohe]
  $Bh(x)$ ist die Anzahl von schwarzen Knoten auf einem Weg, ohne den Knoten $x$  selbst von $x$ zu einem Blatt und hei"st
  Schwarzh"ohe von $x$.
\end{definition}

\begin{satz}[Die H"ohe von Rot-Schwarz-B"aumen] \label{rshoehe}
  Die H"ohe eines Rot-Schwarz-Baumes mit $n$ Knoten ist kleinergleich $2\log(n+1)$
\end{satz}
  
\begin{satz} \label{schwarzhoehe}
Sei $x$ die Wurzel eines Rot-Schwarz-Baumes. Dann hat der Baum mindestens $2^{Bh(x)}-1$ Knoten.
  Der Beweis kann induktiv "uber die H"ohe des Baumes erfolgen.
\end{satz}

Um Speicherplatz zu sparen, bietet es sich an, nur ein Nil-Blatt abzuspeichern. Dann m"ussen nat"urlich alle Zeiger entsprechend gesetzt
werden.
 
 \begin{figure}[H]
    \begin{center}\input{241103c.latex}\end{center}
    \caption{Nur ein NIL-Blatt}
    \label{241103c}
 \end{figure}

\begin{beweis} 
 IA : Sei $h=0$. Dann handelt es um einen Baum, der nur aus einem NIL-Blatt besteht. Dann ist $Bh(x)=0$ und nach Satz \ref{schwarzhoehe} 
 die Anzahl der inneren Knoten mindestens $2^0-1=0$. Damit ist der Induktionsanfang f"ur Satz \ref{schwarzhoehe} gezeigt. 
 \medskip
 
 \noindent IS : Sei nun h'$<$h. Dazu betrachten wir die zwei Teilb"aume eines Baumes mit der H"ohe $h$ und der Wurzel $x$.
Dann gibt es jeweils f"ur den rechten und den linken Teilbaum zwei F"alle. Dabei sind die F"alle f"ur die beiden Teilb"aume analog und
 werden deswegen gleichzeitig abgearbeitet.

    \begin{figure}[H]
    \begin{center}\input{241103d.latex}\end{center}
    \caption{Baumh"ohe mit Bh(li[x])=Bh(x)-1 und Bh(re[x])=Bh(x)}
    \label{241103d}
 \end{figure} 
 
    \textbf{1.Fall}:  Sei $Bh(li[x]) = Bh[x]$. Da eine untere Schranke gezeigt werden soll und in diesem Fall der Baum nicht weniger 
    Knoten
    hat als im zweiten Fall, reicht es den Beweis f"ur den zweiten (kritischeren) Fall zu f"uhren (Der zweite Fall ist kritischer, da
    der Teilbaum weniger Knoten haben kann als im ersten Fall und damit eher in der Lage ist, die untere Schranke zu durchbrechen).
     
    \textbf{2.Fall}:  Sei $Bh(li[x]) = Bh[x]-1$, dann gibt mindestens soviele innere Knoten, wie die beiden
    Teilb"aume nach Induktionsvoraussetzung zusammen haben. Damit hat der gesamte Baum mindestens   
    $1+2^{Bh(x)-1}-1+2^{Bh(x)-1}-1$  $=$ $2 \cdot 2^{Bh(x)-1}+1-2 = 2^{Bh(x)}-1$. Damit ist Satz \ref{schwarzhoehe} bewiesen. 
    
Zum Beweis von Satz \ref{rshoehe} wird Eigenschaft 4 der Rot-Schwarz-B"aume ausgenutzt. Daraus folgt direkt, da"s auf dem Weg von der Wurzel zu den Bl"attern,
mindestens die H"alfte der Knoten schwarz ist. Sei $h$ wieder die H"ohe des Baumes, dann gilt damit 

\begin{quote}       
        $Bh(root) \geq \frac{h}{2}\stackrel{\scriptstyle{S.}\, \ref{schwarzhoehe}}{\longrightarrow}$\\
        $n \geq 2^{Bh(root)}-1 \geq 2^{\frac{h}{2}}-1 \Rightarrow 2^{\frac{h}{2}}\leq n+1$ \hspace{2cm}$|$ $\log$\\
        $\Leftrightarrow \frac{h}{2}\leq \log(n+1)$  \hfill q.e.d  
\end{quote}
\end{beweis} 

% Mitschrift vom 26.11.03, Michael Prei"s
\subsection{Operationen in RS-B"aumen}

Die Operationen Tree-Insert und -Delete haben bei Anwendung auf einen RS-Baum eine Laufzeit von O($\log n$). Da
sie den Baum ver"andern, kann es vorkommen, da"s die Eigenschaften des Rot-Schwarz-Baumes verletzt werden. Um diese wieder herzustellen,
m"ussen die
Farben einiger Knoten im Baum sowie die Baumstruktur selbst ver"andert werden. Dies soll mittels Rotationen realisiert werden, dabei
gibt es die \textsc{Linksrotation} und die \textsc{Rechtsrotation}.
%\textcolor{red}{wieso ist die rechtsrotation algorithmisch etwas komplizierter?}

\begin{center}
\begin{figure}[H]
\input{261103a.latex}
\caption{Die Rotation schematisch}
\end{figure}
\end{center}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Linksrotation\textnormal{(T, z)}}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
                   Tree-Search, Treepreorder, Treeinorder, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Linksrotation\textnormal{(T, z)}}, gobble=4]{Linksrotation(T,z)}
    y := re[x]
    re[x] := li[y]
    p[li[y]] := x
    p[y] := p[x]
    if p[x] = NIL
      then root[T] := y
      else if x = li[p[x]]
        then li[p[x]] := y
        else re[p[x]] := y
    li[y] := x
    p[x] := y
\end{lstlisting}
\end{Algorithmus}

\begin{satz}
Rotationen "andern die G"ultigkeit der Suchbaumeigenschaft nicht.
\end{satz}

Wie die Skizze vermuten l"a"st, ist der Code f"ur die \textsc{Retchsrotation} symmetrisch zu dem f"ur die \textsc{Linksrotation}. Beide
Operationen erfordern O(1) Zeit, da mit jeder Rotation nur eine konstante Anzahl von Zeigern von umgesetzt wird und der Rest
unver"andert bleibt.

Sei T ein Rot-Schwarz-Baum. Ziel ist, da"s T auch nach Einf"ugen eines Knotens z ein Rot-Schwarz-Baum ist. T soll also 
nach Anwendung von \textsc{RS-Insert}(T, z) und einer eventuellen Korrektur die Bedingungen f"ur Rot-Schwarz-B"aume erf"ullen. Im folgenden Beispiel wird die "`3"'
eingef"ugt.

\begin{figure}[H]
\begin{center}
\input{261103b.latex}\input{261103c.latex}\hspace{5mm}\input{261103d.latex}
\end{center}
\caption{Funktionsweise der Rotation}
\end{figure}
Die letzte Rotation sollte zur "Ubung selbst nachvollzogen werden. Der fertige Baum als Ergebnis dieser letzten Roation steht im Anhang
auf Seite \pageref{rsrotation}.

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{RS-Insert\textnormal{(T, z)}}}
%\lstset{emph={Korrigiere}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
                   Korrigiere, Treepreorder, Treeinorder, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{RS-Insert\textnormal{(T, z)}}, gobble=1]{RS-Insert(T,z)}
 y := NIL[T]
 x := root[T]
 while x $\not=$ NIL[T] do
   y := x
   if key[z] < key[x]
     then x := li[x]
     else x := re[x]
 p[z] := y
 if y = NIL[T]
   then root[T] := z
   else if key[x] < key[y]
     then li[y] := z
     else re[y] := z
 li[z] := NIL[T]
 re[z] := NIL[T]
 Farbe[z] := ROT
 Korrigiere (T, z)
\end{lstlisting}
\end{Algorithmus}

Fr"uher wurden die Knoten gef"arbt, mittlerweile ist man aber dazu "ubergegangen, die Kanten zu f"arben. Dabei gilt, da"s eine rote
Kante auf einen fr"uher rot gef"arbten Knoten zeigt und eine schwarze Kante auf Knoten, die fr"uher schwarz gef"arbt
wurden. Die F"arbungen sind auch auf andere B"aume "ubertragbar. Die F"arbung der Kanten hat den Nachteil, da"s sich die Schwarzh"ohe so
ergibt, da"s die roten Kanten auf einem Weg nicht mitgez"ahlt werden. Einfacher und damit sicherer ist es, wenn nur die schwarzen Knoten
gez"ahlt werden.

F"ur alle h"ohenbalancierten B"aume gilt, da"s ihre H"ohe in O($\log n$) liegt, allerdings haben Rot-Schwarz-B"aume den Vorteil, da"s
sie leichter zu implementieren sind. Da die NIL-Bl"atter schwarz sind, kann ein einzuf"ugender Knoten auch erstmal einmal rot gef"arbt
sein.

%\textbf{Funktionsweise:} \dots \textcolor{red}{fehlt noch!}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Korrigiere\textnormal{(T, z)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
                   Korrigiere, Linksrotation, Rechtsrotation, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Korrigiere\textnormal{(T, z)}}, gobble=1]{Korrigiere(T,z)}
 while Farbe[p[z]] = ROT do
   if p[z] = li[p[p[z]]] ~\hspace{20mm}  $\vartriangleright$\textnormal{Vater ist linker Sohn von Opa von z}~
     then y := re[p[p[z]]] ~\hspace{26.1mm} $\vartriangleright$\textnormal{y ist Onkel von z}~
       if Farbe[y] = ROT ~\hspace{31mm} $\vartriangleright$\textnormal{Vater und Onkel rot}~
         then Farbe[p[z]] := SCHWARZ ~\hspace{5.3mm} $\vartriangleright$\textnormal{Vater wird schwarz}~
           Farbe[y] := SCHWARZ ~\hspace{18.7mm} $\vartriangleright$\textnormal{Onkel wird schwarz}~
           Farbe[p[p[z]]] := ROT ~\hspace{14.5mm} $\vartriangleright$\textnormal{Opa wird schwarz}~
           z := p[p[z]]
         else if z = re[p[z]]
           then z := p[z]
             Linksrotation (T, z)
         Farbe[p[z]] := SCHWARZ
         Farbe[p[p[z]]] := ROT
         Rechtsrotation(T, p[p[z]])
    else wie bisher, nur li und re vertauschen
 Farbe[Root[T]] := SCHWARZ
\end{lstlisting}
\end{Algorithmus}

Eine m"ogliche Anwendung f"ur einen Rot-Schwarz-Baum ist das bereits einleitend erw"ahnte Segmentschnitt-Problem. Dabei kann ein
Rot-Schwarz-Baum f"ur die Verwaltung der Sweepline-Status-Struktur verwendet werden (Menge Y, dazu siehe auch Anhang \ref{planesweep}).

% 01.12.2003 Sylvia Andersch
\section{Optimale bin"are Suchb"aume}

Seien wie im folgenden Beispiel Schl"usselwerte $a_1\:<\:...\:<\:a_n$ mit festen bekannten Wahrscheinlichkeiten $p_1,...,p_n$ gegeben, wobei gilt $p_i\:
\geq\:0$ und $\sum_{i=1}^n p_i\:=\:1$. Dabei bezeichnet $p_i$ die Wahrscheinlichkeit mit der auf den Wert $a_i$ zugegriffen wird.

\begin{description}
    \item[Beispiel:]  Sei nun $a_1\:=\:1,\:a_2\:=\:2,...,\:a_6\:=\:6$, 
      $p_1\:=\:0,25,\:p_2\:=\:0,28,\:p_3\:=\:0,1,\:p_4\:=\:0,2,\:p_5\:=\:0,13,\:p_6\:=\:0,04\:$ und
      der bin"are Suchbaum s"ahe wie folgt aus:
      \begin{center}
      \setlength{\unitlength}{4000sp}%
%
\begingroup\makeatletter\ifx\SetFigFont\undefined%
\gdef\SetFigFont#1#2#3#4#5{%
  \reset@font\fontsize{#1}{#2pt}%
  \fontfamily{#3}\fontseries{#4}\fontshape{#5}%
  \selectfont}%
\fi\endgroup%
\begin{picture}(2300,2425)(1000,-3425)
\thinlines
\put(1650,-1150){\circle{300}}
\put(1600,-1230){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}5}}}
\put(1650,- 1300){\line(-1,-1){450}}
\put(1650,- 1300){\line(1,-1){450}}
\put(1200,-1900){\circle{300}}
\put(1150,-1980){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}2}}}
\put(2100,-1900){\circle{300}}
\put(2050,-1980){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}6}}}
\put(1200,-2050){\line(1,-1){450}}
\put(1200,-2050){\line(-1,-1){450}}
\put(750,-2650){\circle{300}}
\put(700,-2730){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}1}}}
\put(1650,-2650){\circle{300}}%\put(1650,-1150){\circle{300}}
\put(1600,-2730){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}3}}}
\put(1650,- 2800){\line(1,-1){450}}
\put(2100,-3400){\circle{300}}
\put(2050,-3480){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}4}}}
\end{picture}
\end{center}      
      
\end{description}

An diesem Beispiel wird deutlich, da"s es bis zur $4$ ein relativ langer Weg ist, d.h. auch die Suche nach $4$ dauert
im Vergleich zu anderen lange. Nun hat aber die 4 eine hohe Wahrscheinlichkeit und wird deshalb oft abgefragt
werden. Also wird die durchschnittliche Rechenzeit relativ lang sein. Es w"are also sch"oner, wenn die $4$ weiter oben im Baum st"unde.
Diese "Uberlegungen lassen sich fortf"uhren und es fraglich erscheinen, ob sich durch Umplazieren von anderen Schl"usselwerten,
die Rechenzeit noch weiter verk"urzen l"a"st (z.B. k"onnte die $6$ weit nach unten, da sie die kleinste Wahrscheinlichkeit
hat und somit nur selten abgefragt wird). Aber wo liegen die Grenzen dieses Prozesses? Es ist ja klar, da"s er
irgendwie begrenzt sein muss. Es bleibt also die Frage: 

Gibt es irgendwelche Schranken, durch die die durchschnittliche
Rechenzeit beschr"ankt ist und wenn ja, wie sehen diese aus? 

Sicher ist, da"s die Rechenzeit eng mit der H"ohe des
Baumes zusammenh"angt. Die Frage ist also "aquivalent dazu, ob es Grenzen f"ur die durchschnittliche Knotentiefe gibt und falls ja, wie 
diese aussehen.
Allgemein sollen in diesem Kapitel folgende Fragen beantwortet werden:
\begin{enumerate}
    \item Wie muss der bin"are Suchbaum konstruiert werden, damit er optimal ist?
    \item Durch welche Grenzen wird die mittlere Knotentiefe beschr"ankt?
\end{enumerate}
\noindent
Dazu mu"s zuerst \textbf{Optimalit"at} definiert werden. Hier ist damit folgendes gemeint:
 \begin{definition}
   Ein bin"arer Suchbaum heisst \textbf{optimal}, wenn die Summe $\sum_{i=1}^n p_i\:(t_i+1)$ minimal ist, wobei
      $t_i$ die Tiefe des Knotens von $a_i$ angibt. Die Addition von "`1"' zur Tiefe erfolgt, damit auch der Wert f"ur die Wurzel in
      das Ergebnis eingeht.
    \end{definition}
Fakt ist, da"s jeder Teilbaum in einem optimalen Suchbaum wieder optimal ist. Der Sachverhalt, da"s eine optimale L"osung des
Gesamtproblems auch jedes Teilproblem optimal l"ost, hei"st "`Optimalit"atskriterium von Bellmann"'.

Diese Tatsache kann nun benutzt werden, um mittels dynamischer Programmierung einen optimalen Suchbaum zu konstruieren.

\subsection{Bottom-Up-Verfahren}

Gegeben ist folgendes:
\begin{itemize}
    \item Gesamtproblem $(a_1, \dots,a_n)$, d.h. $n$ Schl"usselwerte mit den dazu geh"origen Wahrscheinlichkeiten $p_i$ f"ur $i=1 \dots n$
    \item Tiefen der Knoten $t_1,\dots,t_n$
    \item Die mittlere Suchzeit, gegeben durch $\sum_{i=1}^n p_i\:(t_i+1)$
\end{itemize}
\begin{description}
    \item[Idee] Das Gesamtproblem wird in Teilprobleme aufgesplittet, d.h. gr"ossere optimale Suchb"aume werden
     aus kleineren optimalen Suchb"aumen  berechnet und zwar in einem rekursiven Verfahren.
     
     Angenommen alle m"oglichen optimalen Suchb"aume mit weniger als den $n$ gegebenen Schl"usselwerten sind schon bekannt.
     Der optimale Suchbaum mit $n$ Schl"usselwerten besteht aus einer Wurzel, einem rechten und einem linken Teilbaum, wobei
     f"ur die Teilb"aume die optimale Darstellung schon gegeben ist. Zu suchen ist noch die Wurzel $a_k$, f"ur die die
     Summe der mittleren Suchzeiten der beiden Teilb"aume minimal ist.
\end{description}
Dazu definiert man die Teilprobleme $(i,j)\:=\:(a_i,\dots,a_j)$. Der Baum f"ur das optimale Teilproblem wird mit $T(i,j)$
bezeichnet. Weiter ist $$p(i,j):=\sum_{m=i}^j p_m$$ die Wahrscheinlichkeit, da"s ein Wert zwischen $a_i$ und $a_j$
erfragt wird.\\ Ziel ist es, die mittlere Teilsuchzeit $$t(i,j):=\sum_{m=i}^j p_m(t_m+1)$$ zu minimieren. Dies beinhaltet
das Problem, die optimale Wurzel zu suchen. Man w"ahlt also ein beliebiges $k\in [i,j]$, setzt $a_k$ als Wurzel
an und berechnet das dazugeh"orige $t(i,j)$ \\
Betrachten Teilbaum $T(i,j)$:\\ Sei $a_k$ die Wurzel und $j>0$. Die Suchzeit berechnet sich durch\\
$t(i,j)=$ Anteil der Wurzel + Anteil linker Teilbaum + Anteil rechter Teilbaum
$$\Rightarrow\:t(i,j)=\:p_k\cdot 1\:+\:p(i,k-1)\:+\:t(i,k-1)\:+\:p(k+1,j)\:+\:t(k+1,j)$$
wegen $p_k\:+\:p(i,k-1)\:+\:p(k+1,j)=p(i,j)$ folgt
$$t(i,j)=\left\{\begin{array}{ll}
    0, & i>j \\
    p(i,j)+ \min_{i\leq k\leq j}[t(i,k-1)+t(k+1,j)], & sonst \\ \end{array}\right.    $$

\begin{description}
    \item[Beispiel:] Zahlen wie oben
    \begin{table}[h]
     \begin{tabular}{{c}|{c}{c}{c}{c}{c}{c}{c}{c}{c}}
    i$\backslash$ j & 1  &   2  &  3   &  4   &  5   & 6  \\
    \hline
    1             & 0,25/1 & 0,78/2 & 0,98/2 & .../2  & .../2  &.../2 \\
    2             &        & 0,28/2 & 0,48/2 & 0,98/2 & .../2  & .../2 \\
    3             &        &        & 0,1/3  & 0,4/4  & .../4  & .../4\\
    4             &        &        &        & 0,2/4  & .../4  & .../4\\
    5             &        &        &        &        & 0,13/5 & .../5\\
    6             &        &        &        &        &        & 0,04/6\\
  \end{tabular}
  \caption{Tabelle f"ur $t(i,j)$/ optimale Wurzel $k$}
  \end{table}

$\begin{array}{ccl}
t(1,2) &=& p(1,2)+ \min[\:t(1,0)+t(2,2)\:,\:t(1,1)+t(3,2)\:]\\
       &=& 0,53+\min[\:0+0,28\:,\:0,25+0\:]\\
       &=& 0,53+0,25\:=\:0,78
\end{array}$
$\begin{array}{ccl}
t(2,3) &=& p(2,3)+ \min[\:t(2,1)+t(3,3)\:,\:t(2,2)+t(4,3)\:]\\
       &=& 0,38+\min[\:0+0,1\:,\:0,28+0\:]\\
       &=& 0,38+0,1\:=\:0,48
\end{array}$
$\begin{array}{ccl}
t(2,4) &=& p(2,4)+ \min[\:t(2,1)+t(3,4)\:,\:t(2,2)+t(4,4)\:,\:t(2,3)+t(5,4)]\\
       &=& 0,58+\min[\:0+0,4\:,\:0,28+0,2\:,\:0,48+0]\\
       &=& 0,58+0,4\:=\:0,98
\end{array}$
$\Rightarrow$ optimaler Baum:
\begin{center}
\setlength{\unitlength}{4000sp}%
%
\begingroup\makeatletter\ifx\SetFigFont\undefined%
\gdef\SetFigFont#1#2#3#4#5{%
  \reset@font\fontsize{#1}{#2pt}%
  \fontfamily{#3}\fontseries{#4}\fontshape{#5}%
  \selectfont}%
\fi\endgroup%
\begin{picture}(2300,2225)(1000,-3425)
\thinlines
\put(1650,-1150){\circle{300}}
\put(1600,-1230){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}2}}}
\put(1650,- 1300){\line(-1,-1){450}}
\put(1650,- 1300){\line(1,-1){450}}
\put(1200,-1900){\circle{300}}
\put(1150,-1980){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}1}}}
\put(2100,-1900){\circle{300}}
\put(2050,-1980){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}4}}}
\put(2100,-2050){\line(1,-1){450}}
\put(2100,-2050){\line(-1,-1){450}}
\put(1600,-2650){\circle{300}}
\put(1550,-2730){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}3}}}
\put(2550,-2650){\circle{300}}%\put(1650,-1150){\circle{300}}
\put(2500,-2730){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}5}}}

\put(2550,- 2800){\line(1,-1){450}}
\put(2950,-3400){\circle{300}}
\put(2900,-3480){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}6}}}
\end{picture}
\end{center}
\end{description}
\subsection{Schranken}

Wir wollen nun der Frage nachgehen welche Schranken f"ur die mittlere Rechenzeit gelten, und zwar nach oben und nach unten.
Dazu ein kurzer Einschub zur Entropie:
Gegeben sei ein Zufallsexperiment mit m Ausg"angen und den zugeh"origen Wahrscheinlichkeiten $p_1,..,p_m$
\begin{description}
    \item[Entropie] $H(p_1,...,p_n)$ = Ma"s f"ur die Unbestimmtheit eines Versuchsausganges und berechnet sich durch
     $$H(p_1,...,p_n)=-\sum_{i=1}^m p_i\:log_2\:p_i$$
\end{description}

\begin{satz}
 \begin{itemize}
    \item $H(p_1,...,p_n)\:\leq\:log_2\:m$ (Gleichheit gilt $\Leftrightarrow\:p_1=...=p_m=\frac{1}{m})$
    \item $H(p_1,...,p_n)\:=\:H(p_1,...,p_n,0)$
    \item $H(p_1,...,p_n)\:=H(p_{\pi(1)},...,p_{\pi(m)})$
    \item Satz von Gibb: seien $q_1,..,q_m$ Werte gr"o"ser oder gleich Null und $\sum_{i=1}^m q_i\leq1$, so gilt
     $$H(p_1,...,p_n)\:\leq-\sum_{i=1}^m p_i\:log_2\:q_i$$
 \end{itemize}
\end{satz}

\begin{satz}
 Wenn die Daten nur in den Bl"attern stehen, so ist die Entropie eine untere Schranke f"ur die mittlere Tiefe
 der Bl"atter $\sum_{i=1}^m p_i t_i$ (hier $t_i,\:i=1..n$ Tiefe der Bl"atter), es gilt
 $$H(p_1,...,p_n)\:\leq\sum_{i=1}^m p_i\:t_i$$
\end{satz}
\begin{beweis}
Sei $(q_1,..,q_m)=(2^{-t_1},...,2^{-t_m})$, dann gilt nach obigem Satz:\\
$\begin{array}{cccl}
  H(p_1,...,p_n) & \leq & - &\displaystyle\sum_{i=1}^m p_i\:log_2\:q_i \vspace{0.3mm}\\
                 & =    & - &\displaystyle\sum_{i=1}^m p_i\:log_2\:2^{-t_i} \vspace{0.3mm}\\
                 & =    &   &\displaystyle\sum_{i=1}^m p_i\:t_i \\
\end{array}$\\
Die Werte f"ur q sind korrekt gew"ahlt, da nach der Ungleichung von Kraft gilt: $$\sum_{i=1}^m 2^{-t_i}\leq1$$
\end{beweis}
\begin{flushright} q.~e.~d. \end{flushright}
\begin{satz}
Die mittlere Knotentiefe eines optimalen bin"aren Suchbaumes (Daten im gesamten Suchbaum) liegt im Intervall $$
\begin{bmatrix}
  \displaystyle\frac{ H(p_1,...,p_n)}{log_2\:3}-1\:,\:H(p_1,...,p_n)
\end{bmatrix}$$
wobei $p_i$ die Wahrscheinlichkeit ist, mit der der i-te Knoten abgefragt wird.
\end{satz}
\begin{beweis}
\begin{enumerate}
    \item mittlere Knotentiefe $\geq\displaystyle\frac{ H(p_1,...,p_n)}{log_2\:3}-1$      \vspace{2mm}\\
     Jeder bin"are Suchbaum kann in einen tern"aren Baum transformiert werden, in dem nur die Bl"atter Daten enthalten. Die neuen Bl"atter rutschen eine Ebene tiefer:
 \setlength{\unitlength}{4000sp}%
\begingroup\makeatletter\ifx\SetFigFont\undefined%
\gdef\SetFigFont#1#2#3#4#5{%
  \reset@font\fontsize{#1}{#2pt}%
  \fontfamily{#3}\fontseries{#4}\fontshape{#5}%
  \selectfont}%
\fi\endgroup%
\begin{picture}(2300,2025)(1000,-2425)
\thinlines
\put(1650,-1150){\circle{450}}
\put(1600,-1230){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}a}}}
\put(2800,-1200){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}$\Longrightarrow$}}}
\put(1650,- 1370){\line(-1,-1){450}}
\put(1650,- 1370){\line(1,-1){450}}
\put(1200,-2040){\circle{450}}
\put(1020,-2130){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}LTB}}}
\put(2100,-2040){\circle{450}}
\put(1920,-2130){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}RTB}}}
\put(4650,-1150){\circle{450}}
\put(4650,- 1370){\line(-1,-1){450}}
\put(4650,- 1370){\line(1,-1){450}}
\put(4650,- 1370){\line(0,-1){450}}
\put(4650,-2000){\circle{300}}
\put(4600,-2080){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}a}}}
\put(4200,-2040){\circle{450}}
\put(4020,-2130){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}LTB}}}
\put(5100,-2040){\circle{450}}
\put(4920,-2130){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}RTB}}}
\end{picture}\\
Es gilt:\\
      $\begin{array}{ccl}
        \mbox{mittl. Knotentiefe des bin. Baums} & \geq & \mbox{mittl. Blatttiefe des tern"aren Baums}-1 \\
         & \geq & \displaystyle H_3(p_1,...,p_n)-1\:=\:-\sum_{i=1}^n p_i\cdot log_3 p_i \\
         &  =   & \displaystyle-\frac{1}{log_2 3}\sum_{i=1}^n p_i\cdot log_2 p_i\:-1\\
         &  =   & \displaystyle\frac{1}{log_2 3}H(p_1,...,p_n)\:-1
      \end{array}$

    \item $\displaystyle H(p_1,...,p_n)\geq$ mittlere Knotentiefe  \vspace{2mm}\\
      Um f"ur $p_i,...,p_j$ einen m"oglichst guten Suchbaum zu bestimmen, berechnen wir\\ $q\:=\:\sum_{k=i}^j p_k$ und w"ahlen
      als (Teilbaum)-Wurzel den Knoten $k$ f"ur den gilt: $$\sum_{k=i}^{l-1}p_k\:\leq\frac{q}{2}\:\leq\:\sum_{k=i}^{l}p_k$$
      F"ur die Teilfolgen $(p_i,..,p_{l-1})$ und $(p_{l+1},..,p_j)$ verfahren wir rekursiv. Gestartet wird mit $(p_1,..,p_n)$.\\
      Nun gilt f"ur Tiefe $t_l$ einer jeden Teilbaumwurzel $l$ (mit $i\leq l\leq j$), da"s $$\sum_{k=i}^j p_k \leq 2^{-t_l}$$
      (wegen Wahl von Wurzel). Insbesondere folgt: $p_l\:\leq\:2^{-t_l}$. Dies ergibt:
      $$\mbox{mittlere Suchbaumtiefe}\:=\:\sum_{i=1}^n p_i\cdot t_i\:\leq\:-\sum_{i=1}^n p_i\cdot log_2p_i\:=\:H(p_1,..,p_n)$$
\end{enumerate}
\end{beweis}
\begin{flushright} q.~e.~d. \end{flushright}

\section{Stapel}
Stapel haben eine so gro"se Bedeutung in der Informatik, da"s sich auch im Deutschen das englische "`Stack"' eingeb"urgert hat.
In vielen Algorithmen ist eine Menge zu verwalten, f"ur die sich die einfache Struktur eines Stapels anbietet.
Manche Caches arbeiten nach dem LIFO-Prinzip (Last In First Out), dazu reicht ein simpler Stapel, in den die zu verwaltenden Elemente
der Reihe nach hereingegeben werden und ein Zeiger auf das zuletzt hereingegebene Element gesetzt wird.

\begin{figure}[H]
  \begin{center}\input{031203a.latex}\end{center}
  \label{031203a}
\end{figure}

Die Basisoperationen eines Stapels sind nat"urlich \textsc{Push}, \textsc{Pop} und \textsc{Stack-Empty} (Test ob Stack leer), die Implementierung dieser einfachen
Operationen ist simpel. Dabei steht "`S"' f"ur den Stapel.

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Push}}
%\lstset{emph={Top}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
        Top, Korrigiere, Linksrotation, Rechtsrotation, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Push}, gobble=1]{Push}
  Top[s]:=Top[S]+1
  S[Top[S]]:=x
\end{lstlisting}
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Stack-Empty}}
%\lstset{emph={Top}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
        Top, Korrigiere, Linksrotation, Rechtsrotation, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Stack-Empty}, gobble=1]{Stack-Empty}
  if Top[s]=0 
    Then return true
    Else return false
\end{lstlisting}
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Pop}}
\lstset{emph={Top, Stack-Empty}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
        Top, Stack-Empty, Korrigiere, Linksrotation, Rechtsrotation, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Pop}, gobble=1]{Pop}
  if Stack-Empty
    Then Error
    Else Top[S]:=Top[S]-1
      return S[Top[S]+1]
\end{lstlisting}
\end{Algorithmus}

Eine These besagt, da"s Stapel beim Algorithmenentwurf vor allem f"ur die Verwaltung von Kandidaten verwendet werden. Ein Beispiel
daf"ur ist die Berechnung der konvexen H"ulle einer Menge.

\subsection{Konvexe H"ulle}
CH von englisch Convex Hull bezeichnet die konvexe H"ulle, z.B. CH(P) f"ur ein Polygon oder CH(X) f"ur X $\subseteq \mathbb{R}^2$. Dabei
ist diese mathematisch so definiert:
\begin{definition}
\[\mbox{CH(X)}=\bigcap_{\textnormal{M konvex}\ \wedge \ X \subseteq M} M\]
\end{definition}
Anschaulich gesprochen ist CH(X) damit die kleinste Menge, die X umfa"st. Eine sehr sch"one Beschreibung ist auch diese: Man stelle sich
ein Brett mit hereingeschlagenen N"ageln vor. Dann vollzieht ein Gummiband, welches um die N"agel gelegt wird, die konvexe H"ulle der
Nagelmenge nach. Die Punkte der konvexen H"ulle sind nur die N"agel, die das Gummiband tangiert. 

\begin{satz}
Wenn X endich ist, so ist CH(X) ein Polygon (Jede Punktmenge l"a"st sich auczh als Polygon auffassen).
\end{satz}

Ziel ist die Berechnung der CH(X) f"ur endliche X $\subseteq \mathbb{R}^2$. Erreicht wird dieses Ziel durch den Algorithmus von Graham
(1972), der sich grob wie folgt einteilen l"a"st:

\begin{itemize}
\item Sortiere X bez"uglich wachsender x-Werte (o.B.d.A. habe X allgemeine Lage, Sonderf"alle wie z.B. x$_i$=x$_j$ f"ur i$\not=$j treten
also nicht auf und m"ussen nicht behandelt werden)
\item Setze p$_1$=p$_l \in$ X als den Punkt mit dem kleinsten x-Wert und p$_r \in$ X als den Punkt mit dem gr"o"sten x-Wert 
\item Wende den Algorithmus zur Berechnung von UH(X) an, dabei ist UH(X) die obere konvexe H"ulle von X
\item f"ur LH(X) vertausche die Vorzeichen und benutze UH(X) noch einmal, dabei ist LH(X) die untere konvexe H"ulle von X
\end{itemize}

\begin{figure}[H]
  \begin{center}\input{031203b.latex}\end{center}
  \label{031203b}
\end{figure}

Doch wie funktioniert das nun genau? Zuerst brauchen wir den Algorithmus f"ur UH(X):
\begin{itemize}
\item Eingabe ist die Punktfolge p$_l$=p$_1$, p$_2$, \ldots, p$_n$=p$_r$, dabei sind die x-Werte monoton wachsend und alle p$_i$ liegen
oberhalb von G$_{p_l p_r}$ (dies l"a"st sich notfalls in O($n \log n$) Zeit erreichen)
\item Der Algorithmus arbeitet dann Punkt f"ur Punkt von links nach rechts und bewahrt folgende Invariante
  \begin{enumerate}
  \item Der Stapel S speichert Punkte x$_0$, x$_1$, \ldots, x$_t$=x$_{\textnormal{Top}}$, die eine Teilfolge von p$_n$, p$_1$, p$_2$, \ldots sind
  \item t$\geq$2 $\wedge$ x$_0$=p$_n$ $\wedge$ x$_1$=p$_1$ $\wedge$ im Schritt s gilt: x$_t$=p$_s$, dabei ist s$\geq$2
  \item x$_0$, x$_1$, \ldots, x$_t$ ist UH(\{p$_1$, \ldots, p$_s$\})
  \item x$_1$, \ldots, x$_t$ sind von rechts sortiert
  \end{enumerate}
\end{itemize}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Graham}}
%\lstset{emph={Push, Pop}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Graham}, gobble=1]{Graham}
  Push(S, p$_n$)
  Push(S, p$_1$)
  Push(S, p$_2$)
  s:=2
  while s$\not=$n do
    $\alpha$:=top[S]
    $\beta$:=zweites Element in S
    while (p$_{s+1}, \alpha, \beta$) keine Linksdrehung ist do
      Pop
      $\alpha$:=$\beta$
      $\beta$:=neues zweites Element im Stapel
    Push(S, p$_{s+1}$)
    s:=s+1
  Gib S aus    
\end{lstlisting}
\end{Algorithmus}

Der Drehsinn l"a"st sich dabei mathematisch recht einfach feststellen (siehe Anhang \ref{drehsinn}), die Details sind an dieser Stelle
aber weniger wichtig.
Doch wie kann man das f"ur die Berechnung
der konvexen H"ulle ausnutzen? Zeichnung \ref{konvexedrehungskizze} macht dies sehr anschaulich klar. Die Punkte sind jeweils die
Endpunkte der eingezeichneten Strecken und daher nicht explizit markiert. Im rechten Fall wird der Punkt $p_2$ f"ur die konvexe H"ulle
"uberfl"ussig, da er durch $p_{s+1}$ "`"uberdeckt"' wird. Im linken Bild ist $p_2$ hingegen f"ur die konvexe H"ulle notwendig ($p_2$
liege oberhalb der Strecke $\overline{p_1p_{s+1}}$). Dies verdeutlicht anschaulich, warum ein Punkt der durch
\textsc{Pop} im Laufe der Berechnung der konvexen H"ulle herausfliegt, nicht nochmals angeschaut werden mu"s.

\begin{figure}[H]
\begin{center}
\input{031203d.latex}
\end{center}
\caption{Nutzen des Drehsinns f"ur die Berechnung der konvexen H"ulle}
\label{konvexedrehungskizze}
\end{figure}

Nun bleibt noch die Analyse des Algorithmus. Bei der Analyse nach der Guthabenmethode werden Operationen mit Kosten versehen, die auf ein fiktives Bankkonto
verbucht werden. Hier kostet nun jedes \textsc{Push} 2 \textcolor{red}{Euro-Zeichen in LaTeX?}. Davon wird einer verbraucht und einer
gespart. Von dem Ersparten werden die \textsc{Pop}'s bezahlt; dabei kostet jedes \textsc{Pop} einen . Da wie bereits oben erw"ahnt jeder
Punkt maximal einmal durch ein \textsc{Pop} herausfliegen kann, sind wir fertig. Unser Guthaben reicht aus um alle m"oglichen
\textsc{Pop}'s zu bezahlen. Mittels der Guthabenmethode ergeben sich so amortisierte Kosten von O(n) f"ur die Berechnung der konvexen
H"ulle.

Mithilfe eines Stapels ist also ein sehr effezienter Algorithmus m"oglich. Allerdings sind nat"urlich auch Stapel kein Allheilmittel
f"ur alle Probleme der algorithmischen Goemetrie. Ein anderes wichtiges Mittel sind die sogenannten Segmentb"aume.

\section{Segmentb"aume}
Bereits anfangs wurde das Problem der "Uberschneidung von Rechtecken erw"ahnt. Eingabe sollten Ortho-Rechtecke sein und Ausgabe ein
Bericht der "Uberschneidungen. Dazu kann man
\begin{enumerate}
\item alle Rechteckseiten bestimmen und dann
\item den Sgmentschnitt-Algorithmus benutzen (siehe Anhang \ref{planesweep})
\end{enumerate}

Der dabei verwendete abstrakte Datentyp Dictionary k"onnte mit den bisher kenngelernten Methoden z.B. RS-, AVL- oder
Top-down-2-3-4-Baum realisiert werden.
Wichtiger ist jetzt allerdings, da"s dieses Problem auch auf das Problem der Punkteinschl"usse zur"uckgef"uhrt werden kann. Die Eingabe
ist dann eine Menge von Punkten und die Ausgabe sind dann alle Punkteinschl"usse (p, R) mit $p\, \in \,R$. Dabei sind mit $p_1, \, p_2,
\, \ldots$ die Punkte und mit $R_1, \, R_2, \, \ldots$ die Rechtecke gemeint. Auch hier wird eine Gleitgeradenmethode benutzt; die
Ereignispunkte sind die x-Koordinaten der linken und rechten Kanten und der Punkte.
\begin{itemize}
\item[] (li, x, (y$_1$, y$_2$), R) \hspace{2cm} (o.B.d.A. $y_1 \leq y_2$)\vspace{-0.5ex}
\item[] (re, x, (y$_1$, y$_2$), R) \vspace{-0.5ex}
\item[] (Punkt, x, y, R)
\end{itemize}

An der folgenden Skizze wird offensichtlich, da"s es verschiedene Arten von "Uberschenidungen bzw. Schnitten gibt. 
\begin{figure}[H]
\begin{center}
\input{031203e.latex}
\end{center}
\end{figure}

\subsection{Der Punkteinschlu"s-Algorithmus}
Y wird zu Beginn  mit Y:=$\emptyset$ initalisiert. Dann wird von Ereignispunkt zu Ereignispunkt gegangen, nachdem diese nach wachsendem
x sortiert wurden. Bei $m$ Rechtecken geht dies in O($m \log m$) Schritten.

\begin{enumerate}
\item Falls der aktuelle Ereignispunkt die Form (li, x, ($y_1,\ y_2$), R) hat, so setze Y:=Y $\cup$ \{(R, [$y_1,\ y_2$])\} --\textsc{Insert}
\item Falls der aktuelle Ereignispunkt die Form (re, x, ($y_1,\ y_2$), R) hat, so setze Y:=Y $\backslash$ \{(R, [$y_1,\ y_2$])\}
--\textsc{Delete}
\item Falls der aktuelle Ereignispunkt die Form (Punkt, $x,\ y$, R) hat, so finde alle Intervalle in Y, die $y$  enthalten und gib die
entsprechenden Schnittpaare aus (d.h. wenn f"ur (R$'$, [$y_1,\ y_2$]) in Y gilt: $y \in [y_1,\ y_2]$. Pr"ufe, ob R$' \not=$R ist, wenn
ja, gib (R$'$, R) aus.) --\textsc{Search}
\end{enumerate}
\subsection{Der Segment-Baum}
Der Segment-Baum dient also der Verwaltung von Intervallen, wobei die "ublichen Operationen \textsc{Insert}, \textsc{Delete} und
\textsc{Search} m"ogloch sind.

Mit der Eingabe ist die Menge der $y$-Werte gegeben, welche nach steigendem $y$ geordnet wird. Damit ergibt sich die Folge $y_0,\ y_1,\
\ldots,\ y_n$ bzw. die Folge von Elementarintervallen $[y_0,\ y_1],\ [y_1,\ y_2],\ \ldots,\ [y_{n-1},\ y_n]$. Durch das folgende
Beispiel wird dies vielleicht klarer.

Seien die $y$-Werte 0, 1, 3, 6, 9 als sogenannte Roatorpunkte \textcolor{red}{Stimmt das?}, die zu den Elementarintervallen
[0, 1], [1, 3], [3, 6] und [6, 9] f"uhren und die Rechtecke wie in Skizze \ref{031203f} gegeben. Daraus resultiert dann der Baum wie in
Abbildung \ref{031203g}.

\begin{figure}[H]
\begin{center}
\input{031203f.latex}
\caption{Beispiel f"ur "Uberschneidung von Rechtecken}
\label{031203f}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\input{031203g.latex} \input{031203h.latex}
\caption{Beispiel f"ur einen Segment-Baum}
\label{031203g}
\end{center}
\end{figure}

 a) Y=Y $\cup$ \{(R$_1$, [1, 6])\}=\{(R$_1$, [1, 6])\} \hspace{2em} b) Y=\{(R$_1$, [1, 6]), (R$_2$, [0, 9]) \}

\begin{figure}[H]
\begin{center}
\input{031203i.latex} \hspace{1cm} \input{031203j.latex} \hspace{1cm} \input{031203k.latex}
\caption{Beispiel f"ur einen Segment-Baum im Aufbau}
\label{031203i}
\end{center}
\end{figure}

Aus Abbildung \ref{031203f} geht der Baum hervor, der rechts in der Abbildung \ref{031203g} zu sehen ist. Der dritte Wert jedes Tupels
in jedem Knoten
gibt dabei das Maximum der Tupel im linken Teilbaum dieses Knotens an. Etwas verst"andlicher, aber formal falsch, ist folgende
Formulierung: Der dritte Wert jedes Knotens gibt den maximalen Wert seines linken Teilbaumes an.
Der linke Teil von Abbildung
\ref{031203g} deutet an, wie die Struktur im allgemeinen Fall bzw. als "`Leerstruktur"' aussieht. In der untersten Abbildung zeigen a) und
b) wie die Struktur aufgebaut wird. Teil c) verdeutlicht nochmal die "Uberlappung der Intervallgrenzen (im obigen Beispiel!) und macht
gleichzeitig folgenden Satz klar.

\begin{satz}
F"ur jedes Rechteck gibt es pro Level maximal zwei Eintr"age
\end{satz} 

Doch wie funktioniert die Suche und wieviel Rechenzeit wird gr"o"senordnunsm"a"sig ben"otigt, um alle "Uberlappungen herauszufinden und
auszugeben?

Falls nach (Punkt, x, y, R) gesucht wird, wird im im Baum getestet, in welchen Intervallen (x,y) liegt. 

\textcolor{red}{Stimmt das so? Im
Skript stand folgendes: (Punkt, x, y, R) $\rightarrow$ Suche nach y=(4,5), merke die Beschriftungen auf dem Suchpfad von Rechtecken,
OUTPUT (R$_4$, R$_1$) (R$_4$, R$_2$) (R$_4$, R$_3$).
Wenn aber nach "Uberschendiungen im Intervall y=[4, 5] gesucht wird fehlen doch noch drei "Uberschneidungen, ich werde daraus nicht
schlau. }

F"ur die Suche werden die Beschriftungen auf dem Suchpfad von Rechtecken gemerkt. F"ur den Baum gilt der Satz "uber h"ohenbalancierte
Suchb"aume. Damit funktioniert das Suchen in O($\log m$) Zeit, und die Ausgabe hat die Gr"o"se O($k+ \log m$), dabei ist $k$ die Anzahl
der Schnitte. F"ur das Insert wird eine passende rekursive Prozedur geschrieben, die den Suchpfad abtestet und dei Eintragungen
vornimmt; dies geht in ebenfalls in O($\log m$) Zeit.
Damit haben wir einen output-sensitiven Algorithmus mit einer Gesamtlaufzeit von O($k +m \log m$), dazu siehe auch den Anfang des
Skriptes und Anhang \ref{planesweep}.

\chapter{Verwaltung von Mengen -- kompliziertere Datenstrukturen}
Die Operationen \textsc{Make-Heap}(), \textsc{Insert}(H, x), \textsc{Min}(H) und \textsc{Extract-Min}(H) sollten an dieser Stelle
hinreichend bekannt sein, vielleicht ist auch \textsc{Union}(H$_1$, H$_2$) schon bekan\-nt. Diese f"ur die Verwaltung von Mengen wichtige
Operation vereinigt, wie der Name bereits sagt, zwei Heaps H$_1$ und H$_2$ zu einem Heap H. H enth"alt die Knoten von H$_1$ und H$_2$,
die bei dieser Operation zerst"ort werden (Stichwort "`Mergeable Heaps"'). Zus"atzlich wird noch \textsc{Decrease-Key}(H, x, k) benutzt.

Bei Bin"ar-Heaps funktionieren \textsc{Make-Heap} (\textsc{Build-Heap}) und \textsc{Min} in O(1) und \textsc{Insert} und
\textsc{Extract-Min} in O($\log n$). Allerdings ben"otigt \textsc{Union} O(n). Damit unter\-st"utzen Bin"ar-Heaps kein \textsc{Union}!
Diese wichtige Operation wird aber von \textbf{Binomial-Heaps} unterst"utzt.

\section{Binomialb"aume}
\begin{definition}[Binomialb"aume]
\begin{enumerate}
\item B$_0$:= \textcolor{red}{?kleiner Kreis} ist Binomialbaum
\item B$_k$ $\rightarrow$ B$_{k+1}$ (k$\geq$0)
  \begin{figure}[H]
  \begin{center}
  \input{info3_own001.latex} 
  %\caption{Beispiel f"ur einen Segment-Baum im Aufbau}
  %\label{031203i}
  \end{center}
  \end{figure}
\end{enumerate}
\end{definition}
\subsubsection{Beispiel}
\begin{figure}[H]
  \begin{center}
  \input{info3_own002.latex} 
  %\caption{Beispiel f"ur einen Segment-Baum im Aufbau}
  %\label{031203i}
  \end{center}
  \end{figure}
  Im Beispiel sieht man von links nach rechts B$_0$, B$_1$, B$_2$ und den B$_3$.

Bei B"aumen gilt es verschiedene Typen zu unterscheiden, es gibt Wurzelb"aume, geord\-nete B"aume und Positionsb"aume.
Sei nun mit $(a,nil,b)$ der Baum bezeichnet, in dem in den S"ohnen der Wurzel die Werte $(a,nil,b)$ in dieser Reihenfolge von rechts nach
links gespeichert sind.

Dann gilt f"ur den Wurzelbaum, da"s $(a,nil,b)=(nil,a,b)$ ist. In einem geordneten Baum hingegen ist
$(a,b,c)\not=(b,a,c)$ weil die Reihenfolge der S"ohne relevant ist. In einem Positionsbaum wieder ist
$(a,nil,b)\not=(nil,a,b)$ weil danach geguckt wird, welche Platzstellen belegt sind und welche nicht.

\begin{definition}[Binomialb"aume]
B$_0$ ist der Baum mit genau einem Knoten. F"ur k$\geq$0 erh"alt man B$_{k+1}$ aus B$_k$ dadurch, da"s man zu einem
B$_k$ einen weiteren Sohn an seine Wurzel als zus"atzlichen linkesten Sohn h"angt, dieser ist ebenfalls Wurzel eines
B$_k$.
\end{definition}

\begin{definition}
\begin{itemize}
\item Ein freier Baum ist eine ungerichteter zusammenh"angender kreisfreier Graph.
\item Ein Wurzelbaum (B,x$_0$) ist ein freier Baum B mit dem Knoten x$_0$ als "`ROOT"'.
\item Ein Wald ist ein ungerichteter kreisfreier Graph. 
\end{itemize}
\end{definition}

\section{Binomial-Heaps}  
\begin{satz}
Im Zusammenhang mit Binomial-Heaps wird auch der folgende Satz noch benutzt werden:
\[\binom{n}{i}=\binom{n-1}{i}+\binom{n-1}{i-1}\]
\end{satz}

\begin{satz}["Uber die eindeutige g-adische Darstellung nat"urlicher Zahlen]
(f"ur g $\in \mathbb{N} : g\geq 2$)

$n \in \mathbb{N} \rightarrow$ Darstellung eindeutig, $n=\sum_{i=0}^m a_i g^i, a_i \in \mathbb{N}$
\end{satz}
Mit g=2 erhalten wir die Bin"ardarstellung nat"urlicher Zahlen.

\begin{definition}[Binomialheap]  
Ein Binomialheap ist ein Wald von Binomialb"aumen, der zus"atzlich folgende Bedingung\-en erf"ullt:
\begin{enumerate}
\item Heap-Eigenschaft = alle B"aume sind heap-geordnet, d.h. der Schl"usselwert jedes Knotens ist gr"o"ser oder gleich dem
Schl"usselwert des Vaters
\item Einzigkeitseigenschaft = Von jedem Binomialbaum ist maximal ein Exemplar da, d.h. zu einem gegebenen Wurzelgrad
gibt es maximal einen Binomialbaum im Heap.
\end{enumerate}
\end{definition}

\begin{satz}[Struktursatz "uber Binomialb"aume]
\begin{enumerate}
\item Im Baum gibt es 2$^n$ Knoten
\item Die H"ohe des Baumes B$_n$ ist n
\item Es gibt genau $\binom{n}{i}$ Knoten der Tiefe $i$ im Baum B$_n$
\item In B$_n$ hat die Wurzel den maximalen Grad (degree) und die S"ohne sind von rechts nach links nach wachsendem Grad
geordnet.
\end{enumerate}
\end{satz}

\begin{definition}
$d(n,i):=$ Anzahl der Knoten der Tiefe i im B$_n$
\end{definition}

\begin{beweis}
\begin{enumerate}
\item I.A. $n=0$ trivial, $n-1 \Rightarrow n$, $2^{n-1}+2^{n-1}=2^n$ 
\item $n=0$ trivial, $n-1 \Rightarrow n$ trivial
\item 

I.A. trivial n=0

I.S. $n-1 \Rightarrow n$ Setzen die G"ultigkeit von 3) f"ur $n-1$ alle $i$ voraus und zeigen sie dann f"ur $n$ und alle
$i$

$d(n,i)=d(n-1,i)+d(n-1,i-1)=_{I.V.} \binom{n-1}{i}+ \binom{n-1}{i-1}=_{HS} \binom{n}{i}$
\end{enumerate}
\end{beweis}

Es gibt zu $n \in \mathbb{N}$ \underline{genau} (von der Struktur her) einen Binomialheap, der genau die $n$ Knoten
speichert.
 
\begin{beweis}[Eindeutigkeit der Binomialdarstellung von $n$]
$n=12$ 
\end{beweis} 

\begin{figure}[H]
  \begin{center}
  \input{info3_own003.latex} 
  %\caption{Beispiel f"ur einen Segment-Baum im Aufbau}
  %\label{031203i}
  \end{center}
  \end{figure}
  
F"ur $n$ Werte haben wir O$(\log n)$ (genau $\lfloor \log n \rfloor +1$) Binomialb"aume. Dies ist auch intuitiv
verst"andlich, da jede Zahl $n \in \mathbb{Z}$ zur Darstellung $\log n$ Stellen braucht. Die Eindeutigkeit der Struktur
des Heaps wird auch hier wieder klar, da jede Zahl ($\in \mathbb{Z}$) eindeutig als Bin"arzahl dargestellt werden kann.
Was passiert nun aber, wenn zwei Heaps zusammengef"uhrt werden?

\section{Union}
Bei der Zusammenf"uhrung (\textsc{Union}) zweier Heaps H$_1$ und H$_2$ wird ein neuer Heap H geschaffen, der alle Knoten
enth"alt, H$_1$ und H$_2$ werden dabei zerst"ort.

%\begin{figure}[H]
%  \begin{center}
%  \input{info3_own004.latex} 
%  %\caption{Beispiel f"ur einen Segment-Baum im Aufbau}
%  %\label{031203i}
%  \end{center}
%  \end{figure}

Leider ist wegen eines technischen Problems die folgende Darstellung nicht ganz kor\-rekt, der Kopf jedes Heaps zeigt nur
auf das erste Element der Wurzelliste.

\begin{bundle}{Kopf H$_1$}
\chunk{7 $\rightarrow$ } \chunk{\begin{bundle}{2} \chunk{\begin{bundle}{25} \chunk{30} \end{bundle}} \chunk{3} \end{bundle}} 
\end{bundle} \hspace{15mm}
\begin{bundle}{Kopf H$_2$}
\chunk{16 $\rightarrow$ } \chunk{\begin{bundle}{18 $\rightarrow$}\chunk{45} \end{bundle}}\chunk{\begin{bundle}{4}
\chunk{\begin{bundle}{10} \chunk{11} \end{bundle}} \chunk{21} \end{bundle}} 
\end{bundle}\hspace{15mm}
\begin{bundle}{Kopf H}
\chunk{\begin{bundle}{7 $\rightarrow$ } \chunk{\begin{bundle}{18} \chunk{45} \end{bundle}} \chunk{16} \end{bundle}}
\chunk{\begin{bundle}{2}\chunk{\begin{bundle}{4} \chunk{\begin{bundle}{10} \chunk{11} \end{bundle}} \chunk{21} 
\end{bundle}} \chunk{\begin{bundle}{25} \chunk{30} \end{bundle}} \chunk{3}\end{bundle}}
\end{bundle}

Beim \textsc{Union} werden zuerst die Wurzellisten gemischt. Dann werden, angefangen bei den kleinsten,
Binomialb"aume mit gleicher Knotenanzahl zusammengefa"st. Es werden also zwei B$_i$ zu einem B$_{i+1}$
transformiert.

Jeder Knoten im Baum hat dabei folgende Felder:
\begin{itemize}
\item Einen Verweis auf den Vater
\item Den Schl"usselwert
\item Den Grad des Knotens
\item Einen Verweis auf seinen linkesten Sohn
\item Einen Verweis auf seinen rechten Bruder
\end{itemize}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Union\textnormal{(H$_1$, H$_2$)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Union\textnormal{(H$_1$, H$_2$)}}, gobble=1]{Union(H1,H2)}
 H:= Make-Heap()
 Head[H]:=Merge(H$_1$, H$_2$)
 Zerst~"o~re die Objekte H$_1$, H$_2$ (nicht die entspr. Listen)
 if Head[H]=Nil then
   return H
 prev-x:=Nil
 x:=Head[H]
 next-x:=reB[x]
 while next-x $\not=$ Nil do
   if (degree[x] $\not=$ degree[next-x] OR (reB[next-x] $\not=$ Nil AND 
   degree[reB[next-x]]=degree[x])) then
     prev-x:=x
     x:=next-x
   else if (key[x]$\leq$key[next-x]) then
     reB[x]:=reB[next-x]
     Link[next-x, x]
     else if (prev-x=nil) then
       Head[H]:=next-x
       else reB[prev-x]:=next-x
       Link[x, next-x]
       x:=next-x
   next-x:=reB[x]
 return H    
\end{lstlisting}
\end{Algorithmus}

Priority-Queues=\{\textsc{Make-Heap}, \textsc{Insert}, \textsc{Union}, \textsc{Extract-Max}, \textsc{Decrease-Key},
\textsc{Min}\};
\textsc{Make-Heap} erfordert nur O(1), der Rest geht in O($\log n$).
 

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Insert\textnormal{(H, x)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Insert\textnormal{(H, x)}}, gobble=1]{Insert(H,x)}
 H$_1$:= Make-Heap()
 p[x]:=Nil
 Sohn[x]:=Nil
 reB[x]:=Nil
 degree[x]:=0
 Head[H$_1$]:=x
 Union(H, H$_1$) 
\end{lstlisting}
\end{Algorithmus}

Wie funktioniert nun das Entfernen des Minimums? Zuerst wird das Minimum rausge\-worfen, dann werden die S"ohne des
betroffenen Knotens in umgekehrter Ordnung in eine geordnete Liste H$_1$ gebracht und anschlie"send werden mittels
\textsc{Unon} H und H$_1$ zusammengemischt. Dies klappt logischerweise in O($\log n$), da die Wuzellisten durch O($\log
n$) in ihrer L"ange beschr"ankt sind (bei n Werten im Heap). Damit arbeitet auch \textsc{Extract-Min} in O($\log n$).

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Decrease-Key\textnormal{(H, x, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Decrease-Key\textnormal{(H, x, k)}},gobble=1]{DecreaseKey(H,x,k)}
 if k$>$key[x] then
   Fehler ($\rightarrow$ Abbruch)
 key[x]:=k
 y:=x
 z:=p[y]
 while (z$\not=$ Nil AND key[y]$<$key[z]) do
 vertausche key[y] und key[z]
 y:=z
 z:=p[y]    
\end{lstlisting}
\end{Algorithmus}

\begin{satz}
Die linker-Sohn-rechter-Bruder-Darstellung f"ur Binomial-Heaps erm"oglicht in der ange\-gebenen Form f"ur
Priorit"atswarteschlangen logarithmische Laufzeit.
\end{satz}
Merke: Die L"ange der Wurzelliste bei einem Binomial-Heap ist logarithmisch.

Bisher war immer leicht verst"andlich, wie die Schranke f"ur die Komplexit"at eines Algorithmus zustande kam, doch bei
komplizierteren Datenstrukturen ist das nicht mehr immer so einfach. Manche mehrfach ausgef"uhrte Schritte erfordern am
Anfang viel Operationen, am Ende aber wenig. Dies erschwert die Angabe einer Komplexit"at und deswegen werden im Folgenden drei Methoden zur
Kostenabsch"atzung vorgestellt.

\section{Amortisierte Kosten}
Es gibt drei Methoden um amortisierte Kosten abzusch"atzen, eine davon wurde bereits im Abschnitt zur konvexen H"ulle
benutzt. 
\begin{itemize}
\item die Aggregats-Methode
\item die Guthaben-Methode
\item die Potential-Methode
\end{itemize} 
Ziel aller drei Methoden ist es, auf die Kosten einer Operation im gesamten Algorithmus zu kommen. Dies bietet sich z.B.
an, falls eine Operation im worst-case in einem Schritt des Algorithmus eine Komplexit"at von O($n$) haben kann,
aber auch "uber den gesamten Algorithmus hinweg bei $n$ Schritten nicht mehr als O($n$) Aufwand erfordert. Dann kommt
mittels der Analyse zu den amortisierten Kosten O(1), zu einer Art Durchschnittskosten im Stile von $\frac{T(n)}{n}$.

Bei den letzten beiden Methoden erh"alt man Kosten AK($i$)=AK$_i$ f"ur die Operationen.
Die aktuellen (wirklichen) Kosten werden
mit K($i$)=K$_i$ bezeichnet und werden k"onnen relativ frei gew"ahlt werden. In den meisten F"allen ist K$_i$=O(1),
dabei mu"s aber gelten:
\[\sum_{i=1}^n AK_i \geq \sum_{i=1}^n K_i\]

Damit kann gefolgert werden, da"s jede obere Schranke f"ur die amortisierten Kosten AK auch eine obere Schranke
f"ur die interessierenden tats"achlichen Kosten ist.

Im Fall des Graham-Scans ergeben sich amortsierte Kosten von O(1) f"ur das Multi-Pop, obwohl es schon in einem Schritt
O(n) dauern kann. Mittels dieser amortisierten Kosten erh"alt man $\sum AK_i=O(n) \Rightarrow \sum K_i=O(n)$, also
letztlich, da"s der Algorithmus in O($n$) funtkioniert.

\subsection{Die Potentialmethode}
Die Idee ist, da"s jede Datenstruktur DS mit einem Potential $\Phi$ bezeichnet wird. Es ist DS(0)=DS$_0$ und nach der i-ten
Operation DS(i)=DS$_i$. Das Potential wird definiert als $\Phi(DS(i))=:{\Phi}_i=\Phi(i)$, wobei es nat"urlich
eine Folge von mindestens $i$ Operationen geben mu"s.

Die Kosten ergeben sich als Differenz der Potentiale der Datenstruktur von zwei aufeinander folgenden Zeitpunkten. Die
Kosten der i-ten Operation ergeben sich als ${\Phi}_i-{\Phi}_{i-1}$.

Hierbei mu"s \[\sum_{i=1}^n AK_i=\sum_{i=1}^n (K_i+{\Phi}_i-{\Phi}_{i-1})\] gelten.

Dies (Teleskopsumme, s. Anhang \ref{Teleskopsumme}) l"a"st sich zu \[\sum_{i=1}^n AK_i=\sum_{i=1}^n
K_i+{\Phi}_n-{\Phi}_0\] umformen.

Als drittes mu"s (${\Phi}_n-{\Phi}_0 \geq 0$) sein, speziell ${\Phi}_n \geq 0$, falls ${\Phi}_0=0$. Damit folgt
$\sum AK_i \geq \sum K_i$.

\subsubsection{Beispiel Graham-Scan}

\begin{definition}
$\Phi \mbox{(DS(}i\mbox{))}$=Anzahl der Elemente im Stapel, ${\Phi}_i \geq 0, {\Phi}_0=0$
\end{definition}

Die amortisierten Kosten f"ur  \textsc{Push} sind AK$_i$=K$_i$+${\Phi}_i-{\Phi}_{i-1}=1+1=2$
und f"ur  \textsc{Pop} AK$_i$=K$_i$+${\Phi}_i-{\Phi}_{i-1}=1+(-1)=0$.

Woher ergibt sich das? Nun \textsc{Push} hei"st ein Element einf"ugen $\rightarrow {\Phi}_i$ hat n Elemente,
${\Phi}_{i-1}$
 hat n-1 Elemente $ \rightarrow {\Phi}_i-{\Phi}_{i-1}$=1. Bei \textsc{Pop} hat umgekehrt ${\Phi}_{i-1}$ n Elemente und
 ${\Phi}_i $ n-1
Elemente, damit ist dort ${\Phi}_i-{\Phi}_{i-1}=n-1-n=-1$.  

\section{Fibonacci-Heaps}
Bei den Binomial-Heaps k"onnen wegen der Ordnung alle S"ohne auf bequeme Weise angesprochen werden, bei den
Fibonacci-Heaps fehlt diese Ordnung. Daf"ur hat jeder Knoten Zeiger auf den linken und den rechten Bruder, die S"ohne
eines Knotens sind also durch eine doppelt verkette Liste verkn"upft. Mit min[H] kann auf das Minimum der Liste
zugegriffen werden, n[h] bezeichnet die Anzahl der Knoten im Heap. Es gilt weiterhin, da"s alle B"aume die
Eigenschaften eines Heaps erf"ullen.

Wie lange dauern Operationen mit Fibonacci-Heaps? Das Schaffen der leeren Struktur geht wieder in O(1), da zus"atzlich
zu \textsc{Make-Heap} nur noch n[H]=0 und min[H]=Nil gesetzt werden mu"s.

\subsection{Einfache Operationen}
\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Union\textnormal{(H$_1$, H$_2$)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Union\textnormal{(H$_1$, H$_2$)}},gobble=1]{Union2(H1,H2)}
 H:=Make-Heap
 min[H]:=min[H$_1$, H$_2$]
 Verkn"upfe die Wurzellisten von H$_2$ und H miteinander $\vartriangleleft$ ~\mbox{\textcolor{red}{Fehl?}}~
 if (min[H$_1$]=Nil) OR (min[H$_2\not=$Nil] AND min[H$_2$]<min[H$_1$])
   then min[H]:=min[H$_2$]
 n[H]:=n[H$_1$]+n[H$_2$]
 Zerst~"o~re die Objekte H$_1$ und H$_2$
 Output H     
\end{lstlisting}
Formal korrekt mu"s es nicht min[H], sondern key[min[H]] hei"sen. Auf diese korrekte Schreibweise wurde
zugunsten einer klareren Darstellung verzichtet.
\end{Algorithmus}
Das Aufschneiden der Wurzellisten und Umsetzen der Zeiger (Zeile 3) geht in O(1). Im Allgemeinen wird der Ansatz
verfolgt, Operationen so sp"at wie m"oglich auszuf"uhren. Bei \textsc{Union} wird nicht viel getan, erst bei
\textsc{Extract-Min}.

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Insert\textnormal{(H, x)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Insert\textnormal{(H, x)}}, gobble=1]{Insert2(H,x)}
 degree[x]:=0
 p[x]:=Nil
 Sohn[x]:=Nil
 li[x]:=x
 re[x]:=x
 mark[x]:=false ~\hspace{10mm}~ $\vartriangleleft$ ~\mbox{Bis hier neue Wurzelliste nur aus x}~
 Verkn"upfe mit Wurzelliste H
 if (min[H]=Nil) OR (key[x]<key[min[H]]) then
   min[H]:=x
 n[H]:=n[H]+1   
\end{lstlisting}
\end{Algorithmus}

\subsection{Anwendung der Potentialmethode}
Jetzt wird die vorher erl"auterte Potentialmethode benutzt, um die Kosten zu bestimmen, dabei ist
\begin{itemize}
\item $\Phi$(H)=$t$(H)+$2m$(H) mit
\item $t$[H] als der Anzahl der Knoten in der Wurzelliste und
\item $m$[H] als der Anzahl der markierten Knoten
\end{itemize}
Wenn nachgewiesen werden soll, da"s die Potentialfunktion f"ur Fibonacci-Heaps akzepta\-bel (zweckm"a"sig und zul"assig)
ist, m"ussen die Bedingungen erf"ullt sein. In \cite{ottmann} wird eine andere Potentialmethode benutzt.

F"ur die Kosten AK$_{i, \textsc{Insert}}$ des \textsc{Insert} gilt:
\begin{itemize}
\item $t$(H$'$)=$t$(H)+1
\item $m$(H$'$)=$m$(H)
\item ${\Phi}_i-{\Phi}_{i-1}=t$(H$'$)$+2m$(H$'$)-$t$(H)-$2m$(H)=1
\item K$_i=1$
\item AK$_i$=K$_i$+${\Phi}_i-{\Phi}_{i-1}$=1+1=2 $\in$ O(1)
\end{itemize}

\begin{satz}
\textsc{Insert} hat die amortisierten Kosten O(1)
\end{satz}

Die Kosten von \textsc{Union} sind:
\[\Phi(\mbox{H})-\Phi(\mbox{H}_1)-\Phi(\mbox{H}_2)=t(\mbox{H})-t(\mbox{H}_1)-t(\mbox{H}_2)+2m(\mbox{H})-2m(\mbox{H}_1)-
2m(\mbox{H}_2)=0+0=0\]

\begin{satz}
\textsc{Union} hat die amortisierten Kosten O(1), AK$_{i, \textsc{Union}}$=K$_i$+0=1+0 $\in$ O(1).
\end{satz}

Mit diesen beiden sehr schnellen Operationen sind Fibonacci-Heaps sinnvoll, wenn die genannten Operationen sehr h"aufig
vorkommen, ansonsten ist der Aufwand f"ur die komplizierte Implementierung zu gro"s.

Nochmal zur"uck zur amortisierten Kostenanalyse:
Hier wird fast nur die Potentialme\-thode benutzt, andere Methoden sind f"ur uns nicht wichtig. Wesentlicher Bestandteil
der Potentialmethode ist die geschickte Definition der Potentialfunktion, dabei hat man viel Freiheit, nur die Bedingung $\Phi
(D_n) \geq \Phi(D_0)=_{meist}0$ mu"s erf"ullt sein.

Die amortisierten Kosten ergeben sich als die Summe aus den aktuellen Kosten und dem Potential der Datenstruktur. Die
Bedingung ist dazu da, da"s die Kostenabsch"atzung m"oglich, aber auch brauchbar ist.

\subsection{Aufwendige Operationen}
\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Extract-Min\textnormal{(H)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Extract-Min\textnormal{(H)}}, gobble=1]{Extract-Min(H)}
 z:=min[H]
 if (z$\not=$Nil)
   f"ur jeden Sohn x von z do
     f"uge x in die Wurzelliste ein
     p[x]:=Nil
   entferne z aus der Wurzelliste
   if z=re[z] then
     min[H]:=Nil
   else
     min[H]:=re[z]
     Konsolidiere(H)
   n[H]:=n[H]-1
 return z   
\end{lstlisting}
Beim Konsolidieren wird eine Struktur "ahnlich wie bei den Bin"ar-Heaps geschaffen, dazu gleich mehr.
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Link\textnormal{(H, x, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Link\textnormal{(H, x, k)}}, gobble=1]{Link(H, x, k)}
 Entferne y aus der Wurzelliste
 Mache y zum Sohn von x, degree[x]:=degree[x]+1
 Mark[y]:=0
\end{lstlisting}
Der Grad von x wird erh"oht, da er die Anzahl der S"ohne von x z"ahlt, es ist klar, da"s dies immer in O(1) geht. In
\cite{ottmann} ist hier ein kleiner Fehler.
\end{Algorithmus}

Beim Konsolidieren besteht die L"osung in Verwendung eines Rang-Arrays.
Es wird n=n[H] gesetzt und D(n) als der maximale Wurzelgrad definiert, dabei ist D(n)$\in$O($\log n$). Das Array bekommt 
die Gr"o"se D(n). Dann wird die Wurzelliste durchgegangen und an der entsprechenden Position im Array gespeichert. Falls
an dieser Stelle schon ein Eintrg vorhanden ist, wird ein \textsc{Link} durchgef"uhrt. Dabei wird das Gr"o"sere an das
Kleinere (bzgl. des Wurzelwertes\textcolor{red}{Was denn genau} angeh"angt. Da der Wurzelgrad w"achst, mu"s der Eintrag
im Feld dann eins weiter. So wird die gesamte Wurzelliste durchgegangen und es kann am Ende keine zwei Knoten mit
gleichem Wurzelgrad geben. Dazu mu"s immer das Minimum aktualisiert werden.

\subsubsection{Analyse}
Wir haben die amortisierten Kosten, die Behauptung ist: O(D($n$))=O(log n). Die aktuel\-len Kosten sind die Kosten f"ur den
Durchlauf duch die Wurzelliste, alles andere ist hier vernachl"assigbar. Also sind die aktuellen Kosten
$=t(\mbox{H})+\mbox{D}(n)=\mbox{O}(t(\mbox{H})+\mbox{D}(n))$ und die Potentialdifferenz ist $\leq
\mbox{D}(n)-t(\mbox{H})$

Man zeigt schlie"slich, da"s die amortisierten Kosten $\in$ O(D($n$)) liegen.

\textcolor{red}{Der Abschnitt ist schei"se, verbessern!}

\subsection{Weitere Operationen}
\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Decrease-Key\textnormal{(H, x, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Cut, Korrigiere, Cascading, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Decrease-Key\textnormal{(H, x, k)}}, gobble=1]{Decrease-key2(H, x, k)}
 if k$>$key[x] then
   Fehler!
 key[x]:=k  ~\hspace{10mm}~ $\vartriangleleft$ ~\mbox{Weiter, falls F-Heap kaputt}~ 
 y:=p[x]
 if (y$\not=$Nil AND key[x]$<$key[y] then
   Cut(H, x, k)
   Cascading-Cut(H, y)
 if key[x]$<$key[min[H]] then
   min[H]:=x  
\end{lstlisting}
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Cut\textnormal{(H, x, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Cut, Korrigiere, Cascading, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Cut\textnormal{(H, x, k)}}, gobble=1]{Cut(H, x, k)}
 Entferne x aus der Sohnliste von y
 degree[y]:=degree[y]-1
 F~"u~ge x zur Wurzelliste von H hinzu
 p[x]:=Nil
 Mark[x]:=0 
\end{lstlisting}
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Cascading-Cut\textnormal{(H, y)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Cut, Korrigiere, Cascading, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Cascading-Cut\textnormal{(H, y)}}, gobble=1]{Cascading-Cut(H, y)}
 z:=p[y]
 if (z$\not=$Nil then)
   if Mark[y]=0 then
     Mark[y]:=1
   else
     Cut(H, y, z)
     Cascading-Cut(H, z)  
\end{lstlisting}
\end{Algorithmus}

Im Beispiel markiert $\star$, da"s ein Knoten schon einen Sohn verloren hat.

\begin{bundle}{$\bullet$}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{$\bullet$}\chunk{$\bullet$}\chunk{$\bullet$}\end{bundle}}\end{bundle}} \chunk{\begin{bundle}{$\bullet$} \chunk{\begin{bundle}{$\bullet$} \chunk{$\bullet$} \end{bundle}}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{${\bullet}^{\star}$} \chunk{\begin{bundle}{30$^{\star}$}
\chunk{40}\end{bundle}} \chunk{$\bullet$}\end{bundle}}\end{bundle}} \end{bundle}} 
\end{bundle} \hspace{5mm}$\longrightarrow$\hspace{5mm}
\begin{bundle}{$\bullet$}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{$\bullet$}\chunk{$\bullet$}\chunk{$\bullet$}\end{bundle}}\end{bundle}} \chunk{\begin{bundle}{$\bullet$} \chunk{\begin{bundle}{$\bullet$} \chunk{$\bullet$} \end{bundle}}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{${\bullet}^{\star}$} \chunk{\begin{bundle}{30$^{\star}$}
\chunk{25}\end{bundle}} \chunk{$\bullet$}\end{bundle}}\end{bundle}} \end{bundle}} 
\end{bundle} \hspace{5mm}$\longrightarrow$\hspace{5mm}
\begin{bundle}{$\bullet$}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{$\bullet$}\chunk{$\bullet$}\chunk{$\bullet$}\end{bundle}}\end{bundle}} \chunk{\begin{bundle}{$\bullet$} \chunk{\begin{bundle}{$\bullet$} \chunk{$\bullet$} \end{bundle}}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{${\bullet}^{\star}$} \chunk{30$^{\star}$}
\chunk{$\bullet$}\end{bundle}}\end{bundle}} \end{bundle}} 
\end{bundle} 25\hspace{5mm}$\longrightarrow$\hspace{5mm}
\begin{bundle}{$\bullet$}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{$\bullet$}\chunk{$\bullet$}\chunk{$\bullet$}\end{bundle}}\end{bundle}} \chunk{\begin{bundle}{$\bullet$} \chunk{\begin{bundle}{$\bullet$} \chunk{$\bullet$} \end{bundle}}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{${\bullet}^{\star}$}
\chunk{$\bullet$}\end{bundle}}\end{bundle}} \end{bundle}} 
\end{bundle} 25 30\hspace{5mm}$\longrightarrow$\hspace{5mm}

$\longrightarrow$
\begin{bundle}{$\bullet$}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{$\bullet$}\chunk{$\bullet$}\chunk{$\bullet$}\end{bundle}}\end{bundle}} \chunk{\begin{bundle}{$\bullet$} \chunk{\begin{bundle}{$\bullet$} \chunk{$\bullet$} \end{bundle}}
\chunk{$\bullet$}\end{bundle}} 
\end{bundle} 25 30
\begin{bundle}{$\bullet$} \chunk{$\bullet$}\end{bundle}

\subsubsection{Analyse}
Die aktuellen Kosten f"ur das \textsc{Cut} liegen in O(1), was gilt f"ur das \textsc{Cascading-Cut}?

\[
\begin{array}{ccl}
\mbox{ein \textsc{Cut} dabei:} & - \mbox{ eine Markierung} & -2\\
 &  + \mbox{ eine neue Wurzel} & +1\\
 &  + \mbox{ O(1) f"ur das \textsc{Cut} selbst} & +1\\
 &  & =0
\end{array}\]

Damit hat \textsc{Decrease-Key} O(1) amortisierte Kosten. Denn das einmalige Abschnei\-den kostet O(1) und die Kaskade von
\textsc{Cut}s kostet wegen der Markierung nichts. \textcolor{red}{Sollte das etwas O/Null hei"sen}

\begin{satz}[Lemma 1 "uber F-Heaps]
Sei $v$ Knoten eines F-Heaps. Ordnet man (in Gedanken) die S"ohne von $v$ in der zeitlichen Reihenfolge, in der sie an
$v$ angeh"angt wurden, so gilt: der $i$-te Sohn von $v$ hat mindestens den Grad $(i-2)$. 
\end{satz}
Dies ist der Grund f"ur die Leistungsf"ahigkeit und den Namen der F-Heaps

\begin{beweis}
Damit der $i$-te Sohn auftaucht, m"ussen $v$ und dieser Sohn den Rang (i-1) gehabt haben (wenigstens Rang i, falls beide
denselben Rang hatten). Wenn das unklar ist, sollte man sich das Konsolidieren noch einmal anschauen. Danach kann dieser
Sohn wegen der Markierungsvorschrift maximal einen Sohn verlieren, also ist der Rang mindestens $(i-2)$. 
\end{beweis}

\begin{satz}[Lemma 2 "uber F-Heaps]
Jeder Knoten $v$ vom Rang (Grad) $k$ eines F-Heaps ist Wurzel eines Teilbaumes mit mindestens F$_{k+2}$ Knoten.
\end{satz}

\begin{definition}
F$_0:=0$, F$_1:=1$, F$_{k+2}$:=F$_{k+1}$+F$_{k}$
\end{definition}

\begin{satz}
F$_{k+2} \geq {\phi}^k$, $\phi=\frac{1+\sqrt{5}}{2}\approx 1,6$ 
\end{satz}

\begin{satz}[Hilfssatz]
$\forall k \geq 0 \mbox{ }gilt \mbox{ } F_{k+2}=1+\sum_{i=0}^k F_i$
\end{satz}

\begin{beweis}[Beweis vom Hilfssatz durch Induktion "uber $k$]
$k=0 : F_2=1+\sum_{i=0}^0 F_i = 1+ F_0 =1$

\noindent I.V. $F_{k+1}=1+\sum_{i=0}^{k-1} F_i$

\noindent $F_{k+2}=F_{k}+F_{k+1}=F_k + 1 + \sum_{i=0}^{k-1} F_i = 1+\sum_{i=0}^k F_i$
\end{beweis}

\begin{beweis}[Lemma 2]
S$_k$:=Minimalzahl von Knoten, die Nachfolger eines Knotens $v$ vom Rang $k$ sind ($v$ selbst mitgez"ahlt).

\noindent Habe v den Rang 0 (keinen Sohn), dann ist S$_0$=1 und S$_1$=2. Ab $k \geq 2$ gilt Lemma 1:
\[\mbox{S}_k \geq 2 + \sum_{i=0}^{k-2} \mbox{S}_i \mbox{ f"ur }k \geq 2\]
Jetzt werden die S"ohne von $v$ wieder gedanklich in der Reihenfolge des Anliegens geord\-net, und zusammen mit

\[\mbox{F}_{k+2}=1+\sum_{i=0}^k F_i=2+\sum_{i=2}^k F_i\] gilt \[\mbox{S}_k \geq \mbox{F}_{k+2} \mbox{ f"ur } k \geq 0
\mbox{ (Beweis durch Induktion)}\]

Sei $v^k$ der Knoten $v$ mit dem Rang $k$, dann gilt insgesamt: Anzahl Nachfolger$(v^k) \geq \mbox{S}_k \geq
\mbox{F}_{k+2} \geq {\phi}^k \Rightarrow \mbox{Lemma 2}$ 
\end{beweis}
Daraus folgt, da"s der maximale Grad (Rang) eines Knotens in einem F-Heap mit $n$ Knoten D($n$) $\in$ O($\log n$ ) ist.

\begin{beweis}
Sei $v$ beliebig im F-Heap gew"ahlt und $k=$Rang$(v)$, dann ist $n \geq$Anzahl der Nachfolger$(v)$ $\geq {\phi}^k$, also
$k \leq \log_{\phi} n \in \mbox{ O}(\log n)$. Da $v$ beliebig ist, gilt die Ungleichung auch f"ur den maximalen Rang
$k$. 
\end{beweis}
F-Heaps sind Datenstrukturen, die vor allem zur Implementierung von Priorit"atswarte\-schlangen geeignet sind.
\textsc{Extract-Min} braucht amortisiert logarithmische Zeit, alle anderen Operationen (\textsc{Make-Heap},
\textsc{Union}, \textsc{Insert} und \textsc{Decrease-Key}) brauchen amortisiert O(1) Zeit. Das L"oschen klappt damit
ebenfalls in logarithmischer Zeit, zuerst wird der betroffene Wert mit \textsc{Decrease-Key} auf $- \infty$ gesetzt
und dann mit \textsc{Extract-Min} entfernt.

Falls in einer Anwendung oft Werte eingef"ugt, ver"andert oder zusammengef"uhrt wer\-den, sind F-Heaps favorisiert. Falls
nur selten Werte zusammengef"uhrt werden, bieten sich Bin"ar-Heaps an. 

\chapter{Union-Find-Strukturen}
Der abstrakte Datentyp wird durch die Menge der drei Operationen \{\textsc{Union}, \textsc{Find}, \textsc{Make-Set}\}
gebildet. Union-Find-Strukturen dienen zur Verwaltung von Zerlegungen in disjunkte Mengen. Dabei bekommt jede Menge der
Zerlegung ein "`kanonisches Element"' zugeordnet, dieses dient als Name der Menge. Typisch f"ur eine Union-Find-Struktur
ist eine Frage wie "`In welcher Menge liegt Element $x$?"'.

Die Bedeutung von \textsc{Union} und \textsc{Find} sollte klar sein, doch was macht \textsc{Make-Set}(x)?
Sei $x$ Element unseres Universums, dann erzeugt \textsc{Make-Set} die Einermenge \{$x$\} mit dem kanonischen Element 
$x$.
\section{Darstellung von Union-Find-Strukturen im Rechner}
Bei Listen ist die Zeit f"ur das \textsc{Find} ung"unstig, da die ganze Liste durchlaufen werden mu"s. \textsc{Union}
dauert ebenfalls lange, da dann durch die erste Liste bis zum Ende gegangen werden mu"s und dann die zweite Liste an die
erste geh"angt wird. Hierbei wird auch schon offensichtlich, da"s nach dem \textsc{Union} immer ein neues kanonisches
Element bestimmt werden mu"s. F"ur Union-Find-Strukturen sind "`Disjoint set forests"' das Mittel der Wahl.

\textcolor{red}{Zeichnung}

Den Objekten wird eine Gr"o"se zugeordnet, dabei werden die Knoten mitgez"ahlt, damit keine lineare Liste entsteht. Das
\textsc{Union} erfolgt zuerst nach der Gr"o"se und dann nach der H"ohe, dabei wird der k"urzere Wald an den l"angeren
drangeh"angt.

\section{Der minimale Spannbaum -- Algorithmus von Kruskal}
Ein Graph G=(V, E) sei wie "ublich gegeben. Was ist dann der minimale Spannbaum ("`Minimum Spanning Tree"')?
Dazu m"ussen die Kanten Gewichte haben, so wie beim Algorithmus von Dijkstra. Dann ist der minimale Spannbaum wie folgt
definiert:
\begin{enumerate}
\item T=(V, E$^{\star}$) mit E$^{\star} \subseteq$ E
\item Alle Knoten, die in G zusammenh"angend sind, sind auch in T zusammenh"angend
\item Die Summe der Kantengewichte ist minimal
\end{enumerate}

\textcolor{red}{Zeichnung}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{mst\textnormal{(G, w)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Cut, Extract-Min, Find, Make-Set, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{mst\textnormal{(G, w)}}, gobble=1]{mst(G, w)}
 E$^{\star} := \emptyset$
 K$:= \emptyset$
 Bilde Priorit~"a~tswarteschlange Q zu E bzgl. w
 f~"u~r jedes $x \in V$ do
   Make-Set(x) ~\hspace{15mm}~ $\vartriangleleft$ K besteht aus lauter Einermengen
 while K enth~"a~lt mehr als eine Menge do
   (v, w):=Min(Q)
   ~\textsc{Extract-Min}~
   if Find(v)$\not=$ Find(w) then
     Union(v, w) ~\hspace{15mm}~ $\vartriangleleft$ Hier reicht ganz simples ~\textsc{Union}~
     E$^{\star} := $E$^{\star} \cup \{(v, w)\}$
 return E$^{\star}$   
\end{lstlisting}
Der Algorithmus von Kruskal ist ein gieriger Algorithmus. In \cite{cormen} steht eine leicht andere Variante als im
Beispiel.
\end{Algorithmus}

\begin{tabular}{|l|l|}
aktuellle Kante (v, w) & K:\{a\}, \{b\}, \{c\}, \{d\}, \{e\}, \{f\}, \{g\}, \{h\}, \{i\}, \{j\}, \{k\}\\
\hline
1.  $\star$(i, k) & \{a\}, \{b\}, $\ldots$, \{h\}, \{i, k\}\\
2.  $\star$(h, i) & \{a\}, \{b\}, $\ldots$, \{h, i, k\}\\
3.  $\star$(a, b) & \{a, b\}, $\ldots$, \{h, i, k\}\\
\hline
4.  $\star$(d, e) & \{a, b\}, \{c\}, \{d, e\}, $\ldots$, \{h, i, k\}\\
5.  $\star$(e, h) & \{a, b\}, \{c\}, \{d, e, h, i, k\}, \{f\}, \{g\}\\
6.  $\star$(e, g) & \{a, b\}, \{c\}, \{d, e, g, h, i, k\}, \{f\}\\
\hline
7.  (g, i) & ---\\
8.  (g, h) & ---\\
9.  $\star$(f, h) & \{a, b\}, \{c\}, \{d, e, f, g, h, i, k\}\\
\hline
10.  (d, f) & ---\\
11.  $\star$(c, f) & \{a, b\}, \{c, d, e, f, g, h, i, k\}\\
12.  (c, d) & ---\\
\hline
13.  $\star$(a, c) & \{a, b, c, d, e, f, g, h, i, k\}\\
14.  (c, b) & ---\\
15.  (b, e) & ---\\
\end{tabular}

\textcolor{red}{Zeichnung!}

Das sieht ja alles schon und toll aus, wie wird soetwas aber implementiert? Sehen wir uns dazu weiter das Beispiel an.

\begin{tabular}{l|llllllllll}
v $\in$ V & a & b & c & d & e & f & g & h & i & k\\
\hline
p[v] & a & a & c & i & d & f & i & i & i & i\\
\end{tabular}

Dies verleitet zu der einfachen Annahme, da"s bei \textsc{Union}($e$, $f$) einfach $e$ Vater und kanoisches Element der
Menge wird, zu der $f$ geh"ort (\lstinline[mathescape=true]!p[$f$]:=$e$!). Da immer der kleinere Baum an den gr"o"seren
angeh"angt werden soll (bei uns Gr"o"se nach Knotenanzahl, m"oglich w"are auch die H"ohe als Gr"o"se zu nehmen), ist
dies minimal aufwendiger.

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Union\textnormal{(e, f)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Union\textnormal{(e, f)}},gobble=1]{Union3(e,f)}
 if size[e]$<$size[f] then
   vertausche e und f
 p[f]:=e
 size[e]:=size[e]+size[f]  
\end{lstlisting}
\end{Algorithmus} 

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Make-Set\textnormal{(x)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Make-Set\textnormal{(x)}},gobble=1]{Make-Set(x)}
 p[x]:=x
 size[x]:=1  
\end{lstlisting}
\end{Algorithmus} 

\textsc{Find} ist ebenfalls trivial, es wird einfach durch die Menge gegangen, bis das kanonische Element auftaucht, im
Beispiel liefert \textsc{Find}(e) $e$ $\leadsto$ $d$ $\leadsto$ $i$ $\leadsto$ $i$, return $i$.

\begin{satz}
F"ur Vereinigung nach Gr"o"se gilt: Ein Baum mit H"ohe n hat mindestens 2$^n$ Knoten. 
\end{satz}
Daraus folgt, da"s es hier keine d"unnen B"aume gibt. Der Beweis des Satzes erfolgt mittels Induktion "uber die
Komplexit"at der B"aume, doch dazu mu"s erstmal folgendes definiert werden:
 
\begin{definition}[VNG-Baum]
T ist ein VNG-Baum, genau dann wenn
\begin{enumerate}
\item T nur aus der Wurzel besteht oder
\item T Vereinigung von T$_1$ und T$_2$ mit size[T$_2$]$\leq$size[T$_1$] ist.
\end{enumerate}
\end{definition}
VNG steht dabei f"ur Vereinigung nach Gr"o"se.

\begin{beweis}
\begin{itemize}
\item[I.A.] h=0 (Wuzelbaum) 2$^h=1$
\item[I.V.] gelte f"ur T$_1$, T$_2$ o.B.d.A. size[T$_2$]$\leq$size[T$_1$], size[T$_2$]$\geq 2^{h_2}$, size[T$_1$]$\geq
2^{h_1}$
\item[I.B.] size[T]$\geq 2^h$
\item[1. Fall] $h=$max$(h_1, h_2)$ $\Rightarrow$ size[T]=size[T$_1$]+size[T$_2$]$\geq 2^{h_1}+2^{h_2} \geq 2^h$
\item[2. Fall] $h=$max$(h_1, h_2)+1$ $\Rightarrow$ size[T]=size[T$_1$]+size[T$_2$]$\geq 2 \cdot $size[T$_2$]$\geq 2
\cdot 2^{h_2} =2^{h_2+1} \geq 2^h$
\end{itemize}
\end{beweis}

\textcolor{red}{Ist der Beweis nicht falsch? Beispiel: 2. Fall: h$_1$=10, h$_2$=4, h=11}

Aus dem Satz ergibt sich, da"s \textsc{Find} bei Vereinigung nach Gr"o"se O($\log n$) kostet.

\begin{satz}
F"ur Vereinigung nach H"ohe gilt: Ein Baum der H"ohe $h$ hat mindestens 2$^h$ Knoten.
\end{satz}
\textcolor{red}{Was soll die Verarsche, wo ist der Unterschied zum vorherigen Satz? Die Existenz von d"unnen B"aume?}
Also geht auch hier das \textsc{Find} in O($\log n$).

Das \textsc{Make-Set}(x) funktioniert immer in O(1), das \textsc{Find} geht in O($\log n$) und \textsc{Union} klappt in
O(1) falls nur ein Pointer umgesetzt werden mu"s. In einer Idealvorstellung, in der alle Elemente auf das kanonische
Element zeigen, geht \textsc{Find} in O(1), das \textsc{Union} braucht dann aber O($n$). Solche Idealvorstellungen sind
nat"urlich meist sehr realit"atsfern und nicht praktikabel. Doch lassen sich Union-Find-Strukturen noch mit anderen
Mitteln verbessern?

Ja, allerdings. Dazu bedient man sich der Pfad-Kompression und der inversen einer Funktion, die starke "Ahnlichkeit mit
der Ackermann-Funktion aufweist. Dies zusammen f"uhrt zu dem noch folgenden Satz von Tarjan. Dazu wird die
Union-Find-Struktur mit einer amortisierten Analyse auf ihre Kosten untersucht. Wir betrachten eine Folge von insgesamt 
$m$ \textsc{Union-Find}-Operationen mit $n$ \textsc{Make-Set}-Operationen dabei. Dazu brauchen wir zuerst die
Ackermann-Funktion und ihre Inverse.
\begin{definition}[Die Ackermann-Funktion und ihre Inverse]
 Die Ackermann-Funktion
 \begin{align*}
 A&(1,j)=2^j   & j\geq 1\\
 A&(i,1)=A(i-1,2)   & i\geq 2\\
 A&(i,j)=A(i-1, A(i,j-1))   & i,j \geq 2\\
 \end{align*}
 und ihre Inverse
 \[\alpha(m,n):=\mbox{min}\{i \geq 1 : A\left(i,\left\lfloor\frac{m}{n}\right\rfloor\right)> \log n\]
\end{definition}
Wegen obigem kann $n$ nicht gr"o"ser als $m$ sein. F"ur den Grenzwert der Inversen gilt nat"urlich
\[\lim_{\substack{n \rightarrow \infty \\n \leq m }}=\infty\]
Da die Funktion aber extrem schnell w"achst und damit ihre Inverse extrem langsam ist in allen konkreten Anwendungen 
$\alpha (m,n)$ aber kleiner als $5$. 

\begin{satz}[Satz von Tarjan]
F"ur $m$ \textsc{Union-Find}-Operationen mit n$\leq$m \textsc{Make-Set}-Operationen ben"otigt man $\Theta( m \cdot
\alpha(m,n))$ Schritte, falls die Operationen in der Datenstruktur "'Disjoint-Set-Forest"` mit \textsc{Union} nach
Gr"o"se (H"ohe) und \textsc{Find} mit Pfadkompression realisiert werden.
\end{satz}
\textcolor{red}{Es wird noch garnicht erl"autert, was die Pfadkompression denn nun genau ist.}
Noch ein Nachsatz zur Pfadkompression: Wenn wir vom Knoten zur Wurzel laufen, dann laufen wir nochmal zur"uck und setzen
alle Zeiger auf den Vater, den wir nun kennen $\rightarrow$ O(2$x$)=O($x$).

Ein weiteres Beispiel ist die Bestimmung der Zusammenhangskomponenten\textcolor{red}{Beispiel wof"ur?}

\chapter{Hashing}
Jeder wird schon einmal vom Hashing geh"ort haben, da"s es irgendwie mit Funktionen zu tun hat, ist wohl auch klar. Was
steckt aber genau dahinter?

F"ur jede Funktion braucht man einen Grundbereich, eine Menge, auf denen diese Funktion operiert. Genau genommen
braucht man eine "`¸Ursprungsmenge"' f"ur die Argu\-mente und eine "`Zielmenge"' f"ur die Ergebnisse. Da aber h"aufig die
"`Ursprungsmenge"' (Urbildmenge) und die "`Zielmenge"'(Bildmenge) gleich sind, betrachtet man ohne weitere Erw"ahnung
eine Menge, die aber (implizit) beides ist.

Im Falle des Hashings ist unser Universum die Urbildmenge U. Dieses enth"alt alle denkbaren Schl"ussel, darin gibt es
aber eine Teilmenge K, die nur alle auch wirklich betrachteten Schl"ussel enth"alt.

Sehen wir uns alle Familiennamen mit maximal 20 Buchstaben an, in U sind alle Zeichenketten mit maximal 20 Buchstaben,
in K aber nur alle tats"achlich vorkommenden Namen.

\section{Perfektes Hashing}
Perfektes Hashing ist noch recht einfach, es wird die Tabelle T direkt adressiert und Werte $x$ direkt eingef"ugt und
gel"oscht. Daf"ur brauchen wir eine Hash-Funktion $h$ und einen Schl"ussel $k$ (f"ur engl. "`key"').
\textcolor{red}{das sollte noch verbessert werden}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Insert\textnormal{(T, x)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Insert\textnormal{(T, x)}},gobble=1]{Insert3(T, x)}
 T[key[x]]:=x
\end{lstlisting}
\end{Algorithmus} 

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Delete\textnormal{(T, x)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Delete\textnormal{(T, x)}},gobble=1]{Delete(T, x)}
 T[key[x]]:=Nil
\end{lstlisting}
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Search\textnormal{(T, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Search\textnormal{(T, k)}},gobble=1]{Search(T, k)}
 return T[k]
\end{lstlisting}
\end{Algorithmus} 

Alle drei Operationen dauern O(1) Schritte, aber
die Hash-Funktion $h$ bildet aus U auf $m-1$ Werte ab ($h: U \rightarrow \{0, \cdots, m-1\}$), bei vielen Werten steigt
damit die Wahrscheinlichkeit f"ur Kollisionen (Abbildung auf den gleichen Funktionswert) sprung\-haft an. 

Dies wird durch das folgende Beispiel vielleicht klarer: Bei 23 Personen
ist die Wahr\-scheinlichkeit, da"s sie alle an verschiedenen Tagen Geburtstag haben etwas geringer als $0,5$, bei 40
Personen ist die Wahrscheinlichkeit daf"ur schon auf ca. 10\% gesunken.

\section{Drei Methoden zur Behandlung der Kollisionen}
\begin{enumerate}
\item Hashing mit Verkettung (Chaining): 
  \begin{itemize}
  \item Bei \textsc{Insert}(T,$x$) wird $x$ hinter dem Kopf der Liste T[h[key[x]]]
    eingef"ugt, jeder Eintrag der Hash-Tabelle kann also eine Liste beinhalten. 
  \item Beim \textsc{Search} wird in der Liste T[h[k]] mit key[$x$]=k nach Objekt $x$ gesucht
  \item \textsc{Delete} funktioniert analog zum \textsc{Search}
  \end{itemize}
\item Divisions-Rest-Methode $h(k):=k \mod m$ (z.B. $m$ Primzahl, die nicht zu dicht an einer Zweierpotenz liegt)  
\item multiplikative Methode $h(k):=\lfloor m(k \cdot \phi -\lfloor k \cdot \phi \rfloor) \rfloor$, dabei ist $m$ die
konstante Tabellengr"o"se und $0 < \phi <1$. Donald E. Knuth schl"agt z.B. $\phi=\frac{sqrt{5}-1}{2} \approx 0,618$ vor.
\end{enumerate} 
Au"ser den drei vorgestellten Methoden gibt es noch weitere, die uns hier aber nicht interessieren\textcolor{red}{war
das so gemeint?}. Die letzten beiden gezeigten Methoden werfen zus"atz\-lich die Frage auf, was eine
"`gute"' Hashfunktion ist.

\section{Analyse des Hashings mit Chaining}
Beim Hashing mit Chaining steht in der $j$-ten Zelle der Hash-Tabelle ein Zeiger auf die Liste mit $n_j$ Elementen. Um
die Anzahl der Kollisionen abzusch"atzen, gibt es einen sogenannten Ladefaktor $\alpha$.
%Beim Hashing mit Chaining gibt es einen sogenannten Ladefaktor \[\alpha:=\frac{n}{m}\]
Jetzt wird einfaches uniformes Hashing angenommen. Dann ist die Wahrscheinlichkeit f"ur ein gegebenes Element in eine bestimmte
Zelle zu kommen, f"ur alle Zellen gleich. Daraus folgt auch die idealisierte Annahme, da"s alle Listen gleich lang sind
(sonst dauert das \textsc{Search} schlimmstenfalls noch l"anger). Dann gilt \[\mathbb{E}[n_j]=\frac{n}{m}=\alpha\]

\begin{satz}[Theorem 1]
Wenn beim Hashing Kollisionen mit Verkettung gel"ost werden, dann braucht eine erfolg\-lose Suche $\Theta(1+\alpha)$
Zeit.
\end{satz}
\begin{beweis}
$k$ gesucht $\rightarrow h(k)$ berechnet $\rightarrow h(k)=j$ $j$-te Liste durchlaufen $\Rightarrow \Theta(1+\alpha)$
\end{beweis}

\begin{satz}[Theorem 2]
F"ur eine erfolgreiche Suche werden ebenfalls $\Theta(1+\alpha)$ Schritte gebraucht.
\end{satz}
\begin{beweis}
Wir stellen uns einfach vor, da"s bei \textsc{Insert} Werte am Ende der Liste eingef"ugt werden. Dann brauchen wir
wieder Zeit zum Berechnen des Tabelleneintrages, daran anschlie"send mu"s wieder die Liste durchlaufen werden.
\end{beweis} \textcolor{red}{Was soll die G"ulle, das wird doch jetzt nochmal, blo"s weniger wischi-waschi bewiesen?}
\begin{beweis}
Hier nehmen wir an, da"s erfolgreich nach einem Element gesucht wird, also mu"s diese vorher irgendwann auch eingef"ugt
worden sein. Sei nun dieses Element an $i$-ter Stelle eingef"ugt worden (vorher $i-1$ Elemente in der Liste), die
erwartete L"ange der Liste ist dann nat"urlich $\frac{i-1}{m}$. Jetzt seien $n$ Elemente in der Liste, dann gilt:
\[\frac{1}{n} \cdot \sum_{i=1}^n \left(1+\frac{i-1}{m}\right)=1+\frac{1}{n-m} \sum_{i=1}^n (i-1)=1+\frac{1}{n-m} \cdot
\frac{(n-1)n}{2}=1+\frac{\alpha}{2}-\frac{1}{2m} \in \Theta(1+\alpha)\]
\end{beweis}
Bei festem $h$ k"onnen aber auch ung"unstige Inputs problematisch werden. Dieses Problem f"uhrt zum Universal Hashing.

\section{Universal Hashing}
Festgelegte Hashfunktionen k"onnen zu ung"unstigen Schl"usselmengen f"uhren, bei zuf"allig gew"ahlten Hashfunktionen
werden diese ung"unstigen Schl"usselmengen durch die zuf"allige Wahl kompensiert. F"ur das Einf"ugen und Suchen eines Wertes
$x$ mu"s aber immer die gleiche Hashfunktion benutzt werden, dazu wird beim Einf"ugen die Hashfunktion zuf"allig
ausgew"ahlt und dann abgespeichert.

\begin{definition}
Sei H eine Menge von Hashfunktionen, die das Universum U in $\{0, \ldots, m-1\}$ abbilden. Dann hei"st H genau dann
universal, wenn f"ur jedes Paar von verschiedenen Schl"usseln $x, y \in $U gilt:
\[\vert\{h \in \mbox{H} : h(x)=h(y)\}\vert=\frac{\vert \mbox{H}\vert}{m}\]
oder anders ausgedr"uckt:
\[\mbox{H }universal \leftrightarrow Wsk([h(x)=h(y)])=\frac{1}{m} \mbox{ f"ur }x,\, y \in U \wedge x\not=y\]
\end{definition}

Sei $K \subseteq U$ eine Menge von Schl"usseln und $\vert K\vert =n$. Au"serdem sei H eine universale Klasse von
Hashfunktionen. Dann gilt der folgende Satz:
\begin{satz}
Falls $h$ zuf"allig aus H ausgew"ahlt wird, dann ist pro Schl"ussel k $\in$ K die mittlere Anzahl der Kollisionen
(h(x)=h(y)) h"ochstens $\alpha \mbox{ } (=\frac{n}{m})$
\end{satz}
\textcolor{red}{der Beweis erfolgte lt. Skript sp"ater oder in der "Ubung, kam der noch?}
Gibt es soetwas "uberhaupt?

Sei $m$ eine Primzahl, dann ist die folgende Klasse universal:
Die Werte in unserem Universum seien als Bit-Strings der gleichen L"ange gegeben, jeder Schl"ussel $x$ also in der
Gestalt $x=<x_0, x_1, \cdots, x_r>$, wobei die L"ange aller $x_i$ gleich ist.  Dabei ist $0\leq x_i\leq m$ Voraussetzung
bez"uglich $m$ bzw. $r$.
\begin{definition}
\[H:=\{h=h_a : a=<a_0, \ldots, a_r>, a_i \in \{0, \ldots, m-1\}, i=0, \ldots, r\]
\[h_a(x):=\sum_{i=0}^r (a_i \cdot x_i) \mod m\]
\end{definition}
Es ist ersichtlich, da"s es dabei $m^r$ verschiedene Hashfunktionen geben kann.

\begin{satz}
Die so definierte Klasse ist universal.
\end{satz}
F"ur den Beweis brauchen wir noch einen Hilfssatz:
\begin{satz}[Hilfssatz]
Falls $x\not= y$ ist, erf"ullen von den $m^{r+1}$ veschiedenen Hashfunktionen genau $m^r$ St"uck die Bedingung
$h_a(x)=h_a(y)$ $\Rightarrow$ \[\mbox{Wsk}([h(x)=h(y)])=\frac{m^r}{m^{r+1}}=\frac{1}{m}\]
\end{satz}
\begin{beweis}[Beweis des Hilfssatzes und des Satzes]
Sei $x\not=y$ (o.B.d.A $x_0 \not=y_0$)
\[h_a(x)=h_a(y) \Leftrightarrow \sum_{i=0}^r (a_i \cdot x_i)\!\! \mod m=\sum_{i=0}^r (a_i \cdot y_i)\!\! \mod m \leftrightarrow
a_0(x_0-y_0)=\sum_{i=1}^r a_i (x_i- y_i)\]
Da m eine Primzahl ist, kann durch $(x_0-y_0)$ geteilt werden. Damit ist auch die Struktur $(\{0, \ldots, m-1\}, +_{\mod
m}, -_{\mod m})$ ein algebraischer K"orper. Die Umformungen f"uhren schlie"slich zu \[\star  \qquad 
a_0=\frac{1}{x_0-y_0} \cdot \sum_{i=1}^r (a_i (x_i- y_i))\]
Damit kann f"ur jede Wahl von $<a_1, \ldots, a_r> \in \{0, \ldots, m-1\}$ mit $\star$ das zugeh"orige $a_0$ eindeutig
bestimmt werden. Bez"uglich der ersten Komponente von $a$ besteht also keine freie Wahl, oder anders ausgedr"uckt: $a_0$
ist Funktion von $(a_1, \ldots, a_r)$, davon gibt es aber genau $m^r$ St"uck. Es folgt aus $h_a(x)=h_a(y)$ insgesamt,
da"s jedes Tupel $(a_1, \ldots, a_r)$ das $a_0$ eindeutig bestimmt. Und so haben von $m^{r+1}$ m"oglichen Tupeln nur
$m^r$ dei Eigenschaft $h_a(x)=h_a(y)$. Damit ist der Hilfssatz, aus dem der Satz folgt, bewiesen.
\end{beweis}

\section{Open Hashing}
Anfangs wurde die direkte Adressierung erw"ahnt, doch auch dies geht anders. Beim Open Hashing wird eine offene
Adressierung verwendet, dies ist sinnvoll, falls praktisch kein \textsc{Delete} vorkommt. Beim Einf"ugen wird probiert,
ob die ausgesuchte Stelle in der Tabelle frei ist. Falls sie frei ist, wird der Wert eingef"ugt und fertig, falls nicht
wird weitergegangen. So ist $\alpha=\frac{n}{m}<1$ m"oglich.

Die Proben-Sequenz (auch Probier- oder Sondierfolge) ist als Abbildung ein mathema\-tisches Kreuzprodukt:
\[h: U \times \{0, \ldots, m-1\} \stackrel{\rightarrow}{auf}\{0, \ldots, m-1\}\]
Die Algorithmen f"ur die Operationen sind dann nat"urlich etwas komplexer.

\begin{Algorithmus}[H]
Der Einfachheit halber betrachten wir Schl"usselwerte $k$ mit $k \in U$
\addcontentsline{alg}{Algorithmus}{\textsc{Insert\textnormal{(T, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Insert\textnormal{(T, k)}},gobble=1]{Insert4(T, k)}
 i:=0
 repeat j:=h(k,i)
   if T[j]=Nil then
     T[j]:=k
     return j
   else i:=i+1
 until i=m
 Fehler -- Tabelle voll    
\end{lstlisting}
\end{Algorithmus} 

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Search\textnormal{(T, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Search\textnormal{(T, k)}},gobble=1]{Search4(T, k)}
 i:=0
 repeat j:=h(k,i)
   if T[j]=k then
     return j
   else i:=i+1
 until (T[j]=Nil OR i=m)
 return Nil    
\end{lstlisting}
\end{Algorithmus} 

Beim \textsc{Delete} taucht allerdings ein Problem auf. Nil w"urde gesetzt werden und dann w"urde \textsc{Search}
stoppen und nicht mehr weiter suchen. \textcolor{red}{Was soll das jetzt, l"ost das die Porbleme mit dem
\textsc{Delete}?}

\begin{tabular}{l|l|l}
lineares Sondieren & quadratrisches Sondieren & doppeltes Hashing\\
\hline
$h(k,i):=$ & $h(k,i):=$ & $h(k,i)=$\\
$(h'(k)+c \cdot i)\!\! \mod m$ & $(h'(k)+c_1 \cdot i + c_2 \cdot i^2)\!\! \mod m$ & $(h_1(k)+i \cdot h_2(k))\!\! \mod m$\\
Sei eine leere Zelle & & $h_1$ und $h_2$ sind wieder\\
gegeben und davor & & Hashfunktionen\\
$i$ volle & &\\
Dann wird eine Zelle & &\\
mit Wsk $\frac{i+1}{m}$ belegt & &\\
$\Rightarrow$ Prim"arcluster & $\Rightarrow$ Sekund"arcluster& \\
(lange Suchzeiten) & & \\
\end{tabular}

\section{Nochmal zur Annahme des (einfachen) uniformen Hashings}
Beim einfachen uniformen Hashing gilt f"ur jeden Schl"ussel $k$, da"s jeder Tabellenplatz f"ur ihn gleichwahrscheinlich
ist (unabh"angig von den Pl"atzen anderer Schl"usselwerte). Falls k eine zuf"allige reelle Zahl ist, die in $[0, 1]$
gleichverteilt ist, gilt dies f"ur $h(k)=\lfloor k \cdot m\rfloor$
Beim uniformen Hashing gilt f"ur jeden Schl"ussel $k$, da"s jede der $m!$ Permutationen von \{0, \ldots, m-1\} als
Sondierungsfolge gleichwahrscheinlich ist. Eigentlich gibt es kein uniformes Hashing, aber man kann sehr nahe an diese
Forderung (und ihre Folgen!) herankommen.
\begin{itemize}
\item[A] immer darauffolgende Zahl $\rightarrow m! sehr klein$
\item[B] $h'(k_1)=h(k_1, 0)$, $h'(k_2)=h(k_2, 0)$, gleiche Funktion $\rightarrow$ gleicher Platz, $m$ ver\-schiedene
Sondierungsfolgen 
\item[C] gute Wahl von $h_1$ und $h_2 \rightarrow$ nah am uniformen Hashing
\end{itemize}
Damit ist also Variante "'C"` (Doppeltes Hashing) am dichtesten am uniformen Hashing dran, wie bestimmt man nun $h_1$
und $h_2$?
Daf"ur gibt es keine festen Regeln, allerdings sollte
\begin{itemize}
\item $h_2(k)$ relativ prim zu $m$ oder
\item $m$ eine Primzahl sein.
\end{itemize}
Ersteres wird oft eingehalten. M"oglich ist eine Struktur wie 

$\begin{array}{l}
h_1(k)= k \mod m\\
h_2(k)= 1 + (k \mod m')  \mbox{ mit } m' \mbox{ dicht an } m\\
\end{array}$

\subsubsection{Beispiel}
Sei also 

$\begin{array}{l}
h_1(k)= k \mod 13\\
h_2(k)= 1 + (k \mod 11)\\
\end{array}$

\begin{tabular}{lllllll}
Folge & 79 & 69 & 98 & 72 & 50 & 14\\
$h_1$ & 1 & 4 & 7 & 7 & 11 & 1\\
$h(k,i)$ & 1 & 4 & 7 & 8 & 11 & 4\\
\end{tabular}

Bei geschickter Wahl von $h_1$ und $h_2$ erf"ullt das doppelte Hashing die Annahme des uniformen Hashings ausreichend.
 
\begin{satz}[Satz 1 zum Open Hashing]
Gegeben sei eine Open-Hash-Tabelle mit $\alpha=\frac{n}{m}<1$. Dann ist der Erwartungswert f"ur die Anzahl der Proben in
der Sondierungsfolge bei erfolgloser Suche unter Annahme des uniformen Hashings h"ochstens
O$\left(\frac{1}{1-\alpha}\right)$
\end{satz}
Analoges gilt dann f"ur \textsc{Insert}. Je voller die Tabelle ist, umso n"aher ist $\alpha$ an 1 und umso l"anger
dauert das \textsc{Search}.

\begin{satz}[Satz 2 zum das Open Hashing]
Unter der Voraussetzung von Satz 1 gilt bei erfolgreicher Suche, da"s diese weniger als O$\left(\frac{1}{\alpha} \cdot
\ln{\frac{1}{1-\alpha}}\right)$ Schritte braucht (die Annahme des uniformen Hashings sei erf"ullt).
\end{satz}

\begin{beweis}[Nur die Beweisskizze]
Sei das $k$ nach dem gesucht wird, in der Tabelle und als $(i+1)$-ter Wert eingef"ugt worden. Mit der Folgerung vom
ersten Satz zum Open Hashing ist $\alpha=\frac{i}{m}$ beim Einf"ugen von $k$. Die erwartete Anzahl der Proben beim
\textsc{Insert} von $k$ ist danach \[\frac{1}{1-\frac{1}{m}}=\frac{m}{m-i}\]
. Bei $n$ Schl"usseln ist der Mittelwert zu bilden: 
\[\frac{1}{n} \sum_{i=0}^{n-1} \frac{m}{m-i}=\frac{m}{n} \sum_{i=0}^{n-1}
\frac{1}{m-i}=\frac{m}{n}\left(H_m-H_{m-1}\right),\] dabei steht $H_m$ f"ur die harmonische Reihe und es ist
\[H_m=\sum_{mu}^{m}\frac{1}{\mu}\]

Weiter ist
\[\frac{1}{\alpha}\left(H_m-H_{m-1}\right)=\frac{1}{\alpha} \sum_{k=m-n+1}^m \frac{1}{k} \leq \frac{1}{\alpha}\int
\limits_{m-n}
\left(\frac{1}{x}\right)dx=\frac{1}{\alpha} \ln{\frac{m}{m-n}}=\frac{1}{\alpha}\ln{\frac{1}{1-\alpha}}\].
\end{beweis}

Sei z.B. die Tabelle halbvoll $\rightarrow \sim 1,387$ oder zu 90\% voll $\rightarrow \sim 2,56$. Diese Werte gelten
nicht unbedingt f"ur unsere Beispiele wie doppeltes Hashing sondern f"ur idealisertes uniformes Hashing. Falls aber
$h_1$ und $h_2$ geschickt gew"ahlt werden (siehe unser Beispiel), liegen die tats"achlichen Werte nah bei den oben
angegebenen. 

Was bleibt nun als Folgerung? Falls kein \textsc{Delete} erforderlich ist, wird Open Hashing benutzt, sonst Chaining.

  
%\end{document} 
%
