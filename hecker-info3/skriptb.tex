%\documentclass[draft,12pt]{scrreprt}
%
%\usepackage{listings}
%\usepackage{ngerman}
%\usepackage[draft=false]{hyperref}
%\usepackage{fancyvrb}
%\usepackage{amssymb}
%\usepackage{amsmath}
%\usepackage{color} 
%\usepackage{graphicx}
%\usepackage{theorem}
%\usepackage{float}
% \usepackage{ae}  %probably interesting if font-collision
                   % always remember to use pdflatex instead of
                   % dvipdf, fonts are so much better the first way
%
%\fvset{fontsize=\small,frame=single,numbers=left}
%
%\theoremstyle{break}
%\theorembodyfont{\upshape}
%\newtheorem{beweis}{Beweis}
%\theorembodyfont{\itshape}
%\newtheorem{definition}{Definition}
%\newtheorem{satz}{Satz}
%\newfloat{Algorithmus}{h}{alg}[chapter]
%
%\begin{document}
%
\chapter{Sortieren und Selektion}

\( \textup{O}(n\log n) \) ist die obere Schranke für \textsc{MergeSort} (O(n$^2$) für \textsc{Insertionsort}).

Frage: Geht es besser? 
\begin{description}
	\item[- Ja]
		\begin{enumerate}
			\item Mit Parallelrechnern, aber das ist nicht Thema dieser Vorlesung.
			\item Unter bestimmten Bedingungen.
		\end{enumerate}
	\item[- Nein]
		bei allgemeinen Sortierverfahren auf der Basis von Schlüsselvergleichen. Unser Ziel ist der Beweis, daß für  
		allgemeine Sortierverfahren auf der Basis von Schlüsselvergleichen \( \Omega(n\log n)\) eine untere Schranke ist.
\end{description} 

\begin{beweis}
\textbf{Modellieren des Ansatzes: "`Auf der Basis von Schlüsselvergleichen"'.}
  \begin{itemize}
  \item INPUT ist ein Array mit (o.~B.~d.~A.) paarweise verschiedenen Werten 
  \( (a_1, \dots , a_{\textup{n}})\), \(a_i \in S\), \(i=(1, \dots, n)\) auf das nur
  mit der Vergleichsfunktion  \begin{gather*} V(i,j) : = \left\lbrace
  \begin{array}{ll} 1, & a_i < a_j \\ 0, & a_i > a_j \end{array} \right.\end{gather*}
  zugegriffen werden kann.
  \item OUTPUT ist eine Permutation \( \pi \) für die \( a_{\pi (1)} < a_{\pi (2)} < \dots < a_{\pi (n)} \) ist 
\end{itemize}

Sei A ein beliebiges o.~B.~d.~A. deterministisches Sortierverfahren dieser Art.
Die erste Anfrage ist dann nicht \( (a_i < a_j) \) sondern \( (i < j) \)

  \begin{definition}
  \(a(i<j) := \lbrace (a_1, \dots , a_n \vert\, a_i \in S \wedge a_i < a_j \rbrace \)
  \end{definition}

%
% Sebastian Oerding
Der erste Test $V(i,j)$ der Vergleichsfunktion wird immer für dasselbe Indexpaar $(i,j)$ der Eingabe \((a_1, \dots , a_{\textup{n}})\)
ausgeführt, über die der Algorithmus A zu diesem Zeitpunkt noch keinerlei Informationen besitzt.

Falls nun $V(i,j)=1$ ist, d. h. für alle Eingaben, die der Menge a(i$<$j)\(=\{(a_1,\ldots,a_{\textup{n}})\in \mathbb{R}^n :
a_{\textup{i}}<a_{\textup{j}}\}\) angehören, wird der zweite Funktionsaufruf immer dasselbe Indexpaar (k,l) als Parameter
enthalten, da A deterministisch ist und zu diesem Zeitpunkt nur weiß, daß \(a_{\textup{i}}<a_{\textup{j}}\) ist. Analog wird für alle
Folgen a(j$<$i) derselbe Funktionsaufrauf als zweiter ausgeführt. Die Fortführung dieser überlegung führt zu dem vergleichsbasierten
Entscheidungsbaum von Algorithmus A, einem binären Baum, dessen Knoten mit Vergleichen "`\(a_{\textup{i}}<a_{\textup{j}}\)"' beschriftet
sind. An den Kanten steht entweder "`j"' oder "`n"'. Ein kleines Beispiel ist in Abbildung~\ref{271003c} zu sehen.

Genau die Eingabetupel aus der Menge \(\textup{a}(3<4)\cap \textup{a}(3<2)=\{(a_1,\ldots,a_{\textup{n}})\in \mathbb{R}^n : a_3<a_4 \wedge
a_3<a_2\}\) führen zum Knoten $\mathcal{V}$. 

Weil A das Sortierproblem löst, gibt es zu jedem Blatt des Entscheidungsbaumes eine Permutation $\pi$, so das nur die Eingaben mit 
\(\textup{a}_{\pi(1)}<\textup{a}_{\pi(2)}<\ldots<\textup{a}_{\pi(\textup{n})}\) zu diesem Blatt führen. Der Entscheidungsbaum hat 
daher mindestens n! Blätter. Der Beweis dafür stammt fast unverändert aus \cite{klein}.
 
%
%Annette Eisenbraun
  \begin{figure}
    \centering\input{271003c.latex}
  \caption{Entscheidungsbaum}
  \label{271003c}
  \end{figure}

Im Regelfall hat ein Entscheidungsbaum allerdings mehr 
als $n!$ Blätter. Es gibt auch Knoten, die die leere Menge enthalten, oder Knoten, die nie erreicht werden können
(z.~B. Knoten $\mathcal{W}$).
\bigskip

\noindent	
Beispiele für den Baum aus Abbildung~\ref{271003c}:

\noindent
Bei der Eingabe (3, 4, 17, 25) wäre man nach Abarbeitung der Vergleiche 17$<$25, 4$<$17 und 3$<$4 im linkesten Knoten.

\noindent
Bei der Eingabe (17, 4, 3, 25) wäre man nach Abarbeitung der Vergleiche 3$<$25 und 4$<$3 im Knoten $\mathcal{V}$.

\noindent
Wir gehen über $\mathcal{V}$, wenn $a_3 < a_4$ und $a_3 < a_2 $, also für alle Tupel aus \( a(3<4) \cap a(3<2) \).

  \begin{satz}
  Ein Binärbaum der Höhe $h$ hat höchstens \( 2^h \) Blätter.
  \end{satz} 
 
Die Höhe eines Entscheidungsbaumes ist die maximale Weglänge von der Wurzel zu einem Blatt, sie entspricht der Rechenzeit. Wie bereits vorhin angedeutet, muß ein
solcher Baum mindestens n! Blätter haben, wenn er alle Permuationen der Eingabe berücksichtigen können soll (z.B. für das
Sortieren), damit muß gelten

  \begin{gather*}
   2^h \geq n! \leftrightarrow h \geq \log n! \geq \log \left(\frac{n}{2}\right)^{\frac{n}{2}}  
  \end{gather*}

  \begin{gather*}
  \textup{Der Beweis ist trivial da}\
  n! = 1 \cdot 2 \cdot 3 \ldots \cdot n = 1 \cdot 2 \cdot 3 \ldots \cdot \underbrace{\left(\frac{n}{2}+1\right) \cdot \ldots 
  \cdot n}_{\frac{n}{2}} \geq \left(\frac{n}{2}\right)^{\frac{n}{2}} 
  \end{gather*}

\(\log \left(\frac{n}{2}\right)^{\frac{n}{2}} = \frac{n}{2} \cdot \log \frac{n}{2} = \frac{n}{2} \log n - \frac{n}{2}
\underbrace{\log 2}_{1} = \frac{n}{3} \log n + \lbrack \frac{n}{6} \log n - \frac{n}{2} \rbrack \geq \frac {n}{3} \log n \) 

für \( n \geq 8\ \textup{ist}\ \log n \geq 3 \leftrightarrow \frac{n}{6} \log n \geq \frac {n}{2} 
\leftrightarrow \frac{n}{6} \log n - \frac{n}{2} \geq 0\), also \(h \geq n \log n\).

Worst-case-Fall im Sortieren hier ist ein Ablaufen des Baumes von der Wurzel bis zu einem Blatt und dies geht bei einem Baum der Höhe \(n
\log n\) in \(\textup{O}(n \log n)\) Zeit.
\end{beweis}
\hfill q.~e.~d. 

\section{Verschärfung des Hauptsatzes 1. "`Lineares Modell"'}

\( a_i < a_j \leftrightarrow a_j - a_i > 0
\leftrightarrow \exists \,d > 0 : a_j - a_i = d
\leftrightarrow \exists \, d > 0 : a_j - a_i - d = 0 \) 
\bigskip

Von Interesse ist, ob \( g(x_1 , \dots , x_n) < 0 \), wobei \( g(x_1 , \dots , x_n) = c_1 x_1 + \dots + c_n x_n + d \) mit
\( c_1, \dots , c_n, d \) als Konstanten und \( x_1, \dots, x_n  \) als Variablen.
Da Variablen nur in linearer Form vorkommen, nennt man dies "`Lineares Modell"'.

\begin{satz}
Im linearen Modell gilt für das Sortieren eine untere Zeitschranke von \( \Omega (n \log n) \).
\end{satz}
		
Der Beweis erfolgt dabei über die Aufgabenstellung "`$\varepsilon$-closeness"'. Denn, wenn die Komplexität der $\varepsilon$-closeness
in einer unsortierten Folge \( \Omega (n \log n) \) und in einer sortierten Folge \( \textup{O}(n)\) ist, dann muß die Komplexität des
Sortierens \( \Omega (n \log n)\) sein.
		
Beim Elementtest ist eine Menge \( \mathbb{M} \), \( \mathbb{M} \subseteq \mathbb{R}^n\) gegeben, sowie ein variabler Vektor \(
(x_1, \dots , x_n) \). Es wird getestet, ob \( (x_1, \dots , x_n) \in \mathbb{M} \), wobei \( \mathbb{M} \) natürlich fest ist. 
	
Bei der $\varepsilon$-closeness sieht die Fragestellung etwas anders aus. Als Eingabe sind wieder n reelle Zahlen \(a_1, \dots , a_n \)
und \( \varepsilon > 0 \) gegeben. Von Interesse ist aber, ob es zwei Zahlen in der Folge gibt, deren Abstand kleiner oder gleich
$\varepsilon$ ist.

Viel kürzer ist die mathematische Schreibweise: \( \exists \, i,  j \ (1 \leq i, j \leq n):\vert a_i - a_j \vert < \varepsilon \)?

Trivalerweise ist bei einer bereits sortierten Eingabe $\varepsilon$-closeness in O($n$) entscheidbar. Dazu wird einfach nacheinander
geprüft, ob \( \vert a_1 - a_2 \vert < \varepsilon \) oder \( \vert a_2 - a_3 \vert < \varepsilon \) oder \dots \( \vert a_{n-1} -
a_n \vert < \varepsilon \).

\begin{satz} 
Wenn $\varepsilon$-closeness die Komplexität $\Omega (n\, log\, n)$ hat, so auch das Sortieren.
\end{satz}		
		
\begin{satz}
Die Menge  \( \mathbb{M}  \) habe $m$ Zusammenhangskomponenten. Dann gilt, dass jeder Algorithmus im linearen Modell im worst case
mindestens \( \log m \) Schritte braucht, wenn er den Elementtest löst.
\end{satz}
		 	
\begin{beweis}
Jeder Algorithmus A im linearen Modell liefert einen Entscheidungsbaum mit Knoten, in denen für alle möglichen Rechenabläufe
gefragt wird, ob \( g( x_1, \dots , x_n) < 0 \)  ist. Jetzt genügt es zu zeigen, daß der Entscheidungsbaum mindestens soviele
Blätter hat, wie die Menge \( \mathbb{M}  \) Zusammenhangskomponenten. Mit dem eben bewiesenen folgt, daß dies äqivalent zu einer Höhe
des Entscheidungsbaumes von \(\log (card(\mathbb{M}))=\log m\) ist. Nun sei $b$ ein Blatt des Baumes.

  \begin{definition}
  \(E(b) := \lbrace \vec x \in \mathbb{R}^n : \) Alg. A landet bei der Eingabe von \( \vec x = (x_1, \dots ,x_n) \) in Blatt \( b \rbrace \)
  \end{definition}
		
  \begin{tabular}{rcl}
  \( g(x_1\), & \dots & , \( x_n) < 0 \) ? \\
  ja / & & \( \backslash \) nein\\
  \( g(x_1, \dots, x_n) < 0 \) & & \( g(x_1, \dots x_n) \geq 0 \)
  \end{tabular}
\bigskip
  
Nach Definition des linearen Modells sind diese Mengen $E(b)$ jeweils Durchschnitt von offenen und abgeschlossenen affinen Teilräumen
\begin{gather*}
  \begin{array}{c}
  \{(x_1,\ldots,x_{\textup{n}}) \in \mathbb{R}^n : g(x_1,\ldots,x_{\textup{n}})<0\}\\
  \{(x_1,\ldots,x_{\textup{n}}) \in \mathbb{R}^n : g(x_1,\ldots,x_{\textup{n}})\geq0\}
  \end{array}
\end{gather*}
Die Mengen $E(b)$ sind konvex und insbesondere zusammenhängend. Für alle Punkte a in $E(b)$ trifft der Algorithmus dieselbe
Entscheidung; folglich gilt entweder \(E(b) \subset \mathbb{M}\) oder \(E(b) \cap \mathbb{M}=\emptyset\). 

Sei $\mathcal{V}$ ein beliebiger Knoten.
Genau die Inputs einer konvexen und zusammenhängenden Menge führen dorthin (= der Durchschnitt von Halbräumen).	
  
  \begin{definition}	
  \( \mathbb{K} \) ist \textit{konvex}
  \(\leftrightarrow  \forall \, p, \, q: p\in \mathbb{K} \wedge q \in \mathbb{K}
  \rightarrow  \overline{pq} \subseteq \mathbb{K}\)
  \end{definition}
  
Nun  gilt \(\mathbb{R}^n = \bigcup_{b \ ist\ Blatt} E(b)\) also
\( \mathbb{M} = \mathbb{R}^n \cap \mathbb{M} = \bigcup_{b \, Blatt} E(b) \cap \mathbb{M} = \dots = \bigcup_{b \in \mathbb{B}} E(b) \)
für eine bestimmte Teilmenge $\mathbb{B}$ aller Blätter des Entscheidungsbaumes. Weil jede Menge $E(b)$ zusammenhängend ist, kann
diese Vereinigung höchstens $\vert\mathbb{B}\vert$ viele Zusammenhangskomponenten haben. Die Anzahl aller Blätter des
Entscheidungsbaumes kann nicht kleiner sein als $\vert\mathbb{B}\vert$, sie beträgt also mindestens $m$.

\hfill q.~e.~d. 

Die Komplexität des Elementtests ist also \( \Omega (\log m) \)	
  \begin{satz}
  Die Komplexität der $\varepsilon$-closeness ist \( \Omega (n \log n) \).
  \end{satz}

  \begin{beweis}
  $\varepsilon$-closeness ist ein Elementtest-Problem!

  \( \mathbb{M}_{\varepsilon^i} := \lbrace ( a_1, \dots , a_n ´) \in \mathbb{R}^n \vert \forall \, i \ne j : \vert a_i - a_j 
  \vert \geq \varepsilon \rbrace\) ist ein spezielles Elementtestproblem.
				
  $\pi$ sei eine Permutation \( \pi(1, \dots ,n) \), dann ist
  \( \mathbb{M}(\pi ) := \lbrace (a_1, \dots , a_n) \in \mathbb{M} \vert a_{\pi (1)} < a_{\pi (2)} < \dots a_{\pi (n)}  \rbrace \)
	
	
  \begin{satz} 
  Die Zusammenhangskomponenten von \( \mathcal{M}_\varepsilon \) sind die Mengen \( \mathcal{M}_\pi  \) (\textup{Beweis übung})
  \end{satz}		
		
  \textit{Folgerung: Jeder Entscheidungsbaum im linearen Modell erfordert \( \log (n!) \) Schritte im worst case. 
  (\( \Rightarrow \Omega (n \log n \)))}
		
  \end{beweis}	
\textit{Folgerung: Sortieren hat im linearen Modell die Komplexität \( \Omega (n \log n) \)}
\bigskip
	
Angenommen das stimmt nicht, dann existiert ein schnelleres Sortierverfahren. 
Dann benutze das für den Input von $\varepsilon$-closeness und löse $\varepsilon$-closeness schneller als in \( \Omega (n \log n)\)
\( \rightarrow \) dann exisitiert für $\varepsilon$-closeness ein Verfahren von geringerer Komplexität als \( \Omega (n \log n) \)
\( \rightarrow \) Widerspruch zur Voraussetzung \( \rightarrow \) Behauptung
		
(relativer Komplexitätsbeweis)

\hfill q.~e.~d.
\end{beweis}


% 
% Janine Roy
Hilfssatz zur Berechnung der Pfadlängen (Ungleichung von Kraft):

\noindent
Sei $t_{i}$ die Pfadlänge für alle m Pfade eines Binärbaumes von der Wurzel zu den m Blättern, dann gilt:
\begin{gather*}
\sum_{i=1}^{m} 2^{-t_{i}}\leq 1
\end{gather*} 


Beweis induktiv über $m$

\noindent
$m = 1:$ trivial

\noindent
$m \geq 2:$ Dann spaltet sich der Baum in maximal zwei Bäume auf, deren Pfadlängen m$_1$ und m$_2$ um eins geringer sind, als die
von m. Die neuen Längen werden mit t$_{1,1}$ \ldots t$_{1,m_1}$ und t$_{2,1}$ \ldots t$_{2,m_2}$ bezeichnet.
\bigskip

Nach Voraussetzung gilt:
\begin{gather*}
\sum_{i=1}^{m_{1}} 2^{-t_{1i}}\leq 1\ \textup{und}  \sum_{i=1}^{m_{1}} 2^{-t_{2i}}\leq 1
\end{gather*}

Jetzt werden Pfadlängen um 1 größer, dann gilt für $t_{1i}$ (und analog für $t_{2i}$):
\begin{gather*}
2^{(-t_{1i}+1)}=2^{-t_{1i}-1}=2^{-1}\,2^{-t_{1i}}
\end{gather*}

Für T folgt also:
\begin{gather*}
\sum_{i=1}^{m_1,\, m_2} 2^{-t_{j}}=2^{-1}\left(\sum_{i=1}^{m_{1}} 2^{-t_{1i}}+\sum_{i=1}^{m_{2}} 2^{-t_{2i}}\right)
\leq 2^{-1}(1+1)=1
\end{gather*}
\hfill  q.e.d

Hilfssatz:
\begin{gather*}
\frac{1}{m}\, \sum_{i=1}^{m}t_{i}\geq \log m
\end{gather*}

Dabei gelten die selben Bezeichnungen wie oben.

Beweis:
\[
\frac{1}{m}\,  \sum_{i=1}^{m}2^{-t_{i}} \geq \sqrt[m]{\pi_{i=1}^{m}\ 2^{-t_{i}}}=\sqrt[m]{2^{-t_i}\, 2^{-t_2}\cdot \ldots \cdot 2^{-t_m}}
=\sqrt[m]{2^{-t_1-\, \ldots \, -t_m}}= 2^{-\frac{1}{m}\sum_{i=1}^{m}t_i}
\]
\[
\Rightarrow m \leq 2^{\frac{1}{m}\, \sum_{i=1}^{m}t_i}
\]
\[
\textup{Also gilt für die Pfadlänge:}\ \log m \leq\frac{1}{m}\, \sum_{i=1}^{m}t_i
\]
\hfill q.e.d.

\begin{satz}
Hauptsatz über das Sortieren

Das Sortieren auf der Basis von Schlüsselvergleichen kostet bei Gleichwahrscheinlichkeit aller Permutationen der Eingabe
$\theta (n\ log\ n)$ (mit den schnellstmöglichen Algorithmen).
\end{satz}

\begin{beweis}
Annahme $m>n!$

\begin{gather*}
\Omega(n\, \log n) \ni \log n!\leq\frac{1}{n!}\, \sum_{i=1}^{m}t_i \leq\frac{1}{m}\, \sum_{i=1}^{m}t_i
\end{gather*}

Da wir bereits die untere Schranke bewiesen haben, muß \(\frac{1}{m}\, \sum_{i=1}^{m}t_i \geq \frac{1}{n!}\, \sum_{i=1}^{m}t_i\) gelten,
also \(\frac{1}{m} \geq \frac{1}{n!}\) und damit \(m \geq n!\) sein.

Falls \(m>n!\), dann ist aber \(\frac{1}{m} < \frac{1}{n!}\).

Widerspruch zur Voraussetzung (untere Schranke) \(\Rightarrow (m \geq n! \wedge \lnot (m>n!)) \rightarrow m=n!\)
\end{beweis}
\hfill q.e.d.

\section{\textsc{Quicksort}}
Bei \textsc{Quicksort} handelt es sich ebenfalls um ein Teile-und-Hersche-Verfahren.

Eingabe ist wieder ein Feld A=(\(a_0, \ldots, a_{\textup{n}}\)), die Werte stehen dabei in den Plätzen \(a_1, \ldots, a_{\textup{n}}\).
Dabei dient $a_0:=-\infty$ als Markenschlüssel, als sogenanntes Sentinel-Element (siehe auch \textsc{MergeSort}) und
v ist das Pivotelement.

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Quicksort}}
%\lstset{emph={Quicksort}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, Quicksort}, emphstyle=\textsc, escapeinside=~~}  
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Quicksort}, gobble=2]{Quicksort}
  1)Quicksort(l,r)
    if r>l then
      i:=Partition(l,r)
      Quicksort(l,i-1)
      Quicksort(i+1,r) 

  2)Quicksort(l,r)
    if r>l then
    v:= a[r] 
    i:= l-1 
    j:= r
    repeat
      repeat 
        i:=i+1 
      until a[i]>=v
      repeat 
        j:=j-1 
      until a[j]<=v
      t:=a[i]  
      a[i]:=a[j] 
      a[j]:=t
    until j<=i
  Quicksort(l,i-1)
  Quicksort(i+1,r) 
\end{lstlisting}
\end{Algorithmus}

Was passiert bei 2)?
\begin{enumerate}
\item Es wird von links nach rechts gesucht, bis ein Element größer v ist
\item Es wird von rechts nach links gesucht, bis ein Element kleiner v ist
\item Dann werden die beiden Elemente vertauscht, falls sie sich treffen, so kommt v an diese Stelle
\end{enumerate}

Beispiel (getauscht werden die \textbf{Fetten}):

\begin{tabular}{lllllllllll}
2 & \textbf{7} & 8 & 9 & 0 & 1 & 5 & \textbf{3} & 6 & 4 & \hspace{15pt} 4 =: Pivotelement\\
2 & 3 & \textbf{8} & 9 & 0 & \textbf{1} & 5 & 7 & 6 & 4 &\\ 
2 & 3 & 1 & \textbf{9} & \textbf{0} & 8 & 5 & 7 & 6 & 4 &\\
2 & 3 & 1 & 0 & \textbf{9} & 8 & 5 & 7 & 6 & \textbf{4} &\\
%2 & 3 & 1 & 0 & 4 & 8 & 5 & 7 & 6 & 9 & \hspace{15pt} $i = 5$, $j = 4$\\
\end{tabular}

%
\begin{figure}[H]
  \centering\input{031103a.latex}
  \label{031103a}
\end{figure}

Am Ende sind alle Zahlen, die kleiner bzw. größer sind als 4, davor bzw. dahinter angeordnet. 
Jetzt wird der Algorithmus rekursiv für die jeweiligen Teilfolgen aufgerufen.

Nach einer anderen Methode von Schöning (nachzulesen in \cite{sedgewick}) sieht die Eingabe 2 7 8 9 0 1 5 7 6 4 nach dem ersten Durchlauf so aus: 
\underline{2 0 1} 4 \underline{7 8 9 5 7 6}

\subsection{Komplexität des \textsc{Quicksort}-Algorithmus'}

T(n) sei die Anzahl der Vergleiche, im besten Fall "`zerlegt"' das Pivotelement die Sequenz in zwei gleich große Teile und es gilt die
bekannte Rekurrenz
\begin{gather*}
T(n)=2T\left(\frac{n}{2}\right)+n \Rightarrow O(n \log n)
\end{gather*}

Im schlechtesten Fall, nämlich bei bereits sortierten Folgen, ist aber \(T(n) \in \Omega(n^2)\) 

\begin{satz}
\textsc{Quicksort} benötigt bei gleichwahrscheinlichen Eingabefolgen im Mittel etwa $1,38 n \log n$ Vergleiche.
\end{satz}

\begin{beweis}
$n=1$:
\begin{gather*}
T(1)=T(0)=0
\end{gather*}
$n \geq 2:$

\begin{gather*}
T(n)=(n+1)+ \frac{1}{n}\sum_{1\leq k\leq n}[T(k-1)+ T(n-k)]= (n+1)+ \frac{2}{n} \sum_{1\leq k\leq n}T(k-1)
\end{gather*}

Zur Lösung dieser Rekurrenz wird zuerst die Summe beseitigt, dies sollte jeder selbst nachvollziehen.
Die ausführliche Rechnung steht im Anhang \ref{quicksort}. Es ergibt sich \(\frac{T(n)}{n+1}= \frac{T(n-1)}{n}+ \frac{2}{n+1}\) und
Einsetzen der Rekurrenz führt zu:

%\begin{eqnarray*}
%n\, T(n)& = & (n-1)\ T(n-1)-n(n+1)-(n-1)n+2T(n-1)\\
%& = & (n-1)\ T(n-1)+2T(n-1)+n((n+1)-(n-1))\\
%& = & (n+1)\ T(n-1)+2n\\
%\end{eqnarray*}

\begin{gather*}
\frac{T(n)}{n+1}= \frac{T(n-1)}{n}+ \frac{2}{n+1}=\frac{T(n-2)}{n-1}+\frac{2}{n}+\frac{2}{n+1}=\ldots
\end{gather*}

\begin{gather*}
\ldots= \frac{T(2)}{3}+ \sum_{3\leq k \leq n}\frac{2}{k+1} \approx 2\sum_{k=1}^{n}\frac{1}{k}\approx 2
\int_{1}^{n}\frac{1}{x} dx =2 \ln n
\end{gather*}

\begin{gather*}
T(n)=2n\, \ln n \approx 1,38 n \, \log n
\end{gather*}
\end{beweis}
\hfill q.e.d.
%
% Susanne Gernandt
 \section{Auswählen (Sortieren)}
 Wie in vorangegangenen Vorlesungen besprochen wurde, braucht \textsc{QUICKSORT} bestenfalls $O(n \log n)$ Zeit und im
 worst case, wenn beim "`Teile und Herrsche"' - Verfahren die Länge der beiden Teilfolgen sehr unterschiedlich ist, $O(n^{2})$ Zeit.

 Um diese benötigte Zeit zu verringern, versuchen wir nun, einen Algorithmus zu finden, mit dem wir den worst case ausschließen
 können.

 Die Idee hierbei ist, ein Element zu finden, daß in der sortierten Folge ungefähr in der Mitte stehen wird und diesen sogenannten
 "`Median"' als Pivot-Element für das "`Teile und Herrsche"' - Verfahren bei \textsc{QUICKSORT} zu verwenden.

 Wie kompliziert ist es nun, diesen Meridian zu ermitteln? Dazu ist zuerst zu sagen, daß bei einer geraden Anzahl von Elementen zwei
 Elemente als Meridian in Frage kommen. Hierbei ist es allerdings egal, ob man sich für das kleinere oder für das größere Element
 entscheidet.

\begin{definition}
 Sei eine Folge $A=(a_{1}, ... , a_{n})$ gegeben, wobei alle $a_{i}$ die Elemente einer linear geordneten Menge sind. 
 Dann ist der Rang eines Elements $a_i$ im Feld $A$ definiert als $Rang(a_{i}:A):=\left|\left\{x|x \in A:x\leq a_{i}\right\}\right|$.
\end{definition}

 Sei $A=(9,-5,2,-7,6,0,1)$, dann ist $Rang(1:A):=4$ (4 Elemente von $A$ sind $\leq 1$)
\bigskip

Sei nun $A$ sortiert, also $A_{sortiert}=(a_{\pi(1)}, ... , a_{\pi(n)})$, dann gilt für das Element $c$ mit $Rang(c:A)=k$ für $1\leq k
\leq n$, daß $c=a_{\pi(k)}$, das heißt:
$a_{\pi(1)}\leq ... \leq a_{\pi(k)}\leq ... \leq a_{\pi(n)}$.
Die ursprüngliche Reihenfolge paarweise gleicher Elemente wird hierbei beibehalten. Im weiteren wird
eine Kurzschreibweise für den Rang verwendet, $a_{(k)}$ ist das Element mit dem Rang k. Ein Feld $A$ mit $n$ Elementen wird kurz mit $A^n$
bezeichnet.

\begin{definition}
Der Median von $A$ ist demzufolge:

$a_{\left(\left\lfloor \frac{n}{2}\right\rfloor\right)}$ (Element vom Rang $\left\lfloor \frac{n}{2}\right\rfloor$ bei n Elementen)
\end{definition}

Hierbei kann allerdings, wie oben schon erwähnt, auch $a_{\left(\left\lceil \frac{n}{2}\right\rceil\right)}$, also die nächstgrößere
ganze Zahl, als Rang festgelegt werden.

Unter Selektion verstehen wir eine Vorbehandlung, die als Eingabe 
$A:=(a_{1}, ... , a_{n})$ erhält und unter der Bedingung $1\leq k\leq n$ als Ausgabe das Pivot-Element $a_{(k)}$ liefert.

 Dieser Algorithmus zur Selektion (brute force) besteht nun aus folgenden zwei Schritten:
 \begin{enumerate}
	\item SORT $A$: $a_{\pi(1)}, a_{\pi(2)}, ... , a_{\pi(n)}$ (braucht $O(n \log n)$ Zeit)
	\item Ausgabe des k-ten Elementes
 \end{enumerate}
 Für die Selektion wird die "`Median-der-Mediane-Technik"' verwendet. 

 \subsection{Algorithmus \texorpdfstring{$SELECT(A^{n},k)$}{SELECT (A\textsuperscript{n},k)}:}
 \addcontentsline{alg}{Algorithmus}{\textsc{Selektion}($A^n,\, k$)}
 Wähle beliebige feste Ganzzahl $Q$ (z.B. 5)
 \begin{description}
  \item[Schritt 1:] 
  
  If $\left|A\right|\leq Q$ Then sortiere $A$ \hfill(z.B. mit Bubblesort)

  \hspace{2cm}Ausgabe des k-ten Elementes \hfill(da Anzahl konstant: $O(1)$)

  \hspace{1cm}Else Zerlege $A$ in $\frac{\left|A\right|}{Q}$ Teilfolgen der maximalen Länge Q
  \item[Schritt 2:] Sortiere jede Teilfolge und bestimme den Median $m_{i}$ \hfill(dauert $O(n)$ Zeit)
  \item[Schritt 3:] $SELECT(\{m_{1}, m_{2}, ... , m_{\frac{\left|A\right|}{Q}}\},\frac{\left|A\right|}{2Q})$, Ausgabe m
  \item[Schritt 4:] Definition von drei Mengen:

  $A_{1}:=\left\{x\in A|x<m\right\}$

  $A_{2}:=\left\{x\in A|x=m\right\}$

  $A_{3}:=\left\{x\in A|x>m\right\}$
  \item[Schritt 5:] If $\left|A_{1}\right|\geq k$ Then $SELECT (A_{1},k)$

  \hspace{1cm}Elseif $\left|A_{1}\right|+\left|A_{2}\right|\geq k$ Then Output m

  \hspace{1cm}Else $SELECT (A_{3},k-(\left|A_{1}\right|+\left|A_{2}\right|))$
 \end{description}
 
  \subsubsection{Zeitanalyse:}
 \hspace{4mm}zu Schritt 1: $max\{O(1),O(n)\}$

 zu Schritt 2: $O(1)$ für jedes Sortieren der $O(n)$ Teilfolgen

 zu Schritt 3: $T(\frac{n}{Q})$

 zu Schritt 4: $O(n)$

 zu Schritt 5: $T(\frac{n}{Q})$
 \bigskip
 
Seien die Mediane $m_{j}$ aller Teilfolgen sortiert. Dann ist $m$, der Median der Mediane, der Median dieser Folge. 
Wieviele Elemente aller Teilfolgen sind größer oder gleich $m$?

 $\frac{\left|A\right|}{2Q}$ Mediane der Teilfolgen sind größer oder gleich $m$ und für jeden dieser $\frac{\left|A\right|}{2Q}$
 Mediane sind $\frac{Q}{2}$ Elemente "`seiner"' Teilfolge größer oder gleich $m$. Damit sind mindestens
 $\frac{\left|A\right|}{2Q}\cdot \frac{Q}{2}=\frac{\left|A\right|}{4}$ Elemente größergleich $m$.
	
 $\Rightarrow\left|A_{1}\right|\leq\frac{3}{4}\left|A\right|\Rightarrow T(n)=O(n)+T(\frac{n}{Q})+T(\frac{3}{4}n)$
und $T(n)=O(n)\Longleftrightarrow\frac{n}{Q}+\frac{3}{4}n<n$

 Dies trifft für $Q\geq 5$ zu. Damit hat \textsc{Selection}($A^n,k$) die Komplexität O(n).

 \subsection{Algorithmus \textsc{S-Quicksort(A)}}

\begin{Algorithmus}[h]
\addcontentsline{alg}{Algorithmus}{\textsc{S-Quicksort}(A)}
%\lstset{emph={S-Quicksort, Select}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, S-Quicksort, Select}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{S-Quicksort}, gobble=1]{S-Quicksort}
 If $\left|A\right|=2$ und $a_{1}<a_{2}$ Then Tausche $a_{1}$ und $a_{2}$
 Elseif $\left|A\right|>2$ Then
   Select $(A,\frac{\left|A\right|}{2})\longrightarrow m$
   $A_{1}:=\left\{x\leq m, x\in A,\ \textup{sodaß} \left|A_{1}\right|=\left\lceil\frac{\left|A\right|}{2}\right\rceil\right\}$
   $A_{2}:=\left\{x\geq m, x\in A,\ \textup{sodaß} \left|A_{2}\right|=\left\lfloor \frac{\left|A\right|}{2}\right\rfloor\right\}$
   S-Quicksort$(A_{1})$
   S-Quicksort$(A_{2})$
 End
 \end{lstlisting}
  \end{Algorithmus}

Der worst case wird durch die Bestimmung des Medians ausgeschlossen, die die Komplexität O(n) hat. Damit gilt die Rekurrenz
$T(n)=2T(\frac{n}{2})+O(n)$ und der Algorithmus funktioniert immer in $O(n \log n)$ Zeit.
 
\section{\textsc{Heapsort}}
Abstrakte Datentypen wurden bereits auf Seite \pageref{ADT} definiert, ebenso der ADT \textbf{Dictionary}. Nun wird ein weiterer
Datentyp, der \textbf{Heap} vorgestellt. Der \textbf{Heap} wird in der Literatur oft auch mit \textbf{Priority Queue} bezeichnet.
Er findet z.B. bei der Verwaltung von Druckaufträgen Anwendung.

\begin{definition}[Heap]
Der abstrakte Datentyp, der das Quintupel der Operationen
\textsc{MakeHeap}, \textsc{Insert}, \textsc{Max} und \textsc{ExtractMax} unterstützt heißt \textbf{Heap} (oft auch \textbf{Priority Queue} genannt).
 \end{definition}

Hierbei handelt es sich um einen sogenannten Max-Heap. Analog kann ein Min-Heap gebildet werden, indem statt des Maximums immer das
Minimum genommen wird (bzw. statt $\geq$ immer $\leq$ im Algorithmus). 
% 
% 10.11.03 Reana Sommerkorn 
%
\begin{definition}[In place]
Ein Verfahren heißt \textbf{In place}, wenn es zur Bearbeitung der Eingabe unabhängig von der Eingabegröße nur zusätzlichen
Speicherplatz konstanter Größe benötigt.
\end{definition}

Auf der Basis der genannten und später erläuterten Operationen mit Heaps kann ein effizientes Sortierverfahren namens
\textsc{Heapsort} definiert werden, das in place funktioniert.

Das wird so erreicht, daß ein direkter Zugriff auf das Feld besteht und das Sortieren an Ort und Stelle und ohne Verwendung weiteren
Speicherplatzes vorgenommen werden kann. Des weiteren wird garantiert, dass n Elemente in O($n \log n$) Schritten sortiert werden,
unabhängig von den Eingabedaten.

Die Vorteile von \textsc{MergeSort} (O($n \log n$)) und \textsc{InsertionSort} (In place) werden also vereint.

\begin{definition}[Binärer Heap]
Ein \textit{Binärer Max-Heap} ist ein spezieller Binärbaum (wird im Folgenden nochmals definiert) mit den Eigenschaften, daß der
Wert jedes Knotens jeweils größergleich den Werten seiner Söhne ist und daß außer dem Level mit der Höhe 0 alle Level voll besetzt
sein müssen. Jeder Level wird von links beginnend aufgefüllt. Hat also ein Blatt eines Heaps die Höhe 1 im gesamten Heap, so haben
auch alle rechts davon stehenden Blätter genau dieselbe Höhe.

Dies ist äquivalent dazu, daß für eine Folge $F=k_{1},\,k_{2},\,...,\,k_{n}$ von Schlüsseln 
für alle i mit $2\leq i\leq n$ die Beziehung $k_{i}\leq k_{\left\lfloor \frac{i}{2}\right\rfloor}$ gilt (Heap-Eigenschaft), wobei kein
Eintrag im Feld undefiniert sein darf.
\end{definition}

Wegen der Speicherung in einem Array werden die Söhne auch als Nachfolger bezeichnet.
Wird ein binärer Heap in einem Array gespeichert, so wird die Wurzel des Baums an Position 1 im Array gespeichert. Die beiden
Söhne eines Knotens an der Arrayposition i werden an den Positionen 2i und 2i+1 gespeichert.
Und mit der Heap-Eigenschaft gilt $k_{i}\geq k_{2i}\,\textup{und}\, k_{i}\geq k_{2i+1}$ für alle $i$ mit $2i<n$  

Anschaulicher ist vielleicht die Vorstellung als Binärbaum
\begin{figure}[H]
	\centering\input{101103a.latex}
	\caption{Binärer Max-Heap}
	\label{101103a}
\end{figure}
Das korrespondierende Array wäre A=(8,7,5,3,1).
\bigskip

Um auf den Vater, den linken oder den rechten Sohn eines Knotens i zuzugreifen, genügen die folgenden einfachen Berechnungen:

\begin{tabular}{l|l}
Ziel & Berechnung\\
\hline
Vater(i) & $\left\lfloor\frac{i}{2}\right\rfloor$\\
LSohn(i) & $2i$\\
RSohn(i) & $2i+1$\\
\end{tabular}

Für die folgenden überlegungen sind noch weitere Definitionen nützlich.

\begin{definition}[Graphentheoretische Definition eines Binärbaumes]
Ein \textbf{Binärbaum} ist ein Baum, bei dem jeder Knoten vom Grad höchstens 3 ist. Ein Knoten mit
höchstens Grad 2 kann dabei als Wurzel fungieren. Ein solcher Knoten existiert immer (im Extremfall ist er ein Blatt, hat also den Grad
1). 
\end{definition}

\begin{definition}[Ein vollständiger Binärbaum]
Ein \textbf{vollständiger Binärbaum} hat zusätzlich die Eigenschaften, daß genau ein Knoten den Grad 2 besitzt und alle Blätter die
gleiche Höhe haben. In diesem Fall wird immer der Knoten vom Grad 2 als Wurzel bezeichnet.
\end{definition}

\begin{satz}
In einem vollständigen (im strengen Sinne) Binärbaum der Höhe h gibt es $2^{h}$ Blätter und $2^h-1$ innere Knoten.
\end{satz}
Der Beweis ist mittels vollständiger Induktion über h möglich.

\begin{satz}
Der linke Teilbaum eines Binärheaps mit n Knoten hat maximal $\frac{2\textup{n}}{3}$ Knoten.
\end{satz}

Beweisidee: Berechne erst wieviele Knoten der rechte Teilbaum hat. Dann benutze dies um die Knotenanzahl des linken Teilbaumes zu
berechnen. Rechne dann das Verhältnis der Knotenanzahlen zueinander aus.
\begin{beweis}
Da der Grenzfall von Interesse ist, wird von einem möglichst asymmetrischen Heap ausgegangen. Sei also der linke Teilbaum voll besetzt
und habe der rechte genau einen kompletten Höhenlevel weniger. Noch mehr Disbalance ist aufgrund der Heap-Eigenschaft nicht möglich.
Dann ist der rechte Teilbaum ebenfalls voll besetzt, hat allerdings einen Level weniger als der linke.

Sei also $i$ die Wurzel eines solchen Baumes mit der Höhe l und $j$ der linke Sohn von $i$. Dann ist $j$ Wurzel des linken Teilbaumes. 
Nun bezeichne $K(v$) die Anzahl der Knoten im Baum mit der Wurzel $v$. Dann soll also gelten 
\(\frac{K(j)}{K(i)} \leq \frac{2}{3}\).
Nach Voraussetzung gilt $K(j)=2^l-1$ und $K(i)=2^l-1+2^{l-1}-1+1=3 \cdot 2^{l-1}-1$, es folgt
\[\frac{K(j)}{K(i)}= \frac{2^l-1}{3\cdot 2^{l-1}-1} \leq \frac{2^l}{3 \cdot 2^{l-1}}= \frac{2}{3}\]

\hfill q.e.d.
\end{beweis}

\begin{satz}
In einem n-elementigen Binärheap gibt es höchstens $\left\lceil \frac{n}{2^{h+1}}\right\rceil$ Knoten der Höhe h.
\end{satz}

\begin{definition}[Höhe eines Baumes]
Die Höhe eines Knoten $\vartheta$ ist die maximale Länge des Abwärtsweges von $\vartheta$ zu einem beliebigen Blatt (also die Anzahl
der Kanten auf dem Weg).
\end{definition}

Beweisidee:
Die Knoten der Höhe 0 sind die Blätter. Dann wird von unten beginnend zur Wurzel hochgelaufen und dabei der
Exponent des Nenners wird immer um eins erhöht
\begin{figure}[H]
	\centering\input{101103b.latex}
	\caption{Binärer Min-Heap}
	\label{101103b}
\end{figure}

Aus der Heap-Eigenschaft folgt, daß das Maximum in der Wurzel steht. Sei nun also F=(8, 6, 7, 3, 4, 5, 2, 1) gegeben. Handelt es sich
dabei um einen Heap?

Ja, da $F_{i}\geq F_{2i}$ und $F_{i}\geq F_{2i+1}$, für alle $i$ mit $5\geq i \geq 1$, da $8\geq 6 \wedge 8\geq 7 \wedge6\geq 3 \wedge 
6\geq 4 \wedge 7\geq 5 \wedge 7\geq 2 \wedge 3\geq 1$.

Dieser Max-Heap sieht dann grafisch wie folgt aus:

\begin{figure}[H]
	\centering\input{101103c.latex}
%	\caption{Binärer Max-Heap}
	\label{101103c}
\end{figure}
\vspace{3mm}

Sei nun eine Folge von Schlüsseln als Max-Heap gegeben und die Ausgabe sortiert in absteigender Reihenfolge gewünscht; für einen
Min-Heap müssen die Relative "`kleiner"' und "`größer"' ausgetauscht werden. Um die Erläuterung einfacher zu halten, wird von dem
größeren Sohn gesprochen, nicht von dem Knoten mit dem größeren gespeicherten Wert. Genauso werden Knoten und nicht Werte vertauscht.
Dies ist allerdings formal falsch!

Für den ersten Wert ist dies einfach, da das Maximum bereits in der Wurzel steht.
Dies läßt sich ausnutzen, indem der erste Wert in die Ausgabe aufgenommen wird und aus dem Heap entfernt wird, danach wird aus den
verbleibenden Elementen wieder ein Heap erzeugt. Dies wird solange wiederholt, bis der Heap leer ist. Da in der Wurzel immer das Maximum
des aktuellen Heaps gespeichert ist, tauchen dort die Werte der Größe nach geordnet auf.

Der neue Heap wird durch Pflücken und Versickern des Elements mit dem größten Index erreicht. Dazu wird die Wurzel des anfänglichen Heaps geleert. Das Element mit dem größten Index wird aus dem Heap gelöscht
(gepflückt) und in die Wurzel eingesetzt.
Nun werden immer die beiden Söhne des Knotens verglichen, in dem der gepflückte Wert steht. Der größere der beiden Söhne wird mit
dem Knoten, in dem der gepflückte Wert steht, vertauscht.

Der gepflückte Wert sickert allmählich durch alle Level des Heaps, bis die
Heap-Eigenschaft wieder hergestellt ist und wir einen Heap mit n-1 Elementen erhalten haben.

Im obigen Beispiel heißt das also:
Wenn man nun die Wurzel (hier: 8) wieder entfernt, wandert die 1 nach oben und in diesem Moment ist es kein Binärheap. D.h. es muß
ein neuer Heap erzeugt werden und dies geschieht unter zu Zuhilfenahme von Heapify (Versickern).
In der grafischen Darstellung wurde die Position im Array rechts neben die Knoten geschrieben:
\begin{figure}[H]
	\centering\input{101103d.latex} \hspace{2cm}\input{101103e.latex}
%	\caption{Binärer Max-Heap}
	\label{101103de}
\end{figure}
\vspace{3mm}
\noindent
$\rightarrow$ entnehme die Wurzel \hspace{5cm}
$\rightarrow$ setze 1 an die Wurzel

\begin{figure}[H]
	\centering\input{101103f.latex}\hspace{6mm}\input{101103g.latex}\hspace{6mm}\input{101103h.latex}
%	\caption{Binärer Min-Heap}
	\label{101103f}
\end{figure}
$\rightarrow$ Heapify (Versickern) der "`1"' 

Als Algorithmus:
  \begin{Algorithmus}[h]
  \addcontentsline{alg}{Algorithmus}{\textsc{Heapify}(A, i)}
%  \lstset{emph={Heapify}}
  \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
  \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Heapify\textnormal{(A)}}, gobble=4]{Heapify}
    l:= LSohn(i)
    r:= RSohn(i)
    if l <= Heapsize[A] und A[l]=Succ(A[i])
      then Max:= l
      else Max:= i
    if r <= Heapsize[A] und A[r]=Succ(A[Max])
      then Max:= r
    if Max != i
      then tausche A[i] und A[Max]
        Heapify(A,Max) 
    \end{lstlisting}
  \end{Algorithmus}

\textbf{INPUT:} F bzw. A in einen Heap überführen
\begin{Algorithmus}[h]
\addcontentsline{alg}{Algorithmus}{\textsc{Build-Heap\textnormal{(A)}}}
%\lstset{emph={Heapify}}
  \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Build-Heap\textnormal{(A)}}, gobble=2]{Build-Heap}  
   Heapsize[A]:= L~\ttfamilyä~nge[A]
   for i:= $\left\lfloor \texttt{Länge(}\frac{A}{2}\texttt{)} \right\rfloor$ down to 1
      Heapify(A,i)
    \end{lstlisting}
  \end{Algorithmus}

  \begin{Algorithmus}[h]
  \addcontentsline{alg}{Algorithmus}{\textsc{Heapsort}(A)}
%  \lstset{emph={Build-Heap, Heapify}}
  \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, Heapify,
  Build-Heap}, emphstyle=\textsc, escapeinside=~~}  
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Heapsort\textnormal{(A)}}, gobble=4]{Heapsort}
    Build-Heap(A)
    for i:= Laenge[A] down to 2
      do tausche A[1] und A[i]
        Heapsize[A]:= Heapsize[A]-1
        Heapify(A,1)
    \end{lstlisting}
  \end{Algorithmus}
  
Komplexität der einzelnen Algorithmen

Für \textsc{Heapify} gilt die Rekurrenz T(n)=T($2/3$n)+O(1), damit gilt T(n) $\in$ O($\log n$)

Für \textsc{Build-Heap} ist dies etwas komplizierter Sei $h$ die Höhe eines Knotens und $c$ eine Konstante größer 0, dann gilt:
 
\[\sum_{h=0}^{\left\lfloor(\log n)\right\rfloor}\left\lceil\frac{n}{2^{h+1}}\right\rceil \cdot ch \in O\left(n\cdot\sum_{h=0}^{\lfloor(\log
n)\rfloor}
\frac{n}{2^{h+1}}\right) \in O\left(n\cdot\underbrace{\sum_{h=0}^{\infty}
\frac{h}{2^{h+1}}}_{=2}\right)=O(n)\] 
Damit kostet \textsc{Build-Heap} nur O($n$).

Damit hat \textsc{Heapsort} die Komplexität O($n \log n$). In jedem der O($n$) Aufrufe von \textsc{Build-Heap}  braucht
\textsc{Heapify} O($\log n$) Zeit.
%
% 12.11.03 Kay Schieck
\subsection{Priority Queues}
In der Literatur wird die Begriffe Heap und Priority Queue (Prioritätswarteschlange) oftmals synonym benutzt. Hier wird begrifflich
etwas unterschieden und Heaps werden für die Implementierung von solchen Warteschlangen benutzt. Auch die Bezeichnungen für
\textsc{BUILDHEAP} ist nicht einheitlich, in einigen Büchern wird stattdessen \textsc{MAKEHEAP} oder \textsc{MAKE} verwendet.

\begin{definition}
Der abstrakte Datentyp, der die Operationen \textsc{Make-Heap, Insert, Max, ExtractMax} und \textsc{Increase Key} unterstützt wird
\textbf{Priority
Queue} genannt.
\end{definition}

Wie bei einem Heap kann natürlich auch hier immer mit dem Minimum gearbeitet werden, die Operationen wären dann \textsc{Buildheap,
Insert, Min, ExtractMin} und \textsc{Decrease Key}

Behauptung: Binary Heaps unterstützen Warteschlangen.

\begin{Algorithmus}[h]
\addcontentsline{alg}{Algorithmus}{\textsc{ExtractMax}(A)}
%\lstset{emph={Heapify}}
  \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{ExtractMax\textnormal{(A)}}, gobble=4]{ExtractMax(A)}
    if heap-size[A] < 1
         then Fehler
    Max := A[1]
    A[1] := A[heap-size[A]]
    heap-size[A] := heap-size[A]-1
    Heapify(A, 1)
    Ausgabe Max
\end{lstlisting}

Der Heap wird mittels A an Extractmax übergeben. Diese Funktion merkt sich die Wurzel (das Element mit dem größten Schlüssel).
Dann nimmt es das letzte im Heap gespeicherte Blatt und setzt es als die Wurzel ein. Mit dem Aufruf von Heapify() wird die
Heap-Eigenschaft wieder hergestellt. Das gemerkte Wurzel wird nun ausgegeben.
Falls die Anzahl der Elemente in A (heap-size) kleiner als 1 ist, wird eine Fehlermeldung ausgelöst.
\end{Algorithmus}

Increasekey sorgt dafür, das nach änderungen die Heapbedingung wieder gilt. Es vertauscht solange ein Element mit dem Vater, bis
das Element kleiner ist.

\begin{Algorithmus}[h]
\addcontentsline{alg}{Algorithmus}{\textsc{IncreaseKey}(A, x, k)}
%\lstset{emph={Add}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Add, MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{IncreaseKey\textnormal{(A, x, k)}}, gobble=4]{IncreaseKey(A, x, k)}
     if key[x] > k
         then Fehler älter Schl~\ttfamily{ü}~sselwert ist gr~\ttfamily{öß}~er"    
     key[x] := k
     y := x
     z := p[y]
     while z $\not=$ NIL und key[y] > key[z]
         do tausche key[y] mit key[z]
         y := z
         z := p[y]
\end{lstlisting}

Die übergebene Variable x ist der Index im Array und k ist der neue Schlüsselwert. Der Vater des Knotens x in
der Baumstruktur wird mit p[x] bezeichnet.
\end{Algorithmus}
\textsc{IncreaseKey} kostet $O(\log n)$, dazu siehe auch Skizze~\ref{121103a}.
Damit wird \textsc{IncreaseKey} unterstützt und wir wenden uns der Operation \textsc{Insert} zu.
Dabei wandert key solange nach oben, bis die Heap-Eigenschaft wieder gilt. Damit kostet auch \textsc{Insert} $O(\log n)$.

\begin{figure}[H]
  \centering\input{121103a.latex}
  \caption{\textsc{Heapify}}
  \label{121103a}
\end{figure}
Ist der im Vaterknoten gespeicherte Wert größer als der im Sohn gespeicherte, vertauscht  \textsc{Increasekey} die beiden Werte.

\begin{figure}[H]
  \centering\input{121103b.latex}
  \caption{Insert}
  \label{121103b}
\end{figure}

Damit unterstützen binäre Heaps:
\begin{itemize}
\item \textsc{BuildHeap} $O(1)$ - (im Sinne von \textsc{MakeHeap} = Schaffen der leeren Struktur)
\item \textsc{Insert} $O(log\ n)$
\item \textsc{Max} $O(1)$
\item \textsc{ExtractMax} $O(log\ n)$
\item \textsc{IncreaseKey} $O(log\ n)$
\end{itemize}
Somit ist die Behauptung erfüllt.

Ein interesantes Anwendungsbeispiel ist, alle $\leq$ durch $\geq$ zu ersetzen. Also \textsc{Max} durch \textsc{Min},
\textsc{ExtractMax} durch \textsc{ExtractMin} und \textsc{IncreaseKey} durch \textsc{Decreasekey} zu ersetzen und nur \textsc{Insert}
zu belassen.

\section{\textsc{Dijkstra}}

 Problem:
\begin{figure}[H]
  \centering\input{121103c.latex}
  \caption{Kürzesten Weg finden}
  \label{121103c}
\end{figure}

Sehr wichtig für die Informatik sind Graphen und darauf basierende Algorithmen. Viele anscheinend einfache Fragestellungen erfordern
recht komplexe Algorithmen.

So sei z.B. ein unwegsames Gelände mit Hindernissen gegeben und der kürzeste Weg dadurch herauszufinden. Für die algorithmische
Fragestellung ist es völlig egal, ob es sich um ein Gelände oder eine andere Fläche handelt. Deswegen wird soweit abstrahiert, daß
aus der Fläche und den Hindernissen Polygone werden. Doch unverändert lautet die Fragestellung, wie man hier den kürzesten Weg
finden kann. Es ist zu erkennen, das dies nur über die Eckpunkte (von Eckpunkt zu Eckpunkt)
zu bewerkstelligen ist. Dabei darf natürlich nicht der zugrunde liegende Graph verlassen werden. Ist eine aus Strecken zusammengesetzte
Linie der kürzeste Weg?

\begin{definition}[Visibility Graph]
M = Menge der Ecken = Menge der Polygonecken.
$a, b \in M \rightarrow \overline{ab}$ ist Kante des Graphen $\leftrightarrow \overline{ab}$ ganz innerhalb des Polygons liegt.
\end{definition}

\begin{satz}
Der kürzeste Pfad verläuft entlang der Kanten des Sichtbarkeitsgraphen (Lorenzo-Parez).
\end{satz}
Problem \textsc{all-to-one shortest paths}

One (In Abbildung~\ref{121103d} ist das der Punkt a) ist der Startpunkt und von allen anderen
Punkten wird der kürzeste Weg dahin berechnet.
Die Gewichte an den Kanten sind dabei immmer $\geq 0$.

\begin{figure}[h]
  \centering\input{121103d.latex}
  \caption{Graph mit gerichteten Kanten}
  \label{121103d}
\end{figure}

Zur Lösung des Problems verwenden wir den Algorithmus von Dijkstra (1959).

\begin{description}
\item[Paradigma: ] Es ist ein Greedy (gieriger) Algorithmus.
(einfach formuliert: ich denk nicht groß nach, ich nehm einfach den kürzesten Weg um von Punkt a weg zu kommen)
\item[Start: ] $V$ ist die Menge der Knoten, $W$ ist die Menge der erreichten Knoten.
\end{description}

\begin{figure}[H]
  \centering\input{121103e.latex}
  \caption{Menge W wird aufgeblasen}
  \label{121103e}
\end{figure}
Nun zum eigentlichen Algorithmus:

\begin{Algorithmus}[h]
\addcontentsline{alg}{Algorithmus}{\textsc{Dijkstra}}
%\lstset{emph={ExtractMin, DecreaseKey}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={ExtractMin,
DecreaseKey, MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Dijkstra}, gobble=1]{Dijkstra}
 for all v in V do d(v) := + $\infty$
 d(s) := 0; W := $\emptyset$
 Initalisiere Struktur V (mit d(v), v in V)
 while V \ W $\not= \emptyset$ do
   v := Min(V \ W); ExtractMin(V \ W);
   W := W$\cup${v}
   for all w in Succ(v) \ W do
     if d(r) + l(vw) < d(w)
       then DecreaseKey(w, d(v) + l(vw))
\end{lstlisting}

Der Nachweis der Korrektheit dieses Algorithmus ist sehr schwer und soll nicht Gegenstand
dieses Skriptes sein.
\end{Algorithmus}   % noch nicht fertig

Ein einfaches Beispiel soll seine Funktion veranschaulichen. Die verwendeten Knoten und Kanten entsprechen denen
aus Abbildung~\ref{121103d}. Jede Zeile der Tabelle entspricht einer Ausführung des Rumpfes der while Schleife (ab Zeile 5)
und zeigt die Werte, welche die verschiedenen Variablen annehmen. Aus Gründen der übersicht werden nur 3 Spalten
augeführt, der / tritt deshalb nochmal als Trennhilfe auf:

\centering
\begin{tabular}{|l|l|l|}\hline
$(v,d(v)):v \in W$ /		& $v = min(V \setminus W)$ /	& $v=min(V \setminus W),$ \\
$ (v,d(v)): v \in V\cup W$	& $SUCC(v)$		& $w \in SUCC(v)\setminus (W\cup {v}),$ \\
				&			& $(w,l(\vec{vw})$ / \\
				&			& $min(d(w),d(v)+l(\vec{vw}))$ \\ \hline

$\emptyset$ /			& $a$ /			& $v=a:(c,13),(b,7)$ / \\
$(a,0),(b,\infty),(c,\infty),$	& $c,b$			& $(c,13),(b,7)$ \\
$(d,\infty),(e,\infty)$		& 			& \\ \hline

$\{(a,0)\}$ /			& $b$ /			& $v=b:w\in \{c,d,e\}\setminus \{a,b\},$ \\
$(b,7),(c,13),(d,\infty),(e,\infty)$ & $c,d,e$		& $(c,5),(d,12),(e,4)$ / \\
				&			& $(c,12),(d,19),(e,11)$ \\ \hline

$(a,0),(b,7)$ /			& $e$ /			& $v=e, w\in\{a,d\}\setminus \{a,b,e\}$ \\
$(e,11),(c,12),(d,19)$		&			& $=\{d\}, (d,7)$ / \\
				& $a,d$			& $min(19,18):(d,18)$ \\ \hline

$(a,0),(b,7),(e,11)$ /		& $c$ /			& \\
$(c,12),(d,18)$			& $b,d$			& \\
nächste Zeile $(d,13)$		&			& \\ \hline
\end{tabular}


Nun interessiert uns natürlich die Komplexität des Algorithmus. Dazu wird die jeweilige Rechenzeit
der einzelnen Zeilen betrachtet, wobei $|V|=n$ und $|{Knoten}| = m$.
\begin{enumerate}
\item $O(|V|) = O(n)$
\item $O(n)$
\item meist in $O(1)$
\item nicht zu beantworten
\item nicht zu beantworten
\item $O(1)$ im Regelfall (es kann auch komplexer sein, da es darauf ankommt, wie $W$ verwaltet wird)
\item
\item $O(1)$
\item die Komplexität von DECREASEKEY ist auch nicht zu beantworten
\end{enumerate}

Die Laufzeit hängt wesentlich davon ab, wie die Priority Queue verwaltet wird.
\begin{enumerate}
\item Möglichkeit: $V$ wird in einem Array organisiert \hfill $\rightarrow O(n^{2})$
\item Möglichkeit: Binärer Heap für V$\backslash$W, dann geht EXTRACTMIN in $O(n\ log\ n)$ und DECREASEKEY in $O(m\ log\ n)$ \hfill $\rightarrow O((m+n)\ log\ n)$
\item Möglichkeit: V in einen Fibonacci-Heap \hfill $\rightarrow O((n \log n)+m)$
\end{enumerate} 
%
% 17.11.03 Alexander Hofmeister
%
  \section{\textsc{Counting Sort}}
  Einen ganz anderen Weg zum Sortieren von Zahlen beschreitet \textsc{Counting Sort}.
  Es funktioniert, unter den richtigen Bedingungen angewendet, schneller als in O($n \log n$) und basiert nicht auf
  Schlüsselvergleichen.

  \begin{Algorithmus}[H]
  \addcontentsline{alg}{Algorithmus}{\textsc{Counting Sort}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={ExtractMin,
DecreaseKey, MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}  
    \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Counting Sort}, gobble=4]{Counting Sort}
    for i := 0 to k do
     C[i]:= 0;
     for j := 1 to Laenge[A] do
       C[A[j]] := C[A[j]]+1;
     for i := 1 to k do
       C[i] := C[i] + C[i-1];
     for j := Laenge[A] downto 1 do
       B[C[A[j]]] := A[j];
       C[A[j]] := C[A[j]]-1
    \end{lstlisting}

  Das Sortierverfahren \textsc{Counting Sort}  belegt eventuell auf Grund der Verwendung von zwei zusätzlichen Feldern 
  B und C sehr viel Speicherplatz.
  Es funktioniert ohne die Verwendung von Schlüsselvergleichen. Es wird zu Begin ein Zähler (Feld C) erzeugt 
  dessen Größe abhängig ist von der Anzahl der möglichen in A 
  enthaltenen Zahlen (Zeile 1 und 2). Für jeden möglichen Wert in A, also für jeden Wert des Zahlenraumes, wird eine Zelle des Feldes
  reserviert. Seien die Werte in A z.B. vom Typ Integer mit 16 Bit. Dann gibt es 2$^{16}$ mögliche Werte und das Feld C würde für
  jeden der 65536 möglichen Werte eine Zelle erhalten; dabei wird C[0] dem ersten Wert des Zahlenraums zugeordnet, C[1] dem zweiten
  Wert des Zahlenraumes usw.
  Anschließend wird die Anzahl der in 
  A enthaltenen Elemente schrittweise in C geschrieben. Bei mehrfach 
  vorhandenen Elementen wird der entsprechende Wert erhöht, daher kommt auch 
  der Name Counting Sort (Zeile 3 und 4). Nun werden die Adressen im Feld
  berechnet. Dazu wird die Anzahl eines Elemente mit der Anzahl eines 
  Vorgängerelements addiert um die entsprechende Anzahl im Ausgabefeld frei 
  zu halten (Zeile 5 und 6). Zum Schluss wird die richtige Reihenfolge durch 
  zurücklaufen des Arrays A und der Bestimmung der richtigen Stelle, mit Hilfe 
  von C, in B geschrieben. Bei \textsc{Counting Sort} handelt es sich um ein stabiles Sortierverfahren.
  \end{Algorithmus}
  
  \begin{definition}[Stabile Sortierverfahren]
  Ein Sortierverfahren heißt \textbf{stabil}, falls mehrfach vorhandene Elemente in der Ausgabe in der Reihenfolge auftauchen, in der sie auch
  in der Eingabe stehen.
  \end{definition}
  
  \subsection{\textsc{Counting Sort} an einem Beispiel}

  \begin{description}
    \item[Input:] $A\,=\,(\,1_a\,,\,3\,,\,2_a\,,\,1_b\,,\,2_b\,,\,2_c\,,\,1_c\,)$ und $k\,=\,3$
    \item[Output:] $B\,=\,(\,1_a\,,\,1_b\,,\,1_c\,,\,2_a\,,\,2_b\,,\,2_c\,,\,3\,)$, $C\,=\,(\,0,\,0,\,3,\,6\,)$
    \item[Ablauf:]
    \begin{tabular}[t]{*{3}{c}}
      Zeile & Feld $C$ & Erläuterung\\
      \hline
      nach 1 und 2 & $<\,0,\,0,\,0,\,0\,>$ & Zähler wird erzeugt und 0 gesetzt\\
      nach 3 und 4 & $<\,0,\,3,\,3,\,1>$   & Anzahl der Elemente wird ``gezählt''\\
      nach 5 und 6 & $<\,0,\,3,\,6,\,7>$   & Enthält Elementzahl kleiner gleich i\\
      nach 9       & $<\,0,\,0,\,3,\,6>$   & $A[\,i\,]$ werden in B richtig positioniert\\
    \end{tabular}
    
    
  \end{description}

  \subsection{Komplexität von \textsc{Counting Sort}}
   Die Zeitkomplexität von \textsc{Counting Sort} für einen Input von $A^n$ mit 
   \(k \in \textup{O}(\textup{n})\) ist 
   \\ $T(\,n\,)\, \in \,O(\,n\,)\,\cup\,O(\,k\,)$.
  \begin{satz}
   Falls $k\,\in \,O(\,n\,)$, so funktioniert \textsc{Counting Sort} in O(n).
  \end{satz}

Die Stärke von \textsc{Counting Sort} ist gleichzeitig auch Schwäche. So ist aus dem obigen bereits ersichtlich, daß dieses Verfahren
z.B. zum Sortieren von Fließkommazahlen sehr ungeeignet ist, da dann im Regelfall riesige Zählfelder erzeugt werden, die mit vielen
Nullen besetzt sind, aber trotzdem Bearbeitungszeit (und Speicherplatz!) verschlingen.

\section{Weitere Sortieralgorithmen}
  Außer den hier aufgeführten Sortieralgorithmen sind für uns noch \textsc{Bucket Sort} und
  \textsc{Radix Sort} von Interesse.

  \chapter{Einfache Datenstrukturen: Stapel, Bäume, Suchbäume}
  Bevor mit den einfachen Datenstrukturen begonnen wird,
  noch eine Bemerkung zum Begriff des abtrakten Datentyps (siehe Seite \pageref{ADT}).
  Auch dieser Begriff wird leider nicht immer einheitlich verwendet. Mal wird er wie eingangs definiert oder als Menge von Operationen,
  dann wieder wie in der
  folgenden Definition oder noch abstrakter, wie z.B. in \cite{guting}, wo ein ADT als Signatur vereinigt mit Axiomen für die
  Operationen definiert wird.

  \begin{definition}[ADT]
    Ein Abstrakter Datentyp ist eine (oder mehrere) Menge(n) von Objekten und
    darauf definierten Operationen
  \end{definition}

  \subsubsection{Einige Operationen auf ADT's}
  \begin{description}
  \item Q sei eine dynamische Menge
   
   \begin{tabular}[t]{@{}ll@{}} %{*{2}{l}}
      Operation & Erläuterung \\
      \hline
      \textsc{Init}(Q) & Erzeugt eine leere Menge Q \\
      \textsc{Stack-Empty} & Prüft ob der Stack leer ist \\
      \textsc{Push}(Q,x),\textsc{Insert}(Q,x) & Fügt Element x in Q ein ( am Ende ) \\
      \textsc{Pop}(Q,x),\textsc{Delete}(Q,x) & Entfernt Element x aus Q (das Erste x was auftritt)\\
      \textsc{Pop} & Entfernt letztes Element aus Q\\
      \textsc{Top} & Zeigt oberstes Element an \\
      \textsc{Search}(Q,x) & Sucht El. x in Q (gibt erstes Vorkommende aus)\\
      \textsc{Min}(Q) & Gibt Zeiger auf den kleinsten Schlüsselwert zurück \\
      \textsc{Max}Q) & Gibt Zeiger auf den größten Schlüsselwert zurück \\
      \textsc{Succ}(Q,x) & Gibt Zeiger zurück auf das nächst gr. El. nach x\\
      \textsc{Pred}(Q,x) & Gibt Zeiger zurück auf das nächst kl. El. nach x 
   \end{tabular}
    
  \end{description}

  \section{Binäre Suchbäume}

   \begin{definition}[Binärer Suchbäume]
    Ein binärer Suchbaum ist ein Binärbaum folgender Suchbaumeigenschaft:
    Sei x Knoten des binären Suchbaumes und Ahn vom Knoten y. Falls der Weg von x nach y über den linken Sohn von x erfolgt, ist
    $key[\,y\,]\leq key[\,x\,]$. Andernfalls istt $key[\,y\,]> key[\,x\,]$.
   \end{definition}

   \begin{definition}
   Ein Suchbaum heißt \textbf{Blattsuchbaum}, falls die Elemente der dynamischen Menge im Gegensatz zum normalen Suchbaum nur in den
   Blättern gespeichert werden.
   \end{definition}

  \subsection{Beispiel für einen binären Suchbaum}
  \begin{figure}[h]
  \centering \input{171103a.latex} 
   \caption{Binärer Suchbaum}
   \label{171103a}
  \end{figure}

  \begin{itemize}
   \item Rotes Blatt wird erst durch INSERT hinzugefügt
   \item $Q\,=\,\{\,5,\,2,\,1,\,3,\,8,\,7,\,9\,\}$
   \item \textsc{Search}(Q,4): \\
      Beim Suchen wird der Baum von der Wurzel an durchlaufen und der zu 
      suchende Wert mit dem Wert des Knoten verglichen. Ist der zu suchende 
      Wert kleiner als der Knotenwert wird im linken Teilbaum weitergesucht. 
      Ist er größer wie der Knotenwert wird im rechten Teilbaum 
      weitergesucht. Zurückgegeben wird der zuerst gefundene Wert. Ist das 
      Element nicht im Suchbaum enthalten, wird NIL bei Erreichen eines 
      Blattes zurückgegeben.\\
      Im Beispiel wird der Suchbaum in der Reihenfole 5, 2, 3 durchlaufen 
      und dann auf Grund des Fehlens weiterer Knoten mit der Rückgabe von 
      NIL verlassen.
   \item INSERT(Q,4): \\
      Beim Einfügen wird das einzufügende Element mit dem jeweiligen 
      Element des aktuellen Knotens verglichen. Begonnen wird dabei in der 
      Wurzel. Ist das einzufügende Element größer, wird im Baum nach 
      rechts gegangen, ist es kleiner, nach links. Ist in ein 
      Blatt erreicht, wird dann, die Suchbaumeigenschaft erhaltend, entweder rechts oder 
      links vom Blatt aus eingefügt.\\
      Im Beispiel wird der Baum in der Reihenfolge 5, 2, 3 
      durchlaufen und die 4 dann rechts von der 3 als neues Blatt mit dem Wert 4
      eingefügt.
   \item Nach Einfügen: $Q\,=\,\{\,5,\,2,\,1,\,3,\,4,\,9,\,7,\,8\,\}$
  \end{itemize}

  \subsection{Operationen in binären Suchbäumen}

  \begin{Algorithmus}[H]
    \addcontentsline{alg}{Algorithmus}{\textsc{Tree-Search}}
    %\lstset{emph={Tree-Search}}
    \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={ExtractMin,
    Tree-Search, DecreaseKey, MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
    \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Tree-Search}, gobble=4]{Tree-Search}
    if x=NIL or k=key[x]
       then Ausgabe x
    if k<key[x]
       then Ausgabe Tree-Search(li[x],k)
       else Ausgabe Tree-Search(re[x],k)
    \end{lstlisting}

    Bei \textsc{TREE-SEARCH} wird der Baum von der Wurzel aus durchlaufen. 
    Gesucht wird dabei nach dem Wert k. Dabei ist x der Zeiger, der auf den 
    Wert des aktuellen Knotens zeigt. In den ersten beiden Zeilen 
    wird der Zeiger zurückgegeben wenn ein Blatt erreicht oder der 
    zu suchende Wert gefunden ist (Abbruchbedingung für Rekursion). In 
    Zeile 3 wird der zu suchende Wert mit dem aktuellen Knotenwert 
    verglichen und anschl"ießend in den Zeilen 4 und 5 entsprechend im 
    Baum weitergegangen. Es erfolgt jeweils ein rekursiver Aufruf.\\
    Die Funktion wird beendet wenn der Algorithmus in einem Blatt 
    angekommen ist oder der Suchwert gefunden wurde.
    \end{Algorithmus}
 
 \subsubsection{Traversierung von Bäumen}
   \begin{Algorithmus}[H]
   \addcontentsline{alg}{Algorithmus}{\textsc{Treepostorder}}
   %\lstset{emph={Treepostorder}}
   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search}, emphstyle=\textsc, escapeinside=~~}
   \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Treepostorder\textnormal{(x)}}, gobble=4]{Treepostorder(x)}
     if x $\not=$ NIL
       then Treepostorder(li[x])
            Treepostorder(re[x])
	    Print key[x]
    \end{lstlisting}

   Bei \textsc{Treepostorder} handelt es sich um einen rekursiven Algorithmus.
   Es wird zuerst der linke, dann der rechte Teilbaum und erst zum Schluß die Wurzel durchlaufen.
   \end{Algorithmus}

  \begin{figure}[H]
  \centering \input{171103b.latex} 
   \caption{\textsc{Treepostorder}(x)}
   \label{171103b}
  \end{figure}
  \begin{itemize}
   \item x ist der Zeiger auf dem Knoten
   \item Die Ausgabereihenfolge ist \{1,4,3,2,7,9,8,5\}
  \end{itemize}
 
 \subsubsection{\textsc{Treepreorder}(x)}
   \begin{Algorithmus}[H]   
   \addcontentsline{alg}{Algorithmus}{\textsc{Treepreorder}}
   %\lstset{emph={Treepreorder}}
   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search, Treepreorder}, emphstyle=\textsc, escapeinside=~~}
    \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Treepreorder\textnormal{(x)}}, gobble=4]{Treepreorder(x)}
     if x $\not=$ NIL
        then Print key[x]
             Treepreorder(li[x])
             Treepreorder(re[x])
    \end{lstlisting}

   Beim ebenfalls rekursiven \textsc{Treepreorder} wird bei der Wurzel begonnen, dann wird der 
   linke Teilbaum und anschließend der rechte Teilbaum durchlaufen.
   \end{Algorithmus}
Beispiel für \textsc{Treepreorder}
   
  \begin{figure}[H]
  \centering \input{171103c.latex} 
   \caption{\textsc{Treepreorder }}
   \label{171103c}
  \end{figure}
 \begin{itemize}
  \item x ist der Zeiger auf dem Knoten
  \item Die Ausgabenreihenfolge ist \{5,2,1,3,4,8,7,9\}
 \end{itemize}
 
 \subsubsection{\textsc{Treeinorder}(x)}
   \begin{Algorithmus}[H]	
   \addcontentsline{alg}{Algorithmus}{\textsc{Treeinorder}}
   %\lstset{emph={Treeinorder}}
   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search, Treepreorder, Treeinorder}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Treeinorder\textnormal{(x)}}, gobble=4]{Treeinorder(x)}
     if x $\not=$ NIL
        then Treeinorder(li[x])
        	Print key[x]
             Treeinorder(re[x])
\end{lstlisting}

    Bei \textsc{Treeinorder} wird zuerst der linke Teilbaum, dann die 
    Wurzel und anschließend der rechte Teilbaum durchlaufen.
\end{Algorithmus}

  Beispiel für \textsc{Treeinorder}
  \begin{figure}[H]
  \centering \input{171103d.latex} 
   \caption{\textsc{Treeinorder}}
   \label{171103d}
  \end{figure}
 \begin{itemize}
  \item x ist der Zeiger auf dem Knoten
  \item Die Ausgabenreihenfolge ist \{1,2,3,4,5,7,8,9\}
 \end{itemize}
 
  \begin{satz}
 Bei gegebenem binären Suchbaum ist die Ausgabe mit allen drei Verfahren (\textsc{Inorder}, 
 \textsc{Preorder} und \textsc{Postorder}) in $\Theta(\,n\,)$ möglich.
 \end{satz}
 \begin{description}
  \item[Folgerung:] Der Aufbau eines binären Suchbaumes kostet $\Omega(\,n\,log\,n\,)$ Zeit.
 \end{description}
 
 \subsubsection{\textsc{Tree-Successor}(x)}
 
   \begin{Algorithmus}[H]
   \addcontentsline{alg}{Algorithmus}{\textsc{Min}(x)}
   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search, Treepreorder, Treeinorder}, emphstyle=\textsc, escapeinside=~~}
    \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Min\textnormal{(x)}}, gobble=4]{Min(x)}
     while li[x] $\not=$ NIL do
       x:= li[x]
     return x
    \end{lstlisting}

   \textsc{Min}(x) liefert das Minimum des Teilbaumes, dessen Wurzel x ist.
   \end{Algorithmus}
   
   \begin{Algorithmus}[H]
    \addcontentsline{alg}{Algorithmus}{\textsc{Tree-Successor}(x)}
    %\lstset{emph={Min}}
    \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search, Treepreorder, Treeinorder, Min}, emphstyle=\textsc, escapeinside=~~}
    \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Tree-Successor\textnormal{(x)}}, gobble=4]{Tree-Successor(x)}
     if re[x] $\not=$ NIL
        then return Min(re[x])
     y:=p[x]
     while y $\not=$ NIL and x=re[y]
        do x:=y
           y:=p[y]
     return y
    \end{lstlisting}

   Beim \textsc{Tree-Successor} werden zwei Fälle unterschieden. Falls x 
   einen rechten Teilbaum besitzt, dann ist der Nachfolger das Blatt, das im 
   rechten Teilbaum am weitesten links liegt (\textsc{MIN}(x)). Besitzt x keinen 
   rechten Teilbaum, so ist der successor y der Knoten dessen linker Sohn 
   am nächsten mit x verwandt ist. Zu beachten ist dabei, daß sich der Begriff Nachfolger auf einen Knoten bezieht, der Algorithmus
   aber den Knoten liefert, dessen gespeicherter Wert im Baum Nachfolger des im ersten Knoten gespeicherten Wertes ist. 
   \end{Algorithmus}
   
	% Vorlesungsskript vom 19.11.03
	% Christian Lütz
	% M.-Nr.: 62311
	% e-mail: Krisy0910@gmx.de
Die Operationen zum Löschen und Einfügen von Knoten sind etwas komplizierter, da sie die Baumstruktur stark verändern können und
erhalten deswegen jeweils einen eigenen Abschnitt.
\subsection{Das Einfügen}

  	\begin{description}
			\item{Beim \textsc{Tree-Insert} werden zwei Parameter übergeben, wobei}
			\begin{itemize}
				\item T der Baum ist, in dem eingefügt werden soll und
		 		\item z der Knoten, so daß	
		 		\begin{itemize}
					\item key[z]   = v (einzufügender Schlüssel),
					\item left[z]  = NIL und
					\item right[z] = NIL
				\end{itemize}
				ist.
			\end{itemize}
		\end{description}
 	Erklärung:
	Bei diesem Einfügealgorithmus werden die neuen Knoten immer als 
	Blätter in den binären Suchbaum T eingefügt. Der einzufügende 
	Knoten z hat keine Söhne. Die genaue Position des Blattes wird 
	durch den Schlüssel des neuen Knotens bestimmt. Wenn ein neuer 
	Baum aufgebaut wird, dann ergibt der erste eingefügte Knoten die 
	Wurzel. Der zweite Knoten wird linker Nachfolger der Wurzel, wenn 
	sein Schlüssel kleiner ist als der Schlüssel der Wurzel und rechter 
	Nachfolger, wenn sein Schlüssel größer ist als der Schlüssel der 
	Wurzel. Dieses Verfahren wird fortgesetzt, bis die Einfügeposition bestimmt ist.

	Anmerkungen dazu:
	Dieser Algorithmus zum Einfügen ist sehr einfach. Es finden keine 
	Ausgleichs- oder Reorganisationsoperationen statt, so daß die 
	Reihenfolge des Einfügens das Aussehen des Baumes bestimmt, deswegen 
	entartet der binäre Suchbaum beim Einfügen einer bereits sortierten Eingabe
	zu einer linearen Liste. 

\begin{Algorithmus}
\addcontentsline{alg}{Algorithmus}{\textsc{Tree-Insert}}
   %\lstset{emph={Treeinorder}}
   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search, Treepreorder, Treeinorder}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Tree-Insert}, gobble=1]{Tree-Insert}
 y := NIL
 x := root[T]
 while ( x $\not=$ NIL ) do
   y := x
   if ( key[z] = key[x] ) 
     then x := left[x]
     else x := right[x]
 p[z] := y
 if ( y = NIL )
   then root[T] := z
   elseif ( key[z] < key[y] )
     then left[y]  := z
     else right[y] := z    
	    \end{lstlisting}

Die Laufzeit liegt in O(h), wobei h die Höhe von T ist.
\end{Algorithmus}    
    Da der Knoten immer in einem Blatt eingefügt wird, ist damit zu rechnen,
    daß im worst case das Blatt mit der größten Entfernung von der Wurzel
    genommen wird. Da dieses die Höhe h hat sind folglich auch h Schritte
    notwendig, um zu diesem Blatt zu gelangen.
        
\subsection{Das Löschen eines Knotens}
	Beim Löschen eine Knotens z in eiem binären Suchbaum müssen drei Fälle
	unterschieden werden:\
	\begin{itemize}
	 	\item \textbf{1. Fall:} z hat keine Söhne \\
	 	Der Knoten kann gefahrlos gelöscht werden und es sind
	 	keine weiteren Operationen notwendig.
	 	\centering
	 		\input{191103a.latex}
%	 		\vspace{3em}
	 	 
	 	\item \textbf{2. Fall:} z hat genau einen linken Sohn \\
	 	Der zu löschende Knoten wird entfernt und durch den Wurzelknoten
	 	des linken Teilbaums ersetzt.
	 	\centering
	 		\input{191103b.latex}
%	 		\vspace{3em}
	 	
	 	\item \textbf{3. Fall:} z hat genau einen rechten Sohn \\
		  Analog dem 2. Fall.
%		  \vspace{3em}
		\item \textbf{4. Fall:} z hat zwei Söhne \\
		\textit{Problem:} Wo werden die beiden Unterbäume nach dem Löschen von z
		angehängt?
		\textit{Lösung:} Wir suchen den Knoten mit dem kleinsten Schlüssel im rechten
		Teilbaum von z. Dieser hat keinen linken Sohn, denn sonst gäbe es einen
		Knoten mit einem kleineren Schlüssel. Der gefunden Knoten wird mit dem
		zu löschenden Knoten z vertauscht und der aktuelle Knoten entfernt.
	 	\centering
	 		\input{191103c.latex}
	 	
		\end{itemize}		 		

		Auch beim Löschen (\textsc{Tree-Delete}) werden wieder zwei Parameter übergeben, dabei ist
		\begin{itemize}
			\item T der Baum und
		 	\item z der zu löschende Knoten
		\end{itemize}
		Rückgabewert ist der (tatsächlich) aus dem Baum entfernte Knoten, dies muss nicht z sein, (siehe 4. Fall)
		\begin{Algorithmus}
		\addcontentsline{alg}{Algorithmus}{\textsc{Tree-Delete}}
		   %\lstset{emph={Tree-Successor}}
		   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
                   Tree-Search, Treepreorder, Treeinorder, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
                        \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Tree-Delete}, gobble=4]{Tree-Delete}
	if ( left[z] = NIL or right[z] = NIL )
		then y := z
		else y := Tree-Successor(z)
	if ( left[y] $\not=$ NIL )
		then x := left[y]
		else x := right[y]
	if ( x $\not=$ NIL )
		then root[T] := x
		else if ( y = left[p[y]] )
			then left[p[y]] := x
			else right[p[y]] := x
	if ( y $\not=$ z )
		then key[z] := key[y]
	return y
			\end{lstlisting}

		Laufzeit liegt wieder in O(h), wobei  h wieder die Höhe von T bezeichnet.
		\end{Algorithmus}		

		Im worst case wird \textsc{Tree-Successor} mit einer Laufzeit von O(h) einmal aufgerufen, 
		andere Funktionsaufrufe oder Schleifen gibt es nicht.
\subsubsection{Binäre Suchbäume als Implementierung des ADT Wörterbuch}
 		\centering\input{191103d.latex}
%	  	\vspace{1em}
	  	Wie bereits bei der Funktion \textsc{Tree-Insert} beschrieben, kann eine ungünstige 
	  	Einfügereihenfolge den Suchbaum zu einer linearen Liste entarten lassen. Deswegen sind allgemeine 
	  	binäre Suchbäume nicht geeignet, den ADT Wörterbuch zu implementieren.  	
\section{2-3-4-Bäume}
\begin{definition}[2-3-4-Bäume]
\textbf{2-3-4-Bäume} sind Bäume mit folgenden speziellen Eigenschaften:
\begin{itemize}
				\item Jeder Knoten im Baum enthält einen, zwei oder drei Schlüssel, 
							die von links nach rechts aufsteigend sortiert sind.		
				\item Ein Knoten mit k Schlüsseln hat k+1 Söhne (oder er hat überhaupt keine:``Blatt'') und wird als
							 (k+1)-Knoten bezeichnet.
				\item Für Schlüssel im Baum gilt die verallgemeinerte Suchbaumeigenschaft.
				\item Alle Blätter haben den gleichen Abstand zur Wurzel.	
			\end{itemize}
\end{definition}
Zur Veranschaulichung dienen die folgenden Abbildungen (2-, 3- und 4-Knoten, ein Blatt und ein skizzierter möglicher Baum).
			 	\centering\input{191103e.latex}		
				\centering\input{191103f.latex}
	 			\centering\input{191103g.latex}
		Bei einem 2-3-4-Baum ist die Anzahl der Knoten deutlich geringer als bei
		einem vergleichbaren binären Suchbaum. Damit ist die Zahl der besuchten 
		Knoten bei einer Suche geringer. Daraus folgt, daß das Suchen nach einem
		Schlüssel in einem 2-3-4-Baum effizenter ist, als in einem vergleichbaren binären Suchbaum.
		Allerdings ist der Aufwand beim Einfügen und beim Löschen von Schlüsseln höher.
\subsubsection{Beispiel für einen 2-3-4-Baum}
		\centering\input{191103h.latex}
		\begin{itemize}
			\item Erfolgreiche Suche nach 35
			\item Erfolglose Suche nach 69
		\end{itemize}
		Die Laufzeit für das Suchen liegt wieder in O(h), mit h als Höhe des Baumes.
% 24.11.03 Marcel Konstanz
\subsection{Top Down 2-3-4-Bäume für den ADT Dictionary}

Wenn die Höhe des Baumes logarithmisch ($h \in O(\log n)$) ist, eignet er sich gut für den Datentyp
Wörterbuch, da dann alle Operationen in O($\log n$) gehen. Insbesondere das in einem Wörterbuch zu erwartende häufige Suchen hat
die Komplexität O($\log n$).	

\begin{figure}[H]
    \centering\input{241103a.latex}
    \caption{$h \in O(\log n)$}
    \label{241103a}
 \end{figure}	

Bereits bei den Binärbäumen muß die Baumstruktur nach dem Löschen eines Knotens manchmal repariert werden.
Nun ist klar ersichtlich, daß ein Baum, der immer logarithmische Höhe haben soll, nicht zu einer linearen Liste entarten darf. 	
Falls also das Einfügen eines Elemente in einen Baum die Baumstruktur so ändert, daß die Eigenschaften 
verletzt sind, muß der Baum repariert werden.

Wie in Abbildung \ref{241103b} zu sehen ist, kann das Einfügen eines einzigen Knotens dazu führen, daß eine neue Ebene
einfügt werden muß. Falls dies in der untersten Ebene geschieht (worst-case), kann sich das bis zur Wurzel fortsetzen.	
Top down 2-3-4 Bäume sollen diese Situation verhindern! 
 
 \begin{figure}[H]
    \centering\input{241103b.latex}
    \caption{$h \in O(\log n)$}
    \label{241103b}
 \end{figure}

Der worst case wird dadurch verhindert, daß zwischendurch etwas mehr Aufwand betrieben wird.
So wird beim dem Einfügen vorausgehenden Suchen jeder erreichte 4-Knoten sofort aufgesplittet.
So ist gewährleistet, daß immer Platz für einen neuen Knoten ist.

Allerdings haben Top down 2-3-4-Bäume auch den Nachteil, daß sie schwerer implementierbar sind als die 2-3-4 Bäume. 
    Dafür sind \textbf{Rot-Schwarz-Bäume} besser geeignet. %\textcolor{red}{Wofür besser geeignet? Implementierung?}      

\section{Rot-Schwarz-Bäume}
 
\begin{definition}[Rot-Schwarz-Bäume]
   \textbf{Rot-Schwarz-Bäume} sind binäre Suchbäume mit folgenden zusätzlichen Eigenschaften:
      \begin{enumerate}
	          \item Jeder Knoten ist rot oder schwarz.
	          \item Die Wurzel ist schwarz.
	          \item Jedes Blatt ist schwarz.
	          \item Ein roter Vater darf keinen roten Sohn haben.
	          \item Die Schwarzhöhe $(Bh(j))$ ist die Anzahl der schwarzen Knoten auf einem Weg von einem Knoten $j$ zu einem
		  Blatt. Sie ist für einen Knoten auf allen Wegen gleich.
      \end{enumerate}
\end{definition}

\begin{definition}[Die Schwarzhöhe]
  $Bh(x)$ ist die Anzahl von schwarzen Knoten auf einem Weg, ohne den Knoten $x$  selbst von $x$ zu einem Blatt und heißt
  Schwarzhöhe von $x$.
\end{definition}

\begin{satz}[Die Höhe von Rot-Schwarz-Bäumen] \label{rshoehe}
  Die Höhe eines Rot-Schwarz-Baumes mit $n$ Knoten ist kleinergleich $2\log(n+1)$
\end{satz}
  
\begin{satz} \label{schwarzhoehe}
Sei $x$ die Wurzel eines Rot-Schwarz-Baumes. Dann hat der Baum mindestens $2^{Bh(x)}-1$ Knoten.
  Der Beweis kann induktiv über die Höhe des Baumes erfolgen.
\end{satz}

Um Speicherplatz zu sparen, bietet es sich an, nur ein Nil-Blatt abzuspeichern. Dann müssen natürlich alle Zeiger entsprechend gesetzt
werden.
 
 \begin{figure}[H]
    \centering\input{241103c.latex}
    \caption{Nur ein NIL-Blatt}
    \label{241103c}
 \end{figure}

\begin{beweis} 
 IA : Sei $h=0$. Dann handelt es um einen Baum, der nur aus einem NIL-Blatt besteht. Dann ist $Bh(x)=0$ und nach Satz \ref{schwarzhoehe} 
 die Anzahl der inneren Knoten mindestens $2^0-1=0$. Damit ist der Induktionsanfang für Satz \ref{schwarzhoehe} gezeigt. 
 \medskip
 
 \noindent IS : Sei nun h'$<$h. Dazu betrachten wir die zwei Teilbäume eines Baumes mit der Höhe $h$ und der Wurzel $x$.
Dann gibt es jeweils für den rechten und den linken Teilbaum zwei Fälle. Dabei sind die Fälle für die beiden Teilbäume analog und
 werden deswegen gleichzeitig abgearbeitet.

    \begin{figure}[H]
    \centering\input{241103d.latex}
    \caption{Baumhöhe mit Bh(li[x])=Bh(x)-1 und Bh(re[x])=Bh(x)}
    \label{241103d}
 \end{figure} 
 
    \textbf{1.Fall}:  Sei $Bh(li[x]) = Bh[x]$. Da eine untere Schranke gezeigt werden soll und in diesem Fall der Baum nicht weniger 
    Knoten
    hat als im zweiten Fall, reicht es den Beweis für den zweiten (kritischeren) Fall zu führen (Der zweite Fall ist kritischer, da
    der Teilbaum weniger Knoten haben kann als im ersten Fall und damit eher in der Lage ist, die untere Schranke zu durchbrechen).
     
    \textbf{2.Fall}:  Sei $Bh(li[x]) = Bh[x]-1$, dann gibt mindestens soviele innere Knoten, wie die beiden
    Teilbäume nach Induktionsvoraussetzung zusammen haben. Damit hat der gesamte Baum mindestens   
    $1+2^{Bh(x)-1}-1+2^{Bh(x)-1}-1$  $=$ $2 \cdot 2^{Bh(x)-1}+1-2 = 2^{Bh(x)}-1$. Damit ist Satz \ref{schwarzhoehe} bewiesen. 
    
Zum Beweis von Satz \ref{rshoehe} wird Eigenschaft 4 der Rot-Schwarz-Bäume ausgenutzt. Daraus folgt direkt, daß auf dem Weg von der Wurzel zu den Blättern,
mindestens die Hälfte der Knoten schwarz ist. Sei $h$ wieder die Höhe des Baumes, dann gilt damit 

\begin{quote}       
        $Bh(root) \geq \frac{h}{2}\stackrel{\scriptstyle{S.}\, \ref{schwarzhoehe}}{\longrightarrow}$\\
        $n \geq 2^{Bh(root)}-1 \geq 2^{\frac{h}{2}}-1 \Rightarrow 2^{\frac{h}{2}}\leq n+1$ \hspace{2cm}$|$ $\log$\\
        $\Leftrightarrow \frac{h}{2}\leq \log(n+1)$  \hfill q.e.d  
\end{quote}
\end{beweis} 

% Mitschrift vom 26.11.03, Michael Preiß
\subsection{Operationen in RS-Bäumen}

Die Operationen Tree-Insert und -Delete haben bei Anwendung auf einen RS-Baum eine Laufzeit von O($\log n$). Da
sie den Baum verändern, kann es vorkommen, daß die Eigenschaften des Rot-Schwarz-Baumes verletzt werden. Um diese wieder herzustellen,
müssen die
Farben einiger Knoten im Baum sowie die Baumstruktur selbst verändert werden. Dies soll mittels Rotationen realisiert werden, dabei
gibt es die \textsc{Linksrotation} und die \textsc{Rechtsrotation}.
%\textcolor{red}{wieso ist die rechtsrotation algorithmisch etwas komplizierter?}

\centering
\begin{figure}[H]
\input{261103a.latex}
\caption{Die Rotation schematisch}
\end{figure}


\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Linksrotation\textnormal{(T, z)}}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
                   Tree-Search, Treepreorder, Treeinorder, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Linksrotation\textnormal{(T, z)}}, gobble=4]{Linksrotation(T,z)}
    y := re[x]
    re[x] := li[y]
    p[li[y]] := x
    p[y] := p[x]
    if p[x] = NIL
      then root[T] := y
      else if x = li[p[x]]
        then li[p[x]] := y
        else re[p[x]] := y
    li[y] := x
    p[x] := y
\end{lstlisting}
\end{Algorithmus}

\begin{satz}
Rotationen ändern die Gültigkeit der Suchbaumeigenschaft nicht.
\end{satz}

Wie die Skizze vermuten läßt, ist der Code für die \textsc{Retchsrotation} symmetrisch zu dem für die \textsc{Linksrotation}. Beide
Operationen erfordern O(1) Zeit, da mit jeder Rotation nur eine konstante Anzahl von Zeigern von umgesetzt wird und der Rest
unverändert bleibt.

Sei T ein Rot-Schwarz-Baum. Ziel ist, daß T auch nach Einfügen eines Knotens z ein Rot-Schwarz-Baum ist. T soll also 
nach Anwendung von \textsc{RS-Insert}(T, z) und einer eventuellen Korrektur die Bedingungen für Rot-Schwarz-Bäume erfüllen. Im folgenden Beispiel wird die "`3"'
eingefügt.

\begin{figure}[H]
\centering
\input{261103b.latex}\input{261103c.latex}\hspace{5mm}\input{261103d.latex}

\caption{Funktionsweise der Rotation}
\end{figure}
Die letzte Rotation sollte zur übung selbst nachvollzogen werden. Der fertige Baum als Ergebnis dieser letzten Roation steht im Anhang
auf Seite \pageref{rsrotation}.

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{RS-Insert\textnormal{(T, z)}}}
%\lstset{emph={Korrigiere}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
                   Korrigiere, Treepreorder, Treeinorder, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{RS-Insert\textnormal{(T, z)}}, gobble=1]{RS-Insert(T,z)}
 y := NIL[T]
 x := root[T]
 while x $\not=$ NIL[T] do
   y := x
   if key[z] < key[x]
     then x := li[x]
     else x := re[x]
 p[z] := y
 if y = NIL[T]
   then root[T] := z
   else if key[x] < key[y]
     then li[y] := z
     else re[y] := z
 li[z] := NIL[T]
 re[z] := NIL[T]
 Farbe[z] := ROT
 Korrigiere (T, z)
\end{lstlisting}
\end{Algorithmus}

Früher wurden die Knoten gefärbt, mittlerweile ist man aber dazu übergegangen, die Kanten zu färben. Dabei gilt, daß eine rote
Kante auf einen früher rot gefärbten Knoten zeigt und eine schwarze Kante auf Knoten, die früher schwarz gefärbt
wurden. Die Färbungen sind auch auf andere Bäume übertragbar. Die Färbung der Kanten hat den Nachteil, daß sich die Schwarzhöhe so
ergibt, daß die roten Kanten auf einem Weg nicht mitgezählt werden. Einfacher und damit sicherer ist es, wenn nur die schwarzen Knoten
gezählt werden.

Für alle höhenbalancierten Bäume gilt, daß ihre Höhe in O($\log n$) liegt, allerdings haben Rot-Schwarz-Bäume den Vorteil, daß
sie leichter zu implementieren sind. Da die NIL-Blätter schwarz sind, kann ein einzufügender Knoten auch erstmal einmal rot gefärbt
sein.

%\textbf{Funktionsweise:} \dots \textcolor{red}{fehlt noch!}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Korrigiere\textnormal{(T, z)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
                   Korrigiere, Linksrotation, Rechtsrotation, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Korrigiere\textnormal{(T, z)}}, gobble=1]{Korrigiere(T,z)}
 while Farbe[p[z]] = ROT do
   if p[z] = li[p[p[z]]] ~\hspace{20mm}  $\vartriangleright$\textnormal{Vater ist linker Sohn von Opa von z}~
     then y := re[p[p[z]]] ~\hspace{26.1mm} $\vartriangleright$\textnormal{y ist Onkel von z}~
       if Farbe[y] = ROT ~\hspace{31mm} $\vartriangleright$\textnormal{Vater und Onkel rot}~
         then Farbe[p[z]] := SCHWARZ ~\hspace{5.3mm} $\vartriangleright$\textnormal{Vater wird schwarz}~
           Farbe[y] := SCHWARZ ~\hspace{18.7mm} $\vartriangleright$\textnormal{Onkel wird schwarz}~
           Farbe[p[p[z]]] := ROT ~\hspace{14.5mm} $\vartriangleright$\textnormal{Opa wird schwarz}~
           z := p[p[z]]
         else if z = re[p[z]]
           then z := p[z]
             Linksrotation (T, z)
         Farbe[p[z]] := SCHWARZ
         Farbe[p[p[z]]] := ROT
         Rechtsrotation(T, p[p[z]])
    else wie bisher, nur li und re vertauschen
 Farbe[Root[T]] := SCHWARZ
\end{lstlisting}
\end{Algorithmus}

Eine mögliche Anwendung für einen Rot-Schwarz-Baum ist das bereits einleitend erwähnte Segmentschnitt-Problem. Dabei kann ein
Rot-Schwarz-Baum für die Verwaltung der Sweepline-Status-Struktur verwendet werden (Menge Y, dazu siehe auch Anhang \ref{planesweep}).

% 01.12.2003 Sylvia Andersch
\section{Optimale binäre Suchbäume}

Seien wie im folgenden Beispiel Schlüsselwerte $a_1\:<\:...\:<\:a_n$ mit festen bekannten Wahrscheinlichkeiten $p_1,...,p_n$ gegeben, wobei gilt $p_i\:
\geq\:0$ und $\sum_{i=1}^n p_i\:=\:1$. Dabei bezeichnet $p_i$ die Wahrscheinlichkeit mit der auf den Wert $a_i$ zugegriffen wird.

\begin{description}
    \item[Beispiel:]  Sei nun $a_1\:=\:1,\:a_2\:=\:2,...,\:a_6\:=\:6$, 
      $p_1\:=\:0,25,\:p_2\:=\:0,28,\:p_3\:=\:0,1,\:p_4\:=\:0,2,\:p_5\:=\:0,13,\:p_6\:=\:0,04\:$ und
      der binäre Suchbaum sähe wie folgt aus:
      \centering
      \setlength{\unitlength}{4000sp}%
%
\begingroup\makeatletter\ifx\SetFigFont\undefined%
\gdef\SetFigFont#1#2#3#4#5{%
  \reset@font\fontsize{#1}{#2pt}%
  \fontfamily{#3}\fontseries{#4}\fontshape{#5}%
  \selectfont}%
\fi\endgroup%
\begin{picture}(2300,2425)(1000,-3425)
\thinlines
\put(1650,-1150){\circle{300}}
\put(1600,-1230){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}5}}}
\put(1650,- 1300){\line(-1,-1){450}}
\put(1650,- 1300){\line(1,-1){450}}
\put(1200,-1900){\circle{300}}
\put(1150,-1980){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}2}}}
\put(2100,-1900){\circle{300}}
\put(2050,-1980){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}6}}}
\put(1200,-2050){\line(1,-1){450}}
\put(1200,-2050){\line(-1,-1){450}}
\put(750,-2650){\circle{300}}
\put(700,-2730){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}1}}}
\put(1650,-2650){\circle{300}}%\put(1650,-1150){\circle{300}}
\put(1600,-2730){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}3}}}
\put(1650,- 2800){\line(1,-1){450}}
\put(2100,-3400){\circle{300}}
\put(2050,-3480){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}4}}}
\end{picture}
      
      
\end{description}

An diesem Beispiel wird deutlich, daß es bis zur $4$ ein relativ langer Weg ist, d.h. auch die Suche nach $4$ dauert
im Vergleich zu anderen lange. Nun hat aber die 4 eine hohe Wahrscheinlichkeit und wird deshalb oft abgefragt
werden. Also wird die durchschnittliche Rechenzeit relativ lang sein. Es wäre also schöner, wenn die $4$ weiter oben im Baum stünde.
Diese überlegungen lassen sich fortführen und es fraglich erscheinen, ob sich durch Umplazieren von anderen Schlüsselwerten,
die Rechenzeit noch weiter verkürzen läßt (z.B. könnte die $6$ weit nach unten, da sie die kleinste Wahrscheinlichkeit
hat und somit nur selten abgefragt wird). Aber wo liegen die Grenzen dieses Prozesses? Es ist ja klar, daß er
irgendwie begrenzt sein muss. Es bleibt also die Frage: 

Gibt es irgendwelche Schranken, durch die die durchschnittliche
Rechenzeit beschränkt ist und wenn ja, wie sehen diese aus? 

Sicher ist, daß die Rechenzeit eng mit der Höhe des
Baumes zusammenhängt. Die Frage ist also äquivalent dazu, ob es Grenzen für die durchschnittliche Knotentiefe gibt und falls ja, wie 
diese aussehen.
Allgemein sollen in diesem Kapitel folgende Fragen beantwortet werden:
\begin{enumerate}
    \item Wie muss der binäre Suchbaum konstruiert werden, damit er optimal ist?
    \item Durch welche Grenzen wird die mittlere Knotentiefe beschränkt?
\end{enumerate}
\noindent
Dazu muß zuerst \textbf{Optimalität} definiert werden. Hier ist damit folgendes gemeint:
 \begin{definition}
   Ein binärer Suchbaum heisst \textbf{optimal}, wenn die Summe $\sum_{i=1}^n p_i\:(t_i+1)$ minimal ist, wobei
      $t_i$ die Tiefe des Knotens von $a_i$ angibt. Die Addition von "`1"' zur Tiefe erfolgt, damit auch der Wert für die Wurzel in
      das Ergebnis eingeht.
    \end{definition}
Fakt ist, daß jeder Teilbaum in einem optimalen Suchbaum wieder optimal ist. Der Sachverhalt, daß eine optimale Lösung des
Gesamtproblems auch jedes Teilproblem optimal löst, heißt "`Optimalitätskriterium von Bellmann"'.

Diese Tatsache kann nun benutzt werden, um mittels dynamischer Programmierung einen optimalen Suchbaum zu konstruieren.

\subsection{Bottom-Up-Verfahren}

Gegeben ist folgendes:
\begin{itemize}
    \item Gesamtproblem $(a_1, \dots,a_n)$, d.h. $n$ Schlüsselwerte mit den dazu gehörigen Wahrscheinlichkeiten $p_i$ für $i=1 \dots n$
    \item Tiefen der Knoten $t_1,\dots,t_n$
    \item Die mittlere Suchzeit, gegeben durch $\sum_{i=1}^n p_i\:(t_i+1)$
\end{itemize}
\begin{description}
    \item[Idee] Das Gesamtproblem wird in Teilprobleme aufgesplittet, d.h. grössere optimale Suchbäume werden
     aus kleineren optimalen Suchbäumen  berechnet und zwar in einem rekursiven Verfahren.
     
     Angenommen alle möglichen optimalen Suchbäume mit weniger als den $n$ gegebenen Schlüsselwerten sind schon bekannt.
     Der optimale Suchbaum mit $n$ Schlüsselwerten besteht aus einer Wurzel, einem rechten und einem linken Teilbaum, wobei
     für die Teilbäume die optimale Darstellung schon gegeben ist. Zu suchen ist noch die Wurzel $a_k$, für die die
     Summe der mittleren Suchzeiten der beiden Teilbäume minimal ist.
\end{description}
Dazu definiert man die Teilprobleme $(i,j)\:=\:(a_i,\dots,a_j)$. Der Baum für das optimale Teilproblem wird mit $T(i,j)$
bezeichnet. Weiter ist $$p(i,j):=\sum_{m=i}^j p_m$$ die Wahrscheinlichkeit, daß ein Wert zwischen $a_i$ und $a_j$
erfragt wird.\\ Ziel ist es, die mittlere Teilsuchzeit $$t(i,j):=\sum_{m=i}^j p_m(t_m+1)$$ zu minimieren. Dies beinhaltet
das Problem, die optimale Wurzel zu suchen. Man wählt also ein beliebiges $k\in [i,j]$, setzt $a_k$ als Wurzel
an und berechnet das dazugehörige $t(i,j)$ \\
Betrachten Teilbaum $T(i,j)$:\\ Sei $a_k$ die Wurzel und $j>0$. Die Suchzeit berechnet sich durch\\
$t(i,j)=$ Anteil der Wurzel + Anteil linker Teilbaum + Anteil rechter Teilbaum
$$\Rightarrow\:t(i,j)=\:p_k\cdot 1\:+\:p(i,k-1)\:+\:t(i,k-1)\:+\:p(k+1,j)\:+\:t(k+1,j)$$
wegen $p_k\:+\:p(i,k-1)\:+\:p(k+1,j)=p(i,j)$ folgt
$$t(i,j)=\left\{\begin{array}{ll}
    0, & i>j \\
    p(i,j)+ \min_{i\leq k\leq j}[t(i,k-1)+t(k+1,j)], & sonst \\ \end{array}\right.    $$

\begin{description}
    \item[Beispiel:] Zahlen wie oben
    \begin{table}[h]
     \begin{tabular}{{c}|{c}{c}{c}{c}{c}{c}{c}{c}{c}}
    i$\backslash$ j & 1  &   2  &  3   &  4   &  5   & 6  \\
    \hline
    1             & 0,25/1 & 0,78/2 & 0,98/2 & .../2  & .../2  &.../2 \\
    2             &        & 0,28/2 & 0,48/2 & 0,98/2 & .../2  & .../2 \\
    3             &        &        & 0,1/3  & 0,4/4  & .../4  & .../4\\
    4             &        &        &        & 0,2/4  & .../4  & .../4\\
    5             &        &        &        &        & 0,13/5 & .../5\\
    6             &        &        &        &        &        & 0,04/6\\
  \end{tabular}
  \caption{Tabelle für $t(i,j)$/ optimale Wurzel $k$}
  \end{table}

$\begin{array}{ccl}
t(1,2) &=& p(1,2)+ \min[\:t(1,0)+t(2,2)\:,\:t(1,1)+t(3,2)\:]\\
       &=& 0,53+\min[\:0+0,28\:,\:0,25+0\:]\\
       &=& 0,53+0,25\:=\:0,78
\end{array}$
$\begin{array}{ccl}
t(2,3) &=& p(2,3)+ \min[\:t(2,1)+t(3,3)\:,\:t(2,2)+t(4,3)\:]\\
       &=& 0,38+\min[\:0+0,1\:,\:0,28+0\:]\\
       &=& 0,38+0,1\:=\:0,48
\end{array}$
$\begin{array}{ccl}
t(2,4) &=& p(2,4)+ \min[\:t(2,1)+t(3,4)\:,\:t(2,2)+t(4,4)\:,\:t(2,3)+t(5,4)]\\
       &=& 0,58+\min[\:0+0,4\:,\:0,28+0,2\:,\:0,48+0]\\
       &=& 0,58+0,4\:=\:0,98
\end{array}$
$\Rightarrow$ optimaler Baum:
\centering
\setlength{\unitlength}{4000sp}%
%
\begingroup\makeatletter\ifx\SetFigFont\undefined%
\gdef\SetFigFont#1#2#3#4#5{%
  \reset@font\fontsize{#1}{#2pt}%
  \fontfamily{#3}\fontseries{#4}\fontshape{#5}%
  \selectfont}%
\fi\endgroup%
\begin{picture}(2300,2225)(1000,-3425)
\thinlines
\put(1650,-1150){\circle{300}}
\put(1600,-1230){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}2}}}
\put(1650,- 1300){\line(-1,-1){450}}
\put(1650,- 1300){\line(1,-1){450}}
\put(1200,-1900){\circle{300}}
\put(1150,-1980){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}1}}}
\put(2100,-1900){\circle{300}}
\put(2050,-1980){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}4}}}
\put(2100,-2050){\line(1,-1){450}}
\put(2100,-2050){\line(-1,-1){450}}
\put(1600,-2650){\circle{300}}
\put(1550,-2730){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}3}}}
\put(2550,-2650){\circle{300}}%\put(1650,-1150){\circle{300}}
\put(2500,-2730){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}5}}}

\put(2550,- 2800){\line(1,-1){450}}
\put(2950,-3400){\circle{300}}
\put(2900,-3480){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}6}}}
\end{picture}

\end{description}
\subsection{Schranken}

Wir wollen nun der Frage nachgehen welche Schranken für die mittlere Rechenzeit gelten, und zwar nach oben und nach unten.
Dazu ein kurzer Einschub zur Entropie:
Gegeben sei ein Zufallsexperiment mit m Ausgängen und den zugehörigen Wahrscheinlichkeiten $p_1,..,p_m$
\begin{description}
    \item[Entropie] $H(p_1,...,p_n)$ = Maß für die Unbestimmtheit eines Versuchsausganges und berechnet sich durch
     $$H(p_1,...,p_n)=-\sum_{i=1}^m p_i\:log_2\:p_i$$
\end{description}

\begin{satz}
 \begin{itemize}
    \item $H(p_1,...,p_n)\:\leq\:log_2\:m$ (Gleichheit gilt $\Leftrightarrow\:p_1=...=p_m=\frac{1}{m})$
    \item $H(p_1,...,p_n)\:=\:H(p_1,...,p_n,0)$
    \item $H(p_1,...,p_n)\:=H(p_{\pi(1)},...,p_{\pi(m)})$
    \item Satz von Gibb: seien $q_1,..,q_m$ Werte größer oder gleich Null und $\sum_{i=1}^m q_i\leq1$, so gilt
     $$H(p_1,...,p_n)\:\leq-\sum_{i=1}^m p_i\:log_2\:q_i$$
 \end{itemize}
\end{satz}

\begin{satz}
 Wenn die Daten nur in den Blättern stehen, so ist die Entropie eine untere Schranke für die mittlere Tiefe
 der Blätter $\sum_{i=1}^m p_i t_i$ (hier $t_i,\:i=1..n$ Tiefe der Blätter), es gilt
 $$H(p_1,...,p_n)\:\leq\sum_{i=1}^m p_i\:t_i$$
\end{satz}
\begin{beweis}
Sei $(q_1,..,q_m)=(2^{-t_1},...,2^{-t_m})$, dann gilt nach obigem Satz:\\
$\begin{array}{cccl}
  H(p_1,...,p_n) & \leq & - &\displaystyle\sum_{i=1}^m p_i\:log_2\:q_i \vspace{0.3mm}\\
                 & =    & - &\displaystyle\sum_{i=1}^m p_i\:log_2\:2^{-t_i} \vspace{0.3mm}\\
                 & =    &   &\displaystyle\sum_{i=1}^m p_i\:t_i \\
\end{array}$\\
Die Werte für q sind korrekt gewählt, da nach der Ungleichung von Kraft gilt: $$\sum_{i=1}^m 2^{-t_i}\leq1$$
\end{beweis}
\begin{flushright} q.~e.~d. \end{flushright}
\begin{satz}
Die mittlere Knotentiefe eines optimalen binären Suchbaumes (Daten im gesamten Suchbaum) liegt im Intervall $$
\begin{bmatrix}
  \displaystyle\frac{ H(p_1,...,p_n)}{log_2\:3}-1\:,\:H(p_1,...,p_n)
\end{bmatrix}$$
wobei $p_i$ die Wahrscheinlichkeit ist, mit der der i-te Knoten abgefragt wird.
\end{satz}
\begin{beweis}
\begin{enumerate}
    \item mittlere Knotentiefe $\geq\displaystyle\frac{ H(p_1,...,p_n)}{log_2\:3}-1$      \vspace{2mm}\\
     Jeder binäre Suchbaum kann in einen ternären Baum transformiert werden, in dem nur die Blätter Daten enthalten. Die neuen Blätter rutschen eine Ebene tiefer:
 \setlength{\unitlength}{4000sp}%
\begingroup\makeatletter\ifx\SetFigFont\undefined%
\gdef\SetFigFont#1#2#3#4#5{%
  \reset@font\fontsize{#1}{#2pt}%
  \fontfamily{#3}\fontseries{#4}\fontshape{#5}%
  \selectfont}%
\fi\endgroup%
\begin{picture}(2300,2025)(1000,-2425)
\thinlines
\put(1650,-1150){\circle{450}}
\put(1600,-1230){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}a}}}
\put(2800,-1200){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}$\Longrightarrow$}}}
\put(1650,- 1370){\line(-1,-1){450}}
\put(1650,- 1370){\line(1,-1){450}}
\put(1200,-2040){\circle{450}}
\put(1020,-2130){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}LTB}}}
\put(2100,-2040){\circle{450}}
\put(1920,-2130){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}RTB}}}
\put(4650,-1150){\circle{450}}
\put(4650,- 1370){\line(-1,-1){450}}
\put(4650,- 1370){\line(1,-1){450}}
\put(4650,- 1370){\line(0,-1){450}}
\put(4650,-2000){\circle{300}}
\put(4600,-2080){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}a}}}
\put(4200,-2040){\circle{450}}
\put(4020,-2130){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}LTB}}}
\put(5100,-2040){\circle{450}}
\put(4920,-2130){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}RTB}}}
\end{picture}\\
Es gilt:\\
      $\begin{array}{ccl}
        \mbox{mittl. Knotentiefe des bin. Baums} & \geq & \mbox{mittl. Blatttiefe des ternären Baums}-1 \\
         & \geq & \displaystyle H_3(p_1,...,p_n)-1\:=\:-\sum_{i=1}^n p_i\cdot log_3 p_i \\
         &  =   & \displaystyle-\frac{1}{log_2 3}\sum_{i=1}^n p_i\cdot log_2 p_i\:-1\\
         &  =   & \displaystyle\frac{1}{log_2 3}H(p_1,...,p_n)\:-1
      \end{array}$

    \item $\displaystyle H(p_1,...,p_n)\geq$ mittlere Knotentiefe  \vspace{2mm}\\
      Um für $p_i,...,p_j$ einen möglichst guten Suchbaum zu bestimmen, berechnen wir\\ $q\:=\:\sum_{k=i}^j p_k$ und wählen
      als (Teilbaum)-Wurzel den Knoten $k$ für den gilt: $$\sum_{k=i}^{l-1}p_k\:\leq\frac{q}{2}\:\leq\:\sum_{k=i}^{l}p_k$$
      Für die Teilfolgen $(p_i,..,p_{l-1})$ und $(p_{l+1},..,p_j)$ verfahren wir rekursiv. Gestartet wird mit $(p_1,..,p_n)$.\\
      Nun gilt für Tiefe $t_l$ einer jeden Teilbaumwurzel $l$ (mit $i\leq l\leq j$), daß $$\sum_{k=i}^j p_k \leq 2^{-t_l}$$
      (wegen Wahl von Wurzel). Insbesondere folgt: $p_l\:\leq\:2^{-t_l}$. Dies ergibt:
      $$\mbox{mittlere Suchbaumtiefe}\:=\:\sum_{i=1}^n p_i\cdot t_i\:\leq\:-\sum_{i=1}^n p_i\cdot log_2p_i\:=\:H(p_1,..,p_n)$$
\end{enumerate}
\end{beweis}
\begin{flushright} q.~e.~d. \end{flushright}

\section{Stapel}
Stapel haben eine so große Bedeutung in der Informatik, daß sich auch im Deutschen das englische "`Stack"' eingebürgert hat.
In vielen Algorithmen ist eine Menge zu verwalten, für die sich die einfache Struktur eines Stapels anbietet.
Manche Caches arbeiten nach dem LIFO-Prinzip (Last In First Out), dazu reicht ein simpler Stapel, in den die zu verwaltenden Elemente
der Reihe nach hereingegeben werden und ein Zeiger auf das zuletzt hereingegebene Element gesetzt wird.

\begin{figure}[H]
  \centering\input{031203a.latex}
  \label{031203a}
\end{figure}

Die Basisoperationen eines Stapels sind natürlich \textsc{Push}, \textsc{Pop} und \textsc{Stack-Empty} (Test ob Stack leer), die Implementierung dieser einfachen
Operationen ist simpel. Dabei steht "`S"' für den Stapel.

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Push}}
%\lstset{emph={Top}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
        Top, Korrigiere, Linksrotation, Rechtsrotation, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Push}, gobble=1]{Push}
  Top[s]:=Top[S]+1
  S[Top[S]]:=x
\end{lstlisting}
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Stack-Empty}}
%\lstset{emph={Top}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
        Top, Korrigiere, Linksrotation, Rechtsrotation, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Stack-Empty}, gobble=1]{Stack-Empty}
  if Top[s]=0 
    Then return true
    Else return false
\end{lstlisting}
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Pop}}
\lstset{emph={Top, Stack-Empty}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
        Top, Stack-Empty, Korrigiere, Linksrotation, Rechtsrotation, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Pop}, gobble=1]{Pop}
  if Stack-Empty
    Then Error
    Else Top[S]:=Top[S]-1
      return S[Top[S]+1]
\end{lstlisting}
\end{Algorithmus}

Eine These besagt, daß Stapel beim Algorithmenentwurf vor allem für die Verwaltung von Kandidaten verwendet werden. Ein Beispiel
dafür ist die Berechnung der konvexen Hülle einer Menge.

\subsection{Konvexe Hülle}
CH von englisch Convex Hull bezeichnet die konvexe Hülle, z.B. CH(P) für ein Polygon oder CH(X) für X $\subseteq \mathbb{R}^2$. Dabei
ist diese mathematisch so definiert:
\begin{definition}
\[\mbox{CH(X)}=\bigcap_{\textnormal{M konvex}\ \wedge \ X \subseteq M} M\]
\end{definition}
Anschaulich gesprochen ist CH(X) damit die kleinste Menge, die X umfaßt. Eine sehr schöne Beschreibung ist auch diese: Man stelle sich
ein Brett mit hereingeschlagenen Nägeln vor. Dann vollzieht ein Gummiband, welches um die Nägel gelegt wird, die konvexe Hülle der
Nagelmenge nach. Die Punkte der konvexen Hülle sind nur die Nägel, die das Gummiband tangiert. 

\begin{satz}
Wenn X endich ist, so ist CH(X) ein Polygon (Jede Punktmenge läßt sich auczh als Polygon auffassen).
\end{satz}

Ziel ist die Berechnung der CH(X) für endliche X $\subseteq \mathbb{R}^2$. Erreicht wird dieses Ziel durch den Algorithmus von Graham
(1972), der sich grob wie folgt einteilen läßt:

\begin{itemize}
\item Sortiere X bezüglich wachsender x-Werte (o.B.d.A. habe X allgemeine Lage, Sonderfälle wie z.B. x$_i$=x$_j$ für i$\not=$j treten
also nicht auf und müssen nicht behandelt werden)
\item Setze p$_1$=p$_l \in$ X als den Punkt mit dem kleinsten x-Wert und p$_r \in$ X als den Punkt mit dem größten x-Wert 
\item Wende den Algorithmus zur Berechnung von UH(X) an, dabei ist UH(X) die obere konvexe Hülle von X
\item für LH(X) vertausche die Vorzeichen und benutze UH(X) noch einmal, dabei ist LH(X) die untere konvexe Hülle von X
\end{itemize}

\begin{figure}[H]
  \centering\input{031203b.latex}
  \label{031203b}
\end{figure}

Doch wie funktioniert das nun genau? Zuerst brauchen wir den Algorithmus für UH(X):
\begin{itemize}
\item Eingabe ist die Punktfolge p$_l$=p$_1$, p$_2$, \ldots, p$_n$=p$_r$, dabei sind die x-Werte monoton wachsend und alle p$_i$ liegen
oberhalb von G$_{p_l p_r}$ (dies läßt sich notfalls in O($n \log n$) Zeit erreichen)
\item Der Algorithmus arbeitet dann Punkt für Punkt von links nach rechts und bewahrt folgende Invariante
  \begin{enumerate}
  \item Der Stapel S speichert Punkte x$_0$, x$_1$, \ldots, x$_t$=x$_{\textnormal{Top}}$, die eine Teilfolge von p$_n$, p$_1$, p$_2$, \ldots sind
  \item t$\geq$2 $\wedge$ x$_0$=p$_n$ $\wedge$ x$_1$=p$_1$ $\wedge$ im Schritt s gilt: x$_t$=p$_s$, dabei ist s$\geq$2
  \item x$_0$, x$_1$, \ldots, x$_t$ ist UH(\{p$_1$, \ldots, p$_s$\})
  \item x$_1$, \ldots, x$_t$ sind von rechts sortiert
  \end{enumerate}
\end{itemize}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Graham}}
%\lstset{emph={Push, Pop}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Graham}, gobble=1]{Graham}
  Push(S, p$_n$)
  Push(S, p$_1$)
  Push(S, p$_2$)
  s:=2
  while s$\not=$n do
    $\alpha$:=top[S]
    $\beta$:=zweites Element in S
    while (p$_{s+1}, \alpha, \beta$) keine Linksdrehung ist do
      Pop
      $\alpha$:=$\beta$
      $\beta$:=neues zweites Element im Stapel
    Push(S, p$_{s+1}$)
    s:=s+1
  Gib S aus    
\end{lstlisting}
\end{Algorithmus}

Der Drehsinn läßt sich dabei mathematisch recht einfach feststellen (siehe Anhang \ref{drehsinn}), die Details sind an dieser Stelle
aber weniger wichtig.
Doch wie kann man das für die Berechnung
der konvexen Hülle ausnutzen? Zeichnung \ref{konvexedrehungskizze} macht dies sehr anschaulich klar. Die Punkte sind jeweils die
Endpunkte der eingezeichneten Strecken und daher nicht explizit markiert. Im rechten Fall wird der Punkt $p_2$ für die konvexe Hülle
überflüssig, da er durch $p_{s+1}$ "`überdeckt"' wird. Im linken Bild ist $p_2$ hingegen für die konvexe Hülle notwendig ($p_2$
liege oberhalb der Strecke $\overline{p_1p_{s+1}}$). Dies verdeutlicht anschaulich, warum ein Punkt der durch
\textsc{Pop} im Laufe der Berechnung der konvexen Hülle herausfliegt, nicht nochmals angeschaut werden muß.

\begin{figure}[H]
\centering
\input{031203d.latex}

\caption{Nutzen des Drehsinns für die Berechnung der konvexen Hülle}
\label{konvexedrehungskizze}
\end{figure}

Nun bleibt noch die Analyse des Algorithmus. Bei der Analyse nach der Guthabenmethode werden Operationen mit Kosten versehen, die auf ein fiktives Bankkonto
verbucht werden. Hier kostet nun jedes \textsc{Push} 2 \textcolor{red}{Euro-Zeichen in LaTeX?}. Davon wird einer verbraucht und einer
gespart. Von dem Ersparten werden die \textsc{Pop}'s bezahlt; dabei kostet jedes \textsc{Pop} einen . Da wie bereits oben erwähnt jeder
Punkt maximal einmal durch ein \textsc{Pop} herausfliegen kann, sind wir fertig. Unser Guthaben reicht aus um alle möglichen
\textsc{Pop}'s zu bezahlen. Mittels der Guthabenmethode ergeben sich so amortisierte Kosten von O(n) für die Berechnung der konvexen
Hülle.

Mithilfe eines Stapels ist also ein sehr effezienter Algorithmus möglich. Allerdings sind natürlich auch Stapel kein Allheilmittel
für alle Probleme der algorithmischen Goemetrie. Ein anderes wichtiges Mittel sind die sogenannten Segmentbäume.

\section{Segmentbäume}
Bereits anfangs wurde das Problem der überschneidung von Rechtecken erwähnt. Eingabe sollten Ortho-Rechtecke sein und Ausgabe ein
Bericht der überschneidungen. Dazu kann man
\begin{enumerate}
\item alle Rechteckseiten bestimmen und dann
\item den Sgmentschnitt-Algorithmus benutzen (siehe Anhang \ref{planesweep})
\end{enumerate}

Der dabei verwendete abstrakte Datentyp Dictionary könnte mit den bisher kenngelernten Methoden z.B. RS-, AVL- oder
Top-down-2-3-4-Baum realisiert werden.
Wichtiger ist jetzt allerdings, daß dieses Problem auch auf das Problem der Punkteinschlüsse zurückgeführt werden kann. Die Eingabe
ist dann eine Menge von Punkten und die Ausgabe sind dann alle Punkteinschlüsse (p, R) mit $p\, \in \,R$. Dabei sind mit $p_1, \, p_2,
\, \ldots$ die Punkte und mit $R_1, \, R_2, \, \ldots$ die Rechtecke gemeint. Auch hier wird eine Gleitgeradenmethode benutzt; die
Ereignispunkte sind die x-Koordinaten der linken und rechten Kanten und der Punkte.
\begin{itemize}
\item[] (li, x, (y$_1$, y$_2$), R) \hspace{2cm} (o.B.d.A. $y_1 \leq y_2$)\vspace{-0.5ex}
\item[] (re, x, (y$_1$, y$_2$), R) \vspace{-0.5ex}
\item[] (Punkt, x, y, R)
\end{itemize}

An der folgenden Skizze wird offensichtlich, daß es verschiedene Arten von überschenidungen bzw. Schnitten gibt. 
\begin{figure}[H]
\centering
\input{031203e.latex}

\end{figure}

\subsection{Der Punkteinschluß-Algorithmus}
Y wird zu Beginn  mit Y:=$\emptyset$ initalisiert. Dann wird von Ereignispunkt zu Ereignispunkt gegangen, nachdem diese nach wachsendem
x sortiert wurden. Bei $m$ Rechtecken geht dies in O($m \log m$) Schritten.

\begin{enumerate}
\item Falls der aktuelle Ereignispunkt die Form (li, x, ($y_1,\ y_2$), R) hat, so setze Y:=Y $\cup$ \{(R, [$y_1,\ y_2$])\} --\textsc{Insert}
\item Falls der aktuelle Ereignispunkt die Form (re, x, ($y_1,\ y_2$), R) hat, so setze Y:=Y $\backslash$ \{(R, [$y_1,\ y_2$])\}
--\textsc{Delete}
\item Falls der aktuelle Ereignispunkt die Form (Punkt, $x,\ y$, R) hat, so finde alle Intervalle in Y, die $y$  enthalten und gib die
entsprechenden Schnittpaare aus (d.h. wenn für (R$'$, [$y_1,\ y_2$]) in Y gilt: $y \in [y_1,\ y_2]$. Prüfe, ob R$' \not=$R ist, wenn
ja, gib (R$'$, R) aus.) --\textsc{Search}
\end{enumerate}
\subsection{Der Segment-Baum}
Der Segment-Baum dient also der Verwaltung von Intervallen, wobei die üblichen Operationen \textsc{Insert}, \textsc{Delete} und
\textsc{Search} mögloch sind.

Mit der Eingabe ist die Menge der $y$-Werte gegeben, welche nach steigendem $y$ geordnet wird. Damit ergibt sich die Folge $y_0,\ y_1,\
\ldots,\ y_n$ bzw. die Folge von Elementarintervallen $[y_0,\ y_1],\ [y_1,\ y_2],\ \ldots,\ [y_{n-1},\ y_n]$. Durch das folgende
Beispiel wird dies vielleicht klarer.

Seien die $y$-Werte 0, 1, 3, 6, 9 als sogenannte Roatorpunkte \textcolor{red}{Stimmt das?}, die zu den Elementarintervallen
[0, 1], [1, 3], [3, 6] und [6, 9] führen und die Rechtecke wie in Skizze \ref{031203f} gegeben. Daraus resultiert dann der Baum wie in
Abbildung \ref{031203g}.

\begin{figure}[H]
\centering
\input{031203f.latex}
\caption{Beispiel für überschneidung von Rechtecken}
\label{031203f}

\end{figure}

\begin{figure}[H]
\centering
\input{031203g.latex} \input{031203h.latex}
\caption{Beispiel für einen Segment-Baum}
\label{031203g}

\end{figure}

 a) Y=Y $\cup$ \{(R$_1$, [1, 6])\}=\{(R$_1$, [1, 6])\} \hspace{2em} b) Y=\{(R$_1$, [1, 6]), (R$_2$, [0, 9]) \}

\begin{figure}[H]
\centering
\input{031203i.latex} \hspace{1cm} \input{031203j.latex} \hspace{1cm} \input{031203k.latex}
\caption{Beispiel für einen Segment-Baum im Aufbau}
\label{031203i}

\end{figure}

Aus Abbildung \ref{031203f} geht der Baum hervor, der rechts in der Abbildung \ref{031203g} zu sehen ist. Der dritte Wert jedes Tupels
in jedem Knoten
gibt dabei das Maximum der Tupel im linken Teilbaum dieses Knotens an. Etwas verständlicher, aber formal falsch, ist folgende
Formulierung: Der dritte Wert jedes Knotens gibt den maximalen Wert seines linken Teilbaumes an.
Der linke Teil von Abbildung
\ref{031203g} deutet an, wie die Struktur im allgemeinen Fall bzw. als "`Leerstruktur"' aussieht. In der untersten Abbildung zeigen a) und
b) wie die Struktur aufgebaut wird. Teil c) verdeutlicht nochmal die überlappung der Intervallgrenzen (im obigen Beispiel!) und macht
gleichzeitig folgenden Satz klar.

\begin{satz}
Für jedes Rechteck gibt es pro Level maximal zwei Einträge
\end{satz} 

Doch wie funktioniert die Suche und wieviel Rechenzeit wird größenordnunsmäßig benötigt, um alle überlappungen herauszufinden und
auszugeben?

Falls nach (Punkt, x, y, R) gesucht wird, wird im im Baum getestet, in welchen Intervallen (x,y) liegt. 

\textcolor{red}{Stimmt das so? Im
Skript stand folgendes: (Punkt, x, y, R) $\rightarrow$ Suche nach y=(4,5), merke die Beschriftungen auf dem Suchpfad von Rechtecken,
OUTPUT (R$_4$, R$_1$) (R$_4$, R$_2$) (R$_4$, R$_3$).
Wenn aber nach überschendiungen im Intervall y=[4, 5] gesucht wird fehlen doch noch drei überschneidungen, ich werde daraus nicht
schlau. }

Für die Suche werden die Beschriftungen auf dem Suchpfad von Rechtecken gemerkt. Für den Baum gilt der Satz über höhenbalancierte
Suchbäume. Damit funktioniert das Suchen in O($\log m$) Zeit, und die Ausgabe hat die Größe O($k+ \log m$), dabei ist $k$ die Anzahl
der Schnitte. Für das Insert wird eine passende rekursive Prozedur geschrieben, die den Suchpfad abtestet und dei Eintragungen
vornimmt; dies geht in ebenfalls in O($\log m$) Zeit.
Damit haben wir einen output-sensitiven Algorithmus mit einer Gesamtlaufzeit von O($k +m \log m$), dazu siehe auch den Anfang des
Skriptes und Anhang \ref{planesweep}.

\chapter{Verwaltung von Mengen -- kompliziertere Datenstrukturen}
Die Operationen \textsc{Make-Heap}(), \textsc{Insert}(H, x), \textsc{Min}(H) und \textsc{Extract-Min}(H) sollten an dieser Stelle
hinreichend bekannt sein, vielleicht ist auch \textsc{Union}(H$_1$, H$_2$) schon bekan\-nt. Diese für die Verwaltung von Mengen wichtige
Operation vereinigt, wie der Name bereits sagt, zwei Heaps H$_1$ und H$_2$ zu einem Heap H. H enthält die Knoten von H$_1$ und H$_2$,
die bei dieser Operation zerstört werden (Stichwort "`Mergeable Heaps"'). Zusätzlich wird noch \textsc{Decrease-Key}(H, x, k) benutzt.

Bei Binär-Heaps funktionieren \textsc{Make-Heap} (\textsc{Build-Heap}) und \textsc{Min} in O(1) und \textsc{Insert} und
\textsc{Extract-Min} in O($\log n$). Allerdings benötigt \textsc{Union} O(n). Damit unter\-stützen Binär-Heaps kein \textsc{Union}!
Diese wichtige Operation wird aber von \textbf{Binomial-Heaps} unterstützt.

\section{Binomialbäume}
\begin{definition}[Binomialbäume]
\begin{enumerate}
\item B$_0$:= \textcolor{red}{?kleiner Kreis} ist Binomialbaum
\item B$_k$ $\rightarrow$ B$_{k+1}$ (k$\geq$0)
  \begin{figure}[H]
  \centering
  \input{info3_own001.latex} 
  %\caption{Beispiel für einen Segment-Baum im Aufbau}
  %\label{031203i}
  
  \end{figure}
\end{enumerate}
\end{definition}
\subsubsection{Beispiel}
\begin{figure}[H]
  \centering
  \input{info3_own002.latex} 
  %\caption{Beispiel für einen Segment-Baum im Aufbau}
  %\label{031203i}
  
  \end{figure}
  Im Beispiel sieht man von links nach rechts B$_0$, B$_1$, B$_2$ und den B$_3$.

Bei Bäumen gilt es verschiedene Typen zu unterscheiden, es gibt Wurzelbäume, geord\-nete Bäume und Positionsbäume.
Sei nun mit $(a,nil,b)$ der Baum bezeichnet, in dem in den Söhnen der Wurzel die Werte $(a,nil,b)$ in dieser Reihenfolge von rechts nach
links gespeichert sind.

Dann gilt für den Wurzelbaum, daß $(a,nil,b)=(nil,a,b)$ ist. In einem geordneten Baum hingegen ist
$(a,b,c)\not=(b,a,c)$ weil die Reihenfolge der Söhne relevant ist. In einem Positionsbaum wieder ist
$(a,nil,b)\not=(nil,a,b)$ weil danach geguckt wird, welche Platzstellen belegt sind und welche nicht.

\begin{definition}[Binomialbäume]
B$_0$ ist der Baum mit genau einem Knoten. Für k$\geq$0 erhält man B$_{k+1}$ aus B$_k$ dadurch, daß man zu einem
B$_k$ einen weiteren Sohn an seine Wurzel als zusätzlichen linkesten Sohn hängt, dieser ist ebenfalls Wurzel eines
B$_k$.
\end{definition}

\begin{definition}
\begin{itemize}
\item Ein freier Baum ist eine ungerichteter zusammenhängender kreisfreier Graph.
\item Ein Wurzelbaum (B,x$_0$) ist ein freier Baum B mit dem Knoten x$_0$ als "`ROOT"'.
\item Ein Wald ist ein ungerichteter kreisfreier Graph. 
\end{itemize}
\end{definition}

\section{Binomial-Heaps}  
\begin{satz}
Im Zusammenhang mit Binomial-Heaps wird auch der folgende Satz noch benutzt werden:
\[\binom{n}{i}=\binom{n-1}{i}+\binom{n-1}{i-1}\]
\end{satz}

\begin{satz}[über die eindeutige g-adische Darstellung natürlicher Zahlen]
(für g $\in \mathbb{N} : g\geq 2$)

$n \in \mathbb{N} \rightarrow$ Darstellung eindeutig, $n=\sum_{i=0}^m a_i g^i, a_i \in \mathbb{N}$
\end{satz}
Mit g=2 erhalten wir die Binärdarstellung natürlicher Zahlen.

\begin{definition}[Binomialheap]  
Ein Binomialheap ist ein Wald von Binomialbäumen, der zusätzlich folgende Bedingung\-en erfüllt:
\begin{enumerate}
\item Heap-Eigenschaft = alle Bäume sind heap-geordnet, d.h. der Schlüsselwert jedes Knotens ist größer oder gleich dem
Schlüsselwert des Vaters
\item Einzigkeitseigenschaft = Von jedem Binomialbaum ist maximal ein Exemplar da, d.h. zu einem gegebenen Wurzelgrad
gibt es maximal einen Binomialbaum im Heap.
\end{enumerate}
\end{definition}

\begin{satz}[Struktursatz über Binomialbäume]
\begin{enumerate}
\item Im Baum gibt es 2$^n$ Knoten
\item Die Höhe des Baumes B$_n$ ist n
\item Es gibt genau $\binom{n}{i}$ Knoten der Tiefe $i$ im Baum B$_n$
\item In B$_n$ hat die Wurzel den maximalen Grad (degree) und die Söhne sind von rechts nach links nach wachsendem Grad
geordnet.
\end{enumerate}
\end{satz}

\begin{definition}
$d(n,i):=$ Anzahl der Knoten der Tiefe i im B$_n$
\end{definition}

\begin{beweis}
\begin{enumerate}
\item I.A. $n=0$ trivial, $n-1 \Rightarrow n$, $2^{n-1}+2^{n-1}=2^n$ 
\item $n=0$ trivial, $n-1 \Rightarrow n$ trivial
\item 

I.A. trivial n=0

I.S. $n-1 \Rightarrow n$ Setzen die Gültigkeit von 3) für $n-1$ alle $i$ voraus und zeigen sie dann für $n$ und alle
$i$

$d(n,i)=d(n-1,i)+d(n-1,i-1)=_{I.V.} \binom{n-1}{i}+ \binom{n-1}{i-1}=_{HS} \binom{n}{i}$
\end{enumerate}
\end{beweis}

Es gibt zu $n \in \mathbb{N}$ \underline{genau} (von der Struktur her) einen Binomialheap, der genau die $n$ Knoten
speichert.
 
\begin{beweis}[Eindeutigkeit der Binomialdarstellung von $n$]
$n=12$ 
\end{beweis} 

\begin{figure}[H]
  \centering
  \input{info3_own003.latex} 
  %\caption{Beispiel für einen Segment-Baum im Aufbau}
  %\label{031203i}
  
  \end{figure}
  
Für $n$ Werte haben wir O$(\log n)$ (genau $\lfloor \log n \rfloor +1$) Binomialbäume. Dies ist auch intuitiv
verständlich, da jede Zahl $n \in \mathbb{Z}$ zur Darstellung $\log n$ Stellen braucht. Die Eindeutigkeit der Struktur
des Heaps wird auch hier wieder klar, da jede Zahl ($\in \mathbb{Z}$) eindeutig als Binärzahl dargestellt werden kann.
Was passiert nun aber, wenn zwei Heaps zusammengeführt werden?

\section{Union}
Bei der Zusammenführung (\textsc{Union}) zweier Heaps H$_1$ und H$_2$ wird ein neuer Heap H geschaffen, der alle Knoten
enthält, H$_1$ und H$_2$ werden dabei zerstört.

%\begin{figure}[H]
%  \centering
%  \input{info3_own004.latex} 
%  %\caption{Beispiel für einen Segment-Baum im Aufbau}
%  %\label{031203i}
%  
%  \end{figure}

Leider ist wegen eines technischen Problems die folgende Darstellung nicht ganz kor\-rekt, der Kopf jedes Heaps zeigt nur
auf das erste Element der Wurzelliste.

\begin{bundle}{Kopf H$_1$}
\chunk{7 $\rightarrow$ } \chunk{\begin{bundle}{2} \chunk{\begin{bundle}{25} \chunk{30} \end{bundle}} \chunk{3} \end{bundle}} 
\end{bundle} \hspace{15mm}
\begin{bundle}{Kopf H$_2$}
\chunk{16 $\rightarrow$ } \chunk{\begin{bundle}{18 $\rightarrow$}\chunk{45} \end{bundle}}\chunk{\begin{bundle}{4}
\chunk{\begin{bundle}{10} \chunk{11} \end{bundle}} \chunk{21} \end{bundle}} 
\end{bundle}\hspace{15mm}
\begin{bundle}{Kopf H}
\chunk{\begin{bundle}{7 $\rightarrow$ } \chunk{\begin{bundle}{18} \chunk{45} \end{bundle}} \chunk{16} \end{bundle}}
\chunk{\begin{bundle}{2}\chunk{\begin{bundle}{4} \chunk{\begin{bundle}{10} \chunk{11} \end{bundle}} \chunk{21} 
\end{bundle}} \chunk{\begin{bundle}{25} \chunk{30} \end{bundle}} \chunk{3}\end{bundle}}
\end{bundle}

Beim \textsc{Union} werden zuerst die Wurzellisten gemischt. Dann werden, angefangen bei den kleinsten,
Binomialbäume mit gleicher Knotenanzahl zusammengefaßt. Es werden also zwei B$_i$ zu einem B$_{i+1}$
transformiert.

Jeder Knoten im Baum hat dabei folgende Felder:
\begin{itemize}
\item Einen Verweis auf den Vater
\item Den Schlüsselwert
\item Den Grad des Knotens
\item Einen Verweis auf seinen linkesten Sohn
\item Einen Verweis auf seinen rechten Bruder
\end{itemize}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Union\textnormal{(H$_1$, H$_2$)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Union\textnormal{(H$_1$, H$_2$)}}, gobble=1]{Union(H1,H2)}
 H:= Make-Heap()
 Head[H]:=Merge(H$_1$, H$_2$)
 Zerst~ö~re die Objekte H$_1$, H$_2$ (nicht die entspr. Listen)
 if Head[H]=Nil then
   return H
 prev-x:=Nil
 x:=Head[H]
 next-x:=reB[x]
 while next-x $\not=$ Nil do
   if (degree[x] $\not=$ degree[next-x] OR (reB[next-x] $\not=$ Nil AND 
   degree[reB[next-x]]=degree[x])) then
     prev-x:=x
     x:=next-x
   else if (key[x]$\leq$key[next-x]) then
     reB[x]:=reB[next-x]
     Link[next-x, x]
     else if (prev-x=nil) then
       Head[H]:=next-x
       else reB[prev-x]:=next-x
       Link[x, next-x]
       x:=next-x
   next-x:=reB[x]
 return H    
\end{lstlisting}
\end{Algorithmus}

Priority-Queues=\{\textsc{Make-Heap}, \textsc{Insert}, \textsc{Union}, \textsc{Extract-Max}, \textsc{Decrease-Key},
\textsc{Min}\};
\textsc{Make-Heap} erfordert nur O(1), der Rest geht in O($\log n$).
 

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Insert\textnormal{(H, x)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Insert\textnormal{(H, x)}}, gobble=1]{Insert(H,x)}
 H$_1$:= Make-Heap()
 p[x]:=Nil
 Sohn[x]:=Nil
 reB[x]:=Nil
 degree[x]:=0
 Head[H$_1$]:=x
 Union(H, H$_1$) 
\end{lstlisting}
\end{Algorithmus}

Wie funktioniert nun das Entfernen des Minimums? Zuerst wird das Minimum rausge\-worfen, dann werden die Söhne des
betroffenen Knotens in umgekehrter Ordnung in eine geordnete Liste H$_1$ gebracht und anschließend werden mittels
\textsc{Unon} H und H$_1$ zusammengemischt. Dies klappt logischerweise in O($\log n$), da die Wuzellisten durch O($\log
n$) in ihrer Länge beschränkt sind (bei n Werten im Heap). Damit arbeitet auch \textsc{Extract-Min} in O($\log n$).

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Decrease-Key\textnormal{(H, x, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Decrease-Key\textnormal{(H, x, k)}},gobble=1]{DecreaseKey(H,x,k)}
 if k$>$key[x] then
   Fehler ($\rightarrow$ Abbruch)
 key[x]:=k
 y:=x
 z:=p[y]
 while (z$\not=$ Nil AND key[y]$<$key[z]) do
 vertausche key[y] und key[z]
 y:=z
 z:=p[y]    
\end{lstlisting}
\end{Algorithmus}

\begin{satz}
Die linker-Sohn-rechter-Bruder-Darstellung für Binomial-Heaps ermöglicht in der ange\-gebenen Form für
Prioritätswarteschlangen logarithmische Laufzeit.
\end{satz}
Merke: Die Länge der Wurzelliste bei einem Binomial-Heap ist logarithmisch.

Bisher war immer leicht verständlich, wie die Schranke für die Komplexität eines Algorithmus zustande kam, doch bei
komplizierteren Datenstrukturen ist das nicht mehr immer so einfach. Manche mehrfach ausgeführte Schritte erfordern am
Anfang viel Operationen, am Ende aber wenig. Dies erschwert die Angabe einer Komplexität und deswegen werden im Folgenden drei Methoden zur
Kostenabschätzung vorgestellt.

\section{Amortisierte Kosten}
Es gibt drei Methoden um amortisierte Kosten abzuschätzen, eine davon wurde bereits im Abschnitt zur konvexen Hülle
benutzt. 
\begin{itemize}
\item die Aggregats-Methode
\item die Guthaben-Methode
\item die Potential-Methode
\end{itemize} 
Ziel aller drei Methoden ist es, auf die Kosten einer Operation im gesamten Algorithmus zu kommen. Dies bietet sich z.B.
an, falls eine Operation im worst-case in einem Schritt des Algorithmus eine Komplexität von O($n$) haben kann,
aber auch über den gesamten Algorithmus hinweg bei $n$ Schritten nicht mehr als O($n$) Aufwand erfordert. Dann kommt
mittels der Analyse zu den amortisierten Kosten O(1), zu einer Art Durchschnittskosten im Stile von $\frac{T(n)}{n}$.

Bei den letzten beiden Methoden erhält man Kosten AK($i$)=AK$_i$ für die Operationen.
Die aktuellen (wirklichen) Kosten werden
mit K($i$)=K$_i$ bezeichnet und werden können relativ frei gewählt werden. In den meisten Fällen ist K$_i$=O(1),
dabei muß aber gelten:
\[\sum_{i=1}^n AK_i \geq \sum_{i=1}^n K_i\]

Damit kann gefolgert werden, daß jede obere Schranke für die amortisierten Kosten AK auch eine obere Schranke
für die interessierenden tatsächlichen Kosten ist.

Im Fall des Graham-Scans ergeben sich amortsierte Kosten von O(1) für das Multi-Pop, obwohl es schon in einem Schritt
O(n) dauern kann. Mittels dieser amortisierten Kosten erhält man $\sum AK_i=O(n) \Rightarrow \sum K_i=O(n)$, also
letztlich, daß der Algorithmus in O($n$) funtkioniert.

\subsection{Die Potentialmethode}
Die Idee ist, daß jede Datenstruktur DS mit einem Potential $\Phi$ bezeichnet wird. Es ist DS(0)=DS$_0$ und nach der i-ten
Operation DS(i)=DS$_i$. Das Potential wird definiert als $\Phi(DS(i))=:{\Phi}_i=\Phi(i)$, wobei es natürlich
eine Folge von mindestens $i$ Operationen geben muß.

Die Kosten ergeben sich als Differenz der Potentiale der Datenstruktur von zwei aufeinander folgenden Zeitpunkten. Die
Kosten der i-ten Operation ergeben sich als ${\Phi}_i-{\Phi}_{i-1}$.

Hierbei muß \[\sum_{i=1}^n AK_i=\sum_{i=1}^n (K_i+{\Phi}_i-{\Phi}_{i-1})\] gelten.

Dies (Teleskopsumme, s. Anhang \ref{Teleskopsumme}) läßt sich zu \[\sum_{i=1}^n AK_i=\sum_{i=1}^n
K_i+{\Phi}_n-{\Phi}_0\] umformen.

Als drittes muß (${\Phi}_n-{\Phi}_0 \geq 0$) sein, speziell ${\Phi}_n \geq 0$, falls ${\Phi}_0=0$. Damit folgt
$\sum AK_i \geq \sum K_i$.

\subsubsection{Beispiel Graham-Scan}

\begin{definition}
$\Phi \mbox{(DS(}i\mbox{))}$=Anzahl der Elemente im Stapel, ${\Phi}_i \geq 0, {\Phi}_0=0$
\end{definition}

Die amortisierten Kosten für  \textsc{Push} sind AK$_i$=K$_i$+${\Phi}_i-{\Phi}_{i-1}=1+1=2$
und für  \textsc{Pop} AK$_i$=K$_i$+${\Phi}_i-{\Phi}_{i-1}=1+(-1)=0$.

Woher ergibt sich das? Nun \textsc{Push} heißt ein Element einfügen $\rightarrow {\Phi}_i$ hat n Elemente,
${\Phi}_{i-1}$
 hat n-1 Elemente $ \rightarrow {\Phi}_i-{\Phi}_{i-1}$=1. Bei \textsc{Pop} hat umgekehrt ${\Phi}_{i-1}$ n Elemente und
 ${\Phi}_i $ n-1
Elemente, damit ist dort ${\Phi}_i-{\Phi}_{i-1}=n-1-n=-1$.  

\section{Fibonacci-Heaps}
Bei den Binomial-Heaps können wegen der Ordnung alle Söhne auf bequeme Weise angesprochen werden, bei den
Fibonacci-Heaps fehlt diese Ordnung. Dafür hat jeder Knoten Zeiger auf den linken und den rechten Bruder, die Söhne
eines Knotens sind also durch eine doppelt verkette Liste verknüpft. Mit min[H] kann auf das Minimum der Liste
zugegriffen werden, n[h] bezeichnet die Anzahl der Knoten im Heap. Es gilt weiterhin, daß alle Bäume die
Eigenschaften eines Heaps erfüllen.

Wie lange dauern Operationen mit Fibonacci-Heaps? Das Schaffen der leeren Struktur geht wieder in O(1), da zusätzlich
zu \textsc{Make-Heap} nur noch n[H]=0 und min[H]=Nil gesetzt werden muß.

\subsection{Einfache Operationen}
\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Union\textnormal{(H$_1$, H$_2$)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Union\textnormal{(H$_1$, H$_2$)}},gobble=1]{Union2(H1,H2)}
 H:=Make-Heap
 min[H]:=min[H$_1$, H$_2$]
 Verknüpfe die Wurzellisten von H$_2$ und H miteinander $\vartriangleleft$ ~\mbox{\textcolor{red}{Fehl?}}~
 if (min[H$_1$]=Nil) OR (min[H$_2\not=$Nil] AND min[H$_2$]<min[H$_1$])
   then min[H]:=min[H$_2$]
 n[H]:=n[H$_1$]+n[H$_2$]
 Zerst~ö~re die Objekte H$_1$ und H$_2$
 Output H     
\end{lstlisting}
Formal korrekt muß es nicht min[H], sondern key[min[H]] heißen. Auf diese korrekte Schreibweise wurde
zugunsten einer klareren Darstellung verzichtet.
\end{Algorithmus}
Das Aufschneiden der Wurzellisten und Umsetzen der Zeiger (Zeile 3) geht in O(1). Im Allgemeinen wird der Ansatz
verfolgt, Operationen so spät wie möglich auszuführen. Bei \textsc{Union} wird nicht viel getan, erst bei
\textsc{Extract-Min}.

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Insert\textnormal{(H, x)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Insert\textnormal{(H, x)}}, gobble=1]{Insert2(H,x)}
 degree[x]:=0
 p[x]:=Nil
 Sohn[x]:=Nil
 li[x]:=x
 re[x]:=x
 mark[x]:=false ~\hspace{10mm}~ $\vartriangleleft$ ~\mbox{Bis hier neue Wurzelliste nur aus x}~
 Verknüpfe mit Wurzelliste H
 if (min[H]=Nil) OR (key[x]<key[min[H]]) then
   min[H]:=x
 n[H]:=n[H]+1   
\end{lstlisting}
\end{Algorithmus}

\subsection{Anwendung der Potentialmethode}
Jetzt wird die vorher erläuterte Potentialmethode benutzt, um die Kosten zu bestimmen, dabei ist
\begin{itemize}
\item $\Phi$(H)=$t$(H)+$2m$(H) mit
\item $t$[H] als der Anzahl der Knoten in der Wurzelliste und
\item $m$[H] als der Anzahl der markierten Knoten
\end{itemize}
Wenn nachgewiesen werden soll, daß die Potentialfunktion für Fibonacci-Heaps akzepta\-bel (zweckmäßig und zulässig)
ist, müssen die Bedingungen erfüllt sein. In \cite{ottmann} wird eine andere Potentialmethode benutzt.

Für die Kosten AK$_{i, \textsc{Insert}}$ des \textsc{Insert} gilt:
\begin{itemize}
\item $t$(H$'$)=$t$(H)+1
\item $m$(H$'$)=$m$(H)
\item ${\Phi}_i-{\Phi}_{i-1}=t$(H$'$)$+2m$(H$'$)-$t$(H)-$2m$(H)=1
\item K$_i=1$
\item AK$_i$=K$_i$+${\Phi}_i-{\Phi}_{i-1}$=1+1=2 $\in$ O(1)
\end{itemize}

\begin{satz}
\textsc{Insert} hat die amortisierten Kosten O(1)
\end{satz}

Die Kosten von \textsc{Union} sind:
\[\Phi(\mbox{H})-\Phi(\mbox{H}_1)-\Phi(\mbox{H}_2)=t(\mbox{H})-t(\mbox{H}_1)-t(\mbox{H}_2)+2m(\mbox{H})-2m(\mbox{H}_1)-
2m(\mbox{H}_2)=0+0=0\]

\begin{satz}
\textsc{Union} hat die amortisierten Kosten O(1), AK$_{i, \textsc{Union}}$=K$_i$+0=1+0 $\in$ O(1).
\end{satz}

Mit diesen beiden sehr schnellen Operationen sind Fibonacci-Heaps sinnvoll, wenn die genannten Operationen sehr häufig
vorkommen, ansonsten ist der Aufwand für die komplizierte Implementierung zu groß.

Nochmal zurück zur amortisierten Kostenanalyse:
Hier wird fast nur die Potentialme\-thode benutzt, andere Methoden sind für uns nicht wichtig. Wesentlicher Bestandteil
der Potentialmethode ist die geschickte Definition der Potentialfunktion, dabei hat man viel Freiheit, nur die Bedingung $\Phi
(D_n) \geq \Phi(D_0)=_{meist}0$ muß erfüllt sein.

Die amortisierten Kosten ergeben sich als die Summe aus den aktuellen Kosten und dem Potential der Datenstruktur. Die
Bedingung ist dazu da, daß die Kostenabschätzung möglich, aber auch brauchbar ist.

\subsection{Aufwendige Operationen}
\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Extract-Min\textnormal{(H)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Extract-Min\textnormal{(H)}}, gobble=1]{Extract-Min(H)}
 z:=min[H]
 if (z$\not=$Nil)
   für jeden Sohn x von z do
     füge x in die Wurzelliste ein
     p[x]:=Nil
   entferne z aus der Wurzelliste
   if z=re[z] then
     min[H]:=Nil
   else
     min[H]:=re[z]
     Konsolidiere(H)
   n[H]:=n[H]-1
 return z   
\end{lstlisting}
Beim Konsolidieren wird eine Struktur ähnlich wie bei den Binär-Heaps geschaffen, dazu gleich mehr.
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Link\textnormal{(H, x, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Link\textnormal{(H, x, k)}}, gobble=1]{Link(H, x, k)}
 Entferne y aus der Wurzelliste
 Mache y zum Sohn von x, degree[x]:=degree[x]+1
 Mark[y]:=0
\end{lstlisting}
Der Grad von x wird erhöht, da er die Anzahl der Söhne von x zählt, es ist klar, daß dies immer in O(1) geht. In
\cite{ottmann} ist hier ein kleiner Fehler.
\end{Algorithmus}

Beim Konsolidieren besteht die Lösung in Verwendung eines Rang-Arrays.
Es wird n=n[H] gesetzt und D(n) als der maximale Wurzelgrad definiert, dabei ist D(n)$\in$O($\log n$). Das Array bekommt 
die Größe D(n). Dann wird die Wurzelliste durchgegangen und an der entsprechenden Position im Array gespeichert. Falls
an dieser Stelle schon ein Eintrg vorhanden ist, wird ein \textsc{Link} durchgeführt. Dabei wird das Größere an das
Kleinere (bzgl. des Wurzelwertes\textcolor{red}{Was denn genau} angehängt. Da der Wurzelgrad wächst, muß der Eintrag
im Feld dann eins weiter. So wird die gesamte Wurzelliste durchgegangen und es kann am Ende keine zwei Knoten mit
gleichem Wurzelgrad geben. Dazu muß immer das Minimum aktualisiert werden.

\subsubsection{Analyse}
Wir haben die amortisierten Kosten, die Behauptung ist: O(D($n$))=O(log n). Die aktuel\-len Kosten sind die Kosten für den
Durchlauf duch die Wurzelliste, alles andere ist hier vernachlässigbar. Also sind die aktuellen Kosten
$=t(\mbox{H})+\mbox{D}(n)=\mbox{O}(t(\mbox{H})+\mbox{D}(n))$ und die Potentialdifferenz ist $\leq
\mbox{D}(n)-t(\mbox{H})$

Man zeigt schließlich, daß die amortisierten Kosten $\in$ O(D($n$)) liegen.

\textcolor{red}{Der Abschnitt ist scheiße, verbessern!}

\subsection{Weitere Operationen}
\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Decrease-Key\textnormal{(H, x, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Cut, Korrigiere, Cascading, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Decrease-Key\textnormal{(H, x, k)}}, gobble=1]{Decrease-key2(H, x, k)}
 if k$>$key[x] then
   Fehler!
 key[x]:=k  ~\hspace{10mm}~ $\vartriangleleft$ ~\mbox{Weiter, falls F-Heap kaputt}~ 
 y:=p[x]
 if (y$\not=$Nil AND key[x]$<$key[y] then
   Cut(H, x, k)
   Cascading-Cut(H, y)
 if key[x]$<$key[min[H]] then
   min[H]:=x  
\end{lstlisting}
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Cut\textnormal{(H, x, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Cut, Korrigiere, Cascading, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Cut\textnormal{(H, x, k)}}, gobble=1]{Cut(H, x, k)}
 Entferne x aus der Sohnliste von y
 degree[y]:=degree[y]-1
 F~ü~ge x zur Wurzelliste von H hinzu
 p[x]:=Nil
 Mark[x]:=0 
\end{lstlisting}
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Cascading-Cut\textnormal{(H, y)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Cut, Korrigiere, Cascading, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Cascading-Cut\textnormal{(H, y)}}, gobble=1]{Cascading-Cut(H, y)}
 z:=p[y]
 if (z$\not=$Nil then)
   if Mark[y]=0 then
     Mark[y]:=1
   else
     Cut(H, y, z)
     Cascading-Cut(H, z)  
\end{lstlisting}
\end{Algorithmus}

Im Beispiel markiert $\star$, daß ein Knoten schon einen Sohn verloren hat.

\begin{bundle}{$\bullet$}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{$\bullet$}\chunk{$\bullet$}\chunk{$\bullet$}\end{bundle}}\end{bundle}} \chunk{\begin{bundle}{$\bullet$} \chunk{\begin{bundle}{$\bullet$} \chunk{$\bullet$} \end{bundle}}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{${\bullet}^{\star}$} \chunk{\begin{bundle}{30$^{\star}$}
\chunk{40}\end{bundle}} \chunk{$\bullet$}\end{bundle}}\end{bundle}} \end{bundle}} 
\end{bundle} \hspace{5mm}$\longrightarrow$\hspace{5mm}
\begin{bundle}{$\bullet$}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{$\bullet$}\chunk{$\bullet$}\chunk{$\bullet$}\end{bundle}}\end{bundle}} \chunk{\begin{bundle}{$\bullet$} \chunk{\begin{bundle}{$\bullet$} \chunk{$\bullet$} \end{bundle}}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{${\bullet}^{\star}$} \chunk{\begin{bundle}{30$^{\star}$}
\chunk{25}\end{bundle}} \chunk{$\bullet$}\end{bundle}}\end{bundle}} \end{bundle}} 
\end{bundle} \hspace{5mm}$\longrightarrow$\hspace{5mm}
\begin{bundle}{$\bullet$}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{$\bullet$}\chunk{$\bullet$}\chunk{$\bullet$}\end{bundle}}\end{bundle}} \chunk{\begin{bundle}{$\bullet$} \chunk{\begin{bundle}{$\bullet$} \chunk{$\bullet$} \end{bundle}}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{${\bullet}^{\star}$} \chunk{30$^{\star}$}
\chunk{$\bullet$}\end{bundle}}\end{bundle}} \end{bundle}} 
\end{bundle} 25\hspace{5mm}$\longrightarrow$\hspace{5mm}
\begin{bundle}{$\bullet$}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{$\bullet$}\chunk{$\bullet$}\chunk{$\bullet$}\end{bundle}}\end{bundle}} \chunk{\begin{bundle}{$\bullet$} \chunk{\begin{bundle}{$\bullet$} \chunk{$\bullet$} \end{bundle}}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{${\bullet}^{\star}$}
\chunk{$\bullet$}\end{bundle}}\end{bundle}} \end{bundle}} 
\end{bundle} 25 30\hspace{5mm}$\longrightarrow$\hspace{5mm}

$\longrightarrow$
\begin{bundle}{$\bullet$}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{$\bullet$}\chunk{$\bullet$}\chunk{$\bullet$}\end{bundle}}\end{bundle}} \chunk{\begin{bundle}{$\bullet$} \chunk{\begin{bundle}{$\bullet$} \chunk{$\bullet$} \end{bundle}}
\chunk{$\bullet$}\end{bundle}} 
\end{bundle} 25 30
\begin{bundle}{$\bullet$} \chunk{$\bullet$}\end{bundle}

\subsubsection{Analyse}
Die aktuellen Kosten für das \textsc{Cut} liegen in O(1), was gilt für das \textsc{Cascading-Cut}?

\[
\begin{array}{ccl}
\mbox{ein \textsc{Cut} dabei:} & - \mbox{ eine Markierung} & -2\\
 &  + \mbox{ eine neue Wurzel} & +1\\
 &  + \mbox{ O(1) für das \textsc{Cut} selbst} & +1\\
 &  & =0
\end{array}\]

Damit hat \textsc{Decrease-Key} O(1) amortisierte Kosten. Denn das einmalige Abschnei\-den kostet O(1) und die Kaskade von
\textsc{Cut}s kostet wegen der Markierung nichts. \textcolor{red}{Sollte das etwas O/Null heißen}

\begin{satz}[Lemma 1 über F-Heaps]
Sei $v$ Knoten eines F-Heaps. Ordnet man (in Gedanken) die Söhne von $v$ in der zeitlichen Reihenfolge, in der sie an
$v$ angehängt wurden, so gilt: der $i$-te Sohn von $v$ hat mindestens den Grad $(i-2)$. 
\end{satz}
Dies ist der Grund für die Leistungsfähigkeit und den Namen der F-Heaps

\begin{beweis}
Damit der $i$-te Sohn auftaucht, müssen $v$ und dieser Sohn den Rang (i-1) gehabt haben (wenigstens Rang i, falls beide
denselben Rang hatten). Wenn das unklar ist, sollte man sich das Konsolidieren noch einmal anschauen. Danach kann dieser
Sohn wegen der Markierungsvorschrift maximal einen Sohn verlieren, also ist der Rang mindestens $(i-2)$. 
\end{beweis}

\begin{satz}[Lemma 2 über F-Heaps]
Jeder Knoten $v$ vom Rang (Grad) $k$ eines F-Heaps ist Wurzel eines Teilbaumes mit mindestens F$_{k+2}$ Knoten.
\end{satz}

\begin{definition}
F$_0:=0$, F$_1:=1$, F$_{k+2}$:=F$_{k+1}$+F$_{k}$
\end{definition}

\begin{satz}
F$_{k+2} \geq {\phi}^k$, $\phi=\frac{1+\sqrt{5}}{2}\approx 1,6$ 
\end{satz}

\begin{satz}[Hilfssatz]
$\forall k \geq 0 \mbox{ }gilt \mbox{ } F_{k+2}=1+\sum_{i=0}^k F_i$
\end{satz}

\begin{beweis}[Beweis vom Hilfssatz durch Induktion über $k$]
$k=0 : F_2=1+\sum_{i=0}^0 F_i = 1+ F_0 =1$

\noindent I.V. $F_{k+1}=1+\sum_{i=0}^{k-1} F_i$

\noindent $F_{k+2}=F_{k}+F_{k+1}=F_k + 1 + \sum_{i=0}^{k-1} F_i = 1+\sum_{i=0}^k F_i$
\end{beweis}

\begin{beweis}[Lemma 2]
S$_k$:=Minimalzahl von Knoten, die Nachfolger eines Knotens $v$ vom Rang $k$ sind ($v$ selbst mitgezählt).

\noindent Habe v den Rang 0 (keinen Sohn), dann ist S$_0$=1 und S$_1$=2. Ab $k \geq 2$ gilt Lemma 1:
\[\mbox{S}_k \geq 2 + \sum_{i=0}^{k-2} \mbox{S}_i \mbox{ für }k \geq 2\]
Jetzt werden die Söhne von $v$ wieder gedanklich in der Reihenfolge des Anliegens geord\-net, und zusammen mit

\[\mbox{F}_{k+2}=1+\sum_{i=0}^k F_i=2+\sum_{i=2}^k F_i\] gilt \[\mbox{S}_k \geq \mbox{F}_{k+2} \mbox{ für } k \geq 0
\mbox{ (Beweis durch Induktion)}\]

Sei $v^k$ der Knoten $v$ mit dem Rang $k$, dann gilt insgesamt: Anzahl Nachfolger$(v^k) \geq \mbox{S}_k \geq
\mbox{F}_{k+2} \geq {\phi}^k \Rightarrow \mbox{Lemma 2}$ 
\end{beweis}
Daraus folgt, daß der maximale Grad (Rang) eines Knotens in einem F-Heap mit $n$ Knoten D($n$) $\in$ O($\log n$ ) ist.

\begin{beweis}
Sei $v$ beliebig im F-Heap gewählt und $k=$Rang$(v)$, dann ist $n \geq$Anzahl der Nachfolger$(v)$ $\geq {\phi}^k$, also
$k \leq \log_{\phi} n \in \mbox{ O}(\log n)$. Da $v$ beliebig ist, gilt die Ungleichung auch für den maximalen Rang
$k$. 
\end{beweis}
F-Heaps sind Datenstrukturen, die vor allem zur Implementierung von Prioritätswarte\-schlangen geeignet sind.
\textsc{Extract-Min} braucht amortisiert logarithmische Zeit, alle anderen Operationen (\textsc{Make-Heap},
\textsc{Union}, \textsc{Insert} und \textsc{Decrease-Key}) brauchen amortisiert O(1) Zeit. Das Löschen klappt damit
ebenfalls in logarithmischer Zeit, zuerst wird der betroffene Wert mit \textsc{Decrease-Key} auf $- \infty$ gesetzt
und dann mit \textsc{Extract-Min} entfernt.

Falls in einer Anwendung oft Werte eingefügt, verändert oder zusammengeführt wer\-den, sind F-Heaps favorisiert. Falls
nur selten Werte zusammengeführt werden, bieten sich Binär-Heaps an. 

\chapter{Union-Find-Strukturen}
Der abstrakte Datentyp wird durch die Menge der drei Operationen \{\textsc{Union}, \textsc{Find}, \textsc{Make-Set}\}
gebildet. Union-Find-Strukturen dienen zur Verwaltung von Zerlegungen in disjunkte Mengen. Dabei bekommt jede Menge der
Zerlegung ein "`kanonisches Element"' zugeordnet, dieses dient als Name der Menge. Typisch für eine Union-Find-Struktur
ist eine Frage wie "`In welcher Menge liegt Element $x$?"'.

Die Bedeutung von \textsc{Union} und \textsc{Find} sollte klar sein, doch was macht \textsc{Make-Set}(x)?
Sei $x$ Element unseres Universums, dann erzeugt \textsc{Make-Set} die Einermenge \{$x$\} mit dem kanonischen Element 
$x$.
\section{Darstellung von Union-Find-Strukturen im Rechner}
Bei Listen ist die Zeit für das \textsc{Find} ungünstig, da die ganze Liste durchlaufen werden muß. \textsc{Union}
dauert ebenfalls lange, da dann durch die erste Liste bis zum Ende gegangen werden muß und dann die zweite Liste an die
erste gehängt wird. Hierbei wird auch schon offensichtlich, daß nach dem \textsc{Union} immer ein neues kanonisches
Element bestimmt werden muß. Für Union-Find-Strukturen sind "`Disjoint set forests"' das Mittel der Wahl.

\textcolor{red}{Zeichnung}

Den Objekten wird eine Größe zugeordnet, dabei werden die Knoten mitgezählt, damit keine lineare Liste entsteht. Das
\textsc{Union} erfolgt zuerst nach der Größe und dann nach der Höhe, dabei wird der kürzere Wald an den längeren
drangehängt.

\section{Der minimale Spannbaum -- Algorithmus von Kruskal}
Ein Graph G=(V, E) sei wie üblich gegeben. Was ist dann der minimale Spannbaum ("`Minimum Spanning Tree"')?
Dazu müssen die Kanten Gewichte haben, so wie beim Algorithmus von Dijkstra. Dann ist der minimale Spannbaum wie folgt
definiert:
\begin{enumerate}
\item T=(V, E$^{\star}$) mit E$^{\star} \subseteq$ E
\item Alle Knoten, die in G zusammenhängend sind, sind auch in T zusammenhängend
\item Die Summe der Kantengewichte ist minimal
\end{enumerate}

\textcolor{red}{Zeichnung}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{mst\textnormal{(G, w)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Cut, Extract-Min, Find, Make-Set, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{mst\textnormal{(G, w)}}, gobble=1]{mst(G, w)}
 E$^{\star} := \emptyset$
 K$:= \emptyset$
 Bilde Priorit~ä~tswarteschlange Q zu E bzgl. w
 f~ü~r jedes $x \in V$ do
   Make-Set(x) ~\hspace{15mm}~ $\vartriangleleft$ K besteht aus lauter Einermengen
 while K enth~ä~lt mehr als eine Menge do
   (v, w):=Min(Q)
   ~\textsc{Extract-Min}~
   if Find(v)$\not=$ Find(w) then
     Union(v, w) ~\hspace{15mm}~ $\vartriangleleft$ Hier reicht ganz simples ~\textsc{Union}~
     E$^{\star} := $E$^{\star} \cup \{(v, w)\}$
 return E$^{\star}$   
\end{lstlisting}
Der Algorithmus von Kruskal ist ein gieriger Algorithmus. In \cite{cormen} steht eine leicht andere Variante als im
Beispiel.
\end{Algorithmus}

\begin{tabular}{|l|l|}
aktuellle Kante (v, w) & K:\{a\}, \{b\}, \{c\}, \{d\}, \{e\}, \{f\}, \{g\}, \{h\}, \{i\}, \{j\}, \{k\}\\
\hline
1.  $\star$(i, k) & \{a\}, \{b\}, $\ldots$, \{h\}, \{i, k\}\\
2.  $\star$(h, i) & \{a\}, \{b\}, $\ldots$, \{h, i, k\}\\
3.  $\star$(a, b) & \{a, b\}, $\ldots$, \{h, i, k\}\\
\hline
4.  $\star$(d, e) & \{a, b\}, \{c\}, \{d, e\}, $\ldots$, \{h, i, k\}\\
5.  $\star$(e, h) & \{a, b\}, \{c\}, \{d, e, h, i, k\}, \{f\}, \{g\}\\
6.  $\star$(e, g) & \{a, b\}, \{c\}, \{d, e, g, h, i, k\}, \{f\}\\
\hline
7.  (g, i) & ---\\
8.  (g, h) & ---\\
9.  $\star$(f, h) & \{a, b\}, \{c\}, \{d, e, f, g, h, i, k\}\\
\hline
10.  (d, f) & ---\\
11.  $\star$(c, f) & \{a, b\}, \{c, d, e, f, g, h, i, k\}\\
12.  (c, d) & ---\\
\hline
13.  $\star$(a, c) & \{a, b, c, d, e, f, g, h, i, k\}\\
14.  (c, b) & ---\\
15.  (b, e) & ---\\
\end{tabular}

\textcolor{red}{Zeichnung!}

Das sieht ja alles schon und toll aus, wie wird soetwas aber implementiert? Sehen wir uns dazu weiter das Beispiel an.

\begin{tabular}{l|llllllllll}
v $\in$ V & a & b & c & d & e & f & g & h & i & k\\
\hline
p[v] & a & a & c & i & d & f & i & i & i & i\\
\end{tabular}

Dies verleitet zu der einfachen Annahme, daß bei \textsc{Union}($e$, $f$) einfach $e$ Vater und kanoisches Element der
Menge wird, zu der $f$ gehört (\lstinline[mathescape=true]!p[$f$]:=$e$!). Da immer der kleinere Baum an den größeren
angehängt werden soll (bei uns Größe nach Knotenanzahl, möglich wäre auch die Höhe als Größe zu nehmen), ist
dies minimal aufwendiger.

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Union\textnormal{(e, f)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Union\textnormal{(e, f)}},gobble=1]{Union3(e,f)}
 if size[e]$<$size[f] then
   vertausche e und f
 p[f]:=e
 size[e]:=size[e]+size[f]  
\end{lstlisting}
\end{Algorithmus} 

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Make-Set\textnormal{(x)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Make-Set\textnormal{(x)}},gobble=1]{Make-Set(x)}
 p[x]:=x
 size[x]:=1  
\end{lstlisting}
\end{Algorithmus} 

\textsc{Find} ist ebenfalls trivial, es wird einfach durch die Menge gegangen, bis das kanonische Element auftaucht, im
Beispiel liefert \textsc{Find}(e) $e$ $\leadsto$ $d$ $\leadsto$ $i$ $\leadsto$ $i$, return $i$.

\begin{satz}
Für Vereinigung nach Größe gilt: Ein Baum mit Höhe n hat mindestens 2$^n$ Knoten. 
\end{satz}
Daraus folgt, daß es hier keine dünnen Bäume gibt. Der Beweis des Satzes erfolgt mittels Induktion über die
Komplexität der Bäume, doch dazu muß erstmal folgendes definiert werden:
 
\begin{definition}[VNG-Baum]
T ist ein VNG-Baum, genau dann wenn
\begin{enumerate}
\item T nur aus der Wurzel besteht oder
\item T Vereinigung von T$_1$ und T$_2$ mit size[T$_2$]$\leq$size[T$_1$] ist.
\end{enumerate}
\end{definition}
VNG steht dabei für Vereinigung nach Größe.

\begin{beweis}
\begin{itemize}
\item[I.A.] h=0 (Wuzelbaum) 2$^h=1$
\item[I.V.] gelte für T$_1$, T$_2$ o.B.d.A. size[T$_2$]$\leq$size[T$_1$], size[T$_2$]$\geq 2^{h_2}$, size[T$_1$]$\geq
2^{h_1}$
\item[I.B.] size[T]$\geq 2^h$
\item[1. Fall] $h=$max$(h_1, h_2)$ $\Rightarrow$ size[T]=size[T$_1$]+size[T$_2$]$\geq 2^{h_1}+2^{h_2} \geq 2^h$
\item[2. Fall] $h=$max$(h_1, h_2)+1$ $\Rightarrow$ size[T]=size[T$_1$]+size[T$_2$]$\geq 2 \cdot $size[T$_2$]$\geq 2
\cdot 2^{h_2} =2^{h_2+1} \geq 2^h$
\end{itemize}
\end{beweis}

\textcolor{red}{Ist der Beweis nicht falsch? Beispiel: 2. Fall: h$_1$=10, h$_2$=4, h=11}

Aus dem Satz ergibt sich, daß \textsc{Find} bei Vereinigung nach Größe O($\log n$) kostet.

\begin{satz}
Für Vereinigung nach Höhe gilt: Ein Baum der Höhe $h$ hat mindestens 2$^h$ Knoten.
\end{satz}
\textcolor{red}{Was soll die Verarsche, wo ist der Unterschied zum vorherigen Satz? Die Existenz von dünnen Bäume?}
Also geht auch hier das \textsc{Find} in O($\log n$).

Das \textsc{Make-Set}(x) funktioniert immer in O(1), das \textsc{Find} geht in O($\log n$) und \textsc{Union} klappt in
O(1) falls nur ein Pointer umgesetzt werden muß. In einer Idealvorstellung, in der alle Elemente auf das kanonische
Element zeigen, geht \textsc{Find} in O(1), das \textsc{Union} braucht dann aber O($n$). Solche Idealvorstellungen sind
natürlich meist sehr realitätsfern und nicht praktikabel. Doch lassen sich Union-Find-Strukturen noch mit anderen
Mitteln verbessern?

Ja, allerdings. Dazu bedient man sich der Pfad-Kompression und der inversen einer Funktion, die starke ähnlichkeit mit
der Ackermann-Funktion aufweist. Dies zusammen führt zu dem noch folgenden Satz von Tarjan. Dazu wird die
Union-Find-Struktur mit einer amortisierten Analyse auf ihre Kosten untersucht. Wir betrachten eine Folge von insgesamt 
$m$ \textsc{Union-Find}-Operationen mit $n$ \textsc{Make-Set}-Operationen dabei. Dazu brauchen wir zuerst die
Ackermann-Funktion und ihre Inverse.
\begin{definition}[Die Ackermann-Funktion und ihre Inverse]
 Die Ackermann-Funktion
 \begin{align*}
 A&(1,j)=2^j   & j\geq 1\\
 A&(i,1)=A(i-1,2)   & i\geq 2\\
 A&(i,j)=A(i-1, A(i,j-1))   & i,j \geq 2\\
 \end{align*}
 und ihre Inverse
 \[\alpha(m,n):=\mbox{min}\{i \geq 1 : A\left(i,\left\lfloor\frac{m}{n}\right\rfloor\right)> \log n\]
\end{definition}
Wegen obigem kann $n$ nicht größer als $m$ sein. Für den Grenzwert der Inversen gilt natürlich
\[\lim_{\substack{n \rightarrow \infty \\n \leq m }}=\infty\]
Da die Funktion aber extrem schnell wächst und damit ihre Inverse extrem langsam ist in allen konkreten Anwendungen 
$\alpha (m,n)$ aber kleiner als $5$. 

\begin{satz}[Satz von Tarjan]
Für $m$ \textsc{Union-Find}-Operationen mit n$\leq$m \textsc{Make-Set}-Operationen benötigt man $\Theta( m \cdot
\alpha(m,n))$ Schritte, falls die Operationen in der Datenstruktur "'Disjoint-Set-Forest"` mit \textsc{Union} nach
Größe (Höhe) und \textsc{Find} mit Pfadkompression realisiert werden.
\end{satz}
\textcolor{red}{Es wird noch garnicht erläutert, was die Pfadkompression denn nun genau ist.}
Noch ein Nachsatz zur Pfadkompression: Wenn wir vom Knoten zur Wurzel laufen, dann laufen wir nochmal zurück und setzen
alle Zeiger auf den Vater, den wir nun kennen $\rightarrow$ O(2$x$)=O($x$).

Ein weiteres Beispiel ist die Bestimmung der Zusammenhangskomponenten\textcolor{red}{Beispiel wofür?}

\chapter{Hashing}
Jeder wird schon einmal vom Hashing gehört haben, daß es irgendwie mit Funktionen zu tun hat, ist wohl auch klar. Was
steckt aber genau dahinter?

Für jede Funktion braucht man einen Grundbereich, eine Menge, auf denen diese Funktion operiert. Genau genommen
braucht man eine "`¸Ursprungsmenge"' für die Argu\-mente und eine "`Zielmenge"' für die Ergebnisse. Da aber häufig die
"`Ursprungsmenge"' (Urbildmenge) und die "`Zielmenge"'(Bildmenge) gleich sind, betrachtet man ohne weitere Erwähnung
eine Menge, die aber (implizit) beides ist.

Im Falle des Hashings ist unser Universum die Urbildmenge U. Dieses enthält alle denkbaren Schlüssel, darin gibt es
aber eine Teilmenge K, die nur alle auch wirklich betrachteten Schlüssel enthält.

Sehen wir uns alle Familiennamen mit maximal 20 Buchstaben an, in U sind alle Zeichenketten mit maximal 20 Buchstaben,
in K aber nur alle tatsächlich vorkommenden Namen.

\section{Perfektes Hashing}
Perfektes Hashing ist noch recht einfach, es wird die Tabelle T direkt adressiert und Werte $x$ direkt eingefügt und
gelöscht. Dafür brauchen wir eine Hash-Funktion $h$ und einen Schlüssel $k$ (für engl. "`key"').
\textcolor{red}{das sollte noch verbessert werden}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Insert\textnormal{(T, x)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Insert\textnormal{(T, x)}},gobble=1]{Insert3(T, x)}
 T[key[x]]:=x
\end{lstlisting}
\end{Algorithmus} 

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Delete\textnormal{(T, x)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Delete\textnormal{(T, x)}},gobble=1]{Delete(T, x)}
 T[key[x]]:=Nil
\end{lstlisting}
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Search\textnormal{(T, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Search\textnormal{(T, k)}},gobble=1]{Search(T, k)}
 return T[k]
\end{lstlisting}
\end{Algorithmus} 

Alle drei Operationen dauern O(1) Schritte, aber
die Hash-Funktion $h$ bildet aus U auf $m-1$ Werte ab ($h: U \rightarrow \{0, \cdots, m-1\}$), bei vielen Werten steigt
damit die Wahrscheinlichkeit für Kollisionen (Abbildung auf den gleichen Funktionswert) sprung\-haft an. 

Dies wird durch das folgende Beispiel vielleicht klarer: Bei 23 Personen
ist die Wahr\-scheinlichkeit, daß sie alle an verschiedenen Tagen Geburtstag haben etwas geringer als $0,5$, bei 40
Personen ist die Wahrscheinlichkeit dafür schon auf ca. 10\% gesunken.

\section{Drei Methoden zur Behandlung der Kollisionen}
\begin{enumerate}
\item Hashing mit Verkettung (Chaining): 
  \begin{itemize}
  \item Bei \textsc{Insert}(T,$x$) wird $x$ hinter dem Kopf der Liste T[h[key[x]]]
    eingefügt, jeder Eintrag der Hash-Tabelle kann also eine Liste beinhalten. 
  \item Beim \textsc{Search} wird in der Liste T[h[k]] mit key[$x$]=k nach Objekt $x$ gesucht
  \item \textsc{Delete} funktioniert analog zum \textsc{Search}
  \end{itemize}
\item Divisions-Rest-Methode $h(k):=k \mod m$ (z.B. $m$ Primzahl, die nicht zu dicht an einer Zweierpotenz liegt)  
\item multiplikative Methode $h(k):=\lfloor m(k \cdot \phi -\lfloor k \cdot \phi \rfloor) \rfloor$, dabei ist $m$ die
konstante Tabellengröße und $0 < \phi <1$. Donald E. Knuth schlägt z.B. $\phi=\frac{sqrt{5}-1}{2} \approx 0,618$ vor.
\end{enumerate} 
Außer den drei vorgestellten Methoden gibt es noch weitere, die uns hier aber nicht interessieren\textcolor{red}{war
das so gemeint?}. Die letzten beiden gezeigten Methoden werfen zusätz\-lich die Frage auf, was eine
"`gute"' Hashfunktion ist.

\section{Analyse des Hashings mit Chaining}
Beim Hashing mit Chaining steht in der $j$-ten Zelle der Hash-Tabelle ein Zeiger auf die Liste mit $n_j$ Elementen. Um
die Anzahl der Kollisionen abzuschätzen, gibt es einen sogenannten Ladefaktor $\alpha$.
%Beim Hashing mit Chaining gibt es einen sogenannten Ladefaktor \[\alpha:=\frac{n}{m}\]
Jetzt wird einfaches uniformes Hashing angenommen. Dann ist die Wahrscheinlichkeit für ein gegebenes Element in eine bestimmte
Zelle zu kommen, für alle Zellen gleich. Daraus folgt auch die idealisierte Annahme, daß alle Listen gleich lang sind
(sonst dauert das \textsc{Search} schlimmstenfalls noch länger). Dann gilt \[\mathbb{E}[n_j]=\frac{n}{m}=\alpha\]

\begin{satz}[Theorem 1]
Wenn beim Hashing Kollisionen mit Verkettung gelöst werden, dann braucht eine erfolg\-lose Suche $\Theta(1+\alpha)$
Zeit.
\end{satz}
\begin{beweis}
$k$ gesucht $\rightarrow h(k)$ berechnet $\rightarrow h(k)=j$ $j$-te Liste durchlaufen $\Rightarrow \Theta(1+\alpha)$
\end{beweis}

\begin{satz}[Theorem 2]
Für eine erfolgreiche Suche werden ebenfalls $\Theta(1+\alpha)$ Schritte gebraucht.
\end{satz}
\begin{beweis}
Wir stellen uns einfach vor, daß bei \textsc{Insert} Werte am Ende der Liste eingefügt werden. Dann brauchen wir
wieder Zeit zum Berechnen des Tabelleneintrages, daran anschließend muß wieder die Liste durchlaufen werden.
\end{beweis} \textcolor{red}{Was soll die Gülle, das wird doch jetzt nochmal, bloß weniger wischi-waschi bewiesen?}
\begin{beweis}
Hier nehmen wir an, daß erfolgreich nach einem Element gesucht wird, also muß diese vorher irgendwann auch eingefügt
worden sein. Sei nun dieses Element an $i$-ter Stelle eingefügt worden (vorher $i-1$ Elemente in der Liste), die
erwartete Länge der Liste ist dann natürlich $\frac{i-1}{m}$. Jetzt seien $n$ Elemente in der Liste, dann gilt:
\[\frac{1}{n} \cdot \sum_{i=1}^n \left(1+\frac{i-1}{m}\right)=1+\frac{1}{n-m} \sum_{i=1}^n (i-1)=1+\frac{1}{n-m} \cdot
\frac{(n-1)n}{2}=1+\frac{\alpha}{2}-\frac{1}{2m} \in \Theta(1+\alpha)\]
\end{beweis}
Bei festem $h$ können aber auch ungünstige Inputs problematisch werden. Dieses Problem führt zum Universal Hashing.

\section{Universal Hashing}
Festgelegte Hashfunktionen können zu ungünstigen Schlüsselmengen führen, bei zufällig gewählten Hashfunktionen
werden diese ungünstigen Schlüsselmengen durch die zufällige Wahl kompensiert. Für das Einfügen und Suchen eines Wertes
$x$ muß aber immer die gleiche Hashfunktion benutzt werden, dazu wird beim Einfügen die Hashfunktion zufällig
ausgewählt und dann abgespeichert.

\begin{definition}
Sei H eine Menge von Hashfunktionen, die das Universum U in $\{0, \ldots, m-1\}$ abbilden. Dann heißt H genau dann
universal, wenn für jedes Paar von verschiedenen Schlüsseln $x, y \in $U gilt:
\[\vert\{h \in \mbox{H} : h(x)=h(y)\}\vert=\frac{\vert \mbox{H}\vert}{m}\]
oder anders ausgedrückt:
\[\mbox{H }universal \leftrightarrow Wsk([h(x)=h(y)])=\frac{1}{m} \mbox{ für }x,\, y \in U \wedge x\not=y\]
\end{definition}

Sei $K \subseteq U$ eine Menge von Schlüsseln und $\vert K\vert =n$. Außerdem sei H eine universale Klasse von
Hashfunktionen. Dann gilt der folgende Satz:
\begin{satz}
Falls $h$ zufällig aus H ausgewählt wird, dann ist pro Schlüssel k $\in$ K die mittlere Anzahl der Kollisionen
(h(x)=h(y)) höchstens $\alpha \mbox{ } (=\frac{n}{m})$
\end{satz}
\textcolor{red}{der Beweis erfolgte lt. Skript später oder in der übung, kam der noch?}
Gibt es soetwas überhaupt?

Sei $m$ eine Primzahl, dann ist die folgende Klasse universal:
Die Werte in unserem Universum seien als Bit-Strings der gleichen Länge gegeben, jeder Schlüssel $x$ also in der
Gestalt $x=<x_0, x_1, \cdots, x_r>$, wobei die Länge aller $x_i$ gleich ist.  Dabei ist $0\leq x_i\leq m$ Voraussetzung
bezüglich $m$ bzw. $r$.
\begin{definition}
\[H:=\{h=h_a : a=<a_0, \ldots, a_r>, a_i \in \{0, \ldots, m-1\}, i=0, \ldots, r\]
\[h_a(x):=\sum_{i=0}^r (a_i \cdot x_i) \mod m\]
\end{definition}
Es ist ersichtlich, daß es dabei $m^r$ verschiedene Hashfunktionen geben kann.

\begin{satz}
Die so definierte Klasse ist universal.
\end{satz}
Für den Beweis brauchen wir noch einen Hilfssatz:
\begin{satz}[Hilfssatz]
Falls $x\not= y$ ist, erfüllen von den $m^{r+1}$ veschiedenen Hashfunktionen genau $m^r$ Stück die Bedingung
$h_a(x)=h_a(y)$ $\Rightarrow$ \[\mbox{Wsk}([h(x)=h(y)])=\frac{m^r}{m^{r+1}}=\frac{1}{m}\]
\end{satz}
\begin{beweis}[Beweis des Hilfssatzes und des Satzes]
Sei $x\not=y$ (o.B.d.A $x_0 \not=y_0$)
\[h_a(x)=h_a(y) \Leftrightarrow \sum_{i=0}^r (a_i \cdot x_i)\!\! \mod m=\sum_{i=0}^r (a_i \cdot y_i)\!\! \mod m \leftrightarrow
a_0(x_0-y_0)=\sum_{i=1}^r a_i (x_i- y_i)\]
Da m eine Primzahl ist, kann durch $(x_0-y_0)$ geteilt werden. Damit ist auch die Struktur $(\{0, \ldots, m-1\}, +_{\mod
m}, -_{\mod m})$ ein algebraischer Körper. Die Umformungen führen schließlich zu \[\star  \qquad 
a_0=\frac{1}{x_0-y_0} \cdot \sum_{i=1}^r (a_i (x_i- y_i))\]
Damit kann für jede Wahl von $<a_1, \ldots, a_r> \in \{0, \ldots, m-1\}$ mit $\star$ das zugehörige $a_0$ eindeutig
bestimmt werden. Bezüglich der ersten Komponente von $a$ besteht also keine freie Wahl, oder anders ausgedrückt: $a_0$
ist Funktion von $(a_1, \ldots, a_r)$, davon gibt es aber genau $m^r$ Stück. Es folgt aus $h_a(x)=h_a(y)$ insgesamt,
daß jedes Tupel $(a_1, \ldots, a_r)$ das $a_0$ eindeutig bestimmt. Und so haben von $m^{r+1}$ möglichen Tupeln nur
$m^r$ dei Eigenschaft $h_a(x)=h_a(y)$. Damit ist der Hilfssatz, aus dem der Satz folgt, bewiesen.
\end{beweis}

\section{Open Hashing}
Anfangs wurde die direkte Adressierung erwähnt, doch auch dies geht anders. Beim Open Hashing wird eine offene
Adressierung verwendet, dies ist sinnvoll, falls praktisch kein \textsc{Delete} vorkommt. Beim Einfügen wird probiert,
ob die ausgesuchte Stelle in der Tabelle frei ist. Falls sie frei ist, wird der Wert eingefügt und fertig, falls nicht
wird weitergegangen. So ist $\alpha=\frac{n}{m}<1$ möglich.

Die Proben-Sequenz (auch Probier- oder Sondierfolge) ist als Abbildung ein mathema\-tisches Kreuzprodukt:
\[h: U \times \{0, \ldots, m-1\} \stackrel{\rightarrow}{auf}\{0, \ldots, m-1\}\]
Die Algorithmen für die Operationen sind dann natürlich etwas komplexer.

\begin{Algorithmus}[H]
Der Einfachheit halber betrachten wir Schlüsselwerte $k$ mit $k \in U$
\addcontentsline{alg}{Algorithmus}{\textsc{Insert\textnormal{(T, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Insert\textnormal{(T, k)}},gobble=1]{Insert4(T, k)}
 i:=0
 repeat j:=h(k,i)
   if T[j]=Nil then
     T[j]:=k
     return j
   else i:=i+1
 until i=m
 Fehler -- Tabelle voll    
\end{lstlisting}
\end{Algorithmus} 

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Search\textnormal{(T, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Search\textnormal{(T, k)}},gobble=1]{Search4(T, k)}
 i:=0
 repeat j:=h(k,i)
   if T[j]=k then
     return j
   else i:=i+1
 until (T[j]=Nil OR i=m)
 return Nil    
\end{lstlisting}
\end{Algorithmus} 

Beim \textsc{Delete} taucht allerdings ein Problem auf. Nil würde gesetzt werden und dann würde \textsc{Search}
stoppen und nicht mehr weiter suchen. \textcolor{red}{Was soll das jetzt, löst das die Porbleme mit dem
\textsc{Delete}?}

\begin{tabular}{l|l|l}
lineares Sondieren & quadratrisches Sondieren & doppeltes Hashing\\
\hline
$h(k,i):=$ & $h(k,i):=$ & $h(k,i)=$\\
$(h'(k)+c \cdot i)\!\! \mod m$ & $(h'(k)+c_1 \cdot i + c_2 \cdot i^2)\!\! \mod m$ & $(h_1(k)+i \cdot h_2(k))\!\! \mod m$\\
Sei eine leere Zelle & & $h_1$ und $h_2$ sind wieder\\
gegeben und davor & & Hashfunktionen\\
$i$ volle & &\\
Dann wird eine Zelle & &\\
mit Wsk $\frac{i+1}{m}$ belegt & &\\
$\Rightarrow$ Primärcluster & $\Rightarrow$ Sekundärcluster& \\
(lange Suchzeiten) & & \\
\end{tabular}

\section{Nochmal zur Annahme des (einfachen) uniformen Hashings}
Beim einfachen uniformen Hashing gilt für jeden Schlüssel $k$, daß jeder Tabellenplatz für ihn gleichwahrscheinlich
ist (unabhängig von den Plätzen anderer Schlüsselwerte). Falls k eine zufällige reelle Zahl ist, die in $[0, 1]$
gleichverteilt ist, gilt dies für $h(k)=\lfloor k \cdot m\rfloor$
Beim uniformen Hashing gilt für jeden Schlüssel $k$, daß jede der $m!$ Permutationen von \{0, \ldots, m-1\} als
Sondierungsfolge gleichwahrscheinlich ist. Eigentlich gibt es kein uniformes Hashing, aber man kann sehr nahe an diese
Forderung (und ihre Folgen!) herankommen.
\begin{itemize}
\item[A] immer darauffolgende Zahl $\rightarrow m! sehr klein$
\item[B] $h'(k_1)=h(k_1, 0)$, $h'(k_2)=h(k_2, 0)$, gleiche Funktion $\rightarrow$ gleicher Platz, $m$ ver\-schiedene
Sondierungsfolgen 
\item[C] gute Wahl von $h_1$ und $h_2 \rightarrow$ nah am uniformen Hashing
\end{itemize}
Damit ist also Variante "'C"` (Doppeltes Hashing) am dichtesten am uniformen Hashing dran, wie bestimmt man nun $h_1$
und $h_2$?
Dafür gibt es keine festen Regeln, allerdings sollte
\begin{itemize}
\item $h_2(k)$ relativ prim zu $m$ oder
\item $m$ eine Primzahl sein.
\end{itemize}
Ersteres wird oft eingehalten. Möglich ist eine Struktur wie 

$\begin{array}{l}
h_1(k)= k \mod m\\
h_2(k)= 1 + (k \mod m')  \mbox{ mit } m' \mbox{ dicht an } m\\
\end{array}$

\subsubsection{Beispiel}
Sei also 

$\begin{array}{l}
h_1(k)= k \mod 13\\
h_2(k)= 1 + (k \mod 11)\\
\end{array}$

\begin{tabular}{lllllll}
Folge & 79 & 69 & 98 & 72 & 50 & 14\\
$h_1$ & 1 & 4 & 7 & 7 & 11 & 1\\
$h(k,i)$ & 1 & 4 & 7 & 8 & 11 & 4\\
\end{tabular}

Bei geschickter Wahl von $h_1$ und $h_2$ erfüllt das doppelte Hashing die Annahme des uniformen Hashings ausreichend.
 
\begin{satz}[Satz 1 zum Open Hashing]
Gegeben sei eine Open-Hash-Tabelle mit $\alpha=\frac{n}{m}<1$. Dann ist der Erwartungswert für die Anzahl der Proben in
der Sondierungsfolge bei erfolgloser Suche unter Annahme des uniformen Hashings höchstens
O$\left(\frac{1}{1-\alpha}\right)$
\end{satz}
Analoges gilt dann für \textsc{Insert}. Je voller die Tabelle ist, umso näher ist $\alpha$ an 1 und umso länger
dauert das \textsc{Search}.

\begin{satz}[Satz 2 zum das Open Hashing]
Unter der Voraussetzung von Satz 1 gilt bei erfolgreicher Suche, daß diese weniger als O$\left(\frac{1}{\alpha} \cdot
\ln{\frac{1}{1-\alpha}}\right)$ Schritte braucht (die Annahme des uniformen Hashings sei erfüllt).
\end{satz}

\begin{beweis}[Nur die Beweisskizze]
Sei das $k$ nach dem gesucht wird, in der Tabelle und als $(i+1)$-ter Wert eingefügt worden. Mit der Folgerung vom
ersten Satz zum Open Hashing ist $\alpha=\frac{i}{m}$ beim Einfügen von $k$. Die erwartete Anzahl der Proben beim
\textsc{Insert} von $k$ ist danach \[\frac{1}{1-\frac{1}{m}}=\frac{m}{m-i}\]
. Bei $n$ Schlüsseln ist der Mittelwert zu bilden: 
\[\frac{1}{n} \sum_{i=0}^{n-1} \frac{m}{m-i}=\frac{m}{n} \sum_{i=0}^{n-1}
\frac{1}{m-i}=\frac{m}{n}\left(H_m-H_{m-1}\right),\] dabei steht $H_m$ für die harmonische Reihe und es ist
\[H_m=\sum_{mu}^{m}\frac{1}{\mu}\]

Weiter ist
\[\frac{1}{\alpha}\left(H_m-H_{m-1}\right)=\frac{1}{\alpha} \sum_{k=m-n+1}^m \frac{1}{k} \leq \frac{1}{\alpha}\int
\limits_{m-n}
\left(\frac{1}{x}\right)dx=\frac{1}{\alpha} \ln{\frac{m}{m-n}}=\frac{1}{\alpha}\ln{\frac{1}{1-\alpha}}\].
\end{beweis}

Sei z.B. die Tabelle halbvoll $\rightarrow \sim 1,387$ oder zu 90\% voll $\rightarrow \sim 2,56$. Diese Werte gelten
nicht unbedingt für unsere Beispiele wie doppeltes Hashing sondern für idealisertes uniformes Hashing. Falls aber
$h_1$ und $h_2$ geschickt gewählt werden (siehe unser Beispiel), liegen die tatsächlichen Werte nah bei den oben
angegebenen. 

Was bleibt nun als Folgerung? Falls kein \textsc{Delete} erforderlich ist, wird Open Hashing benutzt, sonst Chaining.

  
%\end{document} 
%
