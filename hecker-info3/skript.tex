% Einige zusätzliche Informationen für rubber
%  rubber erkennt nicht, dass die Datei weg kann, daher sagen wir es ihm
% rubber: clean $base.thm
%  rubber soll nach Änderungen an der Datei nochmal bauen
% rubber: watch $base.thm
% rubber: index.tool      xindy
% rubber: index.language  german-din
%
% scrreprt trifft am Besten die Bedürfnisse eines Skripts, das ganze wird
% zweiseitig (twoside), d.h. es wird zwischen linker und rechter Seite
% unterschieden, und wir verwenden zwischen den Absätzen einen Abstand
% von einer halben Zeile (halfparskip) und dafür keinen Absatzeinzug,
% wobei die letzte Zeile eines Absatzes zu min. 1/4 leer ist.


%% TODO:
%%       - Befehl für Algorithmen anlegen
%%       - Algorithmen mit algorithm2e setzen
%%       - Pakete epic, ecltree durch TikZ eliminieren
%%       - autoref beibringen, Algorithmus 3.2.1 zu schreiben

\RequirePackage[l2tabu,orthodox]{nag}  % nag überprüft den Text auf veraltete
                   % Befehle oder solche, die man nicht in LaTeX verwenden
                   % soll -- l2tabu-Checker in LaTeX

\RequirePackage[ngerman=ngerman-x-latest]{hyphsubst} % einbinden der neuen
                   % Trennmuster, diese korrigieren einige Fehler der alten
                   % und bieten mehr Trennstellen

\documentclass[ngerman,draft,parskip=half*,twoside]{scrreprt}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{babel}
\usepackage[draft=false,colorlinks,bookmarksnumbered,linkcolor=blue,breaklinks]{hyperref}
\usepackage{fancyvrb}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[all,warning]{onlyamsmath}  % warnt bei Verwendung von nicht
                                       % amsmath-Umgebungen z.\,B. $$...$$
\usepackage{xcolor} 
\usepackage{graphicx}
\usepackage[amsmath,thmmarks,hyperref]{ntheorem} % für die Theorem-Umgebungen
                                                 % (satz, defini, bemerk)
\usepackage{float}
\usepackage{ecltree}
\usepackage{epic}
\usepackage{fixmath}     % ISO-konforme griech. Buchstaben
\usepackage{svn}         % Zum Auswerten und ordentlichen Darstellen der
                         % SVN-Schlüsselwörter (s. vor \begin{document})
                         % dafür muss in SVN noch das Flag svn:keywords
                         % auf "LastChangedRevision LastChangedDate"
                         % gesetzt werden
\usepackage{tikz}
\usepackage{mathtools}   % Zur Definition von \abs und \norm
\usepackage{nicefrac}
\usepackage{todonotes}
\usepackage{paralist}    % besseres enumerate und itemize und neue
                         % compactenum/compactitem; s. texdoc paralist
\usepackage[german,boxruled,algosection,linesnumbered]{algorithm2e}
\usepackage{braket}
\usepackage{eurosym}

% Damit auch die Zeichen im Mathemode in Überschriften fett sind
% <news:lzfyyvx3pt.fsf@tfkp12.physik.uni-erlangen.de>
\addtokomafont{sectioning}{\boldmath}

% nach dem Theoremkopf wird ein Zeilenumbruch eingefügt, die Schrift des
% Körpers ist normal und der Kopf wird fett gesetzt
\theoremstyle{break}
\theoremnumbering{arabic}
\theorembodyfont{\normalfont}
\theoremheaderfont{\normalfont\bfseries}

% Das Ende von Umgebungen, für die kein Beweis erbracht wurde, soll mit einer
% leeren Box gekennzeichnet werden. Wenn jedoch ein Beweis erbracht wurde,
% soll kein Zeichen ausgegeben werden (die ausgefüllte Box vom proof wird
% verwendet); man beachte die spezielle Definition von \theoremheaderfont für
% die Umgebung proof
% \newboolean{hasproof}
% \theoremheaderfont{\global\hasprooffalse\normalfont\bfseries}
% \theoremsymbol{\ifthenelse{\boolean{hasproof}}{}{\ensuremath{_\Box}}}

% Die folgenden Umgebungen werden einzeln nummeriert und am Ende jedes
% Kapitels zurückgesetzt
\newtheorem{satz}{Satz}[chapter]
\newtheorem{bemerk}{Bemerkung}[chapter]
\newtheorem{definition}{Definition}[chapter]
\newtheorem{bsp}{Beispiel}[chapter]
\newtheorem{festl}{Festlegung}[chapter]

% Die folgenden Theoremumgebungen bekommen keine Nummer
\theoremstyle{nonumberbreak}
\newtheorem{fakt}{Fakt}

% \theoremheaderfont{\global\hasprooftrue\scshape}
\theoremheaderfont{\scshape}
\theorembodyfont{\normalfont}
% Das Zeichen am Ende eines Beweises
%\theoremsymbol{\ensuremath{_\blacksquare}}
\theoremsymbol{q.\,e.\,d.}
\newtheorem{beweis}{Beweis:}

% Hier die Definition, wie \autoref die Umgebungen nennen soll, die mit
% \newtheorem definiert wurden
\newcommand*{\satzautorefname}{Satz}
\newcommand*{\bemerkautorefname}{Bemerkung}
\newcommand*{\definitionautorefname}{Definition}
\newcommand*{\bspautorefname}{Beispiel}
\newcommand*{\festlautorefname}{Festlegung}
\newcommand*{\algorithmautorefname}{Algorithmus}
% Zwischen Unter- und Unterunterabschnitten sollte nicht unterschieden
% werden.
\renewcommand*{\subsectionautorefname}{Abschnitt}
\renewcommand*{\subsubsectionautorefname}{Abschnitt}

\pagestyle{headings}

\newfloat{Algorithmus}{h}{alg}[chapter]

\newcommand*{\OO}{\mathcal{O}}      % Zeichen für das Big-O
\newcommand*{\B}{\mathbb{B}}
\newcommand*{\K}{\mathbb{K}}
\newcommand*{\M}{\mathbb{M}}
\newcommand*{\N}{\mathbb{N}}        % natürliche Zahlen
\newcommand*{\R}{\mathbb{R}}        % reelle Zahlen
\newcommand*{\Z}{\mathbb{Z}}        % ganze Zahlen
\DeclareMathOperator{\Bh}{Bh}       % Schwarzhöhe eines Baumes
\DeclareMathOperator{\rg}{Rang}     % Rang eines Elements

% Um sicherzustellen, dass jeder Betrag/jede Norm links und rechts die
% Striche bekommt, sind diese Befehle da. Damit kann man nicht die
% rechten Striche vergessen und es wird etwas übersichtlicher. Aus
% mathtools.pdf, z. B. \abs[\big]{\abs{a}-\abs{b}} \leq \abs{a+b}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}

% Für die Gaußklammer empfiehlt sich ebenso eine Definition mit
% Benutzung von mathtools.
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}


\SVN $LastChangedRevision$
\SVN $LastChangedDate$

\begin{document}

%\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort}, emphstyle=\textsc, escapeinside=~~}
% 13.10.2003 Adrian Knoth <adi@thur.de>

\author{Prof.\,Dr.\,Hans-Dietrich Hecker}
\title{Algorithmen und Datenstrukturen}
\date{Wintersemester~2003/04}
\maketitle
%\begin{abstract}
%Zur Entst
%
%
%Ergänzend zur Vorlesung wird Literatur empfohlen. Dabei handelt es sich um die ersten vier Werke im Literaturverzeichnis auf der
%letzten Seite dieses Dokuments.
%
%Außer Prof. Dr. Hecker waren natürlich noch weitere Personen an der Entstehung des Skriptes beteiligt. Das waren die
%Studenten die Mitschriften anfertigten, Sebastian Oerding als studentischer Leiter des Projektes "`Skripts"' und Dr. Grajetzki, die die
%Mitschriften auf verbliebene Fehler üeberprüfte.
%\end{abstract}
						
\tableofcontents 
\listof{Algorithmus}{Algorithmenverzeichnis}

%\frontmatter  % entfernt Kapitel-Numerierungen, römische Seitenzahlen
\chapter*{Vorwort}

{\itshape
  Dieses Dokument wurde als Skript für die auf der
  Titelseite genannte Vorlesung erstellt und wird jetzt im Rahmen des
  Projekts
  "`\href{http://uni-skripte.lug-jena.de/}
  {Vorlesungsskripte der Fakultät für Mathematik}
  \href{http://uni-skripte.lug-jena.de/}{und Informatik}"'
  weiter betreut. Das
  Dokument wurde nach bestem Wissen und Gewissen angefertigt. Dennoch
  garantiert weder der auf der Titelseite genannte Dozent, die Personen,
  die an dem Dokument mitgewirkt haben, noch die
  Mitglieder des Projekts für dessen Fehlerfreiheit. Für etwaige Fehler
  und dessen Folgen wird von keiner der genannten Personen eine Haftung
  übernommen. Es steht jeder Person frei, dieses Dokument zu lesen, zu
  verändern oder auf anderen Medien verfügbar zu machen, solange ein
  Verweis auf die Internetadresse des Projekts
  \url{http://uni-skripte.lug-jena.de/}
  enthalten ist.

  Diese Ausgabe trägt die Versionsnummer~\SVNLastChangedRevision{} und ist vom
  \SVNDate{}. Eine neue Ausgabe könnte auf der Webseite des Projekts verfügbar
  sein.

  Jeder ist dazu aufgerufen, Verbesserungen, Erweiterungen und
  Fehlerkorrekturen für das Skript einzureichen bzw. zu melden oder diese
  selbst einzupflegen -- einfach eine E-Mail an die
  \href{mailto:uni-skripte@lug-jena.de}{Mailingliste
  \nolinkurl{<uni-skripte@lug-jena.de>}} senden. Weitere Informationen
  sind unter der oben genannten Internetadresse verfügbar.

  Hiermit möchten wir allen Personen, die an diesem Skript mitgewirkt
  haben, vielmals danken:
  \begin{itemize}
   \item Sebastian Oerding (Erfassung der einzelnen Mitschriften)
   \item Dr.\,Jana Grajetzki (Fachliche Korrektur)
  \end{itemize}
}

\chapter{Einführung}

Liest man heutzutage eine nahezu beliebige Einführung in die theoretische
Informatik, so werden zwei Begriffe immer in einem Atemzug genannt:
\textit{Algorithmen} und \textit{Datenstrukturen}. Sie gehören zusammen
wie die Marktlücke zum Unternehmensgründer. Und tatsächlich kann der 
wirtschaftliche Erfolg einer EDV-basierten Idee existentiell von der Wahl
der passenden Algorithmen und Datenstrukturen abhängen.

Während ein \textbf{Algorithmus} die \emph{notwendigen Schritte zum Lösen
eines Problems} beschreibt, dienen \textbf{Datenstrukturen}
\emph{zum Speichern und Verwalten} der verwendeten Daten.  

\begin{figure}
  \centering
  \begin{tikzpicture}
    \draw (1,1) -- (3,1) -- (3,2.8) -- (1,2.8) -- (1,1);
    \draw (2,3) -- (3,3) -- (3,3.5) -- (2,3.5) -- (2,3);
    \draw (2.6,2) -- (6.5,2) -- (6.5,4) -- (2.6,4) -- (2.6,2);
  \end{tikzpicture}
%  \input{131003a.latex}
  \caption{Überlappen sich die drei Rechtecke?}
  \label{131003a}
\end{figure}

Wie so oft beginnt ein Vorhaben mit den zwei Fragen "`Was \emph{habe}
ich?"' und "`Was \emph{möchte} ich erhalten?"'. In \autoref{131003a}
sind drei einander schneidende Rechtecke zu sehen. Sie bilden die
Ausgangssituation -- das "`Was habe ich?"'. Man spricht im Allgemeinen vom
\textbf{Input}. Gesucht ist ein Algorithmus, der die zugehörigen
Überlappungen dieser drei Rechtecke ermittelt und ausgibt. Das Ergebnis (Menge der Überlappungen)
heißt \textbf{Output}.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \draw (2,2) -- (2,5);
    \draw (3,4) -- (3,5);
    \draw (5,3.4) -- (5,4.6);
    \draw (1.8,4.4) -- (4,4.4);
    \draw (0,4.1) -- (2.5,4.1);
    \draw (3.6,4) -- (6,4);
    \draw (1,2.8) -- (3,2.8);
  \end{tikzpicture}
%  \input{131003b.latex}
  \caption{Das Segmentschnitt-Problem}
  \label{131003b}
\end{figure}

Zum besseren Verständnis ist es jedoch sinnvoll, das allgemeinere
\textsc{Segmentschnitt-Problem} zu betrachten. Eine
gegebene Menge von horizontal und vertikal verlaufenden Liniensegmenten
in der Ebene soll auf sich schneidende Segmente untersucht werden.

Die in \autoref{131003b} dargestellten $n$"=Linien beschreiben ein
solches Segmentschnitt-Problem. Die triviale Idee, jede Strecke mit jeder
anderen zu vergleichen und somit alle möglichen Schnittpunkte zu
ermitteln, charakterisiert einen sogenannten
\textbf{Brute-Force-Algorithmus}, der mittels erschöpfenden Probierens zu
einer Lösung gelangt. Solche Algorithmen sind im Allgemeinen sehr
ineffizient und tatsächlich benötigt dieses Verfahren $n^2$~Vergleiche
($n$: Anzahl der Rechteckkanten). Besteht der Input nämlich nicht wie
hier aus einigen wenigen Linien, sondern sollen statt dessen eine Million
Strecken untersucht werden, dann sind bereits $10^{12}$~Vergleiche
erforderlich.

Im ungünstigsten Fall können alle horizontal verlaufenden Linien
alle vertikalen schneiden, und die Hälfte der Kanten verläuft
horizontal, die andere Hälfte vertikal, die Strecken bilden dann eine
Art Schachbrett (siehe \autoref{131003c}). Jede Linie (Dimension) hat $\frac{n}{2}$ Segmente,
die sich mit den anderen $\frac{n}{2}$ Linien schneiden.  Die Ausgabe
(Output) besteht folglich aus $\frac{n^2}{4}$ Paaren. 

\begin{figure}
  \centering
  \begin{tikzpicture}
    \draw[step=.6cm] (-1.4,-1.4) grid (1.4,1.4);
  \end{tikzpicture}
%  \input{131003c.latex}
  \caption{Ungünstigste Kantenanordnung, $\frac{n^2}{4}$ Kreuzungspunkte}
  \label{131003c}
\end{figure}

Es ist somit eine berechtigte Frage, ob es überhaupt einen besseren
Algorithmus als den bereits vorgestellten geben kann.

In praktischen Anwendungen, zum Beispiel dem Chip-Design, ist so ein
Kreuzungsbild wie in \autoref{131003c} eher unwahrscheinlich. Die
Anzahl der Kreuzungspunkte wächst üblicherweise in linearer
Abhängigkeit zu $n$ oder noch geringer. Die Suche nach einem neuen, im
Allgemeinen besseren, Algorithmus ist also sinnvoll.

Wenn wir $k$ als die Anzahl der resultierenden Schnitte für das
Segmentschnitt-Problem auffassen, können wir das Problem wie folgt
formulieren: Existiert ein Algorithmus mit besserer Laufzeit als $n^2$ in 
Abhängigkeit von $n$ und $k$?

Mit dieser neuen Problemstellung wird die Zeitkomplexität von der Länge
der Eingabe \underline{und} der Länge der Ausgabe abhängig, man spricht
von einem \textbf{output-sensitiven} Algorithmus.

Eine grundlegende Idee ist dabei die Reduktion der Komplexität durch
Verminderung der Dimension. Wie kann man also den Test in der
zweidimensionalen Ebene vermeiden und mit einem Test über eine
Dimension, z.\,B. entlang einer Horizontalen, alle Kantenschnitte
finden?

Dazu bedient man sich einer Gleitgeraden, die von links nach rechts
über die gegebenen Liniensegmente streicht und nur an ihrer aktuellen
Position prüft. Dieses Verfahren heißt \textsc{Plane-Sweep-Algorithmus},
eine detaillierte Beschreibung findet sich in \autoref{planesweep}.

Würde der Strahl stetig über die Segmente streichen, gäbe es
überabzählbar viele Prüfpunkte und das Verfahren wäre nicht maschinell
durchführbar. Daher ist es auch hier wie überall sonst in der
elektronischen Datenverarbeitung notwendig, mit diskreten Werten zu
arbeiten.
 
Zu diesem Zweck definiert man eine Menge mit endlich vielen \textbf{event
points} (Ereignispunkte), an denen Schnitte gesucht werden sollen. Im
vorliegenden Fall kommen dafür nur die $x$-Werte in Frage, an denen ein
horizontales Segment beginnt oder endet bzw. die $x$-Koordinaten der
vertikal verlaufenden Linien. Sie bilden die event points, an denen das
Überstreichen simuliert wird. Eine solche Simulation ist einfach zu
beschreiben:

\textit{Speichere die horizontalen Linien und prüfe beim Auftreten einer
senkrechten Linie, ob sie sich mit einer aktuell gemerkten horizontalen
schneidet.}

Prinzipiell geht man nach folgendem Algorithmus vor:
\begin{enumerate}
 \item Ordne die event points nach wachsender $x$-Koordinate (dies sei hier
  als möglich vorausgesetzt, eine Diskussion über Sortierverfahren erfolgt
  später)
 \item Menge $Y \coloneqq \emptyset$  
  \begin{itemize}
   \item \textit{INSERT (Aufnahme)} der horizontalen Strecken bzw.
   \item \textit{DELETE (Entfernen)}
  \end{itemize}
  der zugehörigen $y$-Werte
 \item bei den event-points (vertikale Strecken):
  \begin{itemize}
   \item \textit{SEARCH (Suche)} im Vertikalintervall nach $y$-Werten aus der 
    Menge $Y$
   \item Ausgabe der Schnitte
  \end{itemize}
\end{enumerate}

Die Implementierung des obigen
\textsc{Plane-Sweep-Algorithmus'} in einer konkreten Programmiersprache
sei als zusätzliche Übungsaufgabe überlassen. Dazu ist jedoch ein dynamischer
Datentyp für die Menge $Y$ erforderlich, der die Operationen INSERT,
DELETE und SEARCH unterstützt. Dieser wird in einem späteren Kapitel noch behandelt. 

\begin{definition}[Datenstruktur]
  Die Art und Weise wie Daten problembezogen verwaltet und organisiert
  werden, nennt man Datenstruktur.
\end{definition}

\label{ADT}
\begin{definition}[Unterstützte Operationen]
  Eine Operation heißt unterstützt, wenn sie für eine Eingabe
  der Länge $n$ proportional nicht mehr als $\log(n)$ Teilschritte und
  somit proportional $\log(n)$ Zeiteinheiten benötigt.
\end{definition}

\begin{definition}[Abstrakter Datentyp (ADT)]
  Eine Datenstruktur, zusammen mit den von ihr unterstützten Operationen, 
  heißt abstrakter Datentyp (ADT).
\end{definition}

\begin{definition}[dictionary]
  Der abstrakte Datentyp, der das Tripel der Operationen \textit{INSERT},
  \textit{DELETE}, \textit{SEARCH} unterstützt, heißt \textbf{dictionary}.
\end{definition}

Der obige \textsc{Plane-Sweep}-Algorithmus benötigt $\OO(n\log(n))$~Schritte
für das Sortieren (siehe unten $\OO$"=Notation). Da es sich um einen \textbf{output-sensitiven}
Algorithmus handelt, belaufen sich seine Gesamtkosten auf
$\OO(n\log(n)+k)$, was meist deutlich besser ist als proportionales Verhalten
zu $n^2$.

Dieses Verfahren erweist sich jedoch mit Hilfe von Arrays als nicht
realisierbar, ein guter Algorithmus nützt folglich nichts ohne eine
geeignete Datenstruktur.

Um die Effizienz von Algorithmen genauer betrachten zu können, ist es
erforderlich, sich im Vorfeld über einige Dinge zu verständigen:

\begin{description}
 \item[Maschinenmodell:] RAM (Random access machine)
  \begin{itemize}
   \item Speicherzugriffszeiten werden ignoriert
   \item arbeitet mit Maschinenwörtern fester Länge für Zahlen und Zeichen
   \item Speicher ist unendlich groß
   \item Operationen wie Addition, Subtraktion, Multiplikation und Division
    sind in \underline{einem} Schritt durchführbar
  \end{itemize}
 \item[Zeitkomplexität:] Mißt die Rechenzeit des Algorithmus
  \begin{itemize}
   \item{abhängig von der Größe der Eingabe und der Art}
   \item{immer für einen konkreten Algorithmus}
  \end{itemize}
\end{description}

\begin{definition}
  \begin{description}
   \item[best-case:] minimale Rechenzeit für einen Input der Länge $n$
   \item[average-case:] mittlere Rechenzeit für einen Input der Länge $n$
   \item[worst-case:] maximale Rechenzeit für einen Input der Länge $n$
  \end{description}
\end{definition}

Für die Analyse von Algorithmen ist der \textbf{worst-case} von ganz
besonderer Bedeutung. Über ihn sind viele Dinge bekannt, er wird
damit mathematisch fassbar und gut berechenbar. Ein Algorithmus,
der für den worst-case gut ist, ist auch in allen anderen Fällen gut.

Die worst-case-Zeit $T$ für den Algorithmus $a$ für Eingaben der Länge $n$
ist das Maximum der Laufzeiten für alle möglichen Eingaben dieser
Länge:
\begin{gather*}
  T_a(n) = \max_{w\colon \abs{w}=n}\{T_a(w)\}
\end{gather*}

Es sei an dieser Stelle angemerkt, daß die worst-case-Betrachtung mitunter
einen verzerrten Eindruck liefert und average-case-Betrachtungen aus
praktischen Gründen die bessere Wahl darstellen.

Zur Bewertung von Algorithmen haben sich verschiedene Notationen
etabliert:
\begin{definition}
  \begin{description}
   \item[$\OO$-Notation:]
    Es bedeutet $T_a(n) \in \OO(n^2)$, dass der Algorithmus $a$
    \emph{höchstens} proportional zu $n^2$ viele Schritte benötigt. 
   \item[$\Omega$-Notation:]
    $T(n) \in \Omega (n^2)$ bedeutet, dass der Algorithmus \emph{mindestens}
    proportional zu $n^2$ viele Schritte benötigt. 
   \item[$\Theta$-Notation:]
    Der Algorithmus benötigt höchstens aber auch mindestens so viele
    Schritte wie angegeben (Verknüfpung von $\OO$- und $\Omega$-Notation)
  \end{description}
\end{definition}
Die genaue mathematische Definition erfolgt später.

Zusammen mit dem vereinbarten
Maschinenmodell läßt sich nun die Frage untersuchen, wieviel Schritte
der obige \textsc{Plane-Sweep-Algorithmus} benötigt.
Das Sortieren der event points zu Beginn erfordert $\Theta(n\log~n)$
Operationen (der Beweis hierfür erfolgt später). Damit ist $n\log(n)$
auch für den Gesamtalgorithmus eine untere Schranke.

Wenn die Menge $Y$ in einer Datenstruktur verwaltet wird, die 
\textit{INSERT}, \textit{DELETE} und \textit{SEARCH} unterstützt, so
reichen $\OO(n \log n)$~Schritte sowie zusätzlich $\OO(k)$~Schritte für
die Ausgabe. Der gesamte Algorithmus ist somit in $\OO(n\log n+k)$ zu
bewältigen.

% 15.10.03 Nina Kottenhagen
\section{Über schnelle und langsame Algorithmen}
\begin{table}[h]
  \begin{tabular}{*{7}{|l}|}
    \hline
    Zeit/ & $1$ sec & $10^2$ sec & $10^4$ sec & $10^6$ & $10^8$ & $10^{10}$ \\
    Komplexität & | & $\approx$ 1,7 min & $\approx$ 2,7 Std & 12 Tage & 3 Jahre & 3 Jhd.\\
    \hline
    $1000n$ & $10^3$ & $10^5$ & $10^7$ & $10^9$ & $10^{11}$ & $10^{13}$ \\
    $100n\log n$ & $1,4*10^2$ & $7,7*10^3$ & & & & $2,6*10^{11}$ \\
    $100n^2$ & $10^2$ & $10^3$ & $10^4$ & $10^5$ & $10^6$ & $10^7$\\
    $10n^3$ & 46 & $2,1*10^2$ & $10^3$ & $4,6*10^3$ & $2,1*10^4$ & $10^5$ \\
    \multicolumn{7}{|c|}{\dotfill}\\
%    \hline
%    \hline
    $2^n$ & 19 & 26 & 33 & 39 & 46 & 53\\
    $3^n$ & 12 & 16 & 20 & 25 & 29 & 33\\
    \hline
  \end{tabular}
  \caption{Zeitkomplexität im Verhältnis zur Eingabegröße}
\end{table}

Die Tabelle verdeutlicht die Wichtigkeit schneller Algorithmen. 
In der linken Spalte steht die Rechenzeit, bezogen auf die Eingabegröße, von Algorithmen. 
Die Tabelleneinträge geben an, wie groß eine Eingabe sein darf, damit ihr Ergebnis in der angegebenen Zeit berechnet werden kann. 

Der konkrete Algorithmus ist hier irrelevant; z.\,B. werden mit der ersten Zeile alle Algorithmen mit der Laufzeit 1000 $n$ erfaßt. 

Bei Algorithmen mit einer schnell wachsenden Laufzeit kann auch in wesentlich mehr Zeit bzw. mit wesentlich schnelleren Rechnern nur ein
minimal größeres Problem gelöst werden. Deswegen ist es wichtig, Algorithmen zu finden, deren Rechenzeit bei einer wachsenden
Eingabegröße möglichst langsam wächst.

Die punktierte Linie ist nach Cook "`die Trennung zwischen Gut und
Böse"'. Die Laufzeit der ersten Zeile ist linear und somit sehr gut.
Für kleine Eingaben reichen auch Laufzeiten von $\OO(n^2)$ und von $\OO(n^3)$ aus.

\section{Die Klassen \textit{P} und \textit{NP}}
Das Problem des Handlungsreisenden, manchmal auch mit \textbf{TSP} abgekürzt, sieht wie folgt aus:
Es gibt n Städte, die alle einmal besucht werden sollen. Dabei soll die
Rundreise so kurz wie möglich sein.

Dieses Problem hat eine exponentielle Laufzeit von $\OO(n!)$, da alle
Reihenfolgen durchprobiert werden müssen.  Es dauert zu lange, die beste
Route herauszufinden. Eine Möglichkeit wäre es, nur eine Route zu
testen.

Das TSP ist ein typisches Beispiel für Probleme der Klasse NP. 
Nichtdeterministisch kann es in polynomialer Zeit  gelöst werden, 
deterministisch nur in exponentieller Zeit (es sei denn P=NP). Dies führt zur Definition der Klassen \textit {P} und \textit {NP}.

\begin{description}
 \item[P:] Die Klasse der Probleme, die für eine Eingabe der Größe n in
  $Pol(n)$ (polynomialer) Zeit gelöst werden können.
 \item[NP:] Die Klasse der Probleme, die für eine Eingabe der Größe n
  \textbf{nichtdeterministisch} in $Pol(n)$ Zeit gelöst werden können 
  (nur überprüfen, ob die Lösung richtig ist, typisch sind
  rate-und-prüfe-Verfahren).
\end{description}

Die große Frage lautet: $P=NP$? Kann man nichtdeterministische Algorithmen durch deterministische ersetzen,
die im Sinne der $\OO$"=Notation ebenfalls polynomiale Laufzeit haben?
Dieses offene Problem besteht seit 1970; bisher gibt es keine Ansätze zu einer Lösung. Ein
dazu äquivalentes Problem wurde aber bereits 1953 von G. Asser formuliert.

Mehr dazu gibt es in der Informatik IV, hier beschäftigen wir uns ausschließlich mit effizienten Algorithmen.

\section{Effiziente Algorithmen}

\begin{definition}[Effiziente Algorithmen]
  Ein Algorithmus, dessen Laufzeit im schlimmsten Fall (\emph{worst
  case}) von $\OO(n^k)$ für konstantes $k$ nach oben beschränkt wird, d.\,h. er hat
  \emph{polynomielle Laufzeit}, heißt effizient.
  
  Es gilt also für ein festes $k$:
  \begin{gather*}
    T_a (n)= \OO(n^k)     
  \end{gather*}
\end{definition}

\subsection{Die Zeitanalyse an einem Beispiel}

Allgemein stellt sich ein Sortierproblem wie folgt dar:
\begin{description}
 \item[INPUT] Folge $<a_1, \ldots, a_n>$, $n \in \N$, $a_i$
  Elemente einer linear geordneten Menge (also eine Menge mit einer totalen, transitiven,
  reflexiven und antisymmetrische Relation)
 \item[OUTPUT:] umgeordnete Folge $<a_{\Pi(1)}, \ldots, a_{\Pi(n)}>:
  a_{\Pi(1)} \le a_{\Pi(2)} \le {\ldots} \le a_{\Pi(n)}$, $\Pi$:
  Permutation
\end{description} 
Die zu sortierende Folge liegt meist in einem Feld \verb@A[1@\ldots\verb@n]@ vor.

\begin{definition}[\textsc{InsertionSort}]
  Dieser Algorithmus funktioniert so, wie viele Menschen Karten
  sortieren.
  
  Man nimmt eine Karte auf und vergleicht diese nacheinander
  mit den Karten, die man bereits in der Hand hält. Sobald die Karte
  wertmäßig zwischen zwei aufeinaderfolgenden Karten liegt, wird sie
  dazwischen gesteckt.
  
  Die Karten vor uns auf dem Boden entsprechen der nicht sortierten Folge
  in dem Feld $A$. Der Algorithmus dafür sind nun wie folgt aus:

  \begin{algorithm}
    \Begin{
    \For{$j\leftarrow2$ to length(A)}
    {$key \leftarrow A[j]$\;
    $i\leftarrow j-1$\;
    \While{$i>0$ and $A[i]> key$}
    {$A[i+1]\leftarrow A[i]$\;
    $i\leftarrow i-1$}
    $A[i+1] \leftarrow key$}}
\caption{\textsc{InsertionSort}}
  \end{algorithm}
\end{definition}

\subsection{Beispiel für \textsc{InsertionSort}}

\begin{description}
 \item[INPUT:] $A=<5,1,8,0>$
 \item[OUPUT:] $<0,1,5,8>$
 \item[Problemgröße:] (= Größe der Eingabe) $n$
 \item[Ablauf:]
  \begin{tabular}[t]{*{4}{c}}
    \texttt{for}-Zähler & \texttt{key} & \texttt{i} & Feld $A$
       {(\small Am Ende der \texttt{while})}\\
    \hline
    Anfang& & & $<5,1,8,0>$\\
    $j=2$ & 1 & 0 & $<1,5,8,0>$\\
    $j=3$ & 8 & 1 & $<1,5,8,0>$\\
    $j=4$ & 0 & 3 & $<1,5,0,8>$\\
    $j=4$ & 0 & 2 & $<1,0,5,8>$\\
    $j=4$ & 0 & 1 & $<0,1,5,8>$
  \end{tabular}
\end{description}

Nun stellt sich natürlich die Frage nach der Laufzeit des Algorithmus für eine Eingabe der Größe $n$.
Dazu treffen wir erstmal folgende Festlegungen. Die Laufzeit wird im weiteren als Komplexität des Algorithmus bezeichnet.


 \textbf {Definition:} $c_i$ sind die Kosten für die Ausführung der i-ten Zeile des Algorithmus.

 \textbf {Definition:} $t_j$ ist Anzahl der Ausführungen des Tests der while-Bedingung für A[j].


In der Tabelle wird die Anzahl der Ausführungen jeder Zeile des Pseudocodes angegeben. Daraus errechnen wir dann
die Komplexität des Algorithmus, so erhalten wir eine Aussage über die Güte des Verfahrens.

\begin{table}[h]
  \begin{tabular}{*{3}{l}}
    Befehl & $c_i$ & Anzahl der Aufrufe\\
    \hline
    1 & $c_1$ & $n$\\
    2 & $c_2$ & $n-1$\\
    3 & $c_3$ & $n-1$\\
    4 & $c_4$ & $\sum_{j=2}^{n}t_j$\\
    5 & $c_5$ & $\sum_{j=2}^{n}t_j-1$\\
    6 & $c_6$ & $\sum_{j=2}^{n}t_j-1$\\
    7 & $c_7$ & $n-1$\\
  \end{tabular}
  \caption{Kosten für \textsc{InsertionSort}} 
\end{table}

\begin{align*}
  T(n) &= c_1 n + c_2(n-1) + c_3(n-1) + c_4\sum_{j=2}^{n}t_j
     + (c_5+c_6)\sum_{j=2}^{n}(t_j-1)\\
  &=\underbrace{(c_1+c_2+c_3+c_4+c_7)}_{a}n -
     \underbrace{(c_2+c_3+c_4+c_7)}_{b} +
     \underbrace{(c_4+c_5+c_6)}_{d}\sum_{j=2}^{n}(t_j-1)\\
  &=  a n-b+d\sum_{j=2}^{n}(t_j-1)
\end{align*}

Als Laufzeiten erhalten wir also
\begin{tabbing}
bester Fall (schon sortiert): \= $ \forall j,t_j=1 \Rightarrow$ \= $T(n)=\sum_{j=2}^{n}(j-1)= \frac {n^2-n}{2}$ \= $\OO(n^2)$ \kill
bester Fall (schon sortiert): \> $ \forall j,t_j=1 \Rightarrow$ \> $T(n)=a*n-b$ \> $=\Theta(n)$ \\ 
schlechtester Fall: \> $ \forall j,t_j=j \Rightarrow$ \> $T(n)=\sum_{j=2}^{n}(j-1)= \frac {n^2-n}{2}$ \> $=\OO(n^2)$\\
durchschnittlicher Fall: \> \> $T(n)=\frac{d}{4}n^2+(a-\frac{d}{4})n-b$ \> $=\OO(n^2)$
\end{tabbing}

 Anmerkungen dazu:

Für den durchschnittlichen Fall wird angenommen, daß alle
Inputreihenfolgen gleichwahrscheinlich sind.
Dieser Algorithmus braucht auch im \textbf{average case} $\OO(n^2)$. Für das Sortieren gibt es
bessere Algorithmen.

\section{Die $\OO$-Notation}
$\forall_n^\infty$ bedeutet für alle $n$ bis auf endlich viele
Ausnahmen, gleichbedeutend mit $\exists n_0 \in \N\colon  \forall n\geq n_0$
\begin{align*}
  \OO(g(n)) &\coloneqq \Set{ f(n) | \exists c_2 > 0, n_0 \in \N\
  \forall_n^\infty\colon  0\leq f(n)\leq c_2 g(n)}\\
  \Omega(g(n)) &\coloneqq \Set{ f(n) | \exists{}c_1 > 0, n_0
  \in\N\ \forall_n^\infty\colon  0\leq c_1 g(n)\leq f(n) } \\
  \Theta(g(n)) &\coloneqq \Set{ f(n) | \exists c_1, c_2 > 0, n_0 \in \N\ \forall_n^\infty\colon  c_1 g(n)\leq f(n)\leq c_2 g(n)} = \OO(g(n)) \cap \Omega(g(n))\\
o(g(n)) &\coloneqq \Set{ f(n) | \exists c_2 > 0, n_0 \in \N\ \forall_n^\infty\colon  0\leq f(n)< c_2 g(n)}\\
\omega(g(n)) &\coloneqq \Set{ f(n) | \exists{}c_1 > 0, n_0 \in  \N\ \forall_n^\infty\colon  0\leq c_1 g(n)< f(n)}
\end{align*} 

$f$ wächst mit zunehmendem $n$ proportional zu $g$.

%
%
% Jonas Melzer

\subsection{Ein Beispiel}
Es sei $f(n) = n^2+99n$

\begin{enumerate}
\item   Behauptung: $f \in \OO(n^2)$\\
        Beweis: Gesucht ist ein $c>0$ und ein $n_0 \in \mathbf{N}$, für das gilt $f(n)\leq c n^2 $ 
        für alle $n\geq n_0$\\
        Das bedeutet konkret für unsere Behauptung:\\
        $f(n)= n^2+99n \leq n^2+99n^2 =100 n^2$.\\
        Mit den Werten $c=100$ und $n_0 = 1$ ist unsere Behauptung erfüllt.

\item   Behauptung: $f \in \Omega(n^2)$\\
        Hier werden Werte $c>0, n_0 \in \mathbf{N}$ gesucht für die gilt: $f(n) \geq c n^2\forall n\geq n_0$. Also $n^2+99n \geq
        c n^2$. Das läßt sich umformen zu $99n \geq (c-1)n^2$ und weiter zu $99 \geq (c-1)n$, also ist jedes $c\colon  0<c \leq 1$ eine
        Lösung.

\item   Behauptung: $f \in \Theta(n^2)$\\
        Beweis: $f \in \OO(n^2), \quad f(\Omega(n^2)) \Rightarrow f \in \OO(n^2)\cap \Omega(n^2)=\Theta(n^2)$

\item   Behauptung: $f \in \OO(n^2 \log\log n)$\\
        Beweis: Übung
\end{enumerate}

\subsection{Einige Eigenschaften}
\begin{description}
\item[Transitivität:]
\begin{tabbing}
platzhalter \= \kill
        \> $f(n) \in \OO(g(n))$ und $g(n) \in \OO(h(n))=f(n) \in \OO(h(n))$\\
        \> $f(n) \in o(g(n))$ und $g(n) \in o(h(n)) \Rightarrow f(n) \in o(h(n))$\\
        \> $f(n) \in \Omega(g(n))$ und $g(n) \in \Omega(h(n))=f(n) \in \Omega(h(n))$\\
        \> $f(n) \in \omega(g(n))$ und $g(n) \in \omega(h(n)) \Rightarrow f(n) \in \omega(h(n))$\\
        \> $f(n) \in \Theta(g(n))$ und $g(n) \in \Theta(h(n))=f(n) \in \Theta(h(n))$
\end{tabbing}
\item[Reflexivität:]
        $f(n) \in \OO(f(n)), \quad f(n) \in \Theta(f(n)), \quad f(n) \in \Omega(f(n))$
\item[Symmetrie:]
        $f(n) \in \Theta(g(n)) \Leftrightarrow g(n) \in \Theta(f(n))$
\item[Schiefsymmetrie:]
        $f(n) \in \OO(g(n)) \Leftrightarrow g(n) \in \Omega(f(n))$
\end{description}
$\Theta$ ist eine Äquivalenzrelation auf der Menge der schließlich positiven Funktionen.
$\OO,o,$ $\Omega,\omega$ sind nichtlineare (totale) Ordnungen.

Beispiel: $f(n)=n$ und $g(n)=n^{1+sin(n\Pi)}$ sind nicht vergleichbar mittels der $\OO$-Notation.

\subsection{Konventionen}
\begin{enumerate}
\item   Wir führen die Gaußklammern $\floor{}$ und $\ceil{}$ ein,
  wobei $\floor{x}$ bzw.$\ceil{x}$ die größte bzw. kleinste ganze Zahl
  kleiner oder gleich bzw. größer oder gleich $x$
  bezeichnet. Beispielsweise ist $3=\floor{3,5}\leq
  3,5\leq\ceil{3,5}=4$.
\item Der Logarithmus $\log$ soll immer als $\log_2$, also als dualer
  Logarithmus interpretiert werden. Im Sinne der $\OO$"=Notation ist
  das irrelevant, da Logarithmen mit unterschiedlicher Basis in einem
  konstanten Verhältnis zueinander stehen. Beispielsweise ist $\log_2 n =
  2 \log_4 n$.
\item  Es gilt, $\log^{(0)}n \coloneqq n, \log^{(i)}n \coloneqq
  \log^{(i-1)}\log n$ und
\item   $\log^{*}n \coloneqq \min \Set{i |\log^{(i)}n \leq1}$. Es
  gilt, $\lim_{n \rightarrow \infty}{\log^{*}n} = + \infty$.
\end{enumerate}

\subsection{Eine Beispieltabelle}
Die folgende Tabelle enthält, aufsteigend nach dem Wachstum geordnet, Beispielfunktionen.
Dabei soll gelten: $f(n)=o(g(n));0 < \alpha < \beta,0<a<b,1<A<B,
\alpha,\beta,a, b, A, B \in \R$.

Die Linie zwischen Formel Nummer neun und zehn repräsentiert die bereits erwähnte Cook'sche Linie.


\begin{table}[h]
\begin{tabular}{c|cr}
    Nummer      &Funktion       \\ \hline
    1   &       $\alpha (n)$    \\
    2   &       $\log^{*}n$     \\
    3   &       $\log \log n $  \\
    4   &       ${(\log n)}^{\alpha}$ \\
    5   &       ${(\log n)}^{\beta}$  \\
    6   &       $n^a$           \\
    7   &       $n (\log n)^{\alpha}$      \\
    8   &       $n^{\alpha}{(\log n)}^{\beta}$  \\
    9   &       $n^b$           & noch polynomial\\ \hline
    10  &       $A^n$           & exponentiell\\
    11  &       $A^n n^a$       \\
    12  &       $A^n n^b$       \\
    13  &       $B^n$           
\end{tabular}
\end{table}

Desweiteren gilt die folgende Regel:

$(f_1(n)+ \dots +f_m(n)) \in \OO (\max\{f_1(n),\dotsc,f_m(n)\})$, mengentheoretisch ausgedrückt gilt also: $\OO(f_1(n)) \cup \dots
\cup \OO(f_m(n)) = \OO (\max\{f_1(n),\dotsc,f_m(n)\})$


\chapter{Jetzt geht's los}

\section{Rekurrenzen}
Hier werden ein rekursive Ansätze verwendet. Das Ausgangsproblem wird also in immer kleinere Teilprobleme zerlegt.
Irgendwann liegt, analog zu einem induktiven Beweis, ein Trivialfall vor, der sich einfach lösen läßt. Aus den Lösungen der Trivialfälle
wird dann sukzessiv eine Lösung des Gesamtproblems konstruiert.

\subsection{Sortieren über Divide and Conquer (Teile und Herrsche)}
%
% Christoph Henniger
%
Dazu wird zunächst das Verfahren \textsc{MergeSort} vorgestellt und
anhand eines Beispiels verdeutlicht.
\begin{definition}[\textsc{MergeSort}]
  Eine Folge $A=a_l,\ldots, a_r$ von $n=r-l+1$~Schlüsseln wird
  sortiert, indem sie zunächst rekursiv immer weiter in möglichst
  gleich lange Teilfolgen gesplittet wird. Haben die Teilfolgen die
  Länge 1 können jeweils zwei durch einen direkten Vergleich sortiert
  werden. Dann werden die Teilfolgen wieder schrittweise
  zusammengemischt, bis schließlich die sortierte Eingabe vorliegt.
\end{definition}

%Sebastian Oerding
%\subsection{Ein Beispiel für \textsc{MergeSort}}
\begin{bsp}
  Gegeben sei die Zahlenfolge $1,7,8,3,4,6,5,9$. Wir wollen diese
  mittels \textsc{MergeSort} in die korrekte Reihenfolge bringen. Der
  Algorithmus verläuft in zwei Schritten. Im ersten Schritt wird die
  Eingabe aufgesplittet. Das heißt, die Eingabe wird sukzessive in
  zwei Hälften geteilt.

\begin{tikzpicture}[level 1/.style={sibling distance=40mm},
  level 2/.style={sibling distance=20mm},
  level 3/.style={sibling distance=10mm}]
  \node {$1~7~8~3~4~6~5~9$}
  child {node {$1~7~8~3$}
  child {node {$1~7$}
  child {node {$1$}}
  child {node {$7$}}}
  child {node {$8~3$}
  child {node {$8$}}
  child {node {$3$}}}}
  child {node {$4~6~5~9$}
  child {node {$4~6$}
  child {node {$4$}}
  child {node {$6$}}}
  child {node {$5~9$}
  child {node {$5$}}
  child {node {$9$}}}};
\end{tikzpicture}

Im zweiten Schritt des Algorithmus' werden nun die sorierten Teilfolgen wieder
gemischt und im Ergebnis erhält man die sortierte Liste.

\begin{tikzpicture}[level 1/.style={sibling distance=40mm},
  level 2/.style={sibling distance=20mm},
  level 3/.style={sibling distance=10mm}]
  \node {$1~3~4~5~6~7~8~9$} [grow'=up]
  child {node {$1~3~7~8$}
  child {node {$1~7$}
  child {node {$1$}}
  child {node {$7$}}}
  child {node {$3~8$}
  child {node {$8$}}
  child {node {$3$}}}}
  child {node {$4~5~6~9$}
  child {node {$4~6$}
  child {node {$4$}}
  child {node {$6$}}}
  child {node {$5~9$}
  child {node {$5$}}
  child {node {$9$}}}};
\end{tikzpicture}
\end{bsp}

Das Mischen funktioniert in $\OO(n)$~Zeit, zur Verdeutlichung wird es
nochmal exemplarisch skizziert, dazu werden zwei Folgen mit
$m$~Elementen zusammengemischt.
\begin{gather*}
  \left.
    \begin{array}{l}
      a_1<\ldots<a_m\\
      b_1<\ldots<b_m
    \end{array}
  \right\} c_1<\ldots<c_{2m}
\end{gather*}
Die beiden Folgen werden also zu einer in sich sortierten Folge der doppelten Länge gemischt. Wie im folgenden zu sehen, werden immer genau
zwei Elemente miteinander verglichen. Der Fall, daß zwei Teilfolgen unterschiedliche Länge haben, kann o.\,B.\,d.\,A. ignoriert werden.
\begin{align*}
  b_1<a_1 \leadsto
  \begin{array}{l}
    \downarrow\\
    b_1
  \end{array}
  \leadsto b_2<a_1 \leadsto
  \begin{array}{l}
    \downarrow\\
    b_2
  \end{array}
  \leadsto a_1<b_3 \leadsto
  \begin{array}{l}
    \downarrow\\
    a_1
  \end{array}
  \leadsto \ldots \leadsto b_1, b_2, a_1, \ldots
\end{align*}

An konkreten Zahlen läßt sich das vielleicht noch besser verfolgen, das Mischen im letzten Schritt aus dem Beispiel sähe wie folgt aus.


$1<4 \leadsto$ \raisebox{-2.5ex}{$\stackrel
{\downarrow}
{\text{\textcircled{1}}}$} $\leadsto
3<4 \leadsto$ \raisebox{-2.5ex}{$\stackrel
{\downarrow}
{\text{\textcircled{3}}}$} $\leadsto
7>4 \leadsto$ \raisebox{-2.5ex}{$\stackrel
{\downarrow}
{\text{\textcircled{4}}}$} $\leadsto 
7>5 \leadsto$ \raisebox{-2.5ex}{$\stackrel
{\downarrow}
{\text{\textcircled{5}}}$} $\leadsto
7>6 \leadsto$ \raisebox{-2.5ex}{$\stackrel
{\downarrow}
{\text{\textcircled{6}}}$} $\leadsto$
\medskip

$7<9 \leadsto$ \raisebox{-2.5ex}{$\stackrel
{\downarrow}
{\text{\textcircled{7}}}$} $\leadsto
8<9 \leadsto$ \raisebox{-2.5ex}{$\stackrel
{\downarrow}
{\text{\textcircled{8}}}$} $\leadsto
9<+\infty \leadsto$ \raisebox{-2.5ex}{$\stackrel
{\downarrow}
{\text{\textcircled{9}}}$} $
\leadsto<1,3,4,5,6,7,8,9>$
\medskip

Der Vergleich mit $\infty$ vereinfacht das Mischen, da sich damit eine kompliziertere Fallunterscheidung für den Fall erübrigt, 
daß alle Elemente einer Teilfolge beim Mischen bereits "`verbraucht"' wurden.

Jeder Vergleich von zwei Elementen ergibt ein Element der neuen Folge und es werden immer nur zwei Werte verglichen, was in
$\OO(1)$~Zeit klappt. Also sind zwei Teilfolgen nach $\OO(n)$~Schritten zu einer neuen Folge zusammengemischt und man erhält
$\OO(n)$ als Kosten für das Mischen.
Für das gesamte Verfahren \textsc{MergeSort} ergibt sich die \textit{Rekurrenz} $T(n)=2T(n/2)+\OO(n)$, die zu einer Komplexität von $\OO(n
\log n)$ führt. Verfahren für das Lösen von Rekurrenzen werden nach Angabe des Pseudocodes und einer etwas
formaleren Laufzeitanalyse für \textsc{MergeSort} eingeführt.

% Christoph Henniger
%
\begin{algorithm}
  \Begin{
    \If{$l<r$}{
      $p \leftarrow\floor{\frac{l+r}{2}}$\;
      MergeSort (A,l,p)\;
      MergeSort (A, p+1,r)\;
      Merge}}
  \caption{\textsc{MergeSort}}
\end{algorithm}

\begin{table}[h]
  \begin{tabular}{*{2}{l}}
    Zeile & Asymptotischer Aufwand\\
    \hline
    2 & $\Theta(1)$\\
    3 & $\Theta(1)$\\
    4 & $T(\frac{n}{2})$\\
    5 & $T(\frac{n}{2})$\\
    6 & $\Theta(n)$\\
  \end{tabular}
  \caption{Kosten für \textsc{MergeSort}} 
\end{table}

Zeile 1 und 2 sind elementare Vergleichs- und Zuweisungsoperationen, welche in $\OO(1)$~Zeit möglich sind. 
Dem rekursiven Aufruf von Mergesort in Zeile 3 und 4 wird jeweils nur eine Hälfte der Schlüssel übergeben, 
daher ist der Zeitaufwand je $T(\frac{n}{2})$. Für das Zusammenführen der beiden Teilmengen in Zeile 5 gilt: 
Für zwei Teilmengen der Länge $n_1$ und $n_2$ sind mindestens $min(n_1,n_2)$ und höchstens $n_1 + n_2 -1$ Schlüsselvergleiche notwendig. 
Zum Verschmelzen zweier etwa gleich langer Teilfolgen der Gesamtlänge n, werden also im ungünstigsten Fall $\Theta(n)$ Schlüsselvergleiche benötigt.

Wie bereits gezeigt, gilt für die Gesamtlaufzeit die folgende Rekurrenz $T(n) = 2\cdot T(\frac{n}{2})+\Theta(n)$, zum Lösen einer
Rekurrenz muß aber auch immer eine sog. boundary condition, zu deutsch Randbedingung, bekannt sein, analog zur Lösung des Trivialfalles einer rekursiven
Funktion. Bei \textsc{MergeSort} gilt für $T(1)$ die Randbedingung $T(1) = 1(=\Theta(1))$

Mit Hilfe des Mastertheorems (siehe \autoref{sec:mastertheorem}) ergibt
sich die Lösung, $T(n) = \Theta(n \log n)$. Dabei wurden die
Gaußklammern  weggelassen. Da das vertretbar ist, werden wir das auch
in Zukunft tun.

\begin{description}
	\item [Binärbaum-Paradigma:] 
\end{description}
	
$ \input{221003a.latex} $

\begin{definition}
  Ein Binärbaum ist eine Menge von drei Mengen von Knoten. Zwei davon
  sind wieder Binärbaume sind und heissen linker bzw.  rechter
  Teilbaum, die dritte Menge ist die Einermenge \{ROOT\}. Andernfalls
  ist die Menge leer.
	  
  Bei einem Baum, der nur aus der Wurzel besteht, sind also die Mengen
  linker Teilbaum und rechter Teilbaum jeweils die leere Menge.
\end{definition}

$ \input{221003b.latex} $

Die Anwendung des Binärbaumparadigmas für parallele Algorithmen wird durch die folgende, grobe Schilderung deutlich.
Man stellt sich vor, auf jedem inneren Knoten des Baumes sitzt ein Prozessor, welcher parallel von Level zu Level fortschreitend die
Aufgabe löst.

Paralleles und optimales Sortieren benötigt $\OO(\log n)$~Zeit. Der Beweis hierfür (Richard Cole) ist extrem schwer und wird an dieser Stelle nicht aufgeführt.

Dafür folgt hier ein einfacheres Beispiel: Addiere $n$~Zahlen, die
Eingabe liege wieder als Liste $A=(a_1,\ldots,a_n)$ vor.

\begin{algorithm}
  \Begin{
  Zerlege $A$ in zwei Hälften $A_{1}, A_{2}$\;
  Add ($A_{1}$)\;
  Add ($A_{2}$)\;
  Add ($A_{1}$) $+$ Add ($A_{2}$)\;
  Ausgabe}
\caption{\textsc{ParallelSort}}
\end{algorithm}

$ \input{221003c.latex} $

Für die Laufzeit gilt nun die Rekurrenz $T_{\text{parallel}}(n) =
T_{\text{parallel}}(\frac{n}{2}) + \OO(1)$, also $T(n)=
T(\frac{n}{2})+1$ und nach Auflösen der Rekurrenz $T(n) = \OO(\log
n)$.

\section{Methoden zur Lösung von Rekurrenzen}
\label{sec:methoden-zur-losung-von-rekurr}
Viele Algorithmen enthalten rekursive Aufrufe. Die Zeitkomplexität solcher Algorithmen wird oft durch Rekurrenzen beschrieben.
\begin{definition}[Rekurrenzen]
Rekurrenzen sind Funktionen, welche durch ihren eigenen Wert für kleinere Argumente beschrieben werden. Für den Trivialfall muß wie
bei jeder rekursiven Funktion eine Lösung angegeben werden.
\end{definition}
Dieses Kapitel liefert vier Ansätze zum Lösen von Rekurrenzen.
%
%
Begonnen wird mit der Methode der vollständigen Induktion (Substitutionsmethode),
bei der zur Lösung von Rekurrenzgleichungen im ersten Schritt eine mögliche Lösung der Gleichung erraten wird,
die im zweiten Schritt mittels vollständiger Induktion bestätigt werden muß.		

	\begin{description}
		\item [Beispiel:] $T(n) = 2 T(\frac{n}{2}) + n$, Randbedingung: $T(1)=1$

		Ansatz: $T(n) \leq c n \log n$; c ist konstant, $c \geq 1$
		\begin{tabbing}
			$T(n)$ \= $= 2 T(\frac{n}{2})+n\leq 2 c (\frac{n}{2}) \log(\frac{n}{2})+n$\\
			\> = $c n \log n-c n \log 2+n$\\
			\> = $c n \log n-c n+n \leq c n \log n$\\
		\end{tabbing}
		$\Rightarrow T(n)=\OO(n \log n)$
	\end{description}

	\begin{description}
		\item [Beispiel:] $T(n) = 2 T(\frac{n}{2}) + b$, Randbedingung: $T(1)=1$

		Ansatz: $T(n)=\OO(n)\Rightarrow T(n)\leq c n$; c ist konstant
		\begin{tabbing}
			$T(n)$ \= $= 2 T(\frac{n}{2})+b$\\
			\> = $2c \frac{n}{2} +b$\\
			\> = $c n+b$; Annahme nicht erfüllt, kein ausreichender Beweis
		\end{tabbing}
		neuer Ansatz: $T(n)\leq c n-b$
		\begin{tabbing}
			$T(n)$ \= $=2 T(\frac{n}{2} )+b$\\
			\> $\leq 2(c \frac{n}{2}-b)+b$\\
			\> $=c n-2 b+b$\\
			\> $\leq c n-b$
		\end{tabbing}
		$\Rightarrow T(n)=\OO(n)$
	\end{description}
			
Als nächstes wird die Methode der Variablensubstitution gezeigt,
bei der ein nicht elementarer Teilausdruck durch eine neue Variable substituiert wird. Dabei ist es wesentlich, daß
sich die dadurch neu entstandene Funktion gegenüber der Ausgangsfunktion vereinfacht. Die vereinfachte Rekurrenzgleichung wird
mittels anderer Verfahren gelöst. Das Ergebnis wird anschließend rücksubstituiert.
	\begin{description}
		\item [Beispiel:] $T(n) = 2 T(\ceil{\sqrt{n}}) + \log n$, Randbedingung: $T(1)=1$

		Substitutionsansatz: $k\coloneqq\log n \Rightarrow 2^k=n$, nach Einsetzen gilt also: $T(2^k) = 2 T(2^{\frac{k}{2}})+k$
		
		Jetzt wird $S(k) \coloneqq T(2^k)$ gesetzt und somit gilt $S(k) = 2 S(\frac{k}{2})+k$, die Auflösung dieser
		Rekurrenz z.\,B. mittels Induktion sei als Übungsaufgabe überlassen und ist deswegen im Anhang zu finden. 
		Für die Lösung der Ausgangsrekurrenz muß dann aber noch die Substitution rückgängig gemacht werden, dabei sei
		ebenfalls auf \autoref{lsg_substitutionsansatz} verwiesen. 
	\end{description}	

Als dritttes wird die Methode der Rekursionsbäume vorgestellt.
Hierbei wird die Funktion mittels eines Rekursionsbaumes dargestellt. Dabei wird der nicht-rekursive Funktionsanteil in jeden Knoten
geschrieben. Für jeden rekursiven Aufruf pro Funktionsaufruf erhält jeder Knoten einen Sohnknoten. Dies wird solange fortgeführt bis
in den Blättern ein Wert $< 1$ steht. Die Summe aller Knoteneinträge bezeichnet die Lösung der Rekurrenz.
	\begin{description}
		\item [Beispiel:] $T(n) = 2 T(\frac{n}{2}) + n$, Randbedingung: $T(1)=1$
			
		Ansatz: $T(n) = n + \frac{n}{2} + \frac{n}{2} + \frac{n}{4} + \frac{n}{4} + \frac{n}{4} + \frac{n}{4} + 8 \frac{n}{8} + 
		\ldots + {2^k} \frac{n}{2^{k+1}} T(\frac{n}{2^{k+1}})$
			
		\input{221003d.latex}
			
		Der Aufwand jeder Zeile beträgt $\OO(n)$. Der Baum hat eine Höhe von $\log n$. Damit ergibt sich als Lösung: 
		$\OO(n \log n)$			
		
		\item [Beispiel:] $T(n) = 3 T(\frac{n}{4}) + c n^2$
						
		Ansatz: $T(n) = c n^2 + (\frac{3}{16} ) c n^2 + (\frac{3}{16})^2 c n^2 + \cdots +
		(\frac{3}{16})^{\log (n-1)} c n^2 + \Theta(n^{\log_43})$
			
		[es gilt: $n^{\log_43}=3^{\log_4n}$]
			\begin{tabbing}
				$T(n)$ \= $= \sum_{i=0}^{\log_4(n-1)}(\frac{3}{16})^i c n^2+ \Theta(n^{\log_43}) 
				< \sum_{i=0}^{\infty}(\frac{3}{16})^i c n^2 + \Theta(n^{\log_43})$\\
				\> $= \frac{1}{1-\frac{3}{16}} c n^2 + \Theta(n^{\log_43})=\OO(n^2)$
			\end{tabbing}			
		$ \input{221003e.latex} $
	\end{description}
		
\begin{description}
	\item [Beispiel:] $T(n) = T(\frac{n}{3}) + T(\frac{2\cdot n}{3}) + \OO(n)$, Randbedingung: $T(1)=1$
	$ \input{221003f.latex} $
	\end{description}		
%Sebastian Oerding
%
Die vierte Methode, das Lösen mittels des sogenannten Mastertheorems erhält wegen ihres gänzlich anderen Charakters einen eigenen
Abschnitt.
% Annette Eisenbraun
%
\section{Das Mastertheorem}
\label{sec:mastertheorem}
Hier rückt das Wachstum des nicht-rekursiven Summanden im Vergleich zum Wachstum des rekursiven Anteils in den Mittelpunkt. Die Frage
lautet also, wie $f(n)$ im Vergleich zu $T(\frac{n}{b})$ wächst, die Vergleichsgröße ist dabei $n^{\log_ba}$.
Im ersten Fall wächst $f(n)$ langsamer, im zweiten gleich schnell und im dritten polynomial schneller.

Sei $T(n)=a T(\frac{n}{b})+f(n)$ mit $ a \geq 1,b > 1$, 
dann gilt asymptotisch für große $n$.

\begin{enumerate}
\item $f(n) \in \OO(n^{\log_b a-\varepsilon})$ mit $\varepsilon > 0$ fest $\rightarrow T(n) \in\Theta(n^{\log_ba})$

\item $f(n) \in\Theta(n^{\log_ba}) \rightarrow T(n) \in\Theta(n^{\log_ba} \log n)$

\item $(f(n) \in\Omega(n^{\log_ba+\varepsilon} ) $ (mit $ \varepsilon > 0 $ fest)
$\wedge\exists c < 1 \colon  \forall^{\infty}_{n}
a f(\frac{n}{b})  \leq c f(n)) \rightarrow T(n) \in \Theta(f(n))$
\end{enumerate}

Der Beweis ist etwas schwieriger und für diese Vorlesung auch nicht
von allzu großer Bedeutung.
R.\,Seidel hat auf seiner Homepage
von 1995 den Kernsatz bewiesen, der ganze aber etwas kompliziertere Beweis ist in \cite{cormen} zu finden. 

\subsection{Beispiele}
\subsubsection{Beispiel: Binäre Suche} 

Eingabe ist eine sortierte Folge $a_1 < \dots < a_{n}$ und ein Wert
$b$. Ausgegeben werden soll $\iota i\colon a_{i} \leq b < a_{i+1}$,
falls es existiert und ansonsten eine Fehlermeldung.


EXKURS: Binärer Suchbäum
\begin{figure}[H]
	\centering\input{271003a.latex}
	\caption{Binärbaum}
	\label{271003a}
\end{figure}

Eigenschaften:
\begin{itemize}
	\item Die Werte sind in Bezug auf die Größe vergleichbar
	\item Die rechten Söhne eines Knotens enthalten größere Werte als die linken Söhne.
\end{itemize}

\begin{figure}
	\centering\input{271003b.latex}
	\caption{Suche im Binärbaum}
	\label{271003b}
\end{figure}

Bei einer Anfrage an einen höhenbalancierten binären Suchbaum werden in jedem Schritt die in Frage kommenden Werte halbiert (siehe \autoref{271003b}), es
werden praktisch Fragen der folgenden Form gestellt.

 \quad $ b < a_{\frac{n}{2}} $ oder $ b \geq a_{\frac{n}{2}} $?
 
 \quad $ b < a_{\frac{n}{4}} $ oder $ b \geq a_{\frac{n}{4}} $? bzw. $ b < a_{\frac{3n}{4}} $ oder $ b \geq a_{\frac{3n}{4}} $?

 \quad usw. 
 
Die Suche läuft bis zu der Stelle, an der der gesuchte Wert sein müsste. Wenn er nicht dort ist, ist er nicht in der sortierten Folge
vorhanden.
%
% Sebastian Oerding
Der Einschränkung des Suchraumes durch eine Intervallhalbierung entspricht jeweils ein Abstieg innerhalb des Baumes um einen
Höhenlevel.
D.\,h. die Anzahl der Rechenschritte ist so groß, wie der Baum hoch ist und es gilt die Rekurrenz: 
$T(n)=T(\frac{n}{2}) + \OO(1)$

Zur Veranschaulichung einer alternativen Vorstellung, bei der in einem Feld gesucht wird, gehen wir von folgender Wette aus:

\textit{Denke dir eine natürliche Zahl $a$ zwischen 0 und 1000. Wetten, daß ich mit 10 Fragen herausbekomme, welche Zahl du dir gedacht
hast!}

Nun sei 128 die gedachte Zahl, die Fragen sähen dann so aus:

1. Ist a$<$500? $\Rightarrow$ 0$\leq$a$<$500

2. Ist a$<$250? $\Rightarrow$ 0$\leq$a$<$250

3. Ist a$<$125? $\Rightarrow$ 125$\leq$a$<$250

4. Ist a$<$187? $\Rightarrow$ 125$\leq$a$<$187

5. Ist a$<$156? $\Rightarrow$ 125$\leq$a$<$156

6. Ist a$<$141? $\Rightarrow$ 125$\leq$a$<$141

7. Ist a$<$133? $\Rightarrow$ 125$\leq$a$<$133

8. Ist a$<$129? $\Rightarrow$ 125$\leq$a$<$129

9. Ist a$<$127? $\Rightarrow$ 127$\leq$a$<$129

10. Ist a$<$128? $\Rightarrow$ 128$\leq$a$<$129 $\Rightarrow$ a=128 

Bei einer Million Zahlen reichen übrigens 20 Fragen aus!


% 
% Annette Eisenbraun
Alternative Vorstellung schematisch:

\begin{tabular}{lllllll}
$a_1$     & $a_2$           & \dots   &  $ a_{\frac{n}{2}}$ & $ \vert a_{\frac{n}{2}+1}$ & \dots  & $a_n$ \\
          &       ?         & $\geq$  & oder                  & $<$                          &  ?     &\\
$\vert$   &                 &         & $\gets$               & $\vert$ $\to$                &        & $\vert$\\
$\vert$   & $\gets$ & $\vert$ $\to$ & & $\vert$\\
&         & $\vert$ $\gets$ & $\vert$ $\to$ & $\vert$
\end{tabular}


Wie sieht nun die Einordung der binären Suche in das Mastertheorem aus?
Mit a = 1 und b = 2 gilt $\log_ba = 0$, also $\Theta(n^{\log_ba})=\Theta(1)$ und der zweite Fall kommt zur Anwendung.
Also ist $  T(n) \in \Theta(n^{\log_ba}\log n)=\Theta(\log n) $.

\subsubsection{Weitere Beispiele}
\begin{enumerate}
\item $ T(n) = 9 T(\frac{n}{3}) + 3 n\log n, alsoa = 9,b = 3
undn^{\log_ba} = n^{\log_39} = n^2.$

$f(n) = 3n\log n \in \OO(n^{\log_ba-\varepsilon}) $
z.\,B. $\OO(n^{\frac{3}{2}})$ mit $ \varepsilon = \frac{1}{2} $

$ \Rightarrow $ Erster Fall $ T(n) \in \Theta(n^{\log_ba}) = \Theta (n^2) $.

\item $ T(n) = 3 T(\frac{n}{4}) + 2n\log n$.

Die Lösung dieser Übungsaufgabe steht im \autoref{mastertheorem_Fall3}.

\item $ T(n) = 2 T(\frac{n}{2}) + 4n\log nalso\
a = 2,b = 2undn^{\log_ba} = n $.

Wie man weiß: $4n\log n \in \Omega(n)\forall \epsilon >0$ aber $4n\log n \notin \Omega(n^{1+\epsilon})$
	
Es trifft {\underline {kein}} Fall zu!
\end{enumerate}
Das Mastertheorem deckt nicht alle Fälle ab!
	
%\input{skriptb}
\chapter{Sortieren und Selektion}

$\OO(n\log n)$ ist die obere Schranke für \textsc{MergeSort} ($\OO(n^2)$ für \textsc{Insertionsort}).

Frage: Geht es besser? 
\begin{description}
	\item[- Ja]
		\begin{enumerate}
			\item Mit Parallelrechnern, aber das ist nicht Thema dieser Vorlesung.
			\item Unter bestimmten Bedingungen.
		\end{enumerate}
	\item[- Nein]
		bei allgemeinen Sortierverfahren auf der Basis von Schlüsselvergleichen. Unser Ziel ist der Beweis, daß für  
		allgemeine Sortierverfahren auf der Basis von Schlüsselvergleichen $ \Omega(n\log n)$ eine untere Schranke ist.
\end{description} 

\begin{beweis}
\textbf{Modellieren des Ansatzes: "`Auf der Basis von Schlüsselvergleichen"'.}
  \begin{itemize}
  \item INPUT ist ein Array mit (o.\,B.\,d.\,A.) paarweise verschiedenen Werten 
  $ (a_1, \dots , a_{n})$, $a_i \in S$, $i=(1, \dots, n)$ auf das nur
  mit der Vergleichsfunktion  \begin{gather*} V(i,j) : = \left\lbrace
  \begin{array}{ll} 1, & a_i < a_j \\ 0, & a_i > a_j \end{array} \right.\end{gather*}
  zugegriffen werden kann.
  \item OUTPUT ist eine Permutation $ \pi $ für die $ a_{\pi (1)} < a_{\pi (2)} < \dots < a_{\pi (n)} $ ist 
\end{itemize}

Sei $A$ ein beliebiges o.\,B.\,d.\,A. deterministisches
Sortierverfahren dieser Art.  Die erste Anfrage ist dann nicht $ (a_i
< a_j) $, sondern $ (i < j) $.

  \begin{definition}
  $a(i<j) \coloneqq  \lbrace (a_1, \dots , a_n \vert  a_i \in S \wedge a_i < a_j \rbrace $
  \end{definition}

%
% Sebastian Oerding
Der erste Test $V(i,j)$ der Vergleichsfunktion wird immer für dasselbe Indexpaar $(i,j)$ der Eingabe $(a_1, \dots , a_{n})$
ausgeführt, über die der Algorithmus A zu diesem Zeitpunkt noch keinerlei Informationen besitzt.

Falls nun $V(i,j)=1$ ist, d.\,h. für alle Eingaben, die der Menge
$a(i<j)= \Set{(a_1,\ldots,a_{n})\in \R^n | a_{i} <a_{j}}$ angehören, wird der zweite Funktionsaufruf immer dasselbe Indexpaar $(k,l)$ als Parameter
enthalten, da $A$ deterministisch ist und zu diesem Zeitpunkt nur weiß, daß $a_{i}<a_{j}$ ist. Analog wird für alle
Folgen a(j$<$i) derselbe Funktionsaufrauf als zweiter ausgeführt. Die Fortführung dieser überlegung führt zu dem vergleichsbasierten
Entscheidungsbaum von Algorithmus $A$, einem binären Baum, dessen Knoten mit Vergleichen "`$a_{i}<a_{j}$"' beschriftet
sind. An den Kanten steht entweder "`$j$"' oder "`$n$"'. Ein kleines Beispiel ist in\autoref{271003c} zu sehen.

Genau die Eingabetupel aus der Menge $a(3<4)\cap a(3<2)=\{(a_1,\ldots,a_{n})\in \R^n : a_3<a_4 \wedge
a_3<a_2\}$ führen zum Knoten $\mathcal{V}$. 

Weil $A$ das Sortierproblem löst, gibt es zu jedem Blatt des Entscheidungsbaumes eine Permutation $\pi$, so das nur die Eingaben mit 
$a_{\pi(1)}<a_{\pi(2)}<\ldots<a_{\pi(n)}$ zu diesem Blatt führen. Der Entscheidungsbaum hat 
daher mindestens $n!$~Blätter. Der Beweis dafür stammt fast unverändert aus \cite{klein}.
 
%
%Annette Eisenbraun
  \begin{figure}
    \centering\input{271003c.latex}
  \caption{Entscheidungsbaum}
  \label{271003c}
  \end{figure}

Im Regelfall hat ein Entscheidungsbaum allerdings mehr 
als $n!$ Blätter. Es gibt auch Knoten, die die leere Menge enthalten, oder Knoten, die nie erreicht werden können
(z.\,B. Knoten $\mathcal{W}$).


	
Beispiele für den Baum aus \autoref{271003c}:

Bei der Eingabe (3, 4, 17, 25) wäre man nach Abarbeitung der Vergleiche 17$<$25, 4$<$17 und 3$<$4 im linkesten Knoten.


Bei der Eingabe (17, 4, 3, 25) wäre man nach Abarbeitung der Vergleiche 3$<$25 und 4$<$3 im Knoten $\mathcal{V}$.


Wir gehen über $\mathcal{V}$, wenn $a_3 < a_4$ und $a_3 < a_2 $, also für alle Tupel aus $ a(3<4) \cap a(3<2) $.

  \begin{satz}
  Ein Binärbaum der Höhe $h$ hat höchstens $ 2^h $ Blätter.
  \end{satz} 
 
Die Höhe eines Entscheidungsbaumes ist die maximale Weglänge von der Wurzel zu einem Blatt, sie entspricht der Rechenzeit. Wie bereits vorhin angedeutet, muß ein
solcher Baum mindestens $n!$~Blätter haben, wenn er alle Permuationen der Eingabe berücksichtigen können soll (z.\,B. für das
Sortieren), damit muß gelten

  \begin{gather*}
   2^h \geq n! \leftrightarrow h \geq \log n! \geq \log (\frac{n}{2})^{\frac{n}{2}}  
  \end{gather*}
  Der Beweis ist trivial da
  \begin{gather*}
  n! = 1 \cdot 2 \cdot 3 \ldots \cdot n = 1 \cdot 2 \cdot 3 \ldots \cdot \underbrace{(\frac{n}{2}+1) \cdot \ldots 
  \cdot n}_{\frac{n}{2}} \geq (\frac{n}{2})^{\frac{n}{2}} 
  \end{gather*}

$\log (\frac{n}{2})^{\frac{n}{2}} = \frac{n}{2} \cdot \log \frac{n}{2} = \frac{n}{2} \log n - \frac{n}{2}
\underbrace{\log 2}_{1} = \frac{n}{3} \log n + \lbrack \frac{n}{6} \log n - \frac{n}{2} \rbrack \geq \frac {n}{3} \log n $ 

für $ n \geq 8ist\log n \geq 3 \leftrightarrow \frac{n}{6} \log n \geq \frac {n}{2} 
\leftrightarrow \frac{n}{6} \log n - \frac{n}{2} \geq 0$, also $h \geq n \log n$.

Worst-case-Fall im Sortieren hier ist ein Ablaufen des Baumes von der Wurzel bis zu einem Blatt und dies geht bei einem Baum der Höhe $n
\log n$ in $\OO(n \log n)$~Zeit.
\end{beweis}

\section{Verschärfung des Hauptsatzes 1. "`Lineares Modell"'}

$ a_i < a_j \leftrightarrow a_j - a_i > 0
\leftrightarrow \exists  d > 0 : a_j - a_i = d
\leftrightarrow \exists   d > 0 : a_j - a_i - d = 0 $ 


Von Interesse ist, ob $ g(x_1 , \dots , x_n) < 0 $, wobei $ g(x_1 , \dots , x_n) = c_1 x_1 + \dots + c_n x_n + d $ mit
$ c_1, \dots , c_n, d $ als Konstanten und $ x_1, \dots, x_n  $ als Variablen.
Da Variablen nur in linearer Form vorkommen, nennt man dies "`Lineares Modell"'.

\begin{satz}
Im linearen Modell gilt für das Sortieren eine untere Zeitschranke von $ \Omega (n \log n) $.
\end{satz}
		
Der Beweis erfolgt dabei über die Aufgabenstellung "`$\varepsilon$-closeness"'. Denn, wenn die Komplexität der $\varepsilon$-closeness
in einer unsortierten Folge $\Omega(n\log n)$ und in einer sortierten Folge $\OO(n)$ ist, dann muß die Komplexität des
Sortierens $\Omega (n \log n)$ sein.
		
Beim Elementtest ist eine Menge $ \M $, $ \M \subseteq \R^n$ gegeben, sowie ein variabler Vektor $
(x_1, \dots , x_n) $. Es wird getestet, ob $ (x_1, \dots , x_n) \in \M $, wobei $ \M $ natürlich fest ist. 
	
Bei der $\varepsilon$-closeness sieht die Fragestellung etwas anders aus. Als Eingabe sind wieder n reelle Zahlen $a_1, \dots , a_n $
und $ \varepsilon > 0 $ gegeben. Von Interesse ist aber, ob es zwei Zahlen in der Folge gibt, deren Abstand kleiner oder gleich
$\varepsilon$ ist.

Viel kürzer ist die mathematische Schreibweise: $ \exists   i,  j (1 \leq i, j \leq n):\vert a_i - a_j \vert < \varepsilon $?

Trivalerweise ist bei einer bereits sortierten Eingabe $\varepsilon$-closeness in $\OO(n)$ entscheidbar. Dazu wird einfach nacheinander
geprüft, ob $ \vert a_1 - a_2 \vert < \varepsilon $ oder $ \vert a_2 - a_3 \vert < \varepsilon $ oder \dots $ \vert a_{n-1} -
a_n \vert < \varepsilon $.

\begin{satz} 
Wenn $\varepsilon$-closeness die Komplexität $\Omega (n  log  n)$ hat, so auch das Sortieren.
\end{satz}		
		
\begin{satz}
Die Menge  $ \M  $ habe $m$ Zusammenhangskomponenten. Dann gilt, dass jeder Algorithmus im linearen Modell im worst case
mindestens $ \log m $ Schritte braucht, wenn er den Elementtest löst.
\end{satz}
		 	
\begin{beweis}
Jeder Algorithmus A im linearen Modell liefert einen Entscheidungsbaum mit Knoten, in denen für alle möglichen Rechenabläufe
gefragt wird, ob $ g( x_1, \dots , x_n) < 0 $  ist. Jetzt genügt es zu zeigen, daß der Entscheidungsbaum mindestens soviele
Blätter hat, wie die Menge $ \M  $ Zusammenhangskomponenten. Mit dem eben bewiesenen folgt, daß dies äqivalent zu einer Höhe
des Entscheidungsbaumes von $\log (card(\M))=\log m$ ist. Nun sei $b$ ein Blatt des Baumes.

  \begin{definition}
  $E(b) \coloneqq  \lbrace \vec x \in \R^n : $ Alg. A landet bei der Eingabe von $ \vec x = (x_1, \dots ,x_n) $ in Blatt $ b \rbrace $
  \end{definition}
		
  \begin{tabular}{rcl}
  $ g(x_1$, & \dots & , $ x_n) < 0 $ ? \\
  ja / & & $ \backslash $ nein\\
  $ g(x_1, \dots, x_n) < 0 $ & & $ g(x_1, \dots x_n) \geq 0 $
  \end{tabular}

  
Nach Definition des linearen Modells sind diese Mengen $E(b)$ jeweils Durchschnitt von offenen und abgeschlossenen affinen Teilräumen
\begin{gather*}
  \Set{(x_1,\ldots,x_{n}) \in \R^n | g(x_1,\ldots,x_{n})<0}\\
  \Set{(x_1,\ldots,x_{n}) \in \R^n | g(x_1,\ldots,x_{n})\geq0}
\end{gather*}
Die Mengen $E(b)$ sind konvex und insbesondere zusammenhängend. Für alle Punkte a in $E(b)$ trifft der Algorithmus dieselbe
Entscheidung; folglich gilt entweder $E(b) \subset \M$ oder $E(b) \cap \M=\emptyset$. 

Sei $\mathcal{V}$ ein beliebiger Knoten.
Genau die Inputs einer konvexen und zusammenhängenden Menge führen dorthin (= der Durchschnitt von Halbräumen).	
  
  \begin{definition}	
  $ \K $ ist \textit{konvex}
  $\leftrightarrow  \forall   p,   q: p\in \K \wedge q \in \K
  \rightarrow  \overline{pq} \subseteq \K$
  \end{definition}
  
Nun  gilt $\R^n = \bigcup_{b \text{ist Blatt}} E(b)$ also
$ \M = \R^n \cap \M = \bigcup_{b   Blatt} E(b) \cap \M = \dots = \bigcup_{b \in \B} E(b) $
für eine bestimmte Teilmenge $\B$ aller Blätter des Entscheidungsbaumes. Weil jede Menge $E(b)$ zusammenhängend ist, kann
diese Vereinigung höchstens $\vert\B\vert$ viele Zusammenhangskomponenten haben. Die Anzahl aller Blätter des
Entscheidungsbaumes kann nicht kleiner sein als $\vert\B\vert$, sie beträgt also mindestens $m$.

Die Komplexität des Elementtests ist also $ \Omega (\log m) $	
  \begin{satz}
  Die Komplexität der $\varepsilon$-closeness ist $ \Omega (n \log n) $.
  \end{satz}

  \begin{beweis}
  $\varepsilon$-closeness ist ein Elementtest-Problem!

  $ \M_{\varepsilon^i} \coloneqq  \lbrace ( a_1, \dots , a_n ') \in \R^n \vert \forall   i \ne j : \vert a_i - a_j 
  \vert \geq \varepsilon \rbrace$ ist ein spezielles Elementtestproblem.
				
  $\pi$ sei eine Permutation $ \pi(1, \dots ,n) $, dann ist
  $ \M(\pi ) \coloneqq  \lbrace (a_1, \dots , a_n) \in \M \vert a_{\pi (1)} < a_{\pi (2)} < \dots a_{\pi (n)}  \rbrace $
	
	
  \begin{satz} 
  Die Zusammenhangskomponenten von $ \mathcal{M}_\varepsilon $ sind die Mengen $ \mathcal{M}_\pi$.
  \end{satz}		
		
  \textit{Folgerung: Jeder Entscheidungsbaum im linearen Modell erfordert $\log (n!)$~Schritte im worst case. 
  ($\Rightarrow \Omega (n \log n)$)}
		
  \end{beweis}	
\textit{Folgerung: Sortieren hat im linearen Modell die Komplexität $\Omega (n \log n)$}
	
Angenommen das stimmt nicht, dann existiert ein schnelleres Sortierverfahren. 
Dann benutze das für den Input von $\varepsilon$-closeness und löse $\varepsilon$-closeness schneller als in $ \Omega (n \log n)$
$ \rightarrow $ dann exisitiert für $\varepsilon$-closeness ein Verfahren von geringerer Komplexität als $ \Omega (n \log n) $
$ \rightarrow $ Widerspruch zur Voraussetzung $ \rightarrow $ Behauptung
		
(relativer Komplexitätsbeweis)
\end{beweis}


% 
% Janine Roy
Hilfssatz zur Berechnung der Pfadlängen (Ungleichung von Kraft):


Sei $t_{i}$ die Pfadlänge für alle m Pfade eines Binärbaumes von der Wurzel zu den m Blättern, dann gilt:
\begin{gather*}
\sum_{i=1}^{m} 2^{-t_{i}}\leq 1
\end{gather*} 


Beweis induktiv über $m$


$m = 1:$ trivial


$m \geq 2:$ Dann spaltet sich der Baum in maximal zwei Bäume auf, deren Pfadlängen m$_1$ und m$_2$ um eins geringer sind, als die
von m. Die neuen Längen werden mit t$_{1,1}$ \ldots t$_{1,m_1}$ und t$_{2,1}$ \ldots t$_{2,m_2}$ bezeichnet.


Nach Voraussetzung gilt:
\begin{gather*}
\sum_{i=1}^{m_{1}} 2^{-t_{1i}}\leq 1und  \sum_{i=1}^{m_{1}} 2^{-t_{2i}}\leq 1
\end{gather*}

Jetzt werden Pfadlängen um 1 größer, dann gilt für $t_{1i}$ (und analog für $t_{2i}$):
\begin{gather*}
2^{(-t_{1i}+1)}=2^{-t_{1i}-1}=2^{-1} 2^{-t_{1i}}
\end{gather*}

Für T folgt also:
\begin{gather*}
\sum_{i=1}^{m_1,  m_2} 2^{-t_{j}}=2^{-1}(\sum_{i=1}^{m_{1}} 2^{-t_{1i}}+\sum_{i=1}^{m_{2}} 2^{-t_{2i}})
\leq 2^{-1}(1+1)=1
\end{gather*}

Hilfssatz:
\begin{gather*}
\frac{1}{m}  \sum_{i=1}^{m}t_{i}\geq \log m
\end{gather*}

Dabei gelten die selben Bezeichnungen wie oben.

Beweis:
\[
\frac{1}{m}   \sum_{i=1}^{m}2^{-t_{i}} \geq \sqrt[m]{\pi_{i=1}^{m}2^{-t_{i}}}=\sqrt[m]{2^{-t_i}  2^{-t_2}\cdot \ldots \cdot 2^{-t_m}}
=\sqrt[m]{2^{-t_1-  \ldots   -t_m}}= 2^{-\frac{1}{m}\sum_{i=1}^{m}t_i}
\]
\[
\Rightarrow m \leq 2^{\frac{1}{m}  \sum_{i=1}^{m}t_i}
\]
\[
\text{Also gilt für die Pfadlänge:}\log m \leq\frac{1}{m}  \sum_{i=1}^{m}t_i
\]

\begin{satz}
Hauptsatz über das Sortieren

Das Sortieren auf der Basis von Schlüsselvergleichen kostet bei Gleichwahrscheinlichkeit aller Permutationen der Eingabe
$\theta (n\log n)$ (mit den schnellstmöglichen Algorithmen).
\end{satz}

\begin{beweis}
Annahme $m>n!$

\begin{gather*}
\Omega(n  \log n) \ni \log n!\leq\frac{1}{n!}  \sum_{i=1}^{m}t_i \leq\frac{1}{m}  \sum_{i=1}^{m}t_i
\end{gather*}

Da wir bereits die untere Schranke bewiesen haben, muß $\frac{1}{m}  \sum_{i=1}^{m}t_i \geq \frac{1}{n!}  \sum_{i=1}^{m}t_i$ gelten,
also $\frac{1}{m} \geq \frac{1}{n!}$ und damit $m \geq n!$ sein.

Falls $m>n!$, dann ist aber $\frac{1}{m} < \frac{1}{n!}$.

Widerspruch zur Voraussetzung (untere Schranke) $\Rightarrow (m \geq n! \wedge \lnot (m>n!)) \rightarrow m=n!$
\end{beweis}

\section{\textsc{Quicksort}}
Bei \textsc{Quicksort} handelt es sich ebenfalls um ein Teile-und-Hersche-Verfahren.

Eingabe ist wieder ein Feld $A=(a_0, \ldots, a_{n})$, die Werte stehen
dabei in den Plätzen $a_1, \ldots, a_{n}$.  Dabei dient $a_0\coloneqq
-\infty$ als Markenschlüssel, als sogenanntes Sentinel-Element (siehe
auch \textsc{MergeSort}) und $v$ ist das Pivotelement.

\begin{algorithm}
  \KwIn{$A=(a_0, \ldots, a_{n})$}
  \Begin{
  \If{$r>l$}
  {$i\rightarrow$ \FuncSty{Partition} ($l,r$)\;
   \FuncSty{Quicksort} ($l,i-1$)\;
   \FuncSty{Quicksort} ($i+1,r$)}}
\caption{erste Variante von \textsc{QuickSort}}
\end{algorithm}

\begin{algorithm}
  \KwIn{$A=(a_0, \ldots, a_{n})$}
  \Begin{
  \If{$r>l$}
  {$v\leftarrow a[r]$\;
   $i\leftarrow l-1$\;
   $j\leftarrow r$\;
  \Repeat{$j\leq i$}
  {\Repeat{$a[i]\geq v$}{$i\leftarrow i+1$}
   \Repeat{$a[j]\leq v$}{$j\leftarrow j-1$}
   $t\leftarrow a[i]$\;
   $a[i]\leftarrow a[j]$\;
   $a[j]\leftarrow t$\;}
   }
  \FuncSty{Quicksort}($l,i-1$)\;
  \FuncSty{Quicksort}($i+1,r$)\;
}
\caption{Zweite Variante von \textsc{Quicksort}}
\label{alg:quicksort2}
\end{algorithm}

Was passiert bei \autoref{alg:quicksort2}?
\begin{enumerate}
\item Es wird von links nach rechts gesucht, bis ein Element größer $v$ ist
\item Es wird von rechts nach links gesucht, bis ein Element kleiner $v$ ist
\item Dann werden die beiden Elemente vertauscht, falls sie sich treffen, so kommt $v$ an diese Stelle
\end{enumerate}

Beispiel (getauscht werden die \textbf{Fetten}):

\begin{tabular}{lllllllllll}
2 & \textbf{7} & 8 & 9 & 0 & 1 & 5 & \textbf{3} & 6 & 4 & \hspace{15pt} 4 =: Pivotelement\\
2 & 3 & \textbf{8} & 9 & 0 & \textbf{1} & 5 & 7 & 6 & 4 &\\ 
2 & 3 & 1 & \textbf{9} & \textbf{0} & 8 & 5 & 7 & 6 & 4 &\\
2 & 3 & 1 & 0 & \textbf{9} & 8 & 5 & 7 & 6 & \textbf{4} &\\
%2 & 3 & 1 & 0 & 4 & 8 & 5 & 7 & 6 & 9 & \hspace{15pt} $i = 5$, $j = 4$\\
\end{tabular}

%
\begin{figure}[H]
  \centering
  \input{031103a.latex}
\caption{TODO: Hier muss eine Unterschrift rein!}
  \label{031103a}
\end{figure}

Am Ende sind alle Zahlen, die kleiner bzw. größer sind als 4, davor bzw. dahinter angeordnet. 
Jetzt wird der Algorithmus rekursiv für die jeweiligen Teilfolgen aufgerufen.

Nach einer anderen Methode von Schöning (nachzulesen in \cite{sedgewick}) sieht die Eingabe 2 7 8 9 0 1 5 7 6 4 nach dem ersten Durchlauf so aus: 
\underline{2 0 1} 4 \underline{7 8 9 5 7 6}

\subsection{Komplexität des \textsc{Quicksort}-Algorithmus'}

T(n) sei die Anzahl der Vergleiche, im besten Fall "`zerlegt"' das Pivotelement die Sequenz in zwei gleich große Teile und es gilt die
bekannte Rekurrenz
\begin{gather*}
T(n)=2T(\frac{n}{2})+n \Rightarrow \OO(n \log n)
\end{gather*}

Im schlechtesten Fall, nämlich bei bereits sortierten Folgen, ist aber $T(n) \in \Omega(n^2)$ 

\begin{satz}
\textsc{Quicksort} benötigt bei gleichwahrscheinlichen Eingabefolgen im Mittel etwa $1,38 n \log n$ Vergleiche.
\end{satz}

\begin{beweis}
$n=1$:
\begin{gather*}
T(1)=T(0)=0
\end{gather*}
$n \geq 2:$

\begin{gather*}
T(n)=(n+1)+ \frac{1}{n}\sum_{1\leq k\leq n}[T(k-1)+ T(n-k)]= (n+1)+ \frac{2}{n} \sum_{1\leq k\leq n}T(k-1)
\end{gather*}

Zur Lösung dieser Rekurrenz wird zuerst die Summe beseitigt, dies sollte jeder selbst nachvollziehen.
Die ausführliche Rechnung steht im \autoref{quicksort}. Es ergibt sich $\frac{T(n)}{n+1}= \frac{T(n-1)}{n}+ \frac{2}{n+1}$ und
Einsetzen der Rekurrenz führt zu:

%\begin{align*}
%n  T(n)& = & (n-1)T(n-1)-n(n+1)-(n-1)n+2T(n-1)\\
%& = & (n-1)T(n-1)+2T(n-1)+n((n+1)-(n-1))\\
%& = & (n+1)T(n-1)+2n\\
%\end{align*}

\begin{gather*}
\frac{T(n)}{n+1}= \frac{T(n-1)}{n}+ \frac{2}{n+1}=\frac{T(n-2)}{n-1}+\frac{2}{n}+\frac{2}{n+1}=\ldots
\end{gather*}

\begin{gather*}
\ldots= \frac{T(2)}{3}+ \sum_{3\leq k \leq n}\frac{2}{k+1} \approx 2\sum_{k=1}^{n}\frac{1}{k}\approx 2
\int_{1}^{n}\frac{1}{x} dx =2 \ln n
\end{gather*}

\begin{gather*}
T(n)=2n  \ln n \approx 1,38 n   \log n
\end{gather*}
\end{beweis}

% Susanne Gernandt
 \section{Auswählen (Sortieren)}
 Wie in vorangegangenen Vorlesungen besprochen wurde, braucht \textsc{QUICKSORT} bestenfalls $\OO(n \log n)$ Zeit und im
 worst case, wenn beim "`Teile und Herrsche"' - Verfahren die Länge der beiden Teilfolgen sehr unterschiedlich ist, $\OO(n^{2})$ Zeit.

 Um diese benötigte Zeit zu verringern, versuchen wir nun, einen Algorithmus zu finden, mit dem wir den worst case ausschließen
 können.

 Die Idee hierbei ist, ein Element zu finden, daß in der sortierten Folge ungefähr in der Mitte stehen wird und diesen sogenannten
 "`Median"' als Pivot-Element für das "`Teile und Herrsche"' - Verfahren bei \textsc{QUICKSORT} zu verwenden.

 Wie kompliziert ist es nun, diesen Meridian zu ermitteln? Dazu ist zuerst zu sagen, daß bei einer geraden Anzahl von Elementen zwei
 Elemente als Meridian in Frage kommen. Hierbei ist es allerdings egal, ob man sich für das kleinere oder für das größere Element
 entscheidet.

\begin{definition}
 Sei eine Folge $A=(a_{1}, \ldots , a_{n})$ gegeben, wobei alle $a_{i}$ die Elemente einer linear geordneten Menge sind. 
 Dann ist der Rang eines Elements $a_i$ im Feld $A$ definiert als
 $\rg(a_{i}:A)\coloneqq \abs{\Set{x | x \in A\colon x\leq a_{i}}}$.
\end{definition}

 Sei $A=(9,-5,2,-7,6,0,1)$, dann ist $\rg(1:A)\coloneqq 4$ (4 Elemente von $A$ sind $\leq 1$)


Sei nun $A$ sortiert, also $A_{sortiert}=(a_{\pi(1)}, \ldots , a_{\pi(n)})$, dann gilt für das Element $c$ mit $\rg(c:A)=k$ für $1\leq k
\leq n$, daß $c=a_{\pi(k)}$, das heißt:
$a_{\pi(1)}\leq \ldots \leq a_{\pi(k)}\leq \ldots \leq a_{\pi(n)}$.
Die ursprüngliche Reihenfolge paarweise gleicher Elemente wird hierbei beibehalten. Im weiteren wird
eine Kurzschreibweise für den Rang verwendet, $a_{(k)}$ ist das Element mit dem Rang k. Ein Feld $A$ mit $n$ Elementen wird kurz mit $A^n$
bezeichnet.

\begin{definition}
Der Median von $A$ ist demzufolge:

$a_{(\floor{ \frac{n}{2}})}$ (Element vom Rang $\floor{ \frac{n}{2}}$ bei $n$~Elementen)
\end{definition}

Hierbei kann allerdings, wie oben schon erwähnt, auch $a_{(\ceil{ \frac{n}{2}})}$, also die nächstgrößere
ganze Zahl, als Rang festgelegt werden.

Unter Selektion verstehen wir eine Vorbehandlung, die als Eingabe 
$A\coloneqq (a_{1}, \ldots , a_{n})$ erhält und unter der Bedingung $1\leq k\leq n$ als Ausgabe das Pivot-Element $a_{(k)}$ liefert.

 Dieser Algorithmus zur Selektion (brute force) besteht nun aus folgenden zwei Schritten:
 \begin{enumerate}
	\item SORT $A$: $a_{\pi(1)}, a_{\pi(2)}, \ldots , a_{\pi(n)}$ (braucht $\OO(n \log n)$ Zeit)
	\item Ausgabe des k-ten Elementes
 \end{enumerate}
 Für die Selektion wird die "`Median-der-Mediane-Technik"' verwendet. 

 \subsection{Algorithmus \texorpdfstring{SELECT$(A^{n},k)$}{SELECT (An,k)}}
 \addcontentsline{alg}{Algorithmus}{\textsc{Selektion}($A^n,  k$)}
 Wähle beliebige feste Ganzzahl $Q$ (z.\,B. 5)\todo{In Algorithmus ändern.}
 \begin{enumerate}[Schr{i}tt~1:]
  \item If $\abs{A}\leq Q$ Then sortiere $A$ (z.\,B. mit Bubblesort)
  \hspace{2cm}Ausgabe des k-ten Elementes \hfill(da Anzahl konstant: $\OO(1)$)
  \hspace{1cm}Else Zerlege $A$ in $\frac{\abs{A}}{Q}$ Teilfolgen der maximalen Länge Q
  \item Sortiere jede Teilfolge und bestimme den Median $m_{i}$ \hfill(dauert $\OO(n)$ Zeit)
  \item $SELECT(\{m_{1}, m_{2}, \ldots , m_{\frac{\abs{A}}{Q}}\},\frac{\abs{A}}{2Q})$, Ausgabe $m$
  \item Definition von drei Mengen:
    \begin{align*}
      A_{1}&\coloneqq\Set{x\in A | x<m}\\
    A_{2}&\coloneqq \Set{x\in A | x=m}\\
    A_{3}&\coloneqq \Set{x\in A | x>m}      
    \end{align*}
  \item If $\abs{A_{1}}\geq k$ Then $SELECT (A_{1},k)$

  \hspace{1cm}Elseif $\abs{A_{1}}+\abs{A_{2}}\geq k$ Then Output m

  \hspace{1cm}Else $SELECT (A_{3},k-(\abs{A_{1}}+\abs{A_{2}}))$
 \end{enumerate}
 
 \subsubsection{Zeitanalyse:}
 \hspace{4mm}zu Schritt 1: $\max\{\OO(1),\OO(n)\}$

 zu Schritt 2: $\OO(1)$ für jedes Sortieren der $\OO(n)$ Teilfolgen

 zu Schritt 3: $T(\frac{n}{Q})$

 zu Schritt 4: $\OO(n)$

 zu Schritt 5: $T(\frac{n}{Q})$
 
 
Seien die Mediane $m_{j}$ aller Teilfolgen sortiert. Dann ist $m$, der Median der Mediane, der Median dieser Folge. 
Wieviele Elemente aller Teilfolgen sind größer oder gleich $m$?

 $\frac{\abs{A}}{2Q}$ Mediane der Teilfolgen sind größer oder gleich $m$ und für jeden dieser $\frac{\abs{A}}{2Q}$
 Mediane sind $\frac{Q}{2}$ Elemente "`seiner"' Teilfolge größer oder gleich $m$. Damit sind mindestens
 $\frac{\abs{A}}{2Q}\cdot \frac{Q}{2}=\frac{\abs{A}}{4}$ Elemente größergleich $m$.
	
 $\Rightarrow\abs{A_{1}}\leq\frac{3}{4}\abs{A}\Rightarrow T(n)=\OO(n)+T(\frac{n}{Q})+T(\frac{3}{4}n)$
und $T(n)=\OO(n)\Longleftrightarrow\frac{n}{Q}+\frac{3}{4}n<n$

 Dies trifft für $Q\geq 5$ zu. Damit hat \textsc{Selection}($A^n,k$) die Komplexität $\OO(n)$.

 \subsection{Algorithmus \textsc{S-Quicksort(A)}}

\begin{Algorithmus}[ht]
\addcontentsline{alg}{Algorithmus}{\textsc{S-Quicksort}(A)}
%\lstset{emph={S-Quicksort, Select}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, S-Quicksort, Select}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{S-Quicksort}, gobble=1]{S-Quicksort}
 If $\abs{A}=2$ und $a_{1}<a_{2}$ Then Tausche $a_{1}$ und $a_{2}$
 Elseif $\abs{A}>2$ Then
   Select $(A,\frac{\abs{A}}{2})\longrightarrow m$
   $A_{1}\coloneqq \{x\leq m, x\in A,sodaß \abs{A_{1}}=\ceil{\frac{\abs{A}}{2}}\}$
   $A_{2}\coloneqq \{x\geq m, x\in A, sodaß \abs{A_{2}}=\floor{ \frac{\abs{A}}{2}}\}$
   S-Quicksort$(A_{1})$
   S-Quicksort$(A_{2})$
 End
 \end{lstlisting}
  \end{Algorithmus}

Der worst case wird durch die Bestimmung des Medians ausgeschlossen, die die Komplexität $\OO(n)$ hat. Damit gilt die Rekurrenz
$T(n)=2T(\frac{n}{2})+\OO(n)$ und der Algorithmus funktioniert immer in $\OO(n \log n)$ Zeit.
 
\section{\textsc{Heapsort}}
Abstrakte Datentypen wurden bereits auf Seite \pageref{ADT} definiert, ebenso der ADT \textbf{Dictionary}. Nun wird ein weiterer
Datentyp, der \textbf{Heap} vorgestellt. Der \textbf{Heap} wird in der Literatur oft auch mit \textbf{Priority Queue} bezeichnet.
Er findet z.\,B. bei der Verwaltung von Druckaufträgen Anwendung.

\begin{definition}[Heap]
Der abstrakte Datentyp, der das Quintupel der Operationen
\textsc{MakeHeap}, \textsc{Insert}, \textsc{Max} und \textsc{ExtractMax} unterstützt heißt \textbf{Heap} (oft auch \textbf{Priority Queue} genannt).
 \end{definition}

Hierbei handelt es sich um einen sogenannten Max-Heap. Analog kann ein Min-Heap gebildet werden, indem statt des Maximums immer das
Minimum genommen wird (bzw. statt $\geq$ immer $\leq$ im Algorithmus). 
% 
% 10.11.03 Reana Sommerkorn 
%
\begin{definition}[In place]
Ein Verfahren heißt \textbf{In place}, wenn es zur Bearbeitung der Eingabe unabhängig von der Eingabegröße nur zusätzlichen
Speicherplatz konstanter Größe benötigt.
\end{definition}

Auf der Basis der genannten und später erläuterten Operationen mit Heaps kann ein effizientes Sortierverfahren namens
\textsc{Heapsort} definiert werden, das in place funktioniert.

Das wird so erreicht, daß ein direkter Zugriff auf das Feld besteht und das Sortieren an Ort und Stelle und ohne Verwendung weiteren
Speicherplatzes vorgenommen werden kann. Des weiteren wird garantiert, dass n Elemente in $\OO(n \log n)$~Schritten sortiert werden,
unabhängig von den Eingabedaten.

Die Vorteile von \textsc{MergeSort} ($\OO(n \log n)$) und \textsc{InsertionSort} (In place) werden also vereint.

\begin{definition}[Binärer Heap]
Ein \textit{Binärer Max-Heap} ist ein spezieller Binärbaum (wird im Folgenden nochmals definiert) mit den Eigenschaften, daß der
Wert jedes Knotens jeweils größergleich den Werten seiner Söhne ist und daß außer dem Level mit der Höhe 0 alle Level voll besetzt
sein müssen. Jeder Level wird von links beginnend aufgefüllt. Hat also ein Blatt eines Heaps die Höhe 1 im gesamten Heap, so haben
auch alle rechts davon stehenden Blätter genau dieselbe Höhe.

Dies ist äquivalent dazu, daß für eine Folge $F=k_{1}, k_{2}, \ldots, k_{n}$ von Schlüsseln 
für alle i mit $2\leq i\leq n$ die Beziehung $k_{i}\leq k_{\floor{ \frac{i}{2}}}$ gilt (Heap-Eigenschaft), wobei kein
Eintrag im Feld undefiniert sein darf.
\end{definition}

Wegen der Speicherung in einem Array werden die Söhne auch als Nachfolger bezeichnet.
Wird ein binärer Heap in einem Array gespeichert, so wird die Wurzel des Baums an Position 1 im Array gespeichert. Die beiden
Söhne eines Knotens an der Arrayposition i werden an den Positionen 2i und 2i+1 gespeichert.
Und mit der Heap-Eigenschaft gilt $k_{i}\geq k_{2i} und  k_{i}\geq k_{2i+1}$ für alle $i$ mit $2i<n$  

Anschaulicher ist vielleicht die Vorstellung als Binärbaum
\begin{figure}[H]
	\centering\input{101103a.latex}
	\caption{Binärer Max-Heap}
	\label{101103a}
\end{figure}
Das korrespondierende Array wäre A=(8,7,5,3,1).

Um auf den Vater, den linken oder den rechten Sohn eines Knotens $i$ zuzugreifen, genügen die folgenden einfachen Berechnungen:

\begin{tabular}{l|l}
Ziel & Berechnung\\
\hline
Vater(i) & $\floor{\frac{i}{2}}$\\
LSohn(i) & $2i$\\
RSohn(i) & $2i+1$\\
\end{tabular}

Für die folgenden Überlegungen sind noch weitere Definitionen nützlich.

\begin{definition}[Graphentheoretische Definition eines Binärbaumes]
Ein \textbf{Binärbaum} ist ein Baum, bei dem jeder Knoten vom Grad höchstens 3 ist. Ein Knoten mit
höchstens Grad 2 kann dabei als Wurzel fungieren. Ein solcher Knoten existiert immer (im Extremfall ist er ein Blatt, hat also den Grad
1). 
\end{definition}

\begin{definition}[Ein vollständiger Binärbaum]
Ein \textbf{vollständiger Binärbaum} hat zusätzlich die Eigenschaften, daß genau ein Knoten den Grad 2 besitzt und alle Blätter die
gleiche Höhe haben. In diesem Fall wird immer der Knoten vom Grad 2 als Wurzel bezeichnet.
\end{definition}

\begin{satz}
In einem vollständigen (im strengen Sinne) Binärbaum der Höhe h gibt es $2^{h}$ Blätter und $2^h-1$ innere Knoten.
\end{satz}
Der Beweis ist mittels vollständiger Induktion über h möglich.

\begin{satz}
Der linke Teilbaum eines Binärheaps mit n Knoten hat maximal $\frac{2n}{3}$ Knoten.
\end{satz}

Beweisidee: Berechne erst wieviele Knoten der rechte Teilbaum hat. Dann benutze dies um die Knotenanzahl des linken Teilbaumes zu
berechnen. Rechne dann das Verhältnis der Knotenanzahlen zueinander aus.
\begin{beweis}
Da der Grenzfall von Interesse ist, wird von einem möglichst asymmetrischen Heap ausgegangen. Sei also der linke Teilbaum voll besetzt
und habe der rechte genau einen kompletten Höhenlevel weniger. Noch mehr Disbalance ist aufgrund der Heap-Eigenschaft nicht möglich.
Dann ist der rechte Teilbaum ebenfalls voll besetzt, hat allerdings einen Level weniger als der linke.

Sei also $i$ die Wurzel eines solchen Baumes mit der Höhe l und $j$ der linke Sohn von $i$. Dann ist $j$ Wurzel des linken Teilbaumes. 
Nun bezeichne $K(v$) die Anzahl der Knoten im Baum mit der Wurzel $v$. Dann soll also gelten 
$\frac{K(j)}{K(i)} \leq \frac{2}{3}$.
Nach Voraussetzung gilt $K(j)=2^l-1$ und $K(i)=2^l-1+2^{l-1}-1+1=3 \cdot 2^{l-1}-1$, es folgt
\[\frac{K(j)}{K(i)}= \frac{2^l-1}{3\cdot 2^{l-1}-1} \leq \frac{2^l}{3 \cdot 2^{l-1}}= \frac{2}{3}\]
\end{beweis}

\begin{satz}
In einem n-elementigen Binärheap gibt es höchstens $\ceil{ \frac{n}{2^{h+1}}}$ Knoten der Höhe h.
\end{satz}

\begin{definition}[Höhe eines Baumes]
Die Höhe eines Knoten $\vartheta$ ist die maximale Länge des Abwärtsweges von $\vartheta$ zu einem beliebigen Blatt (also die Anzahl
der Kanten auf dem Weg).
\end{definition}

Beweisidee:
Die Knoten der Höhe 0 sind die Blätter. Dann wird von unten beginnend zur Wurzel hochgelaufen und dabei der
Exponent des Nenners wird immer um eins erhöht
\begin{figure}[H]
	\centering\input{101103b.latex}
	\caption{Binärer Min-Heap}
	\label{101103b}
\end{figure}

Aus der Heap-Eigenschaft folgt, daß das Maximum in der Wurzel steht. Sei nun also F=(8, 6, 7, 3, 4, 5, 2, 1) gegeben. Handelt es sich
dabei um einen Heap?

Ja, da $F_{i}\geq F_{2i}$ und $F_{i}\geq F_{2i+1}$, für alle $i$ mit $5\geq i \geq 1$, da $8\geq 6 \wedge 8\geq 7 \wedge6\geq 3 \wedge 
6\geq 4 \wedge 7\geq 5 \wedge 7\geq 2 \wedge 3\geq 1$.

Dieser Max-Heap sieht dann grafisch wie folgt aus:

\begin{figure}[H]
	\centering\input{101103c.latex}
	\caption{Binärer Max-Heap}
	\label{101103c}
\end{figure}

Sei nun eine Folge von Schlüsseln als Max-Heap gegeben und die Ausgabe sortiert in absteigender Reihenfolge gewünscht; für einen
Min-Heap müssen die Relative "`kleiner"' und "`größer"' ausgetauscht werden. Um die Erläuterung einfacher zu halten, wird von dem
größeren Sohn gesprochen, nicht von dem Knoten mit dem größeren gespeicherten Wert. Genauso werden Knoten und nicht Werte vertauscht.
Dies ist allerdings formal falsch!

Für den ersten Wert ist dies einfach, da das Maximum bereits in der Wurzel steht.
Dies läßt sich ausnutzen, indem der erste Wert in die Ausgabe aufgenommen wird und aus dem Heap entfernt wird, danach wird aus den
verbleibenden Elementen wieder ein Heap erzeugt. Dies wird solange wiederholt, bis der Heap leer ist. Da in der Wurzel immer das Maximum
des aktuellen Heaps gespeichert ist, tauchen dort die Werte der Größe nach geordnet auf.

Der neue Heap wird durch Pflücken und Versickern des Elements mit dem größten Index erreicht. Dazu wird die Wurzel des anfänglichen Heaps geleert. Das Element mit dem größten Index wird aus dem Heap gelöscht
(gepflückt) und in die Wurzel eingesetzt.
Nun werden immer die beiden Söhne des Knotens verglichen, in dem der gepflückte Wert steht. Der größere der beiden Söhne wird mit
dem Knoten, in dem der gepflückte Wert steht, vertauscht.

Der gepflückte Wert sickert allmählich durch alle Level des Heaps, bis die
Heap-Eigenschaft wieder hergestellt ist und wir einen Heap mit n-1 Elementen erhalten haben.

Im obigen Beispiel heißt das also:
Wenn man nun die Wurzel (hier: 8) wieder entfernt, wandert die 1 nach oben und in diesem Moment ist es kein Binärheap. D.h. es muß
ein neuer Heap erzeugt werden und dies geschieht unter zu Zuhilfenahme von Heapify (Versickern).
In der grafischen Darstellung wurde die Position im Array rechts neben die Knoten geschrieben:
\begin{figure}[H]
	\centering\input{101103d.latex} \hspace{2cm}\input{101103e.latex}
	\caption{Binärer Max-Heap}
	\label{101103de}
\end{figure}

$\rightarrow$ entnehme die Wurzel \hspace{5cm}
$\rightarrow$ setze 1 an die Wurzel

\begin{figure}[H]
	\centering\input{101103f.latex}\hspace{6mm}\input{101103g.latex}\hspace{6mm}\input{101103h.latex}
	\caption{Binärer Min-Heap}
	\label{101103f}
\end{figure}
$\rightarrow$ Heapify (Versickern) der "`1"' 

Als Algorithmus:
  \begin{Algorithmus}[ht]
  \addcontentsline{alg}{Algorithmus}{\textsc{Heapify}(A, i)}
%  \lstset{emph={Heapify}}
  \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
  \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Heapify\textnormal{(A)}}, gobble=4]{Heapify}
    l:= LSohn(i)
    r:= RSohn(i)
    if l <= Heapsize[A] und A[l]=Succ(A[i])
      then Max:= l
      else Max:= i
    if r <= Heapsize[A] und A[r]=Succ(A[Max])
      then Max:= r
    if Max != i
      then tausche A[i] und A[Max]
        Heapify(A,Max) 
    \end{lstlisting}
  \end{Algorithmus}

\textbf{INPUT:} F bzw. A in einen Heap überführen
\begin{Algorithmus}[ht]
\addcontentsline{alg}{Algorithmus}{\textsc{Build-Heap\textnormal{(A)}}}
%\lstset{emph={Heapify}}
  \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Build-Heap\textnormal{(A)}}, gobble=2]{Build-Heap}  
   Heapsize[A]:= L~\ttfamilyä~nge[A]
   for i:= $\floor{ \texttt{Länge(}\frac{A}{2}\texttt{)} }$ down to 1
      Heapify(A,i)
    \end{lstlisting}
  \end{Algorithmus}

  \begin{Algorithmus}[ht]
  \addcontentsline{alg}{Algorithmus}{\textsc{Heapsort}(A)}
%  \lstset{emph={Build-Heap, Heapify}}
  \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, Heapify,
  Build-Heap}, emphstyle=\textsc, escapeinside=~~}  
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Heapsort\textnormal{(A)}}, gobble=4]{Heapsort}
    Build-Heap(A)
    for i:= Laenge[A] down to 2
      do tausche A[1] und A[i]
        Heapsize[A]:= Heapsize[A]-1
        Heapify(A,1)
    \end{lstlisting}
  \end{Algorithmus}
  
Komplexität der einzelnen Algorithmen

Für \textsc{Heapify} gilt die Rekurrenz
$T(n)=T(\nicefrac{2}{3}n)+\OO(1)$, damit gilt $T(n)\in\OO(\log n)$.

Für \textsc{Build-Heap} ist dies etwas komplizierter Sei $h$ die Höhe eines Knotens und $c$ eine Konstante größer 0, dann gilt:
 
\[\sum_{h=0}^{\floor{(\log n)}}\ceil{\frac{n}{2^{h+1}}} \cdot ch \in \OO(n\cdot\sum_{h=0}^{\floor{(\log
n)}}
\frac{n}{2^{h+1}}) \in \OO(n\cdot\underbrace{\sum_{h=0}^{\infty}
\frac{h}{2^{h+1}}}_{=2})=\OO(n)\] 
Damit kostet \textsc{Build-Heap} nur $\OO(n)$.

Damit hat \textsc{Heapsort} die Komplexität $\OO(n \log n)$. In jedem der $\OO(n)$~Aufrufe von \textsc{Build-Heap}  braucht
\textsc{Heapify} $\OO(\log n)$~Zeit.
%
% 12.11.03 Kay Schieck
\subsection{Priority Queues}
In der Literatur wird die Begriffe Heap und Priority Queue (Prioritätswarteschlange) oftmals synonym benutzt. Hier wird begrifflich
etwas unterschieden und Heaps werden für die Implementierung von solchen Warteschlangen benutzt. Auch die Bezeichnungen für
\textsc{BUILDHEAP} ist nicht einheitlich, in einigen Büchern wird stattdessen \textsc{MAKEHEAP} oder \textsc{MAKE} verwendet.

\begin{definition}
Der abstrakte Datentyp, der die Operationen \textsc{Make-Heap, Insert, Max, ExtractMax} und \textsc{Increase Key} unterstützt wird
\textbf{Priority
Queue} genannt.
\end{definition}

Wie bei einem Heap kann natürlich auch hier immer mit dem Minimum gearbeitet werden, die Operationen wären dann \textsc{Buildheap,
Insert, Min, ExtractMin} und \textsc{Decrease Key}

Behauptung: Binary Heaps unterstützen Warteschlangen.

\begin{Algorithmus}[ht]
\addcontentsline{alg}{Algorithmus}{\textsc{ExtractMax}(A)}
%\lstset{emph={Heapify}}
  \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{ExtractMax\textnormal{(A)}}, gobble=4]{ExtractMax(A)}
    if heap-size[A] < 1
         then Fehler
    Max := A[1]
    A[1] := A[heap-size[A]]
    heap-size[A] := heap-size[A]-1
    Heapify(A, 1)
    Ausgabe Max
\end{lstlisting}

Der Heap wird mittels A an Extractmax übergeben. Diese Funktion merkt sich die Wurzel (das Element mit dem größten Schlüssel).
Dann nimmt es das letzte im Heap gespeicherte Blatt und setzt es als die Wurzel ein. Mit dem Aufruf von Heapify() wird die
Heap-Eigenschaft wieder hergestellt. Das gemerkte Wurzel wird nun ausgegeben.
Falls die Anzahl der Elemente in A (heap-size) kleiner als 1 ist, wird eine Fehlermeldung ausgelöst.
\end{Algorithmus}

Increasekey sorgt dafür, das nach änderungen die Heapbedingung wieder gilt. Es vertauscht solange ein Element mit dem Vater, bis
das Element kleiner ist.

\begin{Algorithmus}[ht]
\addcontentsline{alg}{Algorithmus}{\textsc{IncreaseKey}(A, x, k)}
%\lstset{emph={Add}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Add, MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{IncreaseKey\textnormal{(A, x, k)}}, gobble=4]{IncreaseKey(A, x, k)}
     if key[x] > k
         then Fehler ~ä~lter Schl~ü~sselwert ist gr~öß~er"    
     key[x] := k
     y := x
     z := p[y]
     while z $\neq$ NIL und key[y] > key[z]
         do tausche key[y] mit key[z]
         y := z
         z := p[y]
\end{lstlisting}

Die übergebene Variable x ist der Index im Array und k ist der neue Schlüsselwert. Der Vater des Knotens x in
der Baumstruktur wird mit p[x] bezeichnet.
\end{Algorithmus}
\textsc{IncreaseKey} kostet $\OO(\log n)$, dazu siehe auch \autoref{121103a}.
Damit wird \textsc{IncreaseKey} unterstützt und wir wenden uns der Operation \textsc{Insert} zu.
Dabei wandert key solange nach oben, bis die Heap-Eigenschaft wieder gilt. Damit kostet auch \textsc{Insert} $\OO(\log n)$.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \draw (0,0) -- (2.6,0) -- (2.6,.7) -- (4,.7) -- (2,4) -- (0,0);
    \node (u) at (1,1.2) [circle,draw] {$x$};
    \node (o) at (1.5,2.1) [circle,draw] {$k$};
    \draw[<->] (o) -- (u);
    \draw[<->] (5,0) --  node[right=1pt] {$h=\log n$} (5,4);
  \end{tikzpicture}
  \caption{\textsc{Heapify}}
  \label{121103a}
\end{figure}
Ist der im Vaterknoten gespeicherte Wert größer als der im Sohn gespeicherte, vertauscht  \textsc{Increasekey} die beiden Werte.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \draw (0,0) -- (2.6,0) -- (2.6,.7) -- (4,.7) -- (2,4) -- (0,0);
    \node (l) at (2.1,.2) [circle,draw] {};
    \node (r) at (2.9,.2) [circle,draw] {};
    \node (o) at (2.5,1) [circle,draw] {};
    \draw (o) -- (l);
    \draw (o) -- (r);
    \draw[<->] (5,0) --  node[right=1pt] {$h=\log n$} (5,4);
  \end{tikzpicture}
  \caption{Insert}
  \label{121103b}
\end{figure}

Damit unterstützen binäre Heaps:
\begin{itemize}
\item \textsc{BuildHeap} $\OO(1)$ - (im Sinne von \textsc{MakeHeap} = Schaffen der leeren Struktur)
\item \textsc{Insert} $\OO(\log n)$
\item \textsc{Max} $\OO(1)$
\item \textsc{ExtractMax} $\OO(\log n)$
\item \textsc{IncreaseKey} $\OO(\log n)$
\end{itemize}
Somit ist die Behauptung erfüllt.

Ein interesantes Anwendungsbeispiel ist, alle $\leq$ durch $\geq$ zu ersetzen. Also \textsc{Max} durch \textsc{Min},
\textsc{ExtractMax} durch \textsc{ExtractMin} und \textsc{IncreaseKey} durch \textsc{Decreasekey} zu ersetzen und nur \textsc{Insert}
zu belassen.

\section{\textsc{Dijkstra}}

 Problem:
\begin{figure}[H]
  \centering\input{121103c.latex}
  \caption{Kürzesten Weg finden}
  \label{121103c}
\end{figure}

Sehr wichtig für die Informatik sind Graphen und darauf basierende Algorithmen. Viele anscheinend einfache Fragestellungen erfordern
recht komplexe Algorithmen.

So sei z.\,B. ein unwegsames Gelände mit Hindernissen gegeben und der kürzeste Weg dadurch herauszufinden. Für die algorithmische
Fragestellung ist es völlig egal, ob es sich um ein Gelände oder eine andere Fläche handelt. Deswegen wird soweit abstrahiert, daß
aus der Fläche und den Hindernissen Polygone werden. Doch unverändert lautet die Fragestellung, wie man hier den kürzesten Weg
finden kann. Es ist zu erkennen, das dies nur über die Eckpunkte (von Eckpunkt zu Eckpunkt)
zu bewerkstelligen ist. Dabei darf natürlich nicht der zugrunde liegende Graph verlassen werden. Ist eine aus Strecken zusammengesetzte
Linie der kürzeste Weg?

\begin{definition}[Visibility Graph]
M = Menge der Ecken = Menge der Polygonecken.
$a, b \in M \rightarrow \overline{ab}$ ist Kante des Graphen $\leftrightarrow \overline{ab}$ ganz innerhalb des Polygons liegt.
\end{definition}

\begin{satz}
Der kürzeste Pfad verläuft entlang der Kanten des Sichtbarkeitsgraphen (Lorenzo-Parez).
\end{satz}
Problem \textsc{all-to-one shortest paths}

One (In \autoref{121103d} ist das der Punkt a) ist der Startpunkt und von allen anderen
Punkten wird der kürzeste Weg dahin berechnet.
Die Gewichte an den Kanten sind dabei immmer $\geq 0$.

\begin{figure}[ht]
  \centering\input{121103d.latex}
  \caption{Graph mit gerichteten Kanten}
  \label{121103d}
\end{figure}

Zur Lösung des Problems verwenden wir den Algorithmus von Dijkstra (1959).

\begin{description}
\item[Paradigma: ] Es ist ein Greedy (gieriger) Algorithmus.
(einfach formuliert: ich denk nicht groß nach, ich nehm einfach den kürzesten Weg um von Punkt a weg zu kommen)
\item[Start: ] $V$ ist die Menge der Knoten, $W$ ist die Menge der erreichten Knoten.
\end{description}

\begin{figure}[H]
  \centering\input{121103e.latex}
  \caption{Menge W wird aufgeblasen}
  \label{121103e}
\end{figure}
Nun zum eigentlichen Algorithmus:

\begin{Algorithmus}[ht]
\addcontentsline{alg}{Algorithmus}{\textsc{Dijkstra}}
%\lstset{emph={ExtractMin, DecreaseKey}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={ExtractMin,
DecreaseKey, MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Dijkstra}, gobble=1]{Dijkstra}
 for all v in V do d(v) := + $\infty$
 d(s) := 0; W := $\emptyset$
 Initalisiere Struktur V (mit d(v), v in V)
 while V \ W $\neq \emptyset$ do
   v := Min(V \ W); ExtractMin(V \ W);
   W := W$\cup${v}
   for all w in Succ(v) \ W do
     if d(r) + l(vw) < d(w)
       then DecreaseKey(w, d(v) + l(vw))
\end{lstlisting}

Der Nachweis der Korrektheit dieses Algorithmus ist sehr schwer und soll nicht Gegenstand
dieses Skriptes sein.
\end{Algorithmus}   % noch nicht fertig

Ein einfaches Beispiel soll seine Funktion veranschaulichen. Die verwendeten Knoten und Kanten entsprechen denen
aus \autoref{121103d}. Jede Zeile der Tabelle entspricht einer Ausführung des Rumpfes der while Schleife (ab Zeile 5)
und zeigt die Werte, welche die verschiedenen Variablen annehmen. Aus Gründen der übersicht werden nur 3 Spalten
augeführt, der / tritt deshalb nochmal als Trennhilfe auf:

\begin{tabular}{|l|l|l|}\hline
$(v,d(v)):v \in W$ /		& $v = min(V \setminus W)$ /	& $v=min(V \setminus W),$ \\
$ (v,d(v)): v \in V\cup W$	& $SUCC(v)$		& $w \in SUCC(v)\setminus (W\cup {v}),$ \\
				&			& $(w,l(\vec{vw})$ / \\
				&			& $min(d(w),d(v)+l(\vec{vw}))$ \\ \hline

$\emptyset$ /			& $a$ /			& $v=a:(c,13),(b,7)$ / \\
$(a,0),(b,\infty),(c,\infty),$	& $c,b$			& $(c,13),(b,7)$ \\
$(d,\infty),(e,\infty)$		& 			& \\ \hline

$\{(a,0)\}$ /			& $b$ /			& $v=b:w\in \{c,d,e\}\setminus \{a,b\},$ \\
$(b,7),(c,13),(d,\infty),(e,\infty)$ & $c,d,e$		& $(c,5),(d,12),(e,4)$ / \\
				&			& $(c,12),(d,19),(e,11)$ \\ \hline

$(a,0),(b,7)$ /			& $e$ /			& $v=e, w\in\{a,d\}\setminus \{a,b,e\}$ \\
$(e,11),(c,12),(d,19)$		&			& $=\{d\}, (d,7)$ / \\
				& $a,d$			& $min(19,18):(d,18)$ \\ \hline

$(a,0),(b,7),(e,11)$ /		& $c$ /			& \\
$(c,12),(d,18)$			& $b,d$			& \\
nächste Zeile $(d,13)$		&			& \\ \hline
\end{tabular}


Nun interessiert uns natürlich die Komplexität des Algorithmus. Dazu wird die jeweilige Rechenzeit
der einzelnen Zeilen betrachtet, wobei $\abs{V}=n$ und die Anzahl der
Knoten $m$ ist.
\begin{enumerate}
\item $\OO(\abs{V}) = \OO(n)$
\item $\OO(n)$
\item meist in $\OO(1)$
\item nicht zu beantworten
\item nicht zu beantworten
\item $\OO(1)$ im Regelfall (es kann auch komplexer sein, da es darauf ankommt, wie $W$ verwaltet wird)
\item
\item $\OO(1)$
\item die Komplexität von DECREASEKEY ist auch nicht zu beantworten
\end{enumerate}

Die Laufzeit hängt wesentlich davon ab, wie die Priority Queue verwaltet wird.
\begin{enumerate}
\item Möglichkeit: $V$ wird in einem Array organisiert \hfill $\rightarrow \OO(n^{2})$
\item Möglichkeit: Binärer Heap für V$\backslash$W, dann geht EXTRACTMIN in $\OO(n\log n)$ und DECREASEKEY in $\OO(m\log n)$ \hfill $\rightarrow \OO((m+n)\log n)$
\item Möglichkeit: V in einen Fibonacci-Heap \hfill $\rightarrow \OO((n \log n)+m)$
\end{enumerate} 
%
% 17.11.03 Alexander Hofmeister
%
  \section{\textsc{Counting Sort}}
  Einen ganz anderen Weg zum Sortieren von Zahlen beschreitet \textsc{Counting Sort}.
  Es funktioniert, unter den richtigen Bedingungen angewendet, schneller als in $\OO(n \log n)$ und basiert nicht auf
  Schlüsselvergleichen.

  \begin{Algorithmus}[H]
  \addcontentsline{alg}{Algorithmus}{\textsc{Counting Sort}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={ExtractMin,
DecreaseKey, MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}  
    \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Counting Sort}, gobble=4]{Counting Sort}
    for i := 0 to k do
     C[i]:= 0;
     for j := 1 to Laenge[A] do
       C[A[j]] := C[A[j]]+1;
     for i := 1 to k do
       C[i] := C[i] + C[i-1];
     for j := Laenge[A] downto 1 do
       B[C[A[j]]] := A[j];
       C[A[j]] := C[A[j]]-1
    \end{lstlisting}

  Das Sortierverfahren \textsc{Counting Sort}  belegt eventuell auf Grund der Verwendung von zwei zusätzlichen Feldern 
  B und C sehr viel Speicherplatz.
  Es funktioniert ohne die Verwendung von Schlüsselvergleichen. Es wird zu Begin ein Zähler (Feld C) erzeugt 
  dessen Größe abhängig ist von der Anzahl der möglichen in A 
  enthaltenen Zahlen (Zeile 1 und 2). Für jeden möglichen Wert in A, also für jeden Wert des Zahlenraumes, wird eine Zelle des Feldes
  reserviert. Seien die Werte in A z.\,B. vom Typ Integer mit 16 Bit. Dann gibt es 2$^{16}$ mögliche Werte und das Feld C würde für
  jeden der 65536 möglichen Werte eine Zelle erhalten; dabei wird C[0] dem ersten Wert des Zahlenraums zugeordnet, C[1] dem zweiten
  Wert des Zahlenraumes usw.
  Anschließend wird die Anzahl der in 
  A enthaltenen Elemente schrittweise in C geschrieben. Bei mehrfach 
  vorhandenen Elementen wird der entsprechende Wert erhöht, daher kommt auch 
  der Name Counting Sort (Zeile 3 und 4). Nun werden die Adressen im Feld
  berechnet. Dazu wird die Anzahl eines Elemente mit der Anzahl eines 
  Vorgängerelements addiert um die entsprechende Anzahl im Ausgabefeld frei 
  zu halten (Zeile 5 und 6). Zum Schluss wird die richtige Reihenfolge durch 
  zurücklaufen des Arrays A und der Bestimmung der richtigen Stelle, mit Hilfe 
  von C, in B geschrieben. Bei \textsc{Counting Sort} handelt es sich um ein stabiles Sortierverfahren.
  \end{Algorithmus}
  
  \begin{definition}[Stabile Sortierverfahren]
  Ein Sortierverfahren heißt \textbf{stabil}, falls mehrfach vorhandene Elemente in der Ausgabe in der Reihenfolge auftauchen, in der sie auch
  in der Eingabe stehen.
  \end{definition}
  
  \subsection{\textsc{Counting Sort} an einem Beispiel}

  \begin{description}
    \item[Input:] $A = ( 1_a , 3 , 2_a , 1_b , 2_b , 2_c , 1_c )$ und $k = 3$
    \item[Output:] $B = ( 1_a , 1_b , 1_c , 2_a , 2_b , 2_c , 3 )$, $C = ( 0, 0, 3, 6 )$
    \item[Ablauf:]
    \begin{tabular}[t]{*{3}{c}}
      Zeile & Feld $C$ & Erläuterung\\
      \hline
      nach 1 und 2 & $< 0, 0, 0, 0 >$ & Zähler wird erzeugt und 0 gesetzt\\
      nach 3 und 4 & $< 0, 3, 3, 1>$   & Anzahl der Elemente wird ``gezählt''\\
      nach 5 und 6 & $< 0, 3, 6, 7>$   & Enthält Elementzahl kleiner gleich i\\
      nach 9       & $< 0, 0, 3, 6>$   & $A[ i ]$ werden in B richtig positioniert\\
    \end{tabular}
    
    
  \end{description}

  \subsection{Komplexität von \textsc{Counting Sort}}
   Die Zeitkomplexität von \textsc{Counting Sort} für einen Input von $A^n$ mit 
   $k\in\OO(n)$ ist $T(n)\in\OO(n)\cup\OO(k)$.\todo{Kann man hier
     überhaupt ein $\cup$ verwenden?}
  \begin{satz}
   Falls $k \in  \OO( n )$, so funktioniert \textsc{Counting Sort} in $\OO(n)$.
  \end{satz}

Die Stärke von \textsc{Counting Sort} ist gleichzeitig auch Schwäche. So ist aus dem obigen bereits ersichtlich, daß dieses Verfahren
z.\,B. zum Sortieren von Fließkommazahlen sehr ungeeignet ist, da dann im Regelfall riesige Zählfelder erzeugt werden, die mit vielen
Nullen besetzt sind, aber trotzdem Bearbeitungszeit (und Speicherplatz!) verschlingen.

\section{Weitere Sortieralgorithmen}
  Außer den hier aufgeführten Sortieralgorithmen sind für uns noch \textsc{Bucket Sort} und
  \textsc{Radix Sort} von Interesse.

  \chapter{Einfache Datenstrukturen: Stapel, Bäume, Suchbäume}
  Bevor mit den einfachen Datenstrukturen begonnen wird,
  noch eine Bemerkung zum Begriff des abtrakten Datentyps (siehe Seite \pageref{ADT}).
  Auch dieser Begriff wird leider nicht immer einheitlich verwendet. Mal wird er wie eingangs definiert oder als Menge von Operationen,
  dann wieder wie in der
  folgenden Definition oder noch abstrakter, wie z.\,B. in \cite{guting}, wo ein ADT als Signatur vereinigt mit Axiomen für die
  Operationen definiert wird.

  \begin{definition}[ADT]
    Ein Abstrakter Datentyp ist eine (oder mehrere) Menge(n) von Objekten und
    darauf definierten Operationen
  \end{definition}

  \subsubsection{Einige Operationen auf ADT's}
  \begin{description}
  \item Q sei eine dynamische Menge
   
   \begin{tabular}[t]{@{}ll@{}} %{*{2}{l}}
      Operation & Erläuterung \\
      \hline
      \textsc{Init}(Q) & Erzeugt eine leere Menge Q \\
      \textsc{Stack-Empty} & Prüft ob der Stack leer ist \\
      \textsc{Push}(Q,x),\textsc{Insert}(Q,x) & Fügt Element x in Q ein ( am Ende ) \\
      \textsc{Pop}(Q,x),\textsc{Delete}(Q,x) & Entfernt Element x aus Q (das Erste x was auftritt)\\
      \textsc{Pop} & Entfernt letztes Element aus Q\\
      \textsc{Top} & Zeigt oberstes Element an \\
      \textsc{Search}(Q,x) & Sucht El. x in Q (gibt erstes Vorkommende aus)\\
      \textsc{Min}(Q) & Gibt Zeiger auf den kleinsten Schlüsselwert zurück \\
      \textsc{Max}Q) & Gibt Zeiger auf den größten Schlüsselwert zurück \\
      \textsc{Succ}(Q,x) & Gibt Zeiger zurück auf das nächst gr. El. nach x\\
      \textsc{Pred}(Q,x) & Gibt Zeiger zurück auf das nächst kl. El. nach x 
   \end{tabular}
    
  \end{description}

  \section{Binäre Suchbäume}

   \begin{definition}[Binärer Suchbäume]
    Ein binärer Suchbaum ist ein Binärbaum folgender Suchbaumeigenschaft:
    Sei x Knoten des binären Suchbaumes und Ahn vom Knoten y. Falls der Weg von x nach y über den linken Sohn von x erfolgt, ist
    $key[ y ]\leq key[ x ]$. Andernfalls istt $key[ y ]> key[ x ]$.
   \end{definition}

   \begin{definition}
   Ein Suchbaum heißt \textbf{Blattsuchbaum}, falls die Elemente der dynamischen Menge im Gegensatz zum normalen Suchbaum nur in den
   Blättern gespeichert werden.
   \end{definition}

  \subsection{Beispiel für einen binären Suchbaum}
  \begin{figure}[ht]
  \centering \input{171103a.latex} 
   \caption{Binärer Suchbaum}
   \label{171103a}
  \end{figure}

  \begin{itemize}
   \item Rotes Blatt wird erst durch INSERT hinzugefügt
   \item $Q = \{ 5, 2, 1, 3, 8, 7, 9 \}$
   \item \textsc{Search}(Q,4): \\
      Beim Suchen wird der Baum von der Wurzel an durchlaufen und der zu 
      suchende Wert mit dem Wert des Knoten verglichen. Ist der zu suchende 
      Wert kleiner als der Knotenwert wird im linken Teilbaum weitergesucht. 
      Ist er größer wie der Knotenwert wird im rechten Teilbaum 
      weitergesucht. Zurückgegeben wird der zuerst gefundene Wert. Ist das 
      Element nicht im Suchbaum enthalten, wird NIL bei Erreichen eines 
      Blattes zurückgegeben.\\
      Im Beispiel wird der Suchbaum in der Reihenfole 5, 2, 3 durchlaufen 
      und dann auf Grund des Fehlens weiterer Knoten mit der Rückgabe von 
      NIL verlassen.
   \item INSERT(Q,4): \\
      Beim Einfügen wird das einzufügende Element mit dem jeweiligen 
      Element des aktuellen Knotens verglichen. Begonnen wird dabei in der 
      Wurzel. Ist das einzufügende Element größer, wird im Baum nach 
      rechts gegangen, ist es kleiner, nach links. Ist in ein 
      Blatt erreicht, wird dann, die Suchbaumeigenschaft erhaltend, entweder rechts oder 
      links vom Blatt aus eingefügt.\\
      Im Beispiel wird der Baum in der Reihenfolge 5, 2, 3 
      durchlaufen und die 4 dann rechts von der 3 als neues Blatt mit dem Wert 4
      eingefügt.
   \item Nach Einfügen: $Q = \{ 5, 2, 1, 3, 4, 9, 7, 8 \}$
  \end{itemize}

  \subsection{Operationen in binären Suchbäumen}

  \begin{Algorithmus}[H]
    \addcontentsline{alg}{Algorithmus}{\textsc{Tree-Search}}
    %\lstset{emph={Tree-Search}}
    \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={ExtractMin,
    Tree-Search, DecreaseKey, MergeSort, Heapify}, emphstyle=\textsc, escapeinside=~~}
    \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Tree-Search}, gobble=4]{Tree-Search}
    if x=NIL or k=key[x]
       then Ausgabe x
    if k<key[x]
       then Ausgabe Tree-Search(li[x],k)
       else Ausgabe Tree-Search(re[x],k)
    \end{lstlisting}

    Bei \textsc{TREE-SEARCH} wird der Baum von der Wurzel aus durchlaufen. 
    Gesucht wird dabei nach dem Wert k. Dabei ist x der Zeiger, der auf den 
    Wert des aktuellen Knotens zeigt. In den ersten beiden Zeilen 
    wird der Zeiger zurückgegeben wenn ein Blatt erreicht oder der 
    zu suchende Wert gefunden ist (Abbruchbedingung für Rekursion). In 
    Zeile 3 wird der zu suchende Wert mit dem aktuellen Knotenwert 
    verglichen und anschl"ießend in den Zeilen 4 und 5 entsprechend im 
    Baum weitergegangen. Es erfolgt jeweils ein rekursiver Aufruf.\\
    Die Funktion wird beendet wenn der Algorithmus in einem Blatt 
    angekommen ist oder der Suchwert gefunden wurde.
    \end{Algorithmus}
 
 \subsubsection{Traversierung von Bäumen}
   \begin{Algorithmus}[H]
   \addcontentsline{alg}{Algorithmus}{\textsc{Treepostorder}}
   %\lstset{emph={Treepostorder}}
   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search}, emphstyle=\textsc, escapeinside=~~}
   \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Treepostorder\textnormal{(x)}}, gobble=4]{Treepostorder(x)}
     if x $\neq$ NIL
       then Treepostorder(li[x])
            Treepostorder(re[x])
	    Print key[x]
    \end{lstlisting}

   Bei \textsc{Treepostorder} handelt es sich um einen rekursiven Algorithmus.
   Es wird zuerst der linke, dann der rechte Teilbaum und erst zum Schluß die Wurzel durchlaufen.
   \end{Algorithmus}

  \begin{figure}[H]
  \centering \input{171103b.latex} 
   \caption{\textsc{Treepostorder}(x)}
   \label{171103b}
  \end{figure}
  \begin{itemize}
   \item $x$ ist der Zeiger auf dem Knoten
   \item Die Ausgabereihenfolge ist $\{1,4,3,2,7,9,8,5\}$.
  \end{itemize}
 
 \subsubsection{\textsc{Treepreorder}(x)}
   \begin{Algorithmus}[H]   
   \addcontentsline{alg}{Algorithmus}{\textsc{Treepreorder}}
   %\lstset{emph={Treepreorder}}
   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search, Treepreorder}, emphstyle=\textsc, escapeinside=~~}
    \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Treepreorder\textnormal{(x)}}, gobble=4]{Treepreorder(x)}
     if x $\neq$ NIL
        then Print key[x]
             Treepreorder(li[x])
             Treepreorder(re[x])
    \end{lstlisting}

   Beim ebenfalls rekursiven \textsc{Treepreorder} wird bei der Wurzel begonnen, dann wird der 
   linke Teilbaum und anschließend der rechte Teilbaum durchlaufen.
   \end{Algorithmus}
Beispiel für \textsc{Treepreorder}
   
  \begin{figure}[H]
  \centering \input{171103c.latex} 
   \caption{\textsc{Treepreorder }}
   \label{171103c}
  \end{figure}
 \begin{itemize}
 \item $x$ ist der Zeiger auf dem Knoten
  \item Die Ausgabenreihenfolge ist $\{5,2,1,3,4,8,7,9\}$.
 \end{itemize}
 
 \subsubsection{\textsc{Treeinorder}(x)}
   \begin{Algorithmus}[H]	
   \addcontentsline{alg}{Algorithmus}{\textsc{Treeinorder}}
   %\lstset{emph={Treeinorder}}
   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search, Treepreorder, Treeinorder}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Treeinorder\textnormal{(x)}}, gobble=4]{Treeinorder(x)}
     if x $\neq$ NIL
        then Treeinorder(li[x])
        	Print key[x]
             Treeinorder(re[x])
\end{lstlisting}

    Bei \textsc{Treeinorder} wird zuerst der linke Teilbaum, dann die 
    Wurzel und anschließend der rechte Teilbaum durchlaufen.
\end{Algorithmus}

  Beispiel für \textsc{Treeinorder}
  \begin{figure}[H]
  \centering \input{171103d.latex} 
   \caption{\textsc{Treeinorder}}
   \label{171103d}
  \end{figure}
 \begin{itemize}
  \item $x$ ist der Zeiger auf dem Knoten
  \item Die Ausgabenreihenfolge ist $\{1,2,3,4,5,7,8,9\}$.
 \end{itemize}
 
  \begin{satz}
 Bei gegebenem binären Suchbaum ist die Ausgabe mit allen drei Verfahren (\textsc{Inorder}, 
 \textsc{Preorder} und \textsc{Postorder}) in $\Theta( n )$ möglich.
 \end{satz}
 \begin{description}
  \item[Folgerung:] Der Aufbau eines binären Suchbaumes kostet $\Omega( n \log n )$ Zeit.
 \end{description}
 
 \subsubsection{\textsc{Tree-Successor}(x)}
 
   \begin{Algorithmus}[H]
   \addcontentsline{alg}{Algorithmus}{\textsc{Min}(x)}
   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search, Treepreorder, Treeinorder}, emphstyle=\textsc, escapeinside=~~}
    \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Min\textnormal{(x)}}, gobble=4]{Min(x)}
     while li[x] $\neq$ NIL do
       x:= li[x]
     return x
    \end{lstlisting}

   \textsc{Min}(x) liefert das Minimum des Teilbaumes, dessen Wurzel x ist.
   \end{Algorithmus}
   
   \begin{Algorithmus}[H]
    \addcontentsline{alg}{Algorithmus}{\textsc{Tree-Successor}(x)}
    %\lstset{emph={Min}}
    \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search, Treepreorder, Treeinorder, Min}, emphstyle=\textsc, escapeinside=~~}
    \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Tree-Successor\textnormal{(x)}}, gobble=4]{Tree-Successor(x)}
     if re[x] $\neq$ NIL
        then return Min(re[x])
     y:=p[x]
     while y $\neq$ NIL and x=re[y]
        do x:=y
           y:=p[y]
     return y
    \end{lstlisting}

   Beim \textsc{Tree-Successor} werden zwei Fälle unterschieden. Falls x 
   einen rechten Teilbaum besitzt, dann ist der Nachfolger das Blatt, das im 
   rechten Teilbaum am weitesten links liegt (\textsc{MIN}(x)). Besitzt x keinen 
   rechten Teilbaum, so ist der successor y der Knoten dessen linker Sohn 
   am nächsten mit x verwandt ist. Zu beachten ist dabei, daß sich der Begriff Nachfolger auf einen Knoten bezieht, der Algorithmus
   aber den Knoten liefert, dessen gespeicherter Wert im Baum Nachfolger des im ersten Knoten gespeicherten Wertes ist. 
   \end{Algorithmus}
   
	% Vorlesungsskript vom 19.11.03
	% Christian Lütz
	% M.-Nr.: 62311
	% e-mail: Krisy0910@gmx.de
Die Operationen zum Löschen und Einfügen von Knoten sind etwas komplizierter, da sie die Baumstruktur stark verändern können und
erhalten deswegen jeweils einen eigenen Abschnitt.
\subsection{Das Einfügen}

  	\begin{description}
			\item{Beim \textsc{Tree-Insert} werden zwei Parameter übergeben, wobei}
			\begin{itemize}
				\item T der Baum ist, in dem eingefügt werden soll und
		 		\item z der Knoten, so daß	
		 		\begin{itemize}
					\item key[z]   = v (einzufügender Schlüssel),
					\item left[z]  = NIL und
					\item right[z] = NIL
				\end{itemize}
				ist.
			\end{itemize}
		\end{description}
 	Erklärung:
	Bei diesem Einfügealgorithmus werden die neuen Knoten immer als 
	Blätter in den binären Suchbaum T eingefügt. Der einzufügende 
	Knoten z hat keine Söhne. Die genaue Position des Blattes wird 
	durch den Schlüssel des neuen Knotens bestimmt. Wenn ein neuer 
	Baum aufgebaut wird, dann ergibt der erste eingefügte Knoten die 
	Wurzel. Der zweite Knoten wird linker Nachfolger der Wurzel, wenn 
	sein Schlüssel kleiner ist als der Schlüssel der Wurzel und rechter 
	Nachfolger, wenn sein Schlüssel größer ist als der Schlüssel der 
	Wurzel. Dieses Verfahren wird fortgesetzt, bis die Einfügeposition bestimmt ist.

	Anmerkungen dazu:
	Dieser Algorithmus zum Einfügen ist sehr einfach. Es finden keine 
	Ausgleichs- oder Reorganisationsoperationen statt, so daß die 
	Reihenfolge des Einfügens das Aussehen des Baumes bestimmt, deswegen 
	entartet der binäre Suchbaum beim Einfügen einer bereits sortierten Eingabe
	zu einer linearen Liste. 

\begin{Algorithmus}
\addcontentsline{alg}{Algorithmus}{\textsc{Tree-Insert}}
   %\lstset{emph={Treeinorder}}
   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
    Tree-Search, Treepreorder, Treeinorder}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Tree-Insert}, gobble=1]{Tree-Insert}
 y := NIL
 x := root[T]
 while ( x $\neq$ NIL ) do
   y := x
   if ( key[z] = key[x] ) 
     then x := left[x]
     else x := right[x]
 p[z] := y
 if ( y = NIL )
   then root[T] := z
   elseif ( key[z] < key[y] )
     then left[y]  := z
     else right[y] := z    
	    \end{lstlisting}

Die Laufzeit liegt in $\OO(h)$, wobei $h$ die Höhe von $T$ ist.
\end{Algorithmus}    
    Da der Knoten immer in einem Blatt eingefügt wird, ist damit zu rechnen,
    daß im worst case das Blatt mit der größten Entfernung von der Wurzel
    genommen wird. Da dieses die Höhe h hat sind folglich auch h Schritte
    notwendig, um zu diesem Blatt zu gelangen.
        
\subsection{Das Löschen eines Knotens}
Beim Löschen eine Knotens z in eiem binären Suchbaum müssen drei Fälle
unterschieden werden:
\begin{enumerate}[1.\,F{a}ll]
\item $z$ hat keine Söhne.
  Der Knoten kann gefahrlos gelöscht werden und es sind
  keine weiteren Operationen notwendig.
  \input{191103a.latex}
	 	 
\item $z$ hat genau einen linken Sohn \\
  Der zu löschende Knoten wird entfernt und durch den Wurzelknoten
  des linken Teilbaums ersetzt.
  \input{191103b.latex}
	 	
\item $z$ hat genau einen rechten Sohn \\
  Analog dem 2.\,Fall.

\item $z$ hat zwei Söhne \\
  \textit{Problem:} Wo werden die beiden Unterbäume nach dem Löschen von z
  angehängt?
  \textit{Lösung:} Wir suchen den Knoten mit dem kleinsten Schlüssel im rechten
  Teilbaum von $z$. Dieser hat keinen linken Sohn, denn sonst gäbe es einen
  Knoten mit einem kleineren Schlüssel. Der gefunden Knoten wird mit dem
  zu löschenden Knoten $z$ vertauscht und der aktuelle Knoten entfernt.
  \input{191103c.latex}
  
\end{enumerate}		 		

		Auch beim Löschen (\textsc{Tree-Delete}) werden wieder zwei Parameter übergeben, dabei ist
		\begin{itemize}
			\item T der Baum und
		 	\item z der zu löschende Knoten
		\end{itemize}
		Rückgabewert ist der (tatsächlich) aus dem Baum entfernte Knoten, dies muss nicht z sein, (siehe 4. Fall)
		\begin{Algorithmus}
		\addcontentsline{alg}{Algorithmus}{\textsc{Tree-Delete}}
		   %\lstset{emph={Tree-Successor}}
		   \lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
                   Tree-Search, Treepreorder, Treeinorder, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
                        \begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Tree-Delete}, gobble=4]{Tree-Delete}
	if ( left[z] = NIL or right[z] = NIL )
		then y := z
		else y := Tree-Successor(z)
	if ( left[y] $\neq$ NIL )
		then x := left[y]
		else x := right[y]
	if ( x $\neq$ NIL )
		then root[T] := x
		else if ( y = left[p[y]] )
			then left[p[y]] := x
			else right[p[y]] := x
	if ( y $\neq$ z )
		then key[z] := key[y]
	return y
			\end{lstlisting}

		Laufzeit liegt wieder in $\OO(h)$, wobei  $h$ wieder die Höhe von $T$ bezeichnet.
		\end{Algorithmus}		

		Im worst case wird \textsc{Tree-Successor} mit einer Laufzeit von $\OO(h)$ einmal aufgerufen, 
		andere Funktionsaufrufe oder Schleifen gibt es nicht.
\subsubsection{Binäre Suchbäume als Implementierung des ADT Wörterbuch}
 		\input{191103d.latex}
	  	Wie bereits bei der Funktion \textsc{Tree-Insert} beschrieben, kann eine ungünstige 
	  	Einfügereihenfolge den Suchbaum zu einer linearen Liste entarten lassen. Deswegen sind allgemeine 
	  	binäre Suchbäume nicht geeignet, den ADT Wörterbuch zu implementieren.  	
\section{2-3-4-Bäume}
\begin{definition}[2-3-4-Bäume]
\textbf{2-3-4-Bäume} sind Bäume mit folgenden speziellen Eigenschaften:
\begin{itemize}
				\item Jeder Knoten im Baum enthält einen, zwei oder drei Schlüssel, 
							die von links nach rechts aufsteigend sortiert sind.		
				\item Ein Knoten mit k Schlüsseln hat k+1 Söhne (oder er hat überhaupt keine:``Blatt'') und wird als
							 (k+1)-Knoten bezeichnet.
				\item Für Schlüssel im Baum gilt die verallgemeinerte Suchbaumeigenschaft.
				\item Alle Blätter haben den gleichen Abstand zur Wurzel.	
			\end{itemize}
\end{definition}
Zur Veranschaulichung dienen die folgenden Abbildungen (2-, 3- und 4-Knoten, ein Blatt und ein skizzierter möglicher Baum).
			 	\input{191103e.latex}		
				\input{191103f.latex}
	 			\input{191103g.latex}
		Bei einem 2-3-4-Baum ist die Anzahl der Knoten deutlich geringer als bei
		einem vergleichbaren binären Suchbaum. Damit ist die Zahl der besuchten 
		Knoten bei einer Suche geringer. Daraus folgt, daß das Suchen nach einem
		Schlüssel in einem 2-3-4-Baum effizenter ist, als in einem vergleichbaren binären Suchbaum.
		Allerdings ist der Aufwand beim Einfügen und beim Löschen von Schlüsseln höher.
\subsubsection{Beispiel für einen 2-3-4-Baum}
		\input{191103h.latex}
		\begin{itemize}
			\item Erfolgreiche Suche nach 35
			\item Erfolglose Suche nach 69
		\end{itemize}
		Die Laufzeit für das Suchen liegt wieder in $\OO(h)$, mit $h$ als Höhe des Baumes.
% 24.11.03 Marcel Konstanz
\subsection{Top Down 2-3-4-Bäume für den ADT Dictionary}

Wenn die Höhe des Baumes logarithmisch ($h \in \OO(\log n)$) ist, eignet er sich gut für den Datentyp
Wörterbuch, da dann alle Operationen in $\OO(\log n)$ gehen. Insbesondere das in einem Wörterbuch zu erwartende häufige Suchen hat
die Komplexität $\OO(\log n)$.

\begin{figure}[H]
    \centering\input{241103a.latex}
    \caption{$h \in \OO(\log n)$}
    \label{241103a}
 \end{figure}	

Bereits bei den Binärbäumen muß die Baumstruktur nach dem Löschen eines Knotens manchmal repariert werden.
Nun ist klar ersichtlich, daß ein Baum, der immer logarithmische Höhe haben soll, nicht zu einer linearen Liste entarten darf. 	
Falls also das Einfügen eines Elemente in einen Baum die Baumstruktur so ändert, daß die Eigenschaften 
verletzt sind, muß der Baum repariert werden.

Wie in \autoref{241103b} zu sehen ist, kann das Einfügen eines einzigen Knotens dazu führen, daß eine neue Ebene
einfügt werden muß. Falls dies in der untersten Ebene geschieht (worst-case), kann sich das bis zur Wurzel fortsetzen.	
Top down 2-3-4 Bäume sollen diese Situation verhindern! 
 
 \begin{figure}[H]
    \centering\input{241103b.latex}
    \caption{$h \in \OO(\log n)$}
    \label{241103b}
 \end{figure}

Der worst case wird dadurch verhindert, daß zwischendurch etwas mehr Aufwand betrieben wird.
So wird beim dem Einfügen vorausgehenden Suchen jeder erreichte 4-Knoten sofort aufgesplittet.
So ist gewährleistet, daß immer Platz für einen neuen Knoten ist.

Allerdings haben Top down 2-3-4-Bäume auch den Nachteil, daß sie schwerer implementierbar sind als die 2-3-4 Bäume. 
    Dafür sind \textbf{Rot-Schwarz-Bäume} besser geeignet. %\todo{Wofür besser geeignet? Implementierung?}      

\section{Rot-Schwarz-Bäume}
 
\begin{definition}[Rot-Schwarz-Bäume]
   \textbf{Rot-Schwarz-Bäume} sind binäre Suchbäume mit folgenden zusätzlichen Eigenschaften:
      \begin{enumerate}
	          \item Jeder Knoten ist rot oder schwarz.
	          \item Die Wurzel ist schwarz.
	          \item Jedes Blatt ist schwarz.
	          \item Ein roter Vater darf keinen roten Sohn haben.
	          \item Die Schwarzhöhe $(\Bh(j))$ ist die Anzahl der schwarzen Knoten auf einem Weg von einem Knoten $j$ zu einem
		  Blatt. Sie ist für einen Knoten auf allen Wegen gleich.
      \end{enumerate}
\end{definition}

\begin{definition}[Die Schwarzhöhe]
  $\Bh(x)$ ist die Anzahl von schwarzen Knoten auf einem Weg, ohne den Knoten $x$  selbst von $x$ zu einem Blatt und heißt
  Schwarzhöhe von $x$.
\end{definition}

\begin{satz}[Die Höhe von Rot-Schwarz-Bäumen] \label{rshoehe}
  Die Höhe eines Rot-Schwarz-Baumes mit $n$ Knoten ist kleinergleich $2\log(n+1)$
\end{satz}
  
\begin{satz}\label{schwarzhoehe}
Sei $x$ die Wurzel eines Rot-Schwarz-Baumes. Dann hat der Baum mindestens $2^{\Bh(x)}-1$ Knoten.
  Der Beweis kann induktiv über die Höhe des Baumes erfolgen.
\end{satz}

Um Speicherplatz zu sparen, bietet es sich an, nur ein Nil-Blatt abzuspeichern. Dann müssen natürlich alle Zeiger entsprechend gesetzt
werden.
 
 \begin{figure}[H]
    \centering\input{241103c.latex}
    \caption{Nur ein NIL-Blatt}
    \label{241103c}
 \end{figure}

\begin{beweis} 
 IA : Sei $h=0$. Dann handelt es um einen Baum, der nur aus einem NIL-Blatt besteht. Dann ist $\Bh(x)=0$ und nach \autoref{schwarzhoehe} 
 die Anzahl der inneren Knoten mindestens $2^0-1=0$. Damit ist der Induktionsanfang für \autoref{schwarzhoehe} gezeigt. 
 
  IS : Sei nun h'$<$h. Dazu betrachten wir die zwei Teilbäume eines Baumes mit der Höhe $h$ und der Wurzel $x$.
Dann gibt es jeweils für den rechten und den linken Teilbaum zwei Fälle. Dabei sind die Fälle für die beiden Teilbäume analog und
 werden deswegen gleichzeitig abgearbeitet.

    \begin{figure}[H]
    \centering\input{241103d.latex}
    \caption{Baumhöhe mit $\Bh(li[x])=\Bh(x)-1$ und $\Bh(re[x])=\Bh(x)$}
    \label{241103d}
 \end{figure} 
 
    \textbf{1.Fall}:  Sei $\Bh(li[x]) = \Bh[x]$. Da eine untere Schranke gezeigt werden soll und in diesem Fall der Baum nicht weniger 
    Knoten
    hat als im zweiten Fall, reicht es den Beweis für den zweiten (kritischeren) Fall zu führen (Der zweite Fall ist kritischer, da
    der Teilbaum weniger Knoten haben kann als im ersten Fall und damit eher in der Lage ist, die untere Schranke zu durchbrechen).
     
    \textbf{2.Fall}:  Sei $\Bh(li[x]) = \Bh[x]-1$, dann gibt mindestens soviele innere Knoten, wie die beiden
    Teilbäume nach Induktionsvoraussetzung zusammen haben. Damit hat der gesamte Baum mindestens   
    $1+2^{\Bh(x)-1}-1+2^{\Bh(x)-1}-1$  $=$ $2 \cdot 2^{\Bh(x)-1}+1-2 = 2^{\Bh(x)}-1$. Damit ist \autoref{schwarzhoehe} bewiesen. 
    
Zum Beweis von \autoref{rshoehe} wird Eigenschaft 4 der Rot-Schwarz-Bäume ausgenutzt. Daraus folgt direkt, daß auf dem Weg von der Wurzel zu den Blättern,
mindestens die Hälfte der Knoten schwarz ist. Sei $h$ wieder die Höhe
des Baumes, dann gilt damit\todo{Unten steht das Wort root im Mathesatz.}

\begin{align*}
  \Bh(root) &\geq\frac{h}{2}\stackrel{\text{\autoref{schwarzhoehe}}}{\longrightarrow}\\
  n &\geq 2^{\Bh(root)}-1 \geq 2^{\frac{h}{2}}-1 \Rightarrow 2^{\frac{h}{2}}\leq n+1\\
  &\Leftrightarrow \frac{h}{2}\leq \log(n+1)
\end{align*}
\end{beweis} 

% Mitschrift vom 26.11.03, Michael Preiß
\subsection{Operationen in RS-Bäumen}

Die Operationen Tree-Insert und -Delete haben bei Anwendung auf einen RS-Baum eine Laufzeit von $\OO(\log n)$. Da
sie den Baum verändern, kann es vorkommen, daß die Eigenschaften des Rot-Schwarz-Baumes verletzt werden. Um diese wieder herzustellen,
müssen die
Farben einiger Knoten im Baum sowie die Baumstruktur selbst verändert werden. Dies soll mittels Rotationen realisiert werden, dabei
gibt es die \textsc{Linksrotation} und die \textsc{Rechtsrotation}.
%\todo{wieso ist die rechtsrotation algorithmisch etwas komplizierter?}


\begin{figure}[H]
\centering
\input{261103a.latex}
\caption{Die Rotation schematisch}
\end{figure}


\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Linksrotation\textnormal{(T, z)}}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
                   Tree-Search, Treepreorder, Treeinorder, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Linksrotation\textnormal{(T, z)}}, gobble=4]{Linksrotation(T,z)}
    y := re[x]
    re[x] := li[y]
    p[li[y]] := x
    p[y] := p[x]
    if p[x] = NIL
      then root[T] := y
      else if x = li[p[x]]
        then li[p[x]] := y
        else re[p[x]] := y
    li[y] := x
    p[x] := y
\end{lstlisting}
\end{Algorithmus}

\begin{satz}
Rotationen ändern die Gültigkeit der Suchbaumeigenschaft nicht.
\end{satz}

Wie die Skizze vermuten läßt, ist der Code für die \textsc{Retchsrotation} symmetrisch zu dem für die \textsc{Linksrotation}. Beide
Operationen erfordern $\OO(1)$~Zeit, da mit jeder Rotation nur eine konstante Anzahl von Zeigern von umgesetzt wird und der Rest
unverändert bleibt.

Sei T ein Rot-Schwarz-Baum. Ziel ist, daß T auch nach Einfügen eines Knotens z ein Rot-Schwarz-Baum ist. T soll also 
nach Anwendung von \textsc{RS-Insert}(T, z) und einer eventuellen Korrektur die Bedingungen für Rot-Schwarz-Bäume erfüllen. Im folgenden Beispiel wird die "`3"'
eingefügt.

\begin{figure}[H]
\centering
\input{261103b.latex}\input{261103c.latex}\hspace{5mm}\input{261103d.latex}

\caption{Funktionsweise der Rotation}
\end{figure}
Die letzte Rotation sollte zur übung selbst nachvollzogen werden. Der fertige Baum als Ergebnis dieser letzten Roation steht im Anhang
auf Seite \pageref{rsrotation}.

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{RS-Insert\textnormal{(T, z)}}}
%\lstset{emph={Korrigiere}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
                   Korrigiere, Treepreorder, Treeinorder, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{RS-Insert\textnormal{(T, z)}}, gobble=1]{RS-Insert(T,z)}
 y := NIL[T]
 x := root[T]
 while x $\neq$ NIL[T] do
   y := x
   if key[z] < key[x]
     then x := li[x]
     else x := re[x]
 p[z] := y
 if y = NIL[T]
   then root[T] := z
   else if key[x] < key[y]
     then li[y] := z
     else re[y] := z
 li[z] := NIL[T]
 re[z] := NIL[T]
 Farbe[z] := ROT
 Korrigiere (T, z)
\end{lstlisting}
\end{Algorithmus}

Früher wurden die Knoten gefärbt, mittlerweile ist man aber dazu übergegangen, die Kanten zu färben. Dabei gilt, daß eine rote
Kante auf einen früher rot gefärbten Knoten zeigt und eine schwarze Kante auf Knoten, die früher schwarz gefärbt
wurden. Die Färbungen sind auch auf andere Bäume übertragbar. Die Färbung der Kanten hat den Nachteil, daß sich die Schwarzhöhe so
ergibt, daß die roten Kanten auf einem Weg nicht mitgezählt werden. Einfacher und damit sicherer ist es, wenn nur die schwarzen Knoten
gezählt werden.

Für alle höhenbalancierten Bäume gilt, daß ihre Höhe in $\OO(\log n)$ liegt, allerdings haben Rot-Schwarz-Bäume den Vorteil, daß
sie leichter zu implementieren sind. Da die NIL-Blätter schwarz sind, kann ein einzufügender Knoten auch erstmal einmal rot gefärbt
sein.

%\textbf{Funktionsweise:} \dots \todo{fehlt noch!}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Korrigiere\textnormal{(T, z)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
                   Korrigiere, Linksrotation, Rechtsrotation, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Korrigiere\textnormal{(T, z)}}, gobble=1]{Korrigiere(T,z)}
 while Farbe[p[z]] = ROT do
   if p[z] = li[p[p[z]]] ~\hspace{20mm}  $\vartriangleright$\textnormal{Vater ist linker Sohn von Opa von z}~
     then y := re[p[p[z]]] ~\hspace{26.1mm} $\vartriangleright$\textnormal{y ist Onkel von z}~
       if Farbe[y] = ROT ~\hspace{31mm} $\vartriangleright$\textnormal{Vater und Onkel rot}~
         then Farbe[p[z]] := SCHWARZ ~\hspace{5.3mm} $\vartriangleright$\textnormal{Vater wird schwarz}~
           Farbe[y] := SCHWARZ ~\hspace{18.7mm} $\vartriangleright$\textnormal{Onkel wird schwarz}~
           Farbe[p[p[z]]] := ROT ~\hspace{14.5mm} $\vartriangleright$\textnormal{Opa wird schwarz}~
           z := p[p[z]]
         else if z = re[p[z]]
           then z := p[z]
             Linksrotation (T, z)
         Farbe[p[z]] := SCHWARZ
         Farbe[p[p[z]]] := ROT
         Rechtsrotation(T, p[p[z]])
    else wie bisher, nur li und re vertauschen
 Farbe[Root[T]] := SCHWARZ
\end{lstlisting}
\end{Algorithmus}

Eine mögliche Anwendung für einen Rot-Schwarz-Baum ist das bereits einleitend erwähnte Segmentschnitt-Problem. Dabei kann ein
Rot-Schwarz-Baum für die Verwaltung der Sweepline-Status-Struktur verwendet werden (Menge Y, dazu siehe auch \autoref{planesweep}).

% 01.12.2003 Sylvia Andersch
\section{Optimale binäre Suchbäume}

Seien wie im folgenden Beispiel Schlüsselwerte $a_1<\ldots<a_n$ mit festen bekannten Wahrscheinlichkeiten $p_1,\ldots,p_n$ gegeben, wobei gilt $p_i
\geq0$ und $\sum_{i=1}^n p_i=1$. Dabei bezeichnet $p_i$ die Wahrscheinlichkeit mit der auf den Wert $a_i$ zugegriffen wird.

\begin{description}
    \item[Beispiel:]  Sei nun $a_1=1,a_2=2,\ldots,a_6=6$, 
      $p_1=0,25,p_2=0,28,p_3=0,1,p_4=0,2,p_5=0,13,p_6=0,04$ und
      der binäre Suchbaum sähe wie folgt aus:
      \centering
      \setlength{\unitlength}{4000sp}%
%
\begingroup\makeatletter\ifx\SetFigFont\undefined%
\gdef\SetFigFont#1#2#3#4#5{%
  \reset@font\fontsize{#1}{#2pt}%
  \fontfamily{#3}\fontseries{#4}\fontshape{#5}%
  \selectfont}%
\fi\endgroup%
\begin{picture}(2300,2425)(1000,-3425)
\thinlines
\put(1650,-1150){\circle{300}}
\put(1600,-1230){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}5}}}
\put(1650,- 1300){\line(-1,-1){450}}
\put(1650,- 1300){\line(1,-1){450}}
\put(1200,-1900){\circle{300}}
\put(1150,-1980){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}2}}}
\put(2100,-1900){\circle{300}}
\put(2050,-1980){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}6}}}
\put(1200,-2050){\line(1,-1){450}}
\put(1200,-2050){\line(-1,-1){450}}
\put(750,-2650){\circle{300}}
\put(700,-2730){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}1}}}
\put(1650,-2650){\circle{300}}%\put(1650,-1150){\circle{300}}
\put(1600,-2730){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}3}}}
\put(1650,- 2800){\line(1,-1){450}}
\put(2100,-3400){\circle{300}}
\put(2050,-3480){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}4}}}
\end{picture}
      
      
\end{description}

An diesem Beispiel wird deutlich, daß es bis zur $4$ ein relativ langer Weg ist, d.h. auch die Suche nach $4$ dauert
im Vergleich zu anderen lange. Nun hat aber die 4 eine hohe Wahrscheinlichkeit und wird deshalb oft abgefragt
werden. Also wird die durchschnittliche Rechenzeit relativ lang sein. Es wäre also schöner, wenn die $4$ weiter oben im Baum stünde.
Diese überlegungen lassen sich fortführen und es fraglich erscheinen, ob sich durch Umplazieren von anderen Schlüsselwerten,
die Rechenzeit noch weiter verkürzen läßt (z.\,B. könnte die $6$ weit nach unten, da sie die kleinste Wahrscheinlichkeit
hat und somit nur selten abgefragt wird). Aber wo liegen die Grenzen dieses Prozesses? Es ist ja klar, daß er
irgendwie begrenzt sein muss. Es bleibt also die Frage: 

Gibt es irgendwelche Schranken, durch die die durchschnittliche
Rechenzeit beschränkt ist und wenn ja, wie sehen diese aus? 

Sicher ist, daß die Rechenzeit eng mit der Höhe des
Baumes zusammenhängt. Die Frage ist also äquivalent dazu, ob es Grenzen für die durchschnittliche Knotentiefe gibt und falls ja, wie 
diese aussehen.
Allgemein sollen in diesem Kapitel folgende Fragen beantwortet werden:
\begin{enumerate}
    \item Wie muss der binäre Suchbaum konstruiert werden, damit er optimal ist?
    \item Durch welche Grenzen wird die mittlere Knotentiefe beschränkt?
\end{enumerate}

Dazu muß zuerst \textbf{Optimalität} definiert werden. Hier ist damit folgendes gemeint:
 \begin{definition}
   Ein binärer Suchbaum heisst \textbf{optimal}, wenn die Summe $\sum_{i=1}^n p_i(t_i+1)$ minimal ist, wobei
      $t_i$ die Tiefe des Knotens von $a_i$ angibt. Die Addition von "`1"' zur Tiefe erfolgt, damit auch der Wert für die Wurzel in
      das Ergebnis eingeht.
    \end{definition}
Fakt ist, daß jeder Teilbaum in einem optimalen Suchbaum wieder optimal ist. Der Sachverhalt, daß eine optimale Lösung des
Gesamtproblems auch jedes Teilproblem optimal löst, heißt "`Optimalitätskriterium von Bellmann"'.

Diese Tatsache kann nun benutzt werden, um mittels dynamischer Programmierung einen optimalen Suchbaum zu konstruieren.

\subsection{Bottom-Up-Verfahren}

Gegeben ist folgendes:
\begin{itemize}
    \item Gesamtproblem $(a_1, \dots,a_n)$, d.h. $n$ Schlüsselwerte mit den dazu gehörigen Wahrscheinlichkeiten $p_i$ für $i=1 \dots n$
    \item Tiefen der Knoten $t_1,\dots,t_n$
    \item Die mittlere Suchzeit, gegeben durch $\sum_{i=1}^n p_i(t_i+1)$
\end{itemize}
\begin{description}
    \item[Idee] Das Gesamtproblem wird in Teilprobleme aufgesplittet, d.h. grössere optimale Suchbäume werden
     aus kleineren optimalen Suchbäumen  berechnet und zwar in einem rekursiven Verfahren.
     
     Angenommen alle möglichen optimalen Suchbäume mit weniger als den $n$ gegebenen Schlüsselwerten sind schon bekannt.
     Der optimale Suchbaum mit $n$ Schlüsselwerten besteht aus einer Wurzel, einem rechten und einem linken Teilbaum, wobei
     für die Teilbäume die optimale Darstellung schon gegeben ist. Zu suchen ist noch die Wurzel $a_k$, für die die
     Summe der mittleren Suchzeiten der beiden Teilbäume minimal ist.
\end{description}
Dazu definiert man die Teilprobleme $(i,j)=(a_i,\dots,a_j)$. Der Baum für das optimale Teilproblem wird mit $T(i,j)$
bezeichnet. Weiter ist $p(i,j)\coloneqq \sum_{m=i}^j p_m$ die Wahrscheinlichkeit, daß ein Wert zwischen $a_i$ und $a_j$
erfragt wird.\\ Ziel ist es, die mittlere Teilsuchzeit $t(i,j)\coloneqq \sum_{m=i}^j p_m(t_m+1)$ zu minimieren. Dies beinhaltet
das Problem, die optimale Wurzel zu suchen. Man wählt also ein beliebiges $k\in [i,j]$, setzt $a_k$ als Wurzel
an und berechnet das dazugehörige $t(i,j)$ \\
Betrachten Teilbaum $T(i,j)$:\\ Sei $a_k$ die Wurzel und $j>0$. Die Suchzeit berechnet sich durch\\
$t(i,j)=$ Anteil der Wurzel + Anteil linker Teilbaum + Anteil rechter Teilbaum
$\Rightarrow t(i,j)=p_k\cdot 1+p(i,k-1)+t(i,k-1)+p(k+1,j)+t(k+1,j)$
wegen $p_k+p(i,k-1)+p(k+1,j)=p(i,j)$ folgt
$t(i,j)=\left\{\begin{array}{ll}
    0, & i>j \\
    p(i,j)+ \min_{i\leq k\leq j}[t(i,k-1)+t(k+1,j)], & sonst \\ \end{array}\right.    $

\begin{description}
    \item[Beispiel:] Zahlen wie oben
    \begin{table}[ht]
     \begin{tabular}{{c}|{c}{c}{c}{c}{c}{c}{c}{c}{c}}
    i$\backslash$ j & 1  &   2  &  3   &  4   &  5   & 6  \\
    \hline
    1             & 0,25/1 & 0,78/2 & 0,98/2 & \ldots/2  & \ldots/2  &\ldots/2 \\
    2             &        & 0,28/2 & 0,48/2 & 0,98/2 & \ldots/2  & \ldots/2 \\
    3             &        &        & 0,1/3  & 0,4/4  & \ldots/4  & \ldots/4\\
    4             &        &        &        & 0,2/4  & \ldots/4  & \ldots/4\\
    5             &        &        &        &        & 0,13/5 & \ldots/5\\
    6             &        &        &        &        &        & 0,04/6\\
  \end{tabular}
  \caption{Tabelle für $t(i,j)$/ optimale Wurzel $k$}
  \end{table}

$\begin{array}{ccl}
t(1,2) &=& p(1,2)+ \min[t(1,0)+t(2,2),t(1,1)+t(3,2)]\\
       &=& 0,53+\min[0+0,28,0,25+0]\\
       &=& 0,53+0,25=0,78
\end{array}$
$\begin{array}{ccl}
t(2,3) &=& p(2,3)+ \min[t(2,1)+t(3,3),t(2,2)+t(4,3)]\\
       &=& 0,38+\min[0+0,1,0,28+0]\\
       &=& 0,38+0,1=0,48
\end{array}$
$\begin{array}{ccl}
t(2,4) &=& p(2,4)+ \min[t(2,1)+t(3,4),t(2,2)+t(4,4),t(2,3)+t(5,4)]\\
       &=& 0,58+\min[0+0,4,0,28+0,2,0,48+0]\\
       &=& 0,58+0,4=0,98
\end{array}$
$\Rightarrow$ optimaler Baum:
\centering
\setlength{\unitlength}{4000sp}%
%
\begingroup\makeatletter\ifx\SetFigFont\undefined%
\gdef\SetFigFont#1#2#3#4#5{%
  \reset@font\fontsize{#1}{#2pt}%
  \fontfamily{#3}\fontseries{#4}\fontshape{#5}%
  \selectfont}%
\fi\endgroup%
\begin{picture}(2300,2225)(1000,-3425)
\thinlines
\put(1650,-1150){\circle{300}}
\put(1600,-1230){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}2}}}
\put(1650,- 1300){\line(-1,-1){450}}
\put(1650,- 1300){\line(1,-1){450}}
\put(1200,-1900){\circle{300}}
\put(1150,-1980){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}1}}}
\put(2100,-1900){\circle{300}}
\put(2050,-1980){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}4}}}
\put(2100,-2050){\line(1,-1){450}}
\put(2100,-2050){\line(-1,-1){450}}
\put(1600,-2650){\circle{300}}
\put(1550,-2730){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}3}}}
\put(2550,-2650){\circle{300}}%\put(1650,-1150){\circle{300}}
\put(2500,-2730){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}5}}}

\put(2550,- 2800){\line(1,-1){450}}
\put(2950,-3400){\circle{300}}
\put(2900,-3480){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}6}}}
\end{picture}

\end{description}
\subsection{Schranken}

Wir wollen nun der Frage nachgehen welche Schranken für die mittlere Rechenzeit gelten, und zwar nach oben und nach unten.
Dazu ein kurzer Einschub zur Entropie:
Gegeben sei ein Zufallsexperiment mit m Ausgängen und den zugehörigen Wahrscheinlichkeiten $p_1,\ldots,p_m$
\begin{description}
    \item[Entropie] $H(p_1,\ldots,p_n)$ = Maß für die Unbestimmtheit eines Versuchsausganges und berechnet sich durch
     $H(p_1,\ldots,p_n)=-\sum_{i=1}^m p_i\log_2 p_i$
\end{description}

\begin{satz}
 \begin{itemize}
    \item $H(p_1,\ldots,p_n)\leq\log_2 m$ (Gleichheit gilt
      $\Leftrightarrow p_1=\ldots=p_m=\frac{1}{m})$
    \item $H(p_1,\ldots,p_n) = H(p_1,\ldots,p_n,0)$
    \item $H(p_1,\ldots,p_n)=H(p_{\pi(1)},\ldots,p_{\pi(m)})$
    \item Satz von Gibb: seien $q_1,\ldots,q_m$ Werte größer oder gleich Null und $\sum_{i=1}^m q_i\leq1$, so gilt
     $H(p_1,\ldots,p_n)\leq-\sum_{i=1}^m p_i\log_2q_i$
 \end{itemize}
\end{satz}

\begin{satz}
 Wenn die Daten nur in den Blättern stehen, so ist die Entropie eine untere Schranke für die mittlere Tiefe
 der Blätter $\sum_{i=1}^m p_i t_i$ (hier $t_i,i=1,\ldots,n$ Tiefe der Blätter), es gilt
 $H(p_1,\ldots,p_n)\leq\sum_{i=1}^m p_it_i$
\end{satz}
\begin{beweis}
Sei $(q_1,\ldots,q_m)=(2^{-t_1},\ldots,2^{-t_m})$, dann gilt nach obigem Satz:\\
$\begin{array}{cccl}
  H(p_1,\ldots,p_n) & \leq & - &\displaystyle\sum_{i=1}^m p_i\log_2q_i \vspace{0.3mm}\\
                 & =    & - &\displaystyle\sum_{i=1}^m p_i\log_22^{-t_i} \vspace{0.3mm}\\
                 & =    &   &\displaystyle\sum_{i=1}^m p_it_i \\
\end{array}$\\
Die Werte für q sind korrekt gewählt, da nach der Ungleichung von Kraft gilt: $\sum_{i=1}^m 2^{-t_i}\leq1$
\end{beweis}
\begin{flushright} q.~e.~d. \end{flushright}
\begin{satz}
Die mittlere Knotentiefe eines optimalen binären Suchbaumes (Daten im gesamten Suchbaum) liegt im Intervall $
\begin{bmatrix}
  \displaystyle\frac{ H(p_1,\ldots,p_n)}{\log_23}-1,H(p_1,\ldots,p_n)
\end{bmatrix}$
wobei $p_i$ die Wahrscheinlichkeit ist, mit der der i-te Knoten abgefragt wird.
\end{satz}
\begin{beweis}
\begin{enumerate}
    \item mittlere Knotentiefe $\geq\displaystyle\frac{ H(p_1,\ldots,p_n)}{\log_23}-1$.\\
     Jeder binäre Suchbaum kann in einen ternären Baum transformiert werden, in dem nur die Blätter Daten enthalten. Die neuen Blätter rutschen eine Ebene tiefer:
 \setlength{\unitlength}{4000sp}%
\begingroup\makeatletter\ifx\SetFigFont\undefined%
\gdef\SetFigFont#1#2#3#4#5{%
  \reset@font\fontsize{#1}{#2pt}%
  \fontfamily{#3}\fontseries{#4}\fontshape{#5}%
  \selectfont}%
\fi\endgroup%
\begin{picture}(2300,2025)(1000,-2425)
\thinlines
\put(1650,-1150){\circle{450}}
\put(1600,-1230){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}a}}}
\put(2800,-1200){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}$\Longrightarrow$}}}
\put(1650,- 1370){\line(-1,-1){450}}
\put(1650,- 1370){\line(1,-1){450}}
\put(1200,-2040){\circle{450}}
\put(1020,-2130){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}LTB}}}
\put(2100,-2040){\circle{450}}
\put(1920,-2130){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}RTB}}}
\put(4650,-1150){\circle{450}}
\put(4650,- 1370){\line(-1,-1){450}}
\put(4650,- 1370){\line(1,-1){450}}
\put(4650,- 1370){\line(0,-1){450}}
\put(4650,-2000){\circle{300}}
\put(4600,-2080){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}a}}}
\put(4200,-2040){\circle{450}}
\put(4020,-2130){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}LTB}}}
\put(5100,-2040){\circle{450}}
\put(4920,-2130){\makebox(0,0)[lb]{\smash{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}RTB}}}
\end{picture}\\
Es gilt:\\
      $\begin{array}{ccl}
        \mbox{mittl. Knotentiefe des bin. Baums} & \geq & \mbox{mittl. Blatttiefe des ternären Baums}-1 \\
         & \geq & \displaystyle H_3(p_1,\ldots,p_n)-1=-\sum_{i=1}^n p_i\cdot \log_3 p_i \\
         &  =   & \displaystyle-\frac{1}{\log_2 3}\sum_{i=1}^n p_i\cdot \log_2 p_i-1\\
         &  =   & \displaystyle\frac{1}{\log_2 3}H(p_1,\ldots,p_n)-1
      \end{array}$

    \item $\displaystyle H(p_1,\ldots,p_n)\geq$ mittlere Knotentiefe\\
      Um für $p_i,\ldots,p_j$ einen möglichst guten Suchbaum zu bestimmen, berechnen wir\\ $q=\sum_{k=i}^j p_k$ und wählen
      als (Teilbaum)-Wurzel den Knoten $k$ für den gilt: $\sum_{k=i}^{l-1}p_k\leq\frac{q}{2}\leq\sum_{k=i}^{l}p_k$
      Für die Teilfolgen $(p_i,\ldots,p_{l-1})$ und $(p_{l+1},\ldots,p_j)$ verfahren wir rekursiv. Gestartet wird mit $(p_1,\ldots,p_n)$.\\
      Nun gilt für Tiefe $t_l$ einer jeden Teilbaumwurzel $l$ (mit $i\leq l\leq j$), daß $\sum_{k=i}^j p_k \leq 2^{-t_l}$
      (wegen Wahl von Wurzel). Insbesondere folgt: $p_l\leq2^{-t_l}$. Dies ergibt:
      $\mbox{mittlere Suchbaumtiefe}=\sum_{i=1}^n p_i\cdot t_i\leq-\sum_{i=1}^n p_i\cdot \log_2p_i=H(p_1,\ldots,p_n)$
\end{enumerate}
\end{beweis}
\begin{flushright} q.~e.~d. \end{flushright}

\section{Stapel}
Stapel haben eine so große Bedeutung in der Informatik, daß sich auch im Deutschen das englische "`Stack"' eingebürgert hat.
In vielen Algorithmen ist eine Menge zu verwalten, für die sich die einfache Struktur eines Stapels anbietet.
Manche Caches arbeiten nach dem LIFO-Prinzip (Last In First Out), dazu reicht ein simpler Stapel, in den die zu verwaltenden Elemente
der Reihe nach hereingegeben werden und ein Zeiger auf das zuletzt hereingegebene Element gesetzt wird.

\begin{figure}[H]
  \centering\input{031203a.latex}
  \label{031203a}
\end{figure}

Die Basisoperationen eines Stapels sind natürlich \textsc{Push}, \textsc{Pop} und \textsc{Stack-Empty} (Test ob Stack leer), die Implementierung dieser einfachen
Operationen ist simpel. Dabei steht "`S"' für den Stapel.

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Push}}
%\lstset{emph={Top}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
        Top, Korrigiere, Linksrotation, Rechtsrotation, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Push}, gobble=1]{Push}
  Top[s]:=Top[S]+1
  S[Top[S]]:=x
\end{lstlisting}
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Stack-Empty}}
%\lstset{emph={Top}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
        Top, Korrigiere, Linksrotation, Rechtsrotation, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Stack-Empty}, gobble=1]{Stack-Empty}
  if Top[s]=0 
    Then return true
    Else return false
\end{lstlisting}
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Pop}}
\lstset{emph={Top, Stack-Empty}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
        Top, Stack-Empty, Korrigiere, Linksrotation, Rechtsrotation, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Pop}, gobble=1]{Pop}
  if Stack-Empty
    Then Error
    Else Top[S]:=Top[S]-1
      return S[Top[S]+1]
\end{lstlisting}
\end{Algorithmus}

Eine These besagt, daß Stapel beim Algorithmenentwurf vor allem für die Verwaltung von Kandidaten verwendet werden. Ein Beispiel
dafür ist die Berechnung der konvexen Hülle einer Menge.

\subsection{Konvexe Hülle}
CH von englisch Convex Hull bezeichnet die konvexe Hülle, z.\,B. CH(P) für ein Polygon oder CH(X) für X $\subseteq \R^2$. Dabei
ist diese mathematisch so definiert:
\begin{definition}
\[\mbox{CH(X)}=\bigcap_{\textnormal{M konvex}\wedge X \subseteq M} M\]
\end{definition}
Anschaulich gesprochen ist CH(X) damit die kleinste Menge, die X umfaßt. Eine sehr schöne Beschreibung ist auch diese: Man stelle sich
ein Brett mit hereingeschlagenen Nägeln vor. Dann vollzieht ein Gummiband, welches um die Nägel gelegt wird, die konvexe Hülle der
Nagelmenge nach. Die Punkte der konvexen Hülle sind nur die Nägel, die das Gummiband tangiert. 

\begin{satz}
Wenn X endich ist, so ist CH(X) ein Polygon (Jede Punktmenge läßt sich auczh als Polygon auffassen).
\end{satz}

Ziel ist die Berechnung der CH(X) für endliche X $\subseteq \R^2$. Erreicht wird dieses Ziel durch den Algorithmus von Graham
(1972), der sich grob wie folgt einteilen läßt:

\begin{itemize}
\item Sortiere X bezüglich wachsender x-Werte (o.B.d.A. habe X allgemeine Lage, Sonderfälle wie z.\,B. x$_i$=x$_j$ für i$\neq$j treten
also nicht auf und müssen nicht behandelt werden)
\item Setze p$_1$=p$_l \in$ X als den Punkt mit dem kleinsten x-Wert und p$_r \in$ X als den Punkt mit dem größten x-Wert 
\item Wende den Algorithmus zur Berechnung von UH(X) an, dabei ist UH(X) die obere konvexe Hülle von X
\item für LH(X) vertausche die Vorzeichen und benutze UH(X) noch einmal, dabei ist LH(X) die untere konvexe Hülle von X
\end{itemize}

\begin{figure}[H]
  \centering\input{031203b.latex}
  \label{031203b}
\end{figure}

Doch wie funktioniert das nun genau? Zuerst brauchen wir den Algorithmus für UH(X):
\begin{itemize}
\item Eingabe ist die Punktfolge p$_l$=p$_1$, p$_2$, \ldots, p$_n$=p$_r$, dabei sind die x-Werte monoton wachsend und alle p$_i$ liegen
oberhalb von G$_{p_l p_r}$ (dies läßt sich notfalls in $\OO(n\log n)$~Zeit erreichen)
\item Der Algorithmus arbeitet dann Punkt für Punkt von links nach rechts und bewahrt folgende Invariante
  \begin{enumerate}
  \item Der Stapel S speichert Punkte x$_0$, x$_1$, \ldots, x$_t$=x$_{\textnormal{Top}}$, die eine Teilfolge von p$_n$, p$_1$, p$_2$, \ldots sind
  \item t$\geq$2 $\wedge$ x$_0$=p$_n$ $\wedge$ x$_1$=p$_1$ $\wedge$ im Schritt s gilt: x$_t$=p$_s$, dabei ist s$\geq$2
  \item $x_0, x_1, \ldots, x_t$ ist UH($\{p_1, \ldots, p_s\}$)
  \item $x_1, \ldots, x_t$ sind von rechts sortiert.
  \end{enumerate}
\end{itemize}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Graham}}
%\lstset{emph={Push, Pop}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Tree-Successor}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Graham}, gobble=1]{Graham}
  Push(S, p$_n$)
  Push(S, p$_1$)
  Push(S, p$_2$)
  s:=2
  while s$\neq$n do
    $\alpha$:=top[S]
    $\beta$:=zweites Element in S
    while (p$_{s+1}, \alpha, \beta$) keine Linksdrehung ist do
      Pop
      $\alpha$:=$\beta$
      $\beta$:=neues zweites Element im Stapel
    Push(S, p$_{s+1}$)
    s:=s+1
  Gib S aus    
\end{lstlisting}
\end{Algorithmus}

Der Drehsinn läßt sich dabei mathematisch recht einfach feststellen (siehe \autoref{drehsinn}), die Details sind an dieser Stelle
aber weniger wichtig.
Doch wie kann man das für die Berechnung
der konvexen Hülle ausnutzen?  \autoref{konvexedrehungskizze} macht dies sehr anschaulich klar. Die Punkte sind jeweils die
Endpunkte der eingezeichneten Strecken und daher nicht explizit markiert. Im rechten Fall wird der Punkt $p_2$ für die konvexe Hülle
überflüssig, da er durch $p_{s+1}$ "`überdeckt"' wird. Im linken Bild ist $p_2$ hingegen für die konvexe Hülle notwendig ($p_2$
liege oberhalb der Strecke $\overline{p_1p_{s+1}}$). Dies verdeutlicht anschaulich, warum ein Punkt der durch
\textsc{Pop} im Laufe der Berechnung der konvexen Hülle herausfliegt, nicht nochmals angeschaut werden muß.

\begin{figure}[H]
\centering
\input{031203d.latex}

\caption{Nutzen des Drehsinns für die Berechnung der konvexen Hülle}
\label{konvexedrehungskizze}
\end{figure}

Nun bleibt noch die Analyse des Algorithmus. Bei der Analyse nach der Guthabenmethode werden Operationen mit Kosten versehen, die auf ein fiktives Bankkonto
verbucht werden. Hier kostet nun jedes \textsc{Push} 2~\officialeuro{}. Davon wird einer verbraucht und einer
gespart. Von dem Ersparten werden die \textsc{Pop}'s bezahlt; dabei kostet jedes \textsc{Pop} einen . Da wie bereits oben erwähnt jeder
Punkt maximal einmal durch ein \textsc{Pop} herausfliegen kann, sind wir fertig. Unser Guthaben reicht aus um alle möglichen
\textsc{Pop}'s zu bezahlen. Mittels der Guthabenmethode ergeben sich so amortisierte Kosten von $\OO(n)$ für die Berechnung der konvexen
Hülle.

Mithilfe eines Stapels ist also ein sehr effizienter Algorithmus möglich. Allerdings sind natürlich auch Stapel kein Allheilmittel
für alle Probleme der algorithmischen Goemetrie. Ein anderes wichtiges Mittel sind die sogenannten Segmentbäume.

\section{Segmentbäume}
Bereits anfangs wurde das Problem der überschneidung von Rechtecken erwähnt. Eingabe sollten Ortho-Rechtecke sein und Ausgabe ein
Bericht der überschneidungen. Dazu kann man
\begin{enumerate}
\item alle Rechteckseiten bestimmen und dann
\item den Sgmentschnitt-Algorithmus benutzen (siehe \autoref{planesweep})
\end{enumerate}

Der dabei verwendete abstrakte Datentyp Dictionary könnte mit den bisher kenngelernten Methoden z.\,B. RS-, AVL- oder
Top-down-2-3-4-Baum realisiert werden.
Wichtiger ist jetzt allerdings, daß dieses Problem auch auf das Problem der Punkteinschlüsse zurückgeführt werden kann. Die Eingabe
ist dann eine Menge von Punkten und die Ausgabe sind dann alle Punkteinschlüsse (p, R) mit $p  \in  R$. Dabei sind mit $p_1,   p_2,
  \ldots$ die Punkte und mit $R_1,   R_2,   \ldots$ die Rechtecke gemeint. Auch hier wird eine Gleitgeradenmethode benutzt; die
Ereignispunkte sind die x-Koordinaten der linken und rechten Kanten und der Punkte.
\begin{itemize}
\item[] (li, x, (y$_1$, y$_2$), R) \hspace{2cm} (o.\,B.\,d.\,A. $y_1 \leq y_2$)
\item[] (re, x, (y$_1$, y$_2$), R)
\item[] (Punkt, x, y, R)
\end{itemize}

An der folgenden Skizze wird offensichtlich, daß es verschiedene Arten von überschenidungen bzw. Schnitten gibt. 
\begin{figure}[H]
\centering
\input{031203e.latex}

\end{figure}

\subsection{Der Punkteinschluß-Algorithmus}
$Y$ wird zu Beginn  mit $Y\coloneqq \emptyset$ initalisiert. Dann wird von Ereignispunkt zu Ereignispunkt gegangen, nachdem diese nach wachsendem
$x$ sortiert wurden. Bei $m$ Rechtecken geht dies in $\OO(m\log m)$~Schritten.

\begin{enumerate}
\item Falls der aktuelle Ereignispunkt die Form (li, x, ($y_1,y_2$), R) hat, so setze $Y\coloneqq Y\cup \{(R, [y_1,y_2])\}$ --\textsc{Insert}
\item Falls der aktuelle Ereignispunkt die Form (re, x, ($y_1,y_2$), R) hat, so setze $Y\coloneqq Y\backslash\{(R, [y_1,y_2])\}$
--\textsc{Delete}
\item Falls der aktuelle Ereignispunkt die Form (Punkt, $x,y$, R) hat, so finde alle Intervalle in $Y$, die $y$  enthalten und gib die
entsprechenden Schnittpaare aus (d.\,h. wenn für $(R', [y_1,y_2])$ in
$Y$ gilt: $y \in [y_1,y_2]$. Prüfe, ob $R'\neq R$ ist, wenn ja, gib $(R', R)$ aus.) --\textsc{Search}
\end{enumerate}
\subsection{Der Segment-Baum}
Der Segment-Baum dient also der Verwaltung von Intervallen, wobei die üblichen Operationen \textsc{Insert}, \textsc{Delete} und
\textsc{Search} mögloch sind.

Mit der Eingabe ist die Menge der $y$-Werte gegeben, welche nach steigendem $y$ geordnet wird. Damit ergibt sich die Folge $y_0,y_1,\
\ldots,y_n$ bzw. die Folge von Elementarintervallen $[y_0,y_1],[y_1,y_2],\ldots,[y_{n-1},y_n]$. Durch das folgende
Beispiel wird dies vielleicht klarer.

Seien die $y$-Werte 0, 1, 3, 6, 9 als sogenannte Roatorpunkte \todo{Stimmt das?}, die zu den Elementarintervallen
[0, 1], [1, 3], [3, 6] und [6, 9] führen und die Rechtecke wie in \autoref{031203f} gegeben. Daraus resultiert dann der Baum wie in \autoref{031203g}.

\begin{figure}[H]
\centering
\input{031203f.latex}
\caption{Beispiel für überschneidung von Rechtecken}
\label{031203f}

\end{figure}

\begin{figure}[H]
\centering
\input{031203g.latex} \input{031203h.latex}
\caption{Beispiel für einen Segment-Baum}
\label{031203g}

\end{figure}

 a) $Y=Y \cup \{(R_1, [1, 6])\}=\{(R_1, [1, 6])\} \hspace{2em} b) Y=\{(R_1, [1, 6]), (R_2, [0, 9]) \}$

\begin{figure}[H]
\centering
\input{031203i.latex} \hspace{1cm} \input{031203j.latex} \hspace{1cm} \input{031203k.latex}
\caption{Beispiel für einen Segment-Baum im Aufbau}
\label{031203i}

\end{figure}

Aus \autoref{031203f} geht der Baum hervor, der rechts in der \autoref{031203g} zu sehen ist. Der dritte Wert jedes Tupels
in jedem Knoten
gibt dabei das Maximum der Tupel im linken Teilbaum dieses Knotens an. Etwas verständlicher, aber formal falsch, ist folgende
Formulierung: Der dritte Wert jedes Knotens gibt den maximalen Wert seines linken Teilbaumes an.
Der linke Teil von 
\autoref{031203g} deutet an, wie die Struktur im allgemeinen Fall bzw. als "`Leerstruktur"' aussieht. In der untersten Abbildung zeigen a) und
b) wie die Struktur aufgebaut wird. Teil c) verdeutlicht nochmal die überlappung der Intervallgrenzen (im obigen Beispiel!) und macht
gleichzeitig folgenden Satz klar.

\begin{satz}
Für jedes Rechteck gibt es pro Level maximal zwei Einträge
\end{satz} 

Doch wie funktioniert die Suche und wieviel Rechenzeit wird größenordnunsmäßig benötigt, um alle überlappungen herauszufinden und
auszugeben?

Falls nach (Punkt, x, y, R) gesucht wird, wird im im Baum getestet, in welchen Intervallen (x,y) liegt. 

\todo{Stimmt das so? Im
Skript stand folgendes: (Punkt, x, y, R) $\rightarrow$ Suche nach y=(4,5), merke die Beschriftungen auf dem Suchpfad von Rechtecken,
OUTPUT (R$_4$, R$_1$) (R$_4$, R$_2$) (R$_4$, R$_3$).
Wenn aber nach überschendiungen im Intervall y=[4, 5] gesucht wird fehlen doch noch drei überschneidungen, ich werde daraus nicht
schlau. }

Für die Suche werden die Beschriftungen auf dem Suchpfad von Rechtecken gemerkt. Für den Baum gilt der Satz über höhenbalancierte
Suchbäume. Damit funktioniert das Suchen in $\OO(\log m)$ Zeit, und die Ausgabe hat die Größe $\OO(k+ \log m)$, dabei ist $k$ die Anzahl
der Schnitte. Für das Insert wird eine passende rekursive Prozedur geschrieben, die den Suchpfad abtestet und dei Eintragungen
vornimmt; dies geht in ebenfalls in $\OO(\log m)$ Zeit.
Damit haben wir einen output-sensitiven Algorithmus mit einer Gesamtlaufzeit von $\OO(k +m \log m)$, dazu siehe auch den Anfang des
Skriptes und \autoref{planesweep}.

\chapter{Verwaltung von Mengen -- kompliziertere Datenstrukturen}
Die Operationen \textsc{Make-Heap}(), \textsc{Insert}(H, x), \textsc{Min}(H) und \textsc{Extract-Min}(H) sollten an dieser Stelle
hinreichend bekannt sein, vielleicht ist auch \textsc{Union}(H$_1$, H$_2$) schon bekannt. Diese für die Verwaltung von Mengen wichtige
Operation vereinigt, wie der Name bereits sagt, zwei Heaps H$_1$ und H$_2$ zu einem Heap H. H enthält die Knoten von H$_1$ und H$_2$,
die bei dieser Operation zerstört werden (Stichwort "`Mergeable Heaps"'). Zusätzlich wird noch \textsc{Decrease-Key}(H, x, k) benutzt.

Bei Binär-Heaps funktionieren \textsc{Make-Heap} (\textsc{Build-Heap}) und \textsc{Min} in $\OO(1)$ und \textsc{Insert} und
\textsc{Extract-Min} in $\OO(\log n)$. Allerdings benötigt \textsc{Union} $\OO(n)$. Damit unterstützen Binär-Heaps kein \textsc{Union}!
Diese wichtige Operation wird aber von \textbf{Binomial-Heaps} unterstützt.

\section{Binomialbäume}
\begin{definition}[Binomialbäume]
\begin{enumerate}
\item $B_0\coloneqq$  \todo{?kleiner Kreis} ist Binomialbaum.
\item $B_k\rightarrow B_{k+1}$ mit $k\geq0$.
  \begin{figure}[H]
  \centering
  \input{info3_own001.latex} 
  %\caption{Beispiel für einen Segment-Baum im Aufbau}
  %\label{031203i}
  
  \end{figure}
\end{enumerate}
\end{definition}
\subsubsection{Beispiel}
\begin{figure}[H]
  \centering
  \input{info3_own002.latex} 
  %\caption{Beispiel für einen Segment-Baum im Aufbau}
  %\label{031203i}
  
  \end{figure}
  Im Beispiel sieht man von links nach rechts $B_0$, $B_1$, $B_2$ und den $B_3$.

Bei Bäumen gilt es verschiedene Typen zu unterscheiden, es gibt Wurzelbäume, geordnete Bäume und Positionsbäume.
Sei nun mit $(a,nil,b)$ der Baum bezeichnet, in dem in den Söhnen der Wurzel die Werte $(a,nil,b)$ in dieser Reihenfolge von rechts nach
links gespeichert sind.

Dann gilt für den Wurzelbaum, daß $(a,nil,b)=(nil,a,b)$ ist. In einem
geordneten Baum hingegen ist $(a,b,c)\neq(b,a,c)$ weil die
Reihenfolge der Söhne relevant ist. In einem Positionsbaum wieder ist
$(a,nil,b)\neq(nil,a,b)$ weil danach geguckt wird, welche
Platzstellen belegt sind und welche nicht.\todo{nil im Mathesatz. Korrigieren}

\begin{definition}[Binomialbäume]
  $B_0$ ist der Baum mit genau einem Knoten. Für $k\geq0$ erhält man
  $B_{k+1}$ aus $B_k$ dadurch, daß man zu einem $B_k$ einen weiteren
  Sohn an seine Wurzel als zusätzlichen linkesten Sohn hängt, dieser
  ist ebenfalls Wurzel eines $B_k$.
\end{definition}

\begin{definition}
\begin{itemize}
\item Ein freier Baum ist eine ungerichteter zusammenhängender kreisfreier Graph.
\item Ein Wurzelbaum (B,x$_0$) ist ein freier Baum B mit dem Knoten x$_0$ als "`ROOT"'.
\item Ein Wald ist ein ungerichteter kreisfreier Graph. 
\end{itemize}
\end{definition}

\section{Binomial-Heaps}  
\begin{satz}
Im Zusammenhang mit Binomial-Heaps wird auch der folgende Satz noch benutzt werden:
\[\binom{n}{i}=\binom{n-1}{i}+\binom{n-1}{i-1}\]
\end{satz}

\begin{satz}[über die eindeutige g-adische Darstellung natürlicher Zahlen]
(für g $\in \N : g\geq 2$)

$n \in \N \rightarrow$ Darstellung eindeutig, $n=\sum_{i=0}^m a_i g^i, a_i \in \N$
\end{satz}
Mit g=2 erhalten wir die Binärdarstellung natürlicher Zahlen.

\begin{definition}[Binomialheap]  
Ein Binomialheap ist ein Wald von Binomialbäumen, der zusätzlich folgende Bedingungen erfüllt:
\begin{enumerate}
\item Heap-Eigenschaft = alle Bäume sind heap-geordnet, d.h. der Schlüsselwert jedes Knotens ist größer oder gleich dem
Schlüsselwert des Vaters
\item Einzigkeitseigenschaft = Von jedem Binomialbaum ist maximal ein Exemplar da, d.h. zu einem gegebenen Wurzelgrad
gibt es maximal einen Binomialbaum im Heap.
\end{enumerate}
\end{definition}

\begin{satz}[Struktursatz über Binomialbäume]
\begin{enumerate}
\item Im Baum gibt es 2$^n$ Knoten
\item Die Höhe des Baumes B$_n$ ist n
\item Es gibt genau $\binom{n}{i}$ Knoten der Tiefe $i$ im Baum B$_n$
\item In B$_n$ hat die Wurzel den maximalen Grad (degree) und die Söhne sind von rechts nach links nach wachsendem Grad
geordnet.
\end{enumerate}
\end{satz}

\begin{definition}
$d(n,i)\coloneqq $ Anzahl der Knoten der Tiefe i im B$_n$
\end{definition}

\begin{beweis}
\begin{enumerate}
\item I.A. $n=0$ trivial, $n-1 \Rightarrow n$, $2^{n-1}+2^{n-1}=2^n$ 
\item $n=0$ trivial, $n-1 \Rightarrow n$ trivial
\item 

I.A. trivial n=0

I.S. $n-1 \Rightarrow n$ Setzen die Gültigkeit von 3) für $n-1$ alle $i$ voraus und zeigen sie dann für $n$ und alle
$i$

$d(n,i)=d(n-1,i)+d(n-1,i-1)=_{I.V.} \binom{n-1}{i}+ \binom{n-1}{i-1}=_{HS} \binom{n}{i}$
\end{enumerate}
\end{beweis}

Es gibt zu $n \in \N$ \underline{genau} (von der Struktur her) einen Binomialheap, der genau die $n$ Knoten
speichert.
 
\begin{beweis}[Eindeutigkeit der Binomialdarstellung von $n$]
$n=12$ 
\end{beweis} 

\begin{figure}[H]
  \centering
  \input{info3_own003.latex} 
  %\caption{Beispiel für einen Segment-Baum im Aufbau}
  %\label{031203i}
  
  \end{figure}
  
Für $n$ Werte haben wir $\OO(\log n)$ (genau $\floor{ \log n } +1$) Binomialbäume. Dies ist auch intuitiv
verständlich, da jede Zahl $n \in \Z$ zur Darstellung $\log n$ Stellen braucht. Die Eindeutigkeit der Struktur
des Heaps wird auch hier wieder klar, da jede Zahl ($\in \Z$) eindeutig als Binärzahl dargestellt werden kann.
Was passiert nun aber, wenn zwei Heaps zusammengeführt werden?

\section{Union}
Bei der Zusammenführung (\textsc{Union}) zweier Heaps H$_1$ und H$_2$ wird ein neuer Heap H geschaffen, der alle Knoten
enthält, H$_1$ und H$_2$ werden dabei zerstört.

%\begin{figure}[H]
%  \centering
%  \input{info3_own004.latex} 
%  %\caption{Beispiel für einen Segment-Baum im Aufbau}
%  %\label{031203i}
%  
%  \end{figure}

Leider ist wegen eines technischen Problems die folgende Darstellung nicht ganz korrekt, der Kopf jedes Heaps zeigt nur
auf das erste Element der Wurzelliste.

\begin{bundle}{Kopf H$_1$}
\chunk{7 $\rightarrow$ } \chunk{\begin{bundle}{2} \chunk{\begin{bundle}{25} \chunk{30} \end{bundle}} \chunk{3} \end{bundle}} 
\end{bundle} \hspace{15mm}
\begin{bundle}{Kopf H$_2$}
\chunk{16 $\rightarrow$ } \chunk{\begin{bundle}{18 $\rightarrow$}\chunk{45} \end{bundle}}\chunk{\begin{bundle}{4}
\chunk{\begin{bundle}{10} \chunk{11} \end{bundle}} \chunk{21} \end{bundle}} 
\end{bundle}\hspace{15mm}
\begin{bundle}{Kopf H}
\chunk{\begin{bundle}{7 $\rightarrow$ } \chunk{\begin{bundle}{18} \chunk{45} \end{bundle}} \chunk{16} \end{bundle}}
\chunk{\begin{bundle}{2}\chunk{\begin{bundle}{4} \chunk{\begin{bundle}{10} \chunk{11} \end{bundle}} \chunk{21} 
\end{bundle}} \chunk{\begin{bundle}{25} \chunk{30} \end{bundle}} \chunk{3}\end{bundle}}
\end{bundle}

Beim \textsc{Union} werden zuerst die Wurzellisten gemischt. Dann werden, angefangen bei den kleinsten,
Binomialbäume mit gleicher Knotenanzahl zusammengefaßt. Es werden also zwei B$_i$ zu einem B$_{i+1}$
transformiert.

Jeder Knoten im Baum hat dabei folgende Felder:
\begin{itemize}
\item Einen Verweis auf den Vater
\item Den Schlüsselwert
\item Den Grad des Knotens
\item Einen Verweis auf seinen linkesten Sohn
\item Einen Verweis auf seinen rechten Bruder
\end{itemize}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Union\textnormal{(H$_1$, H$_2$)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Treepostorder,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Union\textnormal{(H$_1$, H$_2$)}}, gobble=1]{Union(H1,H2)}
 H:= Make-Heap()
 Head[H]:=Merge(H$_1$, H$_2$)
 Zerst~ö~re die Objekte H$_1$, H$_2$ (nicht die entspr. Listen)
 if Head[H]=Nil then
   return H
 prev-x:=Nil
 x:=Head[H]
 next-x:=reB[x]
 while next-x $\neq$ Nil do
   if (degree[x] $\neq$ degree[next-x] OR (reB[next-x] $\neq$ Nil AND 
   degree[reB[next-x]]=degree[x])) then
     prev-x:=x
     x:=next-x
   else if (key[x]$\leq$key[next-x]) then
     reB[x]:=reB[next-x]
     Link[next-x, x]
     else if (prev-x=nil) then
       Head[H]:=next-x
       else reB[prev-x]:=next-x
       Link[x, next-x]
       x:=next-x
   next-x:=reB[x]
 return H    
\end{lstlisting}
\end{Algorithmus}

Priority-Queues=\{\textsc{Make-Heap}, \textsc{Insert}, \textsc{Union}, \textsc{Extract-Max}, \textsc{Decrease-Key},
\textsc{Min}\};
\textsc{Make-Heap} erfordert nur $\OO(1)$, der Rest geht in $\OO(\log n)$.
 

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Insert\textnormal{(H, x)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Insert\textnormal{(H, x)}}, gobble=1]{Insert(H,x)}
 H$_1$:= Make-Heap()
 p[x]:=Nil
 Sohn[x]:=Nil
 reB[x]:=Nil
 degree[x]:=0
 Head[H$_1$]:=x
 Union(H, H$_1$) 
\end{lstlisting}
\end{Algorithmus}

Wie funktioniert nun das Entfernen des Minimums? Zuerst wird das Minimum rausgeworfen, dann werden die Söhne des
betroffenen Knotens in umgekehrter Ordnung in eine geordnete Liste H$_1$ gebracht und anschließend werden mittels
\textsc{Unon} H und H$_1$ zusammengemischt. Dies klappt logischerweise in $\OO(\log n)$, da die Wurzellisten durch $\OO(\log
n)$ in ihrer Länge beschränkt sind (bei n Werten im Heap). Damit arbeitet auch \textsc{Extract-Min} in $\OO(\log n)$.

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Decrease-Key\textnormal{(H, x, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Decrease-Key\textnormal{(H, x, k)}},gobble=1]{DecreaseKey(H,x,k)}
 if k$>$key[x] then
   Fehler ($\rightarrow$ Abbruch)
 key[x]:=k
 y:=x
 z:=p[y]
 while (z$\neq$ Nil AND key[y]$<$key[z]) do
 vertausche key[y] und key[z]
 y:=z
 z:=p[y]    
\end{lstlisting}
\end{Algorithmus}

\begin{satz}
Die linker-Sohn-rechter-Bruder-Darstellung für Binomial-Heaps ermöglicht in der angegebenen Form für
Prioritätswarteschlangen logarithmische Laufzeit.
\end{satz}
Merke: Die Länge der Wurzelliste bei einem Binomial-Heap ist logarithmisch.

Bisher war immer leicht verständlich, wie die Schranke für die Komplexität eines Algorithmus zustande kam, doch bei
komplizierteren Datenstrukturen ist das nicht mehr immer so einfach. Manche mehrfach ausgeführte Schritte erfordern am
Anfang viel Operationen, am Ende aber wenig. Dies erschwert die Angabe einer Komplexität und deswegen werden im Folgenden drei Methoden zur
Kostenabschätzung vorgestellt.

\section{Amortisierte Kosten}
Es gibt drei Methoden um amortisierte Kosten abzuschätzen, eine davon wurde bereits im Abschnitt zur konvexen Hülle
benutzt. 
\begin{itemize}
\item die Aggregats-Methode
\item die Guthaben-Methode
\item die Potential-Methode
\end{itemize} 
Ziel aller drei Methoden ist es, auf die Kosten einer Operation im gesamten Algorithmus zu kommen. Dies bietet sich z.\,B.
an, falls eine Operation im worst-case in einem Schritt des Algorithmus eine Komplexität von $\OO(n)$ haben kann,
aber auch über den gesamten Algorithmus hinweg bei $n$ Schritten nicht mehr als $\OO(n)$ Aufwand erfordert. Dann kommt
mittels der Analyse zu den amortisierten Kosten $\OO(1)$, zu einer Art Durchschnittskosten im Stile von $\frac{T(n)}{n}$.

Bei den letzten beiden Methoden erhält man Kosten AK($i$)=AK$_i$ für die Operationen.
Die aktuellen (wirklichen) Kosten werden
mit K($i$)=K$_i$ bezeichnet und werden können relativ frei gewählt werden. In den meisten Fällen ist $K_i=\OO(1)$,
dabei muß aber gelten:
\[\sum_{i=1}^n AK_i \geq \sum_{i=1}^n K_i\]

Damit kann gefolgert werden, daß jede obere Schranke für die amortisierten Kosten AK auch eine obere Schranke
für die interessierenden tatsächlichen Kosten ist.

Im Fall des Graham-Scans ergeben sich amortsierte Kosten von $\OO(1)$ für das Multi-Pop, obwohl es schon in einem Schritt
$\OO(n)$ dauern kann. Mittels dieser amortisierten Kosten erhält man $\sum AK_i=\OO(n) \Rightarrow \sum K_i=\OO(n)$, also
letztlich, daß der Algorithmus in $\OO(n)$ funktioniert.

\subsection{Die Potentialmethode}
Die Idee ist, daß jede Datenstruktur DS mit einem Potential $\Phi$ bezeichnet wird. Es ist DS(0)=DS$_0$ und nach der i-ten
Operation DS(i)=DS$_i$. Das Potential wird definiert als $\Phi(DS(i))=:{\Phi}_i=\Phi(i)$, wobei es natürlich
eine Folge von mindestens $i$ Operationen geben muß.

Die Kosten ergeben sich als Differenz der Potentiale der Datenstruktur von zwei aufeinander folgenden Zeitpunkten. Die
Kosten der i-ten Operation ergeben sich als ${\Phi}_i-{\Phi}_{i-1}$.

Hierbei muß \[\sum_{i=1}^n AK_i=\sum_{i=1}^n (K_i+{\Phi}_i-{\Phi}_{i-1})\] gelten.

Dies (Teleskopsumme, siehe \autoref{Teleskopsumme}) läßt sich zu \[\sum_{i=1}^n AK_i=\sum_{i=1}^n
K_i+{\Phi}_n-{\Phi}_0\] umformen.

Als drittes muß (${\Phi}_n-{\Phi}_0 \geq 0$) sein, speziell ${\Phi}_n \geq 0$, falls ${\Phi}_0=0$. Damit folgt
$\sum AK_i \geq \sum K_i$.

\subsubsection{Beispiel Graham-Scan}

\begin{definition}
$\Phi \mbox{(DS(}i\mbox{))}$=Anzahl der Elemente im Stapel, ${\Phi}_i \geq 0, {\Phi}_0=0$
\end{definition}

Die amortisierten Kosten für  \textsc{Push} sind AK$_i$=K$_i$+${\Phi}_i-{\Phi}_{i-1}=1+1=2$
und für  \textsc{Pop} AK$_i$=K$_i$+${\Phi}_i-{\Phi}_{i-1}=1+(-1)=0$.

Woher ergibt sich das? Nun \textsc{Push} heißt ein Element einfügen $\rightarrow {\Phi}_i$ hat n Elemente,
${\Phi}_{i-1}$
 hat n-1 Elemente $ \rightarrow {\Phi}_i-{\Phi}_{i-1}$=1. Bei \textsc{Pop} hat umgekehrt ${\Phi}_{i-1}$ n Elemente und
 ${\Phi}_i $ n-1
Elemente, damit ist dort ${\Phi}_i-{\Phi}_{i-1}=n-1-n=-1$.  

\section{Fibonacci-Heaps}
Bei den Binomial-Heaps können wegen der Ordnung alle Söhne auf bequeme Weise angesprochen werden, bei den
Fibonacci-Heaps fehlt diese Ordnung. Dafür hat jeder Knoten Zeiger auf den linken und den rechten Bruder, die Söhne
eines Knotens sind also durch eine doppelt verkette Liste verknüpft. Mit $\min[H]$ kann auf das Minimum der Liste
zugegriffen werden, $n[h]$ bezeichnet die Anzahl der Knoten im Heap. Es gilt weiterhin, daß alle Bäume die
Eigenschaften eines Heaps erfüllen.

Wie lange dauern Operationen mit Fibonacci-Heaps? Das Schaffen der leeren Struktur geht wieder in $\OO(1)$, da zusätzlich
zu \textsc{Make-Heap} nur noch $n[H]=0$ und $\min[H]=$ Nil gesetzt werden muß.

\subsection{Einfache Operationen}

\begin{algorithm}
  \KwOut{$H$}
  \Begin{
  $H\leftarrow$ \FuncSty{Make-Heap}\;
  $\min[H]\leftarrow\min[H_{1}, H_{2}]$\;
  Verknüpfe die Wurzellisten von $H_{2}$ und $H$\;
  \If{$\min[H_{1}]=$ NIL OR $\min[H_{2}]\neq$ NIL AND
      $\min[H_{2}]<\min[H_{1}]$}
  {$\min[H]\leftarrow\min[H_{2}]$}
  $n[H]\leftarrow n[H_{1}]+n[H_{2}]$\;
  Zerstöre Objekte $H_{1}, H_{2}$}
\caption{Union ($H_{1}, H_{2}$)}
\end{algorithm}
\todo{Fehl?}
Formal korrekt muß es nicht $\min[H]$, sondern $key[\min[H]]$ heißen. Auf diese korrekte Schreibweise wurde
zugunsten einer klareren Darstellung verzichtet.
%\end{Algorithmus}
Das Aufschneiden der Wurzellisten und Umsetzen der Zeiger (Zeile 3) geht in $\OO(1)$. Im Allgemeinen wird der Ansatz
verfolgt, Operationen so spät wie möglich auszuführen. Bei \textsc{Union} wird nicht viel getan, erst bei
\textsc{Extract-Min}.

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Insert\textnormal{(H, x)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Insert\textnormal{(H, x)}}, gobble=1]{Insert2(H,x)}
 degree[x]:=0
 p[x]:=Nil
 Sohn[x]:=Nil
 li[x]:=x
 re[x]:=x
 mark[x]:=false ~\hspace{10mm}~ $\vartriangleleft$ ~\mbox{Bis hier neue Wurzelliste nur aus x}~
 Verkn~ü~pfe mit Wurzelliste H
 if (min[H]=Nil) OR (key[x]<key[min[H]]) then
   min[H]:=x
 n[H]:=n[H]+1   
\end{lstlisting}
\end{Algorithmus}

\subsection{Anwendung der Potentialmethode}
Jetzt wird die vorher erläuterte Potentialmethode benutzt, um die Kosten zu bestimmen, dabei ist
\begin{itemize}
\item $\Phi$(H)=$t$(H)+$2m$(H) mit
\item $t$[H] als der Anzahl der Knoten in der Wurzelliste und
\item $m$[H] als der Anzahl der markierten Knoten
\end{itemize}
Wenn nachgewiesen werden soll, daß die Potentialfunktion für Fibonacci-Heaps akzeptabel (zweckmäßig und zulässig)
ist, müssen die Bedingungen erfüllt sein. In \cite{ottmann} wird eine andere Potentialmethode benutzt.

Für die Kosten AK$_{i, \textsc{Insert}}$ des \textsc{Insert} gilt:
\begin{itemize}
\item $t$(H$'$)=$t$(H)+1
\item $m$(H$'$)=$m$(H)
\item ${\Phi}_i-{\Phi}_{i-1}=t$(H$'$)$+2m$(H$'$)-$t$(H)-$2m$(H)=1
\item $K_i=1$
\item $AK_i =K_i +\Phi_i-\Phi_{i-1}=1+1=2 \in \OO(1)$
\end{itemize}

\begin{satz}
\textsc{Insert} hat die amortisierten Kosten $\OO(1)$
\end{satz}

Die Kosten von \textsc{Union} sind:
\[\Phi(\mbox{H})-\Phi(\mbox{H}_1)-\Phi(\mbox{H}_2)=t(\mbox{H})-t(\mbox{H}_1)-t(\mbox{H}_2)+2m(\mbox{H})-2m(\mbox{H}_1)-
2m(\mbox{H}_2)=0+0=0\]

\begin{satz}
\textsc{Union} hat die amortisierten Kosten $\OO(1), AK_{i,
  \text{\textsc{Union}}} =K_i +0=1+0\in\OO(1)$.
\end{satz}

Mit diesen beiden sehr schnellen Operationen sind Fibonacci-Heaps sinnvoll, wenn die genannten Operationen sehr häufig
vorkommen, ansonsten ist der Aufwand für die komplizierte Implementierung zu groß.

Nochmal zurück zur amortisierten Kostenanalyse:
Hier wird fast nur die Potentialmethode benutzt, andere Methoden sind für uns nicht wichtig. Wesentlicher Bestandteil
der Potentialmethode ist die geschickte Definition der Potentialfunktion, dabei hat man viel Freiheit, nur die Bedingung $\Phi
(D_n) \geq \Phi(D_0)=_{meist}0$ muß erfüllt sein.

Die amortisierten Kosten ergeben sich als die Summe aus den aktuellen Kosten und dem Potential der Datenstruktur. Die
Bedingung ist dazu da, daß die Kostenabschätzung möglich, aber auch brauchbar ist.

\subsection{Aufwendige Operationen}
\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Extract-Min\textnormal{(H)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Extract-Min\textnormal{(H)}}, gobble=1]{Extract-Min(H)}
 z:=min[H]
 if (z$\neq$Nil)
   f~ü~r jeden Sohn x von z do
     f~ü~ge x in die Wurzelliste ein
     p[x]:=Nil
   entferne z aus der Wurzelliste
   if z=re[z] then
     min[H]:=Nil
   else
     min[H]:=re[z]
     Konsolidiere(H)
   n[H]:=n[H]-1
 return z   
\end{lstlisting}
Beim Konsolidieren wird eine Struktur ähnlich wie bei den Binär-Heaps geschaffen, dazu gleich mehr.
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Link\textnormal{(H, x, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Link\textnormal{(H, x, k)}}, gobble=1]{Link(H, x, k)}
 Entferne y aus der Wurzelliste
 Mache y zum Sohn von x, degree[x]:=degree[x]+1
 Mark[y]:=0
\end{lstlisting}
Der Grad von x wird erhöht, da er die Anzahl der Söhne von x zählt, es ist klar, daß dies immer in $\OO(1)$ geht. In
\cite{ottmann} ist hier ein kleiner Fehler.
\end{Algorithmus}

Beim Konsolidieren besteht die Lösung in Verwendung eines Rang-Arrays.
Es wird n=n[H] gesetzt und D(n) als der maximale Wurzelgrad definiert, dabei ist $D(n)\in\OO(\log n)$. Das Array bekommt 
die Größe $D(n)$. Dann wird die Wurzelliste durchgegangen und an der entsprechenden Position im Array gespeichert. Falls
an dieser Stelle schon ein Eintrg vorhanden ist, wird ein \textsc{Link} durchgeführt. Dabei wird das Größere an das
Kleinere (bzgl. des Wurzelwertes\todo{Was denn genau} angehängt. Da der Wurzelgrad wächst, muß der Eintrag
im Feld dann eins weiter. So wird die gesamte Wurzelliste durchgegangen und es kann am Ende keine zwei Knoten mit
gleichem Wurzelgrad geben. Dazu muß immer das Minimum aktualisiert werden.

\subsubsection{Analyse}
Wir haben die amortisierten Kosten, die Behauptung ist: $\OO(D(n))=\OO(\log n)$. Die aktuellen Kosten sind die Kosten für den
Durchlauf duch die Wurzelliste, alles andere ist hier vernachlässigbar. Also sind die aktuellen Kosten
$=t(\mbox{H})+\mbox{D}(n)=\OO(t(\mbox{H})+\mbox{D}(n))$ und die Potentialdifferenz ist $\leq
\mbox{D}(n)-t(\mbox{H})$

Man zeigt schließlich, daß die amortisierten Kosten in $\OO(D(n))$ liegen.
\todo{Der Abschnitt ist scheiße, verbessern!}

\subsection{Weitere Operationen}
\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Decrease-Key\textnormal{(H, x, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Cut, Korrigiere, Cascading, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Decrease-Key\textnormal{(H, x, k)}}, gobble=1]{Decrease-key2(H, x, k)}
 if k$>$key[x] then
   Fehler!
 key[x]:=k  ~\hspace{10mm}~ $\vartriangleleft$ ~\mbox{Weiter, falls F-Heap kaputt}~ 
 y:=p[x]
 if (y$\neq$Nil AND key[x]$<$key[y] then
   Cut(H, x, k)
   Cascading-Cut(H, y)
 if key[x]$<$key[min[H]] then
   min[H]:=x  
\end{lstlisting}
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Cut\textnormal{(H, x, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Cut, Korrigiere, Cascading, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Cut\textnormal{(H, x, k)}}, gobble=1]{Cut(H, x, k)}
 Entferne x aus der Sohnliste von y
 degree[y]:=degree[y]-1
 F~ü~ge x zur Wurzelliste von H hinzu
 p[x]:=Nil
 Mark[x]:=0 
\end{lstlisting}
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Cascading-Cut\textnormal{(H, y)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Cut, Korrigiere, Cascading, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Cascading-Cut\textnormal{(H, y)}}, gobble=1]{Cascading-Cut(H, y)}
 z:=p[y]
 if (z$\neq$Nil then)
   if Mark[y]=0 then
     Mark[y]:=1
   else
     Cut(H, y, z)
     Cascading-Cut(H, z)  
\end{lstlisting}
\end{Algorithmus}

Im Beispiel markiert $\star$, daß ein Knoten schon einen Sohn verloren hat.

\begin{bundle}{$\bullet$}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{$\bullet$}\chunk{$\bullet$}\chunk{$\bullet$}\end{bundle}}\end{bundle}} \chunk{\begin{bundle}{$\bullet$} \chunk{\begin{bundle}{$\bullet$} \chunk{$\bullet$} \end{bundle}}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{${\bullet}^{\star}$} \chunk{\begin{bundle}{30$^{\star}$}
\chunk{40}\end{bundle}} \chunk{$\bullet$}\end{bundle}}\end{bundle}} \end{bundle}} 
\end{bundle} \hspace{5mm}$\longrightarrow$\hspace{5mm}
\begin{bundle}{$\bullet$}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{$\bullet$}\chunk{$\bullet$}\chunk{$\bullet$}\end{bundle}}\end{bundle}} \chunk{\begin{bundle}{$\bullet$} \chunk{\begin{bundle}{$\bullet$} \chunk{$\bullet$} \end{bundle}}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{${\bullet}^{\star}$} \chunk{\begin{bundle}{30$^{\star}$}
\chunk{25}\end{bundle}} \chunk{$\bullet$}\end{bundle}}\end{bundle}} \end{bundle}} 
\end{bundle} \hspace{5mm}$\longrightarrow$\hspace{5mm}
\begin{bundle}{$\bullet$}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{$\bullet$}\chunk{$\bullet$}\chunk{$\bullet$}\end{bundle}}\end{bundle}} \chunk{\begin{bundle}{$\bullet$} \chunk{\begin{bundle}{$\bullet$} \chunk{$\bullet$} \end{bundle}}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{${\bullet}^{\star}$} \chunk{30$^{\star}$}
\chunk{$\bullet$}\end{bundle}}\end{bundle}} \end{bundle}} 
\end{bundle} 25\hspace{5mm}$\longrightarrow$\hspace{5mm}
\begin{bundle}{$\bullet$}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{$\bullet$}\chunk{$\bullet$}\chunk{$\bullet$}\end{bundle}}\end{bundle}} \chunk{\begin{bundle}{$\bullet$} \chunk{\begin{bundle}{$\bullet$} \chunk{$\bullet$} \end{bundle}}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{${\bullet}^{\star}$}
\chunk{$\bullet$}\end{bundle}}\end{bundle}} \end{bundle}} 
\end{bundle} 25 30\hspace{5mm}$\longrightarrow$\hspace{5mm}

$\longrightarrow$
\begin{bundle}{$\bullet$}
\chunk{\begin{bundle}{$\bullet$}\chunk{\begin{bundle}{$\bullet$}\chunk{$\bullet$}\chunk{$\bullet$}\end{bundle}}\end{bundle}} \chunk{\begin{bundle}{$\bullet$} \chunk{\begin{bundle}{$\bullet$} \chunk{$\bullet$} \end{bundle}}
\chunk{$\bullet$}\end{bundle}} 
\end{bundle} 25 30
\begin{bundle}{$\bullet$} \chunk{$\bullet$}\end{bundle}

\subsubsection{Analyse}
Die aktuellen Kosten für das \textsc{Cut} liegen in $\OO(1)$, was gilt für das \textsc{Cascading-Cut}?

\[
\begin{array}{ccl}
\mbox{ein \textsc{Cut} dabei:} & - \mbox{ eine Markierung} & -2\\
 &  + \mbox{ eine neue Wurzel} & +1\\
 &  + \OO(1) \text{für das \textsc{Cut} selbst} & +1\\
 &  & =0
\end{array}\]

Damit hat \textsc{Decrease-Key} $\OO(1)$ amortisierte Kosten. Denn das einmalige Abschneiden kostet $\OO(1)$ und die Kaskade von
\textsc{Cut}s kostet wegen der Markierung nichts. \todo{Sollte das etwas O/Null heißen}

\begin{satz}[Lemma 1 über F-Heaps]
Sei $v$ Knoten eines F-Heaps. Ordnet man (in Gedanken) die Söhne von $v$ in der zeitlichen Reihenfolge, in der sie an
$v$ angehängt wurden, so gilt: der $i$-te Sohn von $v$ hat mindestens den Grad $(i-2)$. 
\end{satz}
Dies ist der Grund für die Leistungsfähigkeit und den Namen der F-Heaps

\begin{beweis}
Damit der $i$-te Sohn auftaucht, müssen $v$ und dieser Sohn den Rang $(i-1)$ gehabt haben (wenigstens Rang $i$, falls beide
denselben Rang hatten). Wenn das unklar ist, sollte man sich das Konsolidieren noch einmal anschauen. Danach kann dieser
Sohn wegen der Markierungsvorschrift maximal einen Sohn verlieren, also ist der Rang mindestens $(i-2)$. 
\end{beweis}

\begin{satz}[Lemma 2 über F-Heaps]
Jeder Knoten $v$ vom Rang (Grad) $k$ eines F-Heaps ist Wurzel eines Teilbaumes mit mindestens F$_{k+2}$ Knoten.
\end{satz}

\begin{definition}
$F_0\coloneqq 0, F_1\coloneqq 1, F_{k+2}\coloneqq F_{k+1} +F_{k}$
\end{definition}

\begin{satz}
F$_{k+2} \geq {\phi}^k$, $\phi=\frac{1+\sqrt{5}}{2}\approx 1,6$ 
\end{satz}

\begin{satz}[Hilfssatz]
$\forall k \geq 0 \mbox{ }gilt \mbox{ } F_{k+2}=1+\sum_{i=0}^k F_i$
\end{satz}

\begin{beweis}[Beweis vom Hilfssatz durch Induktion über $k$]
$k=0 : F_2=1+\sum_{i=0}^0 F_i = 1+ F_0 =1$

 I.V. $F_{k+1}=1+\sum_{i=0}^{k-1} F_i$

 $F_{k+2}=F_{k}+F_{k+1}=F_k + 1 + \sum_{i=0}^{k-1} F_i = 1+\sum_{i=0}^k F_i$
\end{beweis}

\begin{beweis}[Lemma 2]
$S_k\coloneqq$ Minimalzahl von Knoten, die Nachfolger eines Knotens $v$ vom Rang $k$ sind ($v$ selbst mitgezählt).

Habe $v$ den Rang 0 (keinen Sohn), dann ist $S_0=1$ und $S_1=2$. Ab
$k\geq2$ gilt Lemma~1\todo{Verweis einbauen}:
\begin{gather*}
  S_k \geq 2 + \sum_{i=0}^{k-2} S_i \text{ für }k \geq 2
\end{gather*}
Jetzt werden die Söhne von $v$ wieder gedanklich in der Reihenfolge des Anliegens geordnet, und zusammen mit

\[\mbox{F}_{k+2}=1+\sum_{i=0}^k F_i=2+\sum_{i=2}^k F_i\] gilt \[\mbox{S}_k \geq \mbox{F}_{k+2} \mbox{ für } k \geq 0
\mbox{ (Beweis durch Induktion)}\]

Sei $v^k$ der Knoten $v$ mit dem Rang $k$, dann gilt insgesamt: Anzahl Nachfolger$(v^k) \geq \mbox{S}_k \geq
\mbox{F}_{k+2} \geq {\phi}^k \Rightarrow \mbox{Lemma 2}$ 
\end{beweis}
Daraus folgt, daß der maximale Grad (Rang) eines Knotens in einem F-Heap mit $n$ Knoten $D(n)\in\OO(\log n)$ ist.

\begin{beweis}
Sei $v$ beliebig im F-Heap gewählt und $k=\rg(v)$, dann ist $n$ größer
oder gleich der Anzahl der Nachfolger $(v)$ $\geq {\phi}^k$, also
$k \leq \log_{\phi} n \in \OO(\log n)$. Da $v$ beliebig ist, gilt die Ungleichung auch für den maximalen Rang
$k$. 
\end{beweis}
F-Heaps sind Datenstrukturen, die vor allem zur Implementierung von Prioritätswarteschlangen geeignet sind.
\textsc{Extract-Min} braucht amortisiert logarithmische Zeit, alle anderen Operationen (\textsc{Make-Heap},
\textsc{Union}, \textsc{Insert} und \textsc{Decrease-Key}) brauchen amortisiert $\OO(1)$~Zeit. Das Löschen klappt damit
ebenfalls in logarithmischer Zeit, zuerst wird der betroffene Wert mit \textsc{Decrease-Key} auf $- \infty$ gesetzt
und dann mit \textsc{Extract-Min} entfernt.

Falls in einer Anwendung oft Werte eingefügt, verändert oder zusammengeführt werden, sind F-Heaps favorisiert. Falls
nur selten Werte zusammengeführt werden, bieten sich Binär-Heaps an. 

\chapter{Union-Find-Strukturen}
Der abstrakte Datentyp wird durch die Menge der drei Operationen \{\textsc{Union}, \textsc{Find}, \textsc{Make-Set}\}
gebildet. Union-Find-Strukturen dienen zur Verwaltung von Zerlegungen in disjunkte Mengen. Dabei bekommt jede Menge der
Zerlegung ein "`kanonisches Element"' zugeordnet, dieses dient als Name der Menge. Typisch für eine Union-Find-Struktur
ist eine Frage wie "`In welcher Menge liegt Element $x$?"'.

Die Bedeutung von \textsc{Union} und \textsc{Find} sollte klar sein, doch was macht \textsc{Make-Set}(x)?
Sei $x$ Element unseres Universums, dann erzeugt \textsc{Make-Set} die Einermenge \{$x$\} mit dem kanonischen Element 
$x$.
\section{Darstellung von Union-Find-Strukturen im Rechner}
Bei Listen ist die Zeit für das \textsc{Find} ungünstig, da die ganze Liste durchlaufen werden muß. \textsc{Union}
dauert ebenfalls lange, da dann durch die erste Liste bis zum Ende gegangen werden muß und dann die zweite Liste an die
erste gehängt wird. Hierbei wird auch schon offensichtlich, daß nach dem \textsc{Union} immer ein neues kanonisches
Element bestimmt werden muß. Für Union-Find-Strukturen sind "`Disjoint set forests"' das Mittel der Wahl.

\todo{Zeichnung}

Den Objekten wird eine Größe zugeordnet, dabei werden die Knoten mitgezählt, damit keine lineare Liste entsteht. Das
\textsc{Union} erfolgt zuerst nach der Größe und dann nach der Höhe, dabei wird der kürzere Wald an den längeren
drangehängt.

\section{Der minimale Spannbaum -- Algorithmus von Kruskal}
Ein Graph G=(V, E) sei wie üblich gegeben. Was ist dann der minimale Spannbaum ("`Minimum Spanning Tree"')?
Dazu müssen die Kanten Gewichte haben, so wie beim Algorithmus von Dijkstra. Dann ist der minimale Spannbaum wie folgt
definiert:
\begin{enumerate}
\item T=(V, E$^{\star}$) mit E$^{\star} \subseteq$ E
\item Alle Knoten, die in G zusammenhängend sind, sind auch in T zusammenhängend
\item Die Summe der Kantengewichte ist minimal
\end{enumerate}

\todo{Zeichnung}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{mst\textnormal{(G, w)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Cut, Extract-Min, Find, Make-Set, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{mst\textnormal{(G, w)}}, gobble=1]{mst(G, w)}
 E$^{\star} := \emptyset$
 K$:= \emptyset$
 Bilde Priorit~ä~tswarteschlange Q zu E bzgl. w
 f~ü~r jedes $x \in V$ do
   Make-Set(x) ~\hspace{15mm}~ $\vartriangleleft$ K besteht aus lauter Einermengen
 while K enth~ä~lt mehr als eine Menge do
   (v, w):=Min(Q)
   ~\textsc{Extract-Min}~
   if Find(v)$\neq$ Find(w) then
     Union(v, w) ~\hspace{15mm}~ $\vartriangleleft$ Hier reicht ganz simples ~\textsc{Union}~
     E$^{\star} := $E$^{\star} \cup \{(v, w)\}$
 return E$^{\star}$   
\end{lstlisting}
Der Algorithmus von Kruskal ist ein gieriger Algorithmus. In \cite{cormen} steht eine leicht andere Variante als im
Beispiel.
\end{Algorithmus}

\begin{tabular}{|l|l|}
aktuelle Kante (v, w) & K:\{a\}, \{b\}, \{c\}, \{d\}, \{e\}, \{f\}, \{g\}, \{h\}, \{i\}, \{j\}, \{k\}\\
\hline
1.  $\star$(i, k) & \{a\}, \{b\}, $\ldots$, \{h\}, \{i, k\}\\
2.  $\star$(h, i) & \{a\}, \{b\}, $\ldots$, \{h, i, k\}\\
3.  $\star$(a, b) & \{a, b\}, $\ldots$, \{h, i, k\}\\
\hline
4.  $\star$(d, e) & \{a, b\}, \{c\}, \{d, e\}, $\ldots$, \{h, i, k\}\\
5.  $\star$(e, h) & \{a, b\}, \{c\}, \{d, e, h, i, k\}, \{f\}, \{g\}\\
6.  $\star$(e, g) & \{a, b\}, \{c\}, \{d, e, g, h, i, k\}, \{f\}\\
\hline
7.  (g, i) & ---\\
8.  (g, h) & ---\\
9.  $\star$(f, h) & \{a, b\}, \{c\}, \{d, e, f, g, h, i, k\}\\
\hline
10.  (d, f) & ---\\
11.  $\star$(c, f) & \{a, b\}, \{c, d, e, f, g, h, i, k\}\\
12.  (c, d) & ---\\
\hline
13.  $\star$(a, c) & \{a, b, c, d, e, f, g, h, i, k\}\\
14.  (c, b) & ---\\
15.  (b, e) & ---\\
\end{tabular}

\todo{Zeichnung!}

Das sieht ja alles schon und toll aus, wie wird soetwas aber implementiert? Sehen wir uns dazu weiter das Beispiel an.

\begin{tabular}{l|llllllllll}
v $\in$ V & a & b & c & d & e & f & g & h & i & k\\
\hline
p[v] & a & a & c & i & d & f & i & i & i & i\\
\end{tabular}

Dies verleitet zu der einfachen Annahme, daß bei \textsc{Union}($e$, $f$) einfach $e$ Vater und kanoisches Element der
Menge wird, zu der $f$ gehört (\lstinline[mathescape=true]!p[$f$]\coloneqq $e$!). Da immer der kleinere Baum an den größeren
angehängt werden soll (bei uns Größe nach Knotenanzahl, möglich wäre auch die Höhe als Größe zu nehmen), ist
dies minimal aufwendiger.

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Union\textnormal{(e, f)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Union\textnormal{(e, f)}},gobble=1]{Union3(e,f)}
 if size[e]$<$size[f] then
   vertausche e und f
 p[f]:=e
 size[e]:=size[e]+size[f]  
\end{lstlisting}
\end{Algorithmus} 

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Make-Set\textnormal{(x)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Push, Pop, Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Make-Set\textnormal{(x)}},gobble=1]{Make-Set(x)}
 p[x]:=x
 size[x]:=1  
\end{lstlisting}
\end{Algorithmus} 

\textsc{Find} ist ebenfalls trivial, es wird einfach durch die Menge gegangen, bis das kanonische Element auftaucht, im
Beispiel liefert \textsc{Find}(e) $e$ $\leadsto$ $d$ $\leadsto$ $i$ $\leadsto$ $i$, return $i$.

\begin{satz}
Für Vereinigung nach Größe gilt: Ein Baum mit Höhe n hat mindestens 2$^n$ Knoten. 
\end{satz}
Daraus folgt, daß es hier keine dünnen Bäume gibt. Der Beweis des Satzes erfolgt mittels Induktion über die
Komplexität der Bäume, doch dazu muß erstmal folgendes definiert werden:
 
\begin{definition}[VNG-Baum]
T ist ein VNG-Baum, genau dann wenn
\begin{enumerate}
\item T nur aus der Wurzel besteht oder
\item T Vereinigung von T$_1$ und T$_2$ mit size[T$_2$]$\leq$size[T$_1$] ist.
\end{enumerate}
\end{definition}
VNG steht dabei für Vereinigung nach Größe.

\begin{beweis}
\begin{itemize}
\item[I.A.] h=0 (Wuzelbaum) 2$^h=1$
\item[I.V.] gelte für T$_1$, T$_2$ o.B.d.A. size[T$_2$]$\leq$size[T$_1$], size[T$_2$]$\geq 2^{h_2}$, size[T$_1$]$\geq
2^{h_1}$
\item[I.B.] size[T]$\geq 2^h$
\item[1. Fall] $h=$max$(h_1, h_2)$ $\Rightarrow$ size[T]=size[T$_1$]+size[T$_2$]$\geq 2^{h_1}+2^{h_2} \geq 2^h$
\item[2. Fall] $h=$max$(h_1, h_2)+1$ $\Rightarrow$ size[T]=size[T$_1$]+size[T$_2$]$\geq 2 \cdot $size[T$_2$]$\geq 2
\cdot 2^{h_2} =2^{h_2+1} \geq 2^h$
\end{itemize}
\end{beweis}

\todo{Ist der Beweis nicht falsch? Beispiel: 2. Fall: h$_1$=10, h$_2$=4, h=11}

Aus dem Satz ergibt sich, daß \textsc{Find} bei Vereinigung nach Größe $\OO(\log n)$ kostet.

\begin{satz}
Für Vereinigung nach Höhe gilt: Ein Baum der Höhe $h$ hat mindestens 2$^h$ Knoten.
\end{satz}
\todo{Was soll die Verarsche, wo ist der Unterschied zum vorherigen Satz? Die Existenz von dünnen Bäume?}
Also geht auch hier das \textsc{Find} in $\OO(\log n)$.

Das \textsc{Make-Set}(x) funktioniert immer in $\OO(1)$, das \textsc{Find} geht in $\OO(\log n)$ und \textsc{Union} klappt in
$\OO(1)$ falls nur ein Pointer umgesetzt werden muß. In einer Idealvorstellung, in der alle Elemente auf das kanonische
Element zeigen, geht \textsc{Find} in $\OO(1)$, das \textsc{Union} braucht dann aber $\OO(n)$. Solche Idealvorstellungen sind
natürlich meist sehr realitätsfern und nicht praktikabel. Doch lassen sich Union-Find-Strukturen noch mit anderen
Mitteln verbessern?

Ja, allerdings. Dazu bedient man sich der Pfad-Kompression und der inversen einer Funktion, die starke ähnlichkeit mit
der Ackermann-Funktion aufweist. Dies zusammen führt zu dem noch folgenden Satz von Tarjan. Dazu wird die
Union-Find-Struktur mit einer amortisierten Analyse auf ihre Kosten untersucht. Wir betrachten eine Folge von insgesamt 
$m$ \textsc{Union-Find}-Operationen mit $n$ \textsc{Make-Set}-Operationen dabei. Dazu brauchen wir zuerst die
Ackermann-Funktion und ihre Inverse.
\begin{definition}[Die Ackermann-Funktion und ihre Inverse]
 Die Ackermann-Funktion
 \begin{align*}
 A&(1,j)=2^j   & j\geq 1\\
 A&(i,1)=A(i-1,2)   & i\geq 2\\
 A&(i,j)=A(i-1, A(i,j-1))   & i,j \geq 2\\
 \end{align*}
 und ihre Inverse
 \begin{gather*}
   \alpha(m,n)\coloneqq \min\Set{i \geq 1 | A(i,\floor{\frac{m}{n}})> \log n}
 \end{gather*}
\end{definition}
Wegen obigem kann $n$ nicht größer als $m$ sein. Für den Grenzwert der Inversen gilt natürlich
\[\lim_{\substack{n \rightarrow \infty \\n \leq m }}=\infty\]
Da die Funktion aber extrem schnell wächst und damit ihre Inverse extrem langsam ist in allen konkreten Anwendungen 
$\alpha (m,n)$ aber kleiner als $5$. 

\begin{satz}[Satz von Tarjan]
Für $m$ \textsc{Union-Find}-Operationen mit n$\leq$m \textsc{Make-Set}-Operationen benötigt man $\Theta( m \cdot
\alpha(m,n))$ Schritte, falls die Operationen in der Datenstruktur "'Disjoint-Set-Forest"` mit \textsc{Union} nach
Größe (Höhe) und \textsc{Find} mit Pfadkompression realisiert werden.
\end{satz}
\todo{Es wird noch garnicht erläutert, was die Pfadkompression denn nun genau ist.}
Noch ein Nachsatz zur Pfadkompression: Wenn wir vom Knoten zur Wurzel laufen, dann laufen wir nochmal zurück und setzen
alle Zeiger auf den Vater, den wir nun kennen $\rightarrow \OO(2x)=\OO(x)$.

Ein weiteres Beispiel ist die Bestimmung der Zusammenhangskomponenten\todo{Beispiel wofür?}

\chapter{Hashing}
Jeder wird schon einmal vom Hashing gehört haben, daß es irgendwie mit Funktionen zu tun hat, ist wohl auch klar. Was
steckt aber genau dahinter?

Für jede Funktion braucht man einen Grundbereich, eine Menge, auf denen diese Funktion operiert. Genau genommen
braucht man eine "`¸Ursprungsmenge"' für die Argumente und eine "`Zielmenge"' für die Ergebnisse. Da aber häufig die
"`Ursprungsmenge"' (Urbildmenge) und die "`Zielmenge"'(Bildmenge) gleich sind, betrachtet man ohne weitere Erwähnung
eine Menge, die aber (implizit) beides ist.

Im Falle des Hashings ist unser Universum die Urbildmenge U. Dieses enthält alle denkbaren Schlüssel, darin gibt es
aber eine Teilmenge K, die nur alle auch wirklich betrachteten Schlüssel enthält.

Sehen wir uns alle Familiennamen mit maximal 20 Buchstaben an, in U sind alle Zeichenketten mit maximal 20 Buchstaben,
in K aber nur alle tatsächlich vorkommenden Namen.

\section{Perfektes Hashing}
Perfektes Hashing ist noch recht einfach, es wird die Tabelle T direkt adressiert und Werte $x$ direkt eingefügt und
gelöscht. Dafür brauchen wir eine Hash-Funktion $h$ und einen Schlüssel $k$ (für engl. "`key"').
\todo{das sollte noch verbessert werden}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Insert\textnormal{(T, x)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Insert\textnormal{(T, x)}},gobble=1]{Insert3(T, x)}
 T[key[x]]:=x
\end{lstlisting}
\end{Algorithmus} 

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Delete\textnormal{(T, x)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Delete\textnormal{(T, x)}},gobble=1]{Delete(T, x)}
 T[key[x]]:=Nil
\end{lstlisting}
\end{Algorithmus}

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Search\textnormal{(T, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Search\textnormal{(T, k)}},gobble=1]{Search(T, k)}
 return T[k]
\end{lstlisting}
\end{Algorithmus} 

Alle drei Operationen dauern $\OO(1)$~Schritte, aber
die Hash-Funktion $h$ bildet aus U auf $m-1$ Werte ab ($h: U \rightarrow \{0, \cdots, m-1\}$), bei vielen Werten steigt
damit die Wahrscheinlichkeit für Kollisionen (Abbildung auf den gleichen Funktionswert) sprunghaft an. 

Dies wird durch das folgende Beispiel vielleicht klarer: Bei 23 Personen
ist die Wahrscheinlichkeit, daß sie alle an verschiedenen Tagen Geburtstag haben etwas geringer als $0,5$, bei 40
Personen ist die Wahrscheinlichkeit dafür schon auf ca. 10\% gesunken.

\section{Drei Methoden zur Behandlung der Kollisionen}
\begin{enumerate}
\item Hashing mit Verkettung (Chaining): 
  \begin{itemize}
  \item Bei \textsc{Insert}(T,$x$) wird $x$ hinter dem Kopf der Liste T[h[key[x]]]
    eingefügt, jeder Eintrag der Hash-Tabelle kann also eine Liste beinhalten. 
  \item Beim \textsc{Search} wird in der Liste T[h[k]] mit key[$x$]=k nach Objekt $x$ gesucht
  \item \textsc{Delete} funktioniert analog zum \textsc{Search}
  \end{itemize}
\item Divisions-Rest-Methode $h(k)\coloneqq k \mod m$ (z.\,B. $m$ Primzahl, die nicht zu dicht an einer Zweierpotenz liegt)  
\item multiplikative Methode $h(k)\coloneqq \floor{ m(k \cdot \phi -\floor{ k \cdot \phi }) }$, dabei ist $m$ die
konstante Tabellengröße und $0 < \phi <1$. Donald E. Knuth schlägt z.\,B. $\phi=\frac{sqrt{5}-1}{2} \approx 0,618$ vor.
\end{enumerate} 
Außer den drei vorgestellten Methoden gibt es noch weitere, die uns hier aber nicht interessieren\todo{war
das so gemeint?}. Die letzten beiden gezeigten Methoden werfen zusätzlich die Frage auf, was eine
"`gute"' Hashfunktion ist.

\section{Analyse des Hashings mit Chaining}
Beim Hashing mit Chaining steht in der $j$-ten Zelle der Hash-Tabelle ein Zeiger auf die Liste mit $n_j$ Elementen. Um
die Anzahl der Kollisionen abzuschätzen, gibt es einen sogenannten Ladefaktor $\alpha$.
%Beim Hashing mit Chaining gibt es einen sogenannten Ladefaktor \[\alpha\coloneqq \frac{n}{m}\]
Jetzt wird einfaches uniformes Hashing angenommen. Dann ist die Wahrscheinlichkeit für ein gegebenes Element in eine bestimmte
Zelle zu kommen, für alle Zellen gleich. Daraus folgt auch die idealisierte Annahme, daß alle Listen gleich lang sind
(sonst dauert das \textsc{Search} schlimmstenfalls noch länger). Dann gilt \[\mathbb{E}[n_j]=\frac{n}{m}=\alpha\]

\begin{satz}[Theorem 1]
Wenn beim Hashing Kollisionen mit Verkettung gelöst werden, dann braucht eine erfolglose Suche $\Theta(1+\alpha)$
Zeit.
\end{satz}
\begin{beweis}
$k$ gesucht $\rightarrow h(k)$ berechnet $\rightarrow h(k)=j$ $j$-te Liste durchlaufen $\Rightarrow \Theta(1+\alpha)$
\end{beweis}

\begin{satz}[Theorem 2]
Für eine erfolgreiche Suche werden ebenfalls $\Theta(1+\alpha)$ Schritte gebraucht.
\end{satz}
\begin{beweis}
Wir stellen uns einfach vor, daß bei \textsc{Insert} Werte am Ende der Liste eingefügt werden. Dann brauchen wir
wieder Zeit zum Berechnen des Tabelleneintrages, daran anschließend muß wieder die Liste durchlaufen werden.
\end{beweis} \todo{Was soll die Gülle, das wird doch jetzt nochmal, bloß weniger wischi-waschi bewiesen?}
\begin{beweis}
Hier nehmen wir an, daß erfolgreich nach einem Element gesucht wird, also muß diese vorher irgendwann auch eingefügt
worden sein. Sei nun dieses Element an $i$-ter Stelle eingefügt worden (vorher $i-1$ Elemente in der Liste), die
erwartete Länge der Liste ist dann natürlich $\frac{i-1}{m}$. Jetzt seien $n$ Elemente in der Liste, dann gilt:
\[\frac{1}{n} \cdot \sum_{i=1}^n (1+\frac{i-1}{m})=1+\frac{1}{n-m} \sum_{i=1}^n (i-1)=1+\frac{1}{n-m} \cdot
\frac{(n-1)n}{2}=1+\frac{\alpha}{2}-\frac{1}{2m} \in \Theta(1+\alpha)\]
\end{beweis}
Bei festem $h$ können aber auch ungünstige Inputs problematisch werden. Dieses Problem führt zum Universal Hashing.

\section{Universal Hashing}
Festgelegte Hashfunktionen können zu ungünstigen Schlüsselmengen führen, bei zufällig gewählten Hashfunktionen
werden diese ungünstigen Schlüsselmengen durch die zufällige Wahl kompensiert. Für das Einfügen und Suchen eines Wertes
$x$ muß aber immer die gleiche Hashfunktion benutzt werden, dazu wird beim Einfügen die Hashfunktion zufällig
ausgewählt und dann abgespeichert.

\begin{definition}
Sei $H$ eine Menge von Hashfunktionen, die das Universum $U$ in $\{0, \ldots, m-1\}$ abbilden. Dann heißt $H$ genau dann
universal, wenn für jedes Paar von verschiedenen Schlüsseln $x, y \in
U$ gilt:
\begin{gather*}
  \abs{\Set{h \in H | h(x)=h(y)}} =\frac{\abs{H}}{m}
\end{gather*}
oder anders ausgedrückt:
\[\mbox{H }universal \leftrightarrow Wsk([h(x)=h(y)])=\frac{1}{m}
\mbox{ für }x,  y \in U \wedge x\neq y\]
\end{definition}

Sei $K \subseteq U$ eine Menge von Schlüsseln und $\vert K\vert =n$. Außerdem sei H eine universale Klasse von
Hashfunktionen. Dann gilt der folgende Satz:
\begin{satz}
Falls $h$ zufällig aus H ausgewählt wird, dann ist pro Schlüssel k $\in$ K die mittlere Anzahl der Kollisionen
(h(x)=h(y)) höchstens $\alpha \mbox{ } (=\frac{n}{m})$
\end{satz}
\todo{der Beweis erfolgte lt. Skript später oder in der übung, kam der noch?}
Gibt es soetwas überhaupt?

Sei $m$ eine Primzahl, dann ist die folgende Klasse universal:
Die Werte in unserem Universum seien als Bit-Strings der gleichen Länge gegeben, jeder Schlüssel $x$ also in der
Gestalt $x=<x_0, x_1, \cdots, x_r>$, wobei die Länge aller $x_i$ gleich ist.  Dabei ist $0\leq x_i\leq m$ Voraussetzung
bezüglich $m$ bzw. $r$.
\begin{definition}
\[H\coloneqq \Set{h=h_a | a\leq a_0, \ldots, a_r>, a_i \in \{0, \ldots, m-1\}, i=0, \ldots, r}\]
\[h_a(x)\coloneqq \sum_{i=0}^r (a_i \cdot x_i) \mod m\]
\end{definition}
Es ist ersichtlich, daß es dabei $m^r$ verschiedene Hashfunktionen geben kann.

\begin{satz}
Die so definierte Klasse ist universal.
\end{satz}
Für den Beweis brauchen wir noch einen Hilfssatz:
\begin{satz}[Hilfssatz]
Falls $x\neq y$ ist, erfüllen von den $m^{r+1}$ veschiedenen Hashfunktionen genau $m^r$ Stück die Bedingung
$h_a(x)=h_a(y)$ $\Rightarrow$ \[\mbox{Wsk}([h(x)=h(y)])=\frac{m^r}{m^{r+1}}=\frac{1}{m}\]
\end{satz}
\begin{beweis}[Beweis des Hilfssatzes und des Satzes]
  Sei $x\neq y$ (o.\,B.\,d.\,A. $x_0 \neq y_0$)
\[h_a(x)=h_a(y) \Leftrightarrow \sum_{i=0}^r (a_i \cdot x_i) \mod m=\sum_{i=0}^r (a_i \cdot y_i) \mod m \leftrightarrow
a_0(x_0-y_0)=\sum_{i=1}^r a_i (x_i- y_i)\]
Da m eine Primzahl ist, kann durch $(x_0-y_0)$ geteilt werden. Damit ist auch die Struktur $(\{0, \ldots, m-1\}, +_{\mod
m}, -_{\mod m})$ ein algebraischer Körper. Die Umformungen führen schließlich zu \[\star  \qquad 
a_0=\frac{1}{x_0-y_0} \cdot \sum_{i=1}^r (a_i (x_i- y_i))\]
Damit kann für jede Wahl von $<a_1, \ldots, a_r> \in \{0, \ldots, m-1\}$ mit $\star$ das zugehörige $a_0$ eindeutig
bestimmt werden. Bezüglich der ersten Komponente von $a$ besteht also keine freie Wahl, oder anders ausgedrückt: $a_0$
ist Funktion von $(a_1, \ldots, a_r)$, davon gibt es aber genau $m^r$ Stück. Es folgt aus $h_a(x)=h_a(y)$ insgesamt,
daß jedes Tupel $(a_1, \ldots, a_r)$ das $a_0$ eindeutig bestimmt. Und so haben von $m^{r+1}$ möglichen Tupeln nur
$m^r$ dei Eigenschaft $h_a(x)=h_a(y)$. Damit ist der Hilfssatz, aus dem der Satz folgt, bewiesen.
\end{beweis}

\section{Open Hashing}
Anfangs wurde die direkte Adressierung erwähnt, doch auch dies geht anders. Beim Open Hashing wird eine offene
Adressierung verwendet, dies ist sinnvoll, falls praktisch kein \textsc{Delete} vorkommt. Beim Einfügen wird probiert,
ob die ausgesuchte Stelle in der Tabelle frei ist. Falls sie frei ist, wird der Wert eingefügt und fertig, falls nicht
wird weitergegangen. So ist $\alpha=\frac{n}{m}<1$ möglich.

Die Proben-Sequenz (auch Probier- oder Sondierfolge) ist als Abbildung ein mathematisches Kreuzprodukt:
\[h: U \times \{0, \ldots, m-1\} \stackrel{\rightarrow}{auf}\{0, \ldots, m-1\}\]
Die Algorithmen für die Operationen sind dann natürlich etwas komplexer.

\begin{Algorithmus}[H]
Der Einfachheit halber betrachten wir Schlüsselwerte $k$ mit $k \in U$
\addcontentsline{alg}{Algorithmus}{\textsc{Insert\textnormal{(T, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Insert\textnormal{(T, k)}},gobble=1]{Insert4(T, k)}
 i:=0
 repeat j:=h(k,i)
   if T[j]=Nil then
     T[j]:=k
     return j
   else i:=i+1
 until i=m
 Fehler -- Tabelle voll    
\end{lstlisting}
\end{Algorithmus} 

\begin{Algorithmus}[H]
\addcontentsline{alg}{Algorithmus}{\textsc{Search\textnormal{(T, k)}}}
%\lstset{emph={Korrigiere, Linksrotation, Rechtsrotation, Make-Heap, Merge, Link}}
\lstset{numbers=left, numberstyle=\tiny, numbersep=8pt, stepnumber=1, basicstyle=\ttfamily, emph={Union,
        Make-Heap, Merge, Link}, emphstyle=\textsc, escapeinside=~~}
\begin{lstlisting}[frame=tlrb, mathescape=true, title=\textsc{Search\textnormal{(T, k)}},gobble=1]{Search4(T, k)}
 i:=0
 repeat j:=h(k,i)
   if T[j]=k then
     return j
   else i:=i+1
 until (T[j]=Nil OR i=m)
 return Nil    
\end{lstlisting}
\end{Algorithmus} 

Beim \textsc{Delete} taucht allerdings ein Problem auf. Nil würde gesetzt werden und dann würde \textsc{Search}
stoppen und nicht mehr weiter suchen. \todo{Was soll das jetzt, löst das die Porbleme mit dem
\textsc{Delete}?}

\begin{tabular}{l|l|l}
lineares Sondieren & quadratrisches Sondieren & doppeltes Hashing\\
\hline
$h(k,i)\coloneqq $ & $h(k,i)\coloneqq $ & $h(k,i)=$\\
$(h'(k)+c \cdot i) \mod m$ & $(h'(k)+c_1 \cdot i + c_2 \cdot i^2) \mod m$ & $(h_1(k)+i \cdot h_2(k)) \mod m$\\
Sei eine leere Zelle & & $h_1$ und $h_2$ sind wieder\\
gegeben und davor & & Hashfunktionen\\
$i$ volle & &\\
Dann wird eine Zelle & &\\
mit Wsk $\frac{i+1}{m}$ belegt & &\\
$\Rightarrow$ Primärcluster & $\Rightarrow$ Sekundärcluster& \\
(lange Suchzeiten) & & \\
\end{tabular}

\section{Nochmal zur Annahme des (einfachen) uniformen Hashings}
Beim einfachen uniformen Hashing gilt für jeden Schlüssel $k$, daß jeder Tabellenplatz für ihn gleichwahrscheinlich
ist (unabhängig von den Plätzen anderer Schlüsselwerte). Falls k eine zufällige reelle Zahl ist, die in $[0, 1]$
gleichverteilt ist, gilt dies für $h(k)=\floor{ k \cdot m}$
Beim uniformen Hashing gilt für jeden Schlüssel $k$, daß jede der $m!$ Permutationen von \{0, \ldots, m-1\} als
Sondierungsfolge gleichwahrscheinlich ist. Eigentlich gibt es kein uniformes Hashing, aber man kann sehr nahe an diese
Forderung (und ihre Folgen!) herankommen.
\begin{itemize}
\item[A] immer darauffolgende Zahl $\rightarrow m! sehr klein$
\item[B] $h'(k_1)=h(k_1, 0)$, $h'(k_2)=h(k_2, 0)$, gleiche Funktion $\rightarrow$ gleicher Platz, $m$ verschiedene
Sondierungsfolgen 
\item[C] gute Wahl von $h_1$ und $h_2 \rightarrow$ nah am uniformen Hashing
\end{itemize}
Damit ist also Variante "'C"` (Doppeltes Hashing) am dichtesten am uniformen Hashing dran, wie bestimmt man nun $h_1$
und $h_2$?
Dafür gibt es keine festen Regeln, allerdings sollte
\begin{itemize}
\item $h_2(k)$ relativ prim zu $m$ oder
\item $m$ eine Primzahl sein.
\end{itemize}
Ersteres wird oft eingehalten. Möglich ist eine Struktur wie 

$\begin{array}{l}
h_1(k)= k \mod m\\
h_2(k)= 1 + (k \mod m')  \mbox{ mit } m' \mbox{ dicht an } m\\
\end{array}$

\subsubsection{Beispiel}
Sei also 

$\begin{array}{l}
h_1(k)= k \mod 13\\
h_2(k)= 1 + (k \mod 11)\\
\end{array}$

\begin{tabular}{lllllll}
Folge & 79 & 69 & 98 & 72 & 50 & 14\\
$h_1$ & 1 & 4 & 7 & 7 & 11 & 1\\
$h(k,i)$ & 1 & 4 & 7 & 8 & 11 & 4\\
\end{tabular}

Bei geschickter Wahl von $h_1$ und $h_2$ erfüllt das doppelte Hashing die Annahme des uniformen Hashings ausreichend.
 
\begin{satz}[Satz 1 zum Open Hashing]
Gegeben sei eine Open-Hash-Tabelle mit $\alpha=\frac{n}{m}<1$. Dann ist der Erwartungswert für die Anzahl der Proben in
der Sondierungsfolge bei erfolgloser Suche unter Annahme des uniformen Hashings höchstens
$\OO(\frac{1}{1-\alpha})$
\end{satz}
Analoges gilt dann für \textsc{Insert}. Je voller die Tabelle ist, umso näher ist $\alpha$ an 1 und umso länger
dauert das \textsc{Search}.

\begin{satz}[Satz 2 zum das Open Hashing]
Unter der Voraussetzung von Satz 1 gilt bei erfolgreicher Suche, daß
diese weniger als $\OO(\frac{1}{\alpha} \cdot\ln\frac{1}{1-\alpha})$~Schritte braucht (die Annahme des uniformen Hashings sei erfüllt).
\end{satz}

\begin{beweis}[Nur die Beweisskizze]
Sei das $k$ nach dem gesucht wird, in der Tabelle und als $(i+1)$-ter Wert eingefügt worden. Mit der Folgerung vom
ersten Satz zum Open Hashing ist $\alpha=\frac{i}{m}$ beim Einfügen von $k$. Die erwartete Anzahl der Proben beim
\textsc{Insert} von $k$ ist danach \[\frac{1}{1-\frac{1}{m}}=\frac{m}{m-i}\]
Bei $n$~Schlüsseln ist der Mittelwert zu bilden: 
\[\frac{1}{n} \sum_{i=0}^{n-1} \frac{m}{m-i}=\frac{m}{n} \sum_{i=0}^{n-1}
\frac{1}{m-i}=\frac{m}{n}(H_m-H_{m-1})\] dabei steht $H_m$ für die harmonische Reihe und es ist
\[H_m=\sum_{mu}^{m}\frac{1}{\mu}\]

Weiter ist
\[\frac{1}{\alpha}(H_m-H_{m-1})=\frac{1}{\alpha} \sum_{k=m-n+1}^m \frac{1}{k} \leq \frac{1}{\alpha}\int_{m-n}
(\frac{1}{x})dx=\frac{1}{\alpha} \ln{\frac{m}{m-n}}=\frac{1}{\alpha}\ln{\frac{1}{1-\alpha}}\]
\end{beweis}

Sei z.\,B. die Tabelle halbvoll $\rightarrow \sim 1,387$ oder zu 90\% voll $\rightarrow \sim 2,56$. Diese Werte gelten
nicht unbedingt für unsere Beispiele wie doppeltes Hashing sondern für idealisertes uniformes Hashing. Falls aber
$h_1$ und $h_2$ geschickt gewählt werden (siehe unser Beispiel), liegen die tatsächlichen Werte nah bei den oben
angegebenen. 

Was bleibt nun als Folgerung? Falls kein \textsc{Delete} erforderlich ist, wird Open Hashing benutzt, sonst Chaining.
% 13.10.2003 Adrian Knoth <adi@thur.de>

\appendix    % ab hier ist Anhang

\chapter{Der Plane-Sweep-Algorithmus im Detail}
\label{planesweep}

\begin{figure}
  \centering \input{131003d.latex} 
  \caption{Liniensegmente mit Gleitgerade}
  \label{131003d}
\end{figure}

Der \textsc{Plane-Sweep}-Algorithmus ist eine äußerst bekannte
Methode zur Lösung von Mengenproblemen. Der primäre Gedanke
besteht darin, eine vertikale Gerade, die \textit{Sweepline}, von
links nach rechts durch die Ebene zu schieben und dabei den
Schnitt der Geraden mit der Objektmenge zu beobachten. Es
ist ebenfalls möglich, statt einer vertikalen Sweepline eine
horizontale zu verwenden und diese von oben nach unten über die
Objekte zu führen. Einige Algorithmen benötigen mehrere Sweeps,
durchaus in unterschiedliche Richtungen, um gewonnene Daten aus
vorangegangenen Überstreichungen zu verarbeiten.

Dazu "`merkt"' sich die Sweepline entsprechend ihrer aktuellen
Position die horizontal schneidenden Segmente anhand deren
$y$-Koordinaten. Bei Erreichen eines vertikalen Elements
werden alle in der Sweepline vorhandenen $y$-Koordinaten
ermittelt, die im $y$-Intervall des vertikalen Segments
liegen.

Die Gleitgerade wird nicht stetig über die Objektmenge
geführt, überabzählbar viele Teststellen wären die Folge. 
Lediglich $x$-Koordinaten, an denen ein Ereignis
eintreten kann, werden in der Simulation berücksichtigt.

Im vorliegenden Fall sind das Anfangs- und End-$x$-Koordinaten von
horizontalen Elementen sowie die $x$-Koordinate 
vertikal verlaufender Strecken. Diese Werte bilden die Menge
der \textit{event points}, sie kann häufig statisch implementiert werden.

Die Sweepline-Status-Struktur hingegen muß dynamisch sein, da sich
deren Umfang und Belegung an jedem event point ändern kann.

Zur Beschreibung des Verfahrens ist es erforderlich, die nachfolgende
Terminologie zu vereinbaren. Sie orientiert sich an der mathematischen
Schreibweise:

\begin{description}

\item[$h=(x_1,x_2,y)$:] horizontal verlaufendes Liniensegment mit dem
    Anfangspunkt $(x_1,y)$ und dem Endpunkt $(x_2,y)$
\item[$v=(x,y_1,y_2)$:] vertikal verlaufendes Liniensegment mit dem
    Anfangspunkt $(x,y_1)$ und dem Endpunkt $(x,y_2)$
\item[$t_i$ für $t=(x_1,\dots,x_n)$:] Zugriff auf die $i$-te Komponente des 
   Tupels $t$, also $t_i=x_i$
\item[$\pi_i(S)$:] Für eine Tupelmenge $S$ ist $\pi_i(S)$ die Projektion
   auf die $i$-te Komponente
\end{description} 

Der Algorithmus nach Güting~\cite{guting} gestaltet sich dann in dieser Weise:

\textbf{Algorithmus} \textsc{SegmentIntersectionPS (H,V)}\\
\{Eingabe ist eine Menge horizontaler Segmente H und eine Menge vertikaler
 Segmente V, berechne mit Plane-Sweep die Menge aller Paare $(h,v)$
mit $h{\in}H$ und $v{\in}V$ und $h$ schneidet $v$\}

\begin{enumerate}
\item
Sei
\begin{align*}
  S &= \Set{(x_1, (x_1,x_2,y)) | (x_1,x_2,y)\in H}\cup
  \Set{(x_2,(x_1,x_2,y)) | (x_1,x_2,y)\in H}\\
  &\qquad\cup \Set{(x,(x,y_1,y_2)) | (x,y_1,y_2)\in V}
\end{align*}
($S$ ist also eine gemischte Menge von horizontalen und vertikalen Segmenten,
in der jedes horizontale Segment einmal anhand des linken und einmal
anhand des rechten Endpunkts dargestellt ist. Diese Menge beschreibt die
Sweep-Event-Struktur.)\\
Sortiere $S$ nach der ersten Komponente, also nach $x$-Koordinaten.

\item
Sei $Y$ die Menge horizontaler Segmente, deren $y$-Koordinate als
Schlüssel verwendet wird (Sweepline-Status-Struktur);\\
$Y\coloneqq\emptyset$;\\
durchlaufe $S$: das gerade erreichte Objekt ist
\begin{enumerate}
\item
linker Endpunkt eines horizontalen Segments $h=(x_1,x_2,y)$:\\
$Y \coloneqq Y \cup \{(y,(x_1,x_2,y))\}$ (füge $h$ in $Y$ ein)
\item
rechter Endpunkt von $h=(x_1,x_2,y)$:\\
$Y \coloneqq Y \backslash \{(y,(x_1,x_2,y))\}$ (entferne $h$ aus $Y$)
\item
ein vertikales Segment $v=(x,y_1,y_2)$:\\
$A \coloneqq\pi_2 (\Set{w\in Y | w_0 \in [y_1,y_2]})$;
 \hspace{1cm}  (finde alle Segmente in $Y$, deren
   $y$-Koordinate im $y$-Intervall von $v$ liegt)\\
gibt alle Paare in $A\times\{v\}$ aus
\end{enumerate}
\end{enumerate}
\textbf{end} \textsc{SegmentIntersectionPS} 

\chapter{Beispiele}
%\todo{okay noch ziemlich popelig, ich lasse mir noch mehr einfallen bzw. strukturiere}
\section{Lösung Substitutionsansatz}
\label{lsg_substitutionsansatz}

Die Rekurrenz läßt sich natürlich auch anders als mit vollständiger Induktion lösen, aber da dies mit der entsprechenden Abschätzung ein
gute Rechenübung ist, wird diese Methode benutzt.

Als Verdacht wird S(k)$\leq(k \log k)$ genommen.


$S(k) = 2 S(\frac{k}{2})+k\leq 2 (\frac{k}{2} \log \frac{k}{2})+k=$
$k \log \frac{k}{2}+k=k (\log k-\log 2)+k=(k \log k)$

Die Rücksubstitution ist hier recht einfach und ergibt als Gesamtlaufzeit

$T(n)=T(2^k)=S(k)=\OO(k \log k)=\OO(\log n \log^{(2)} n)$.

\section{Lösung Mastertheorem}
\label{mastertheorem_Fall3}

$ T(n) = 3 T(\frac{n}{4}) + 2n\log n \Rightarrow a=3,b=4$ und $\log_ba<1 \rightarrow 
f(n) \in \Omega(n^{\log_43+\varepsilon})$ da $\forall n \in \N\colon  n>0\colon  n^{\log_43}<2n\log n$.

Es könnte sich also um den dritten Fall handeln, dazu muß aber noch ein $c < 1 \colon  \forall^{\infty}_{n}\colon 
a f(\frac{n}{b})  \leq c f(n)$ existieren. Gibt es also ein 
$c<1\colon  \forall^{\infty}_{n}\colon  3f(\frac{n}{4}) \leq cf(n)$? Ja, denn

\begin{align*}
3f(\frac{n}{4}) & \leq & c f(n)\\
3[2(\frac{n}{4}) \log (\frac{n}{4})] & \leq & c 2n \log n\\
(\frac{3}{4}) \log (\frac{n}{4}) & \leq & c \log n\\
(\frac{3}{4}) \log n- \log 4 & \leq & c \log n\\
(\frac{3}{4}) \log n- 2 & \leq & c \log n\\
(\frac{3}{4}) \frac{\log n}{\log n}-\frac{2}{\log n} & \leq & c\\
(\frac{3}{4}) -\frac{2}{\log n} & \leq & c
\end{align*}
Da $lim_{n \to \infty} \frac{2}{\log n}=0$ gilt die Ungleichung für jedes $c \in [\frac{3}{4},\ldots,1]$. %(offenes Intervall!) 
Also handelt es sich um den dritten Fall des Mastertheorems und 
$ \Rightarrow T(n) \in \Theta (f(n))=\Theta (n\log n)$.

\section{Aufwandsabschätzung \textsc{Quicksort}}
\label{quicksort}

\begin{align}
T(n) & = & (n+1)+ \frac{2}{n} \sum_{k=1}^nT(k-1) \rightarrow\\
T(n-1) & = & n+\frac{2}{n-1} \sum_{k=1}^{n-1}T(k-1)\\
(n-1)T(n-1) - (n-1)n & = & 2\sum_{k=1}^{n-1}T(k-1)
\end{align}

\begin{align*}
n T(n) & = & (n+1)+ 2 \sum_{k=1}^nT(k-1)\\
& = & n(n+1)+2 T(n-1) +2 \sum_{k=1}^{n-1}T(k-1) \hspace{2cm} \text{B.3 einsetzen}\\
& = & (n+1)n+2 T(n-1) + (n-1)T(n-1) -(n-1)n\\
& = & [(n+1)-(n-1)]n+(2+n-1) T(n-1)\\
& = & 2n+(n+1)T(n-1) \rightarrow 
\end{align*}

\begin{gather*}
\frac{T(n)}{n+1}= \frac{T(n-1)}{n}+ \frac{2}{n+1}	
\end{gather*}

\section{Fertiger RS-Baum nach Rotation}
\label{rsrotation}
\begin{figure}[H]
\centering
\input{261103e.latex}

\end{figure}

\section{Der Drehsinn} \label{drehsinn}
Bei vielen Problemen der algorithmischen Geometrie ist wie bei der konvexen Hülle einer Menge der Drehsinn von Punkten (bzw. Geraden) zueinander wichtig.
Dieser läßt sich mittels einer Determinante berechnen, wobei letzten Endes nur wichtig ist, ob die Determinante kleiner oder größer
0 ist. Zur
mathematischen Berechnung des Drehsinns. Dafür braucht man natürlich wie in
\autoref{drehsinnskizze} drei Punkte (bzw. zwei Geraden) zwischen denen überhaupt ein Drehsinn berechnet werden kann.

\begin{figure}[H]
\centering
\input{031203c.latex}
\caption{Beispiel zur Berechnung des Drehsinns}
\label{drehsinnskizze}
\end{figure}

Nun seien $(1,0), (0,1), (1,1)$ und $(0,0)$ die Koordinaten der Punkte $x,y,z$ und $y'$ 
und wie üblich mit $x_1, x_2, y_1, y_2, z_1, z_2$ und $y'_1, y'_2$
bezeichnet. Wie bereits angemerkt ist das Vorzeichen, nicht der genaue Wert der
Determinante, entscheidend. Diese angenommenen Werte vereinfachen nur die Rechnung:

\[ \abs{ 
\begin{array}{ccc}
1 & x_1 & x_2\\
1 & y_1 & y_2\\
1 & z_1 & z_2\\
\end{array} 
}=
\abs{ 
\begin{array}{ccc}
1 & 1 & 0\\
1 & 1 & 1\\
1 & 0 & 1\\
\end{array} 
}=1-1+1=1>0 \]

\[ \abs{ 
\begin{array}{ccc}
1 & x_1 & x_2\\
1 & y'_1 & y'_2\\
1 & z_1 & z_2\\
\end{array} 
}=
\abs{ 
\begin{array}{ccc}
1 & 1 & 0\\
1 & 0 & 0\\
1 & 0 & 1\\
\end{array} 
}=-1<0 \]

In dem einen Fall handelt es sich also um eine Linksdrehung, im anderen um eine Rechtsdrehung. Der Aufwand für die Berechnung des
Drehsinns liegt in $\OO(1)$, da immer nur eine Determinante konstanter Größe berechnet werden muß.

\section{Teleskopsummen} \label{Teleskopsumme}
Ab und zu tauchen Summen ähnlich zu der in der Potentialmethode auf. Folgende Trivialität hilft manchmal sehr
beim Vereinfachen:
\[\sum_{i=0}^n ({\Phi}_i-{\Phi}_{i-1})=
\begin{array}{ll}
& {\Phi}_1-{\Phi}_0\\
+ & {\Phi}_2-{\Phi}_1\\
& \vdots \\
+ & {\Phi}_{n-1}-{\Phi}_{n-2}\\
+ & {\Phi}_n-{\Phi}_{n-1}\\
\end{array}
={\Phi}_n-{\Phi}_0\] 
\begin{thebibliography}{99}
\label{literaturverzeichnis}
%\addcontentsline{toc}{Inhaltsverzeichnis}{Literaturverzeichnis}
\bibitem{cormen}
T.\,H.\,Cormen, C.\,E.\,Leierson, R.\,L.\,Rivest, C.\,Stein; \textit{Introduction to Algorithms}, MIT Press, 2$^{nd}$ edition, 2001
\bibitem{ottmann}
Ottmann, Widmeyer; \textit{Algorithmen und Datenstrukturen}, Spektrum Akademischer Verlag GmbH, ISBN 3-8274-1029-0
\bibitem{schoening}
Schöning; \textit{Algorithmik}, Spektrum Akademischer Verlag GmbH, ISBN 3-8274-1092-4
\bibitem{sedgewick}
Sedgewick; Algorithmen (mehrere unterschiedliche Fassungen verfügbar)
\bibitem{klein}
R.\,Klein; \textit{Algorithmische Geometrie}, Addison--Wesley, 1997
\bibitem{guting}
R.\,Güting; \textit{Algorithmen und Datenstrukturen}, Teubner, 1992
\end{thebibliography}
\end{document}
% LaTeX_output_format: pdf
