
\chapter {Leistungsbewertung}
\section{Kenngrößen der Zeit}

\paragraph{Mittlere Operationszeit:}
\begin{gather*}
  T = \sum_{i=1}^{n} t_i \cdot p_i
\end{gather*}
$t_i$ \ldots Operationszeit des $i$-ten Befehls\\
$p_i$ \ldots Relative Häufigkeit des $i$-ten Befehls im Befehlssatz

\paragraph{Mittlere Operationszeit: } {\textit{alternativer Ausdruck}}
\begin{gather*}
  T = CPI \cdot T_C + S \cdot T_S [s]
\end{gather*}
{\footnotesize $CPI$ \ldots cycles per instruction\\
$T_C$ \ldots Taktzykluszeit\\
$S$ \ldots Speicherbedarf bei Durchschnittsbefehl $[bit]$\\
$T_S$ \ldots Speicherzugriffszeit [s/bit]}

\paragraph{Verweilzeit, Wartezeit, Bedienzeit}
\textit{Warteschlangenmodell}
\begin{center}
\includegraphics[width=10cm]{warteschlangenmodell}
\end{center}
{\footnotesize 
\textbf{Bedienzeit }\ldots Dauer der Ausführung\\
\textbf{Wartezeit }\ldots Dauer des Aufenthalts in der Warteschlange\\
\textbf{Verweilzeit}\ldots Verzögerungszeit, Latenz, Antwortzeit}\\

\section {Kenngrößen zum Durchsatz}
\paragraph{MIPS, MOPS, MFLOPS:}
\begin{gather*}
  \text{MIPS/MOPS:}\quad
     \frac{1}{\text{mittlere Operationszeit}} = \frac{1}{T}
\end{gather*}
\noindent
Da weder die Leistungsfähigkeit der einzelnen Operationen, noch die
relativen Geschwindigkeiten der beteiligten CPUs einberechnet werden
eignet sich dies nicht zum Vergleich von verschiedenen CPUs.
MFLOPS sind ebenfalls ungünstig, da sie sich auf Programme beschränken,
die sehr Fliesskomma-zentriert sind. Auch die relative Komplexität
dieser Operationen wird nicht mit einbezogen.

\paragraph{Grenzdurchsatz, Verlustrate: } Für Warteschlangenmodell
relevant:

In Rechnernetzen spricht man beim \emph{Grenzdurchsatz} von der
\emph{Bandbreite}, wobei dies somit den maximalen Durchsatz darstellt. Da die
\emph{Latenz} in Netzen, die nahe der Grenzfrequenz operieren, oft sehr
hoch ist, ist als interessantes Maß die Anzahl der Aufträge, die
eine bestimmte obere Grenze der Antwortzeit (Latenz) nicht
überschreiten von Interesse.
Die \emph{Verlustrate} hingegen beschreibt die Anzahl der
Einheiten, die pro Zeiteinheit verloren gehen.
\section{Auslastung}
Hierbei von Bedeutung ist das Verhältnis von tatsächlich erreichtem
Durchsatz und Grenzdurchsatz. Dazu wird für folgendes Beispiel der
\emph{Speed-Up}, also die Beschleunigung definiert: 
\begin{gather*}
  S_p(n)=\frac{T_1(n)}{T_p(n)}
\end{gather*}
Es sei $T_p(n)$ die Laufzeit eines Algorithmus, der zur Ausführung
$p$ Prozessoren benötigt, bei einer Eingabe der Länge $n$. Die
Berechnung der \emph{Effizienz} $E_p(n)$ berechnet sich nun aus dem
Verhältnis des berechneten Speed-Ups $S_p(n)$ zu dem maximal
erreichbaren Speed-Up:
\begin{gather*}
  E_p(n)=\frac{S_p(n)}{p}
\end{gather*}
Die maximal erreichbare Beschleunigung ist $p$-fach, da wir davon
ausgehen, dass das Problem sich durch die Verteilung auf $p$-Prozessoren
idealerweise um diesem Faktor beschleunigt. Damit gilt für die Effizienz
folgende Ungleichung:
\begin{gather*}
  \frac{1}{p} \leq E_p(n) \leq 1
\end{gather*}
Daher bezeichnet man einen parallelen Algorithmus mit $E_p(n)=1$ als
\emph{optimal}.

\section{Amdahls Gesetz}

Amdahls Gesetz besagt, dass die \emph{Leistungsverbesserung} durch den
Einsatz neuer Hardware abhängig ist von
\begin{itemize}
\item dem Anteil an Ausführungszeit $r$, die ein Programm erbringt
und
\item der Beschleunigungsrate b, die durch die bessere Hardware erreicht
wird, wenn das \emph{gesamte} Programm getestet wird.
\end{itemize}
Mit anderen Worten besagt Amdahls Gesetz, dass sich der Aufwand für die
Verbesserung nur dann lohne, wenn dadurch der häufig eintretende Fall
verbessere, also die neue Hardware häufig eingesetzt würde. 
\begin{center}
"`making the common case fast"'
\end{center}
Es ergibt sich also der Anteil an Ausführungszeit $r$, der sich
parallelisieren läßt und $(1-r)$, der Anteil für den das nicht
klappt. Damit gilt für die \emph{Gesamtausführungszeit} $T$:
\begin{gather*}
  T = T \cdot (1-r) + T \cdot r\\
  T_{\text{verbessert}} = T \cdot (1-r) + \frac{T\cdot r}{b}\\
  \rightarrow \text{Speed-Up} = \frac{T}{T_\text{verbessert}}
     = \frac{1}{(1-r) +	\frac{r}{b}}
\end{gather*}

\section{Methoden}
Viele Methoden zur Leistungsbewertung lassen sich in die drei Klassen
\emph{analytische Methoden, Messungen und Simulation} einordnen.
Analytische Methoden eignen sich gut zur schnellen Orientierung,
Simulationen sind dagegen sehr flexibel und Messungen liefern genaue
Aussagen.
\paragraph{Analytische Methoden. } Hierbei wird das Bewertungsproblem
auf der Grundlage von \emph{Gleichungen} approximativ oder numerisch gelöst.
Diese Gleichungen sind mathematisch \emph{abgeschlossen} oder bestehen aus
Modellen. Sie werden abgeleitet aus z.Bsp. Warteschlangenmodellen,
Wartenetzen oder stochastischen Petrinetzen. Berechnet werden
quantitative Leistungsmaße wie \emph{Mittelwert, Durchsatz} oder
\emph{Latenz}. Das betroffene System wird dabei immer im \emph{stationären
Zustand} betrachtet. Mit der Zunahme der Komplexität eines Systems wird
auch das Auffinden eines geeigneten analytischen Modells schwieriger
und aufwendiger.
\paragraph{Simulation. } Ohne das der zu bewertetende Rechner existieren
muss, wird eine Annahme über dessen Eigenschaften, also \emph{Struktur,
Betrieb, Betriebsprozesse}, getroffen und darauf ein \emph{Ablaufmodell}
gewonnen. Mit diesem Modell wird versucht genaue Leistungsaussagen über
den Rechner zu gewinnen. Simulationen sind \emph{Programme}, die
geschaffen werden, um das Verhalten eines Rechners auf einen anderen
abzubilden und werden meist in höheren Programmiersprachen, oder
Simulationssprachen geschrieben. Sehr häufig wird dabei eine
\emph{ereignisorientierte} Simulation (event driven simulation)
angewendet. Dabei wird die Zeit \emph{diskretisiert} und die Steuerung
wird durch bestimmte Ereignisse, wie z.Bsp. dem Ende einer
Auftragsbearbeitung ausgelöst. Grundsätzlich werden mit
\emph{deterministischen} und \emph{stochastischen} Simulationen zwei
Arten definiert, von denen es aber auch Mischformen gibt. Bei den
stochastischen Simulationen werden \emph{Zufallswerte} als Betriebsdaten
angenommen. Es ensteht hierbei das Problem der Generierung vernünftiger
Zufallszahlen. Variablen sind während des gesamten Simulationsprozesses
abrufbar und erlauben somit wesentlich aussagekräftigere Feststellungen
als dies bei analytischen Methoden möglich ist. Es lassen sich somit
beispielsweise \emph{einzelne Komponenten} einer Bewertung zuführen.
Auf der anderen Seite stehen die deterministischen Simulationen. Mit
ihnen wird das \emph{funktionale} und \emph{Zeitverhalten} realer
Systeme genauer abgebildet als bei den stochastischen Simulationen. Der
Haupteinsatz dieser Simulation findet sich bei dem Entwurf neuer oder
der Optimierung bereits bestehender \emph{Funktionseinheiten} eines
bereits existierenden Systems. Detaillierte und deterministisch
arbeitende Modelle zu entwerfen ist hier möglich. Es wird hier weiterhin
unterschieden in die \emph{Trace-}gesteuerten und
\emph{ausführungsgesteuerten} Simulationen. Bei ersterem wird
beispielsweise das Verhalten von Speicherhierarchien simuliert. Mit
letzterem werden auf dem Simulationsmodell komplette Programme,
sogenannte \emph{Benchmark-}Programme, ausgeführt und erlauben somit
eine noch realitätsnähere Betrachtung.

Allgemein läß sich für Simulationen festhalten:
\begin{enumerate}
\item Simulationen bieten eine gute Möglichkeit zum Entwurf von
Rechnern.
\item Sie erlauben eine Auswahl an Konfigurationsänderungen
vorzunehmen.
\item Mit der Zunahme der Komplexität steigen jedoch auch ihre Kosten.
\end{enumerate}
\paragraph{Messungen. } An existierenden System können zur
Leistungsfeststellung, also zum Erhalt von \emph{Leistungsmaßzahlen},
Messungen durchgeführt werden. Man nennt dies \emph{beobachtende
Leistungsbewertung}, deren Ziel es sein kann, entweder einzelne
Komponenten oder aber auch komplette Rechenanlagen zu vermessen. In
dieser Zielstellung steckt der Wunsch verschiedenene Rechenanlagen
\emph{vergleichen} zu können, theoretische Leistungsaussagen zu
\emph{validieren}, sowie den Betrieb \emph{überwachen} zu können.
Daher auch der Begriff "`beobachtende Leistungsbewertung"', im
Englischen \emph{monitoring}. Das monitoring kann \emph{ereignisbezogen}
und \emph{ereignisunabhängig} sein. Es werden drei Typen von Messungen
unterschieden:
\begin {enumerate}
\item Monitoring
\item Benchmarking
\item Hybrides Monitoring
\end {enumerate}
Das Monitoring wird weiterhin unterschieden in \emph{Hardwaremonitoring}
und \emph{Softwaremonitoring}. Mit ersterem werden \emph{Messfühler} an
das Messobjekt angelegt und die Ergebnisse vom Hardwaremonitor
aufgesammelt. Vorteilhaft ist hier, dass die Messung den Ablauf des
Objektrechners nicht beinflußt, hingegen muss mit einer großen
Datenflut umgegangen werden. 

\begin{center}
\includegraphics[height=2cm]{hw_monitor}
\end{center}

\noindent Hingegen wird beim Softwaremonitoring
die Messung durch das \emph{Objektprogramm} angestoßen und durch ein
\emph{Messprogramm} dem Monitor zugeführt. Vorteilhaft hier ist, dass
nur wenig Information über den Objektrechner notwendig ist und das die
Messprogramme auf vielen Rechnern laufen. Jedoch ist negativ zu
bemerken, dass durch den Eingriff in den Programmablauf das dynamische
Verhalten des Objektrechners verfälscht wird.
\begin{center}
\includegraphics[height=2cm]{sw_monitor}
\end{center}
Eine spezielle Problemstellung stellt das Monitoring in verteilten
Systemen dar, in denen es keinen Masterrechner gibt, der die Daten über
die beteiligten Knoten zur Verfügung stellen kann. Daher werden 
\emph{Knotenmonitore} zur Überwachung von einzelnen Knoten oder
\emph{Verbindungsmonitore} zur Überwachung von Clustern von Rechnern
eingesetzt. Dabei stellt die \emph{unterschiedliche} Uhrzeit in den
beteiligten Rechnern das Hauptproblem dar, dass durch software- oder
hardwaremässige Synchronisation, oder durch den Einsatz lokaler
Zeitstempel gelöst werden kann.

Das Monitoring erfährt seine Grenzen in der aus der zunehmenden
Integration resultierenden Schwierigkeit auf interne Signale zugreifen
zu können. Weiterhin wird durch \emph{virtuellen Speicher} eine
eindeutige Zuordnung zum Prozessor erschwert. Der Zugriff auf Daten-
oder Befehlscache wird verhindert. RISC-Prozessoren ändern die
Reihenfolge der Befehlsausführung und sorgen damit für Probleme beim
Monitoring. Schliesslich kann die Steigerung der Taktraten zu einer
übermäßigen Datenflut führen.

\noindent Unter dem Begriff \textbf{Benchmarking} versteht man die
\emph{vergleichende Ausführung} ein und desselben (Mess-)programms auf
unterschiedlichen Rechnern. Der Rechner, der das Programm am schnellsten
ausführt ist demnach der schnellere Rechner. Hier liegt das Problem
begraben. In der Praxis ist es häufig vorgekommen, dass Compiler oder
Befehlssätze manipuliert wurden, damit sie auf speziellen Benchmarks
bessere Leistung erzeugen. Benchmarks ermöglichen es, zusätzliche
Informationen über Compiler, Betriebssystem und auch die Genauigkeit
der Ergebnisse zu gewinnen. Es gibt vier Typen von Benchmarks:

\begin{enumerate}
\item Reale Programme (häufig verwendete Programme)
\item Kernels (kurze kritische Passagen aus realen Programmen)
\item Toy Benchmarks (kleine, einfache, schnell übertragbare Programme)
\item Synthetische Benchmarks (testen spezielle Instruktionen)
\end{enumerate}

\noindent Zu den synthetischen Benchmarks gehören die mittlerweile
veralteten Dhrystone und Wheatstone Benchmarks, der LINPACK-Benchmark
und die SPECint, sowie SPECfp-Benchmarks.


\chapter{Fehlertoleranz}

Rechnersysteme werden auch in sensiblen Bereichen eingesetzt, in denen
ein Ausfall oder Fehlfunktion eklatante Folgen haben können. Es wird daher
nach hoher Verlässlichkeit gefragt. Die \emph{Fehlerintoleranz}
beschreibt Methoden zur \emph{Vermeidung} von Fehlverhalten. Die
\emph{Fehlertoleranz} versucht dagegen erwünschtes Verhalten durch
ein System auch dann zu erhalten, wenn mit einer bestimmen Anzahl von
Fehlern gerechnet werden muss. Fehler lassen sich aufgrund verschiedener
Eigenschaften klassifizieren. So beschreibt die \emph{Dauer eines
Fehlers}, ob ein Fehler permanent oder nur vorrübergehend auftritt. Die
\emph{Auswirkung eines Fehlers} erklärt, ob ein Fehler nur auf eine
Komponente wirkt ($\rightarrow$ Einfachfehler) oder ob eine Fehlerquelle
mehrere Fehler erzeugt ($\rightarrow$ Mehrfachfehler). Finde sich ein
\emph{Fehler im Lebenszyklus eines Rechners}, so besteht die Frage ob es
sich um einen Konstruktionsfehler (Entwurfs-, Implementierungs- oder
Herstellungsfehler) handelt oder ob es ein Betriebsfehler ist, also ob
er während des Betriebs(Bedienungs- oder Wartungsfehler) auftritt. Ein
weiteres Klassifizierungsmerkmal ist der \emph{Ort des Fehlers}, also ob
er in der Hardware oder Software angesiedelt ist Ein weiteres
Klassifizierungsmerkmal ist der \emph{Ort des Fehlers}, also ob er in
der Hardware oder Software angesiedelt ist.

\section{Zuverlässigkeit und Verfügbarkeit}
Zur Erfassung der Fehlertoleranz werden quantitative Größen relevant.
Eine wichtige Größe ist die \emph{Zuverlässigkeit} $R(t)$, die
bedingte Wahrscheinlichkeit, dass ein System das Zeitintervall $[0,t)$
überlebt. Es gilt:
\begin{align*}
  R(0) &=1 \longrightarrow \text{~Das Gerät ist zum Zeitpunkt 0
     funktionstüchtig.}\\
  R(\infty) &=0\longrightarrow \text{~Nach genügend langer Zeit fällt das
     Gerät aus.}
\end{align*}
Eine weitere Größe ist die \emph{Ausfallrate} $\lambda(t)$, für die
im nicht konstanten Fall gilt:
\begin{gather*}
  \lambda(t)=-\frac{\delta R(t)}{dt R(t)}
\end{gather*}
Ist die Aufallrate $\lambda$ konstant, so gilt beispielsweise:
\begin{gather*}
  R(t)=e^{-\lambda t}
\end{gather*}
Dieses Beispiel nutzen wir für die Beschreibung einer weiteren Größe,
nämlich der \emph{mittleren Funktionszeit} (mean team to failure):
\begin{gather*}
  \mathit{MTF}=\int_{i=0}^{\infty}R(t)dt= \frac{1}{\lambda}
\end{gather*}
Mit der sogenannten Badewannenfunktion lassen sich Frühausfälle und
Spätausfälle beschreiben. Eine andere wichtige Größe stellt die
\emph{Verfügbarkeit} dar. Für eine nicht-redundante Einheit mit
Ausfallrate $\lambda$ und Reparaturrate $\mu$ (Kehrwert der
Reparaturzeit $\mathit{MTR}$ gilt
\begin{gather*}
  A=\frac{\mathit{MTF}}{\mathit{MTR}+\mathit{MTF}}
     =\frac{\lambda}{\lambda + \mu}
\end{gather*}

\begin{center}
\includegraphics[height=5cm]{badewanne}
\end{center}

\noindent So lassen sich beispielsweise für die Halbleiterproduktion
die Frühausfälle durch die Technik des \emph{burn in} lokalisieren und
die entsprechend fehlerhaften Produkte entfernen, was die Lebensdauer
von bereitgestellten Halbleitern natürlich erhöht. 

\section{Redundanztechniken}
Der Begriff \emph{Redundanz} beschreibt hier das Hinzufügen von
Hardware oder Software in ein System um Fehlererkennung und
Reaktion darauf zu ermöglichen. Die zentralen Begriffe sind hierfür
die \emph{Fehlerdiagnose} und \emph{Fehlerbehandlung}. Die
Fehlerdiagnose findet als \emph{Selbstest} im System im laufenden
Betrieb statt. Die Fehlerbehandlung setzt sich aus der
\emph{Rekonfiguration}, also der Isolierung der fehlerhaften Teile in HW
und SW und aus dem \emph{Wiederanlauf} zusammen. Beim Wiederanlauf wird
das System nach dem Ausschluss fehlerhafter Teile in der Grundzustand
versetzt. Ausserdem sollen \emph{Sicherungspunkte} angelegt werden, die
als spätere Rücksetzpunkte dienen können.

Die Redundanz wird unterschieden in \emph{dynamische} und
\emph{statische} Redundanz. In der statischen Redundanz sind alle
Elemente, die für die Fehlertoleranz notwendig sind, ständig in
Betrieb, während bei der dynamischen Redundanz notwendige Elemente
on-demand zugeschalten werden.
\section{Beispiele fehlertoleranter Systeme}
Hierzu gelten \emph{Universalrechnersysteme}, wie
transaktionsorientierte Datenbankanwendungen, die hohe Zuverlässigkeit
erfordern. Es sind dabei alle funktionalen Komponenten, also
CPU\slash{}Speicher\slash{}Busse\slash{}Peripherie statisch und\slash{}oder
dynamisch redundant.
Ebenso sind Lüfter- und Spannungsversorgung redundant. Als anderes
Beispiel seien \emph{eingebettete Rechnersysteme} genannt, bei denen
geringe Kosten und hohe Kompatibilität im Vordergrund stehen. Hierbei
wurden Mechanismen entwickelt, die den Austausch von Komponenten zur
Laufzeit ermöglichen (hot-swap, hot-plugin). Dazu werden zumeist
Bussysteme verwendet, die dies unterstützenm wie USB. Ein letztes
Beispiel sei mit RAID genannt. RAID steht für \emph{Redundant array of
inexpensive disks} und stellt ein Peripheriesystem dar. Der Grundgedanke
ist, dass mechanische Komponenten wie hier zum Beispiel Festplatten eine
sehr viel geringe $\mathit{MTF}$ haben. Die $\mathit{MTF}$ sinkt sogar noch weiter, wenn man
die Anzahl der Festplatten in einem System erhöht. RAID bietet hierfür
eine Lösung. Drei Eigenschaften sind kennzeichnend:
\begin{enumerate}
\item Eine Menge \emph{physischer} Platten wird durch das Betriebssystem
als \emph{eine logische} Platte aufgefasst.
\item Die Datenspeicherung erfolgt verteilt.
\item Ausnutzen der Redundanz der verfügbaren Kapazitäten um im
Fehlerfall die Wahrscheinlichkeit zur Wiederherstellung zu erhöhen.
\end{enumerate}

\noindent Die Vorteile liegen auf der Hand. Es wird eine wesentlich höhere
Zuverlässigkeit ermöglicht und der parallele Zugriff auf die Platten
erhöht die Leistungsfähigkeit der E/A. 
