% Einige zusätzliche Informationen für rubber
%  rubber erkennt nicht, dass die Datei weg kann, daher sagen wir es ihm
% rubber: clean $base.thm
%  rubber soll nach Änderungen an der Datei nochmal bauen
% rubber: watch $base.thm
% rubber: makeidx.tool      xindy
% rubber: makeidx.language  german-din
% rubber: makeidx.modules   indexstyle.xdy

\RequirePackage[l2tabu,orthodox]{nag}  % nag überprüft den Text auf veraltete
                   % Befehle oder solche, die man nicht in LaTeX verwenden
                   % soll -- l2tabu-Checker in LaTeX

\documentclass[halfparskip*,ngerman,draft,twoside]{scrreprt}

\usepackage{babel}
\usepackage{makeidx}
\usepackage{color}
\usepackage[draft=false,colorlinks,bookmarksnumbered,linkcolor=blue,breaklinks]{hyperref}

\usepackage[latin1]{inputenc}
\usepackage{nicefrac}

\usepackage{lmodern}		% Latin Modern
% \usepackage{type1ec}           % cm-super
\usepackage[T1]{fontenc}        % T1-Schriften notwendig für PDFs
\usepackage{textcomp}           % wird benötigt, damit der \textbullet
                                % für itemize in lmodern gefunden wird.

\usepackage[intlimits,leqno]{amsmath}
\usepackage[all,warning]{onlyamsmath}  % warnt bei Verwendung von nicht
                                       % amsmath-Umgebungen z.\,B. $$...$$
\usepackage{amssymb}     % wird für \R, \C,... gebraucht
\usepackage{fixmath}     % ISO-konforme griech. Buchstaben

\usepackage[amsmath,thmmarks,hyperref]{ntheorem} % für die Theorem-Umgebungen
                                                 % (satz, defini, bemerk)
\usepackage{xspace}      % wird weiter unten gebraucht
\usepackage{paralist}    % besseres enumerate und itemize und neue
                         % compactenum/compactitem; s. texdoc paralist

\usepackage{svn}         % Zum Auswerten und ordentlichen Darstellen der
                         % SVN-Schlüsselwörter (s. vor \begin{document})
                         % dafür muss in SVN noch das Flag svn:keywords
                         % auf "LastChangedRevision LastChangedDate"
                         % gesetzt werden
\usepackage{ellipsis}    % Korrektur für \dots
\usepackage{fixltx2e}
\usepackage[final]{microtype} % Verbesserung der Typographie
\usepackage{amsfonts}
\usepackage{eurosym}     %% Eingabe des Eurosymbols
\usepackage{sistyle}     %% fuer die korrekte Angabe von Einheiten
\usepackage{braket}      %% Zur Eingabe von Mengen: A={x| x>2}
\usepackage{booktabs}    %% schöne Linien in Tabell


% Damit auch die Zeichen im Mathemode in Überschriften fett sind
% <news:lzfyyvx3pt.fsf@tfkp12.physik.uni-erlangen.de>
\addtokomafont{sectioning}{\boldmath}

% nach dem Theoremkopf wird ein Zeilenumbruch eingefügt, die Schrift des
% Körpers ist normal und der Kopf wird fett gesetzt
\theoremstyle{break}
\theorembodyfont{\normalfont}
\theoremheaderfont{\normalfont\bfseries}
\theoremnumbering{arabic}

% Die folgenden Umgebungen werden einzeln nummeriert und am Ende jedes
% Kapitels zurückgesetzt
\newtheorem{satz}{Satz}[chapter]
\newtheorem{bem}[satz]{Bemerkung}
\newtheorem{defin}[satz]{Definition}
\newtheorem{bsp}[satz]{Beispiel}
\newtheorem{verf}[satz]{Verfahren}
\newtheorem{lemma}[satz]{Lemma}
\newtheorem{kor}[satz]{Korollar}

% Die folgenden Theoremumgebungen bekommen keine Nummer
%\theoremstyle{nonumberbreak}
%\newtheorem{fakt}{Fakt}

\theoremheaderfont{\scshape}
\theorembodyfont{\normalfont}
% Das Zeichen am Ende eines Beweises
\theoremsymbol{\ensuremath{_\blacksquare}}
% \theoremsymbol{q.\,e.\,d.}
\newtheorem{proof}{Beweis:}

% Hier die Definition, wie \autoref die Umgebungen nennen soll, die mit
% \newtheorem definiert wurden
\newcommand*{\satzautorefname}{Satz}
\newcommand*{\bemautorefname}{Bemerkung}
\newcommand*{\definautorefname}{Definition}
\newcommand*{\bspautorefname}{Beispiel}
\newcommand*{\verfautorefname}{Verfahren}
\newcommand*{\lemmaautorefname}{Lemma}
\newcommand*{\korautorefname}{Korollar}
% Zwischen Unter- und Unterunterabschnitten sollte nicht unterschieden
% werden.
\renewcommand*{\subsectionautorefname}{Abschnitt}
\renewcommand*{\subsubsectionautorefname}{Abschnitt}

\pagestyle{headings}

\newcommand*{\R}{\mathbb{R}}      % reelle Zahlen
%\newcommand*{\C}{\mathbb{C}}      % komplexe Zahlen
\newcommand*{\N}{\mathbb{N}}      % natürliche Zahlen
%\newcommand*{\Q}{\mathbb{Q}}      % gebrochene Zahlen
%\newcommand*{\Z}{\mathbb{Z}}      % ganze Zahlen

%% kalligrafische Zeichen
\newcommand*{\FF}{\mathcal{F}}

% Wenn irgendwo Unklarheiten zum Inhalt im Skript auftreten, können sie
% einfach mit \help{Ich verstehe das nicht} hervorgehoben werden. Dies
% macht es leichter sie alle zu finden und auch ganz einfach
% auszublenden, indem man den Befehl einfach leer definiert
\newcommand*{\help}[1]{\textcolor{green}{help: #1}}

% \todo ist das gleiche wie \help nur für offene Aufgaben
\newcommand*{\todo}[1]{\textcolor{red}{todo: #1}}

% Um wichtige Begriffe im Text überall gleich vorzuheben (gleiches
% Markup), sollte dieser Befehl verwendet werden. Das Argument wird
% automatisch als Indexeintrag verwendet. Dieser kann aber auch als
% optionales Argument selbst bestimmt werden.
\newcommand*{\highl}[2][]{\textbf{\boldmath{#2}}%
  \ifthenelse{\equal{#1}{}}{\index{#2}}{\index{#1}}%
}

% Definition für Xindy für die Trennung der einzelnen Abschnitte im
% Index. siehe auch die Datei indexstyle.xdy
\newcommand*{\indexsection}{\minisec}

% Für Leute, die nicht gern o.\,B.\,d.\,A. jedesmal eintippen wollen
\newcommand*{\obda}{o.\,B.\,d.\,A.\xspace}
%% unter den Nebenbedingungen ist auch zu lang zum Eintippen
\newcommand*{\unb}{unter den Nebenbedingungen\xspace}

% Diese Befehle sind dafür gedacht, dass die Symbole für "genau dann wenn"
% im ganzen Dokument gleich aussehen. Außerdem erlaubt es eine schnelle
% Veränderung aller Stellen, falls der Prof. doch nicht mehr gdw nimmt,
% sondern \Leftrightarrow.
% \newcommand*{\gdw}{\ifthenelse{\boolean{mmode}}%
% 			       {\mspace{8mu}gdw\mspace{8mu}}%
% 			       {$gdw$\xspace}}
% \newcommand*{\gdwdef}{\ifthenelse{\boolean{mmode}}%
% 			       {\mspace{8mu}gdw_{def}\mspace{8mu}}%
% 			       {$gdw_{def}$\xspace}}

% Um sicherzustellen, dass jeder Betrag-/jede Norm links und rechts die
% Striche bekommt, sind diese Befehle da. Damit kann man nicht die
% rechten Striche vergessen und es wird etwas übersichtlicher. (Vorschlag
% ist aus amsldoc) \abs[\big]{\abs{a}-\abs{b}} \leq \abs{a+b}
\newcommand*{\abs}[2][]{#1\lvert#2#1\rvert}
\newcommand*{\norm}[2][]{#1\lVert#2#1\rVert}

% Das original Epsilon sieht nicht so toll aus
\renewcommand*{\epsilon}{\varepsilon}
% ... und mancheinem gefällt auch das Phi nicht
\renewcommand*{\phi}{\varphi}

\DeclareMathOperator{\co}{co}  %% konvexe Huelle
\DeclareMathOperator{\rg}{rg}  %% Rang
\DeclareMathOperator{\diag}{diag}  %% Diagonalmatrix

\makeindex

\SVN $LastChangedRevision$
\SVN $LastChangedDate$

\begin{document}

\title{Lineare Optimierung}
\author{Prof.\,Walter Alt}
\date{Semester: SS 2006}
\maketitle

\clearpage
\chapter*{Vorwort}

{\itshape
  Dieses Dokument wurde als Skript für die auf der
  Titelseite genannte Vorlesung erstellt und wird jetzt im Rahmen des
  Projekts
  "`\href{http://www.minet.uni-jena.de/~joergs/skripte/}
  {Vorlesungsskripte der Fakultät für Mathematik}
  \href{http://www.minet.uni-jena.de/~joergs/skripte/}{und Informatik}"'
  weiter betreut. Das
  Dokument wurde nach bestem Wissen und Gewissen angefertigt. Denoch
  garantiert weder der auf der Titelseite genannte Dozent, die Personen,
  die an dem Dokument mitgewirkt haben, noch die
  Mitglieder des Projekts für dessen Fehlerfreiheit. Für etwaige Fehler
  und dessen Folgen wird von keiner der genannten Personen eine Haftung
  übernommen. Es steht jeder Person frei, dieses Dokument zu lesen, zu
  verändern oder auf anderen Medien verfügbar zu machen, solange ein
  Verweis auf die Internetadresse des Projekts
  \url{http://www.minet.uni-jena.de/~joergs/skripte/}
  enthalten ist.

  Diese Ausgabe trägt die Versionsnummer~\SVNLastChangedRevision\ und ist
  vom \SVNDate. Eine (mögliche) aktuellere Ausgabe ist auf der Webseite
  des Projekts verfügbar.

  Jeder ist dazu aufgerufen Verbesserungen, Erweiterungen und
  Fehlerkorrekturen für das Skript einzureichen bzw. zu melden oder diese
  selbst einzupflegen -- einfach eine E-Mail an die
  \href{mailto:skripte@listserv.uni-jena.de}{Mailingliste
  \texttt{<skripte@listserv.uni-jena.de>}} senden. Weitere Informationen
  sind unter der oben genannten Internetadresse verfügbar.

  Hiermit möchten wir allen Personen, die an diesem Skript mitgewirkt
  haben, vielmals danken:
  \begin{itemize}
   \item \href{mailto:jens@kubieziel.de}{Jens Kubieziel
    \texttt{<jens@kubieziel.de>}} (2006)
  \end{itemize}
}

\clearpage
\pdfbookmark[0]{Inhaltsverzeichnis}{inhaltsverzeichnis}
\tableofcontents

\clearpage
\pdfbookmark[0]{Auflistung der Sätze}{theoremlist}
\chapter*{Auflistung der Theoreme}

\pdfbookmark[1]{Sätze}{satzlist}
\section*{Sätze}
\theoremlisttype{optname}
\listtheorems{satz}

\pdfbookmark[1]{Definitionen}{definilist}
\section*{Definitionen}
% \theoremlisttype{all}
\listtheorems{defin}

\chapter{Einführung}

\section{Grundbegriffe und Beispiele}

Die Zielstellung in der linearen Optimierung ist die Betrachtung einer
Funktion $f\colon\R^n\rightarrow \R$ mit dem Ziel der Berechnung eines
Minimums von $f$.

\begin{bsp}
  \begin{inparaenum}[(1)]
  \item $f\colon\R\rightarrow\R$ mit $f(x)=x^2$ hat genau ein Minimum.
  \item $f\colon\R\rightarrow\R$ mit $f(x)=x$ ist nicht nach unten
    beschränkt und hat daher kein Minimum.
  \item $f\colon\R\rightarrow\R$ mit $f(x)=\sin x$.
  \end{inparaenum}
\end{bsp}

\begin{bsp}
  Die Funktion $f\colon\R^2\rightarrow\R$ mit $f(x_1,x_2)= x_1 \sin (x_1+
  x_1 x_2\sin(x_2))$.
\end{bsp}

Die Norm auf $\R^n$ ist im allgemeinen die euklidische Norm,
d.\,h. für $x\in \R^n, x=
\begin{pmatrix}
  x_1\\ \vdots\\ x_n
\end{pmatrix}$ ist $\lVert x\rVert=\sqrt{\sum_{i=1}^n x_i^2}$. Für
$\tilde{x}\in\R$ und $r>0$ ist die offene Kugel um $\tilde{x}$ mit
Radius $r$ definiert durch $B(\tilde{x}, r)=\{x\in\R^n\colon \lVert
x-\tilde{x} \rVert< r\}$.

\paragraph{Allgemeine Formulierung des Optimierungsproblems}

Sei $D\subset \R^n$ nichtleer und offen, $f\colon D\rightarrow \R,
\FF\subset D$:
\begin{gather}
  \label{eq:P}
  \min_{x\in\FF} f(x)
\end{gather}

Die Funktion $f$ heißt \highl[Zielfunktion]{Ziel-} oder \highl{Kostenfunktion}.
\begin{description}
\item[$\FF=D$] unrestringiertes Optimierungsproblem
  (Optimierungsproblem ohne Nebenbedingungen)
\item[$\FF\subset D$] restringiertes Optimierungsproblem
  (Optimierungsproblem mit Nebenbedingungen), $\FF$ heißt
  \highl[Menge!zulässige]{zulässige Menge} und wird meist durch (Un-)gleichungen beschrieben.
\end{description}

\begin{bsp}
  $n=1, D=\R, \min f(x)=x^3$, NB: $x\geq 1$.\\
  In diesem Fall ist $\FF=\{x\in\R\colon x\geq 1\}$
\end{bsp}

\begin{defin}[Lokale Minima]
  Ein Punkt $\tilde{x}\in\FF$ (zulässiger Punkt) heißt
  \highl[Minimum!lokales]{lokales Minimum} von $f$ auf $\FF$ oder
  \highl[Lösung!lokale]{lokale Lösung} von
  \autoref{eq:P}, falls es ein $r>0$ mit $f(x)\geq f(\tilde{x})$ für
  alle $x\in\FF\cap B(\tilde{x}, r)$ gibt.

  Ein Punkt $\tilde{x}\in\FF$ heißt \highl[Minimum!striktes
  lokales]{striktes lokales Minimum} von $f$ auf $\FF$ oder
  \highl[Lösung!strikte lokale]{strikte lokale Lösung} von
  \autoref{eq:P}, falls es ein $r>0$ mit $f(x)>f(\tilde{x})$ für alle
  $x\in \FF\cap B(\tilde{x}, r)$ mit $x\neq \tilde{x}$ gibt.
\end{defin}

\begin{defin}[globales Minimum]
  Ein Punkt $\tilde{x}\in\FF$ heißt \highl[Minimum!globales]{globales
  Minimum} von $f$ auf $\FF$, falls gilt $\forall x\in\FF\colon
  f(x)\geq f(\tilde{x})$
\end{defin}

\begin{bem*}
  Die Optimierungsaufgabe $\max g(x)$ unter den Nebenbedingungen
  $x\in\FF$ ist äquivalent zur Minimierung. Das Problem wird als $\min
  f(x) := -g(x)$ definiert.

  Anstatt lokales Minimum sagt man auch
    \highl[Minimum!relatives]{relatives Minimum}. Anstatt striktes
    Minimum sagt man auch
  \highl[Minimum!strenges]{strenges Minimum}.

  Eine linere Optimierungsaufgabe besteht aus einer linearen Funktion
  $f$ und $\FF$ wird aus linearen (Un-)gleichungen beschrieben.
\end{bem*}

\begin{bsp}[Produktionsplanung]
  \label{bsp:prodplan-16}
  Eine Firma produziert vier Lacke $L_1, L_2, L_3, L_4$. Dabei ist der
  Gewinn pro Kilogramm Lack:
  \begin{asparaitem}
  \item 1,50 \euro{} für $L_1$
  \item 1,00 \euro{} für $L_2$
  \item 2,00 \euro{} für $L_3$
  \item 1,40 \euro{} für $L_4$
  \end{asparaitem}
Nebenbedingungen:
\begin{itemize}
\item von $L_1$ und $L_2$ können maximal \SI{1300}{kg} produziert werden
\item von $L_1, L_3, L_4$ können maximal \SI{2000}{kg} produziert werden
\item Mindestproduktion von $L_4$ sind \SI{800}{kg}
\item von $L_3$ sind nicht mehr als \SI{500}{kg} zu verkaufen
\end{itemize}

Ziel ist die Maximierung des Gewinns. Wir bezeichnen mit $x_i$ die
Menge, die von Lack $L_i$ produziert wird. Der Gewinn ist $1,5x_1+ x_2+
2x_3+ 1,4x_4$, d.\,h. es ist $f(x_1,x_2,x_3,x_4)= -1,5x_1 -x_2 -2x_3
-1,4x_4$ unter den Nebenbedingungen:
\begin{align*}
  x_1 + x_2 &\leq 1300 & x_4 &\geq 800 \Leftrightarrow -x_4\leq -800\\
  x_1+x_3+x_4 &\leq 2000 & x_3 &\leq 500
\end{align*}
zu minimieren.
Weiterhin ergibt sich aus praktischen Überlegungen $x_i\geq 0$. Wir
formulieren das Problem in Matrixschreibweise: Sei $c=-
\begin{pmatrix}
  1,5\\1\\2\\1,4
\end{pmatrix}$. Dann hat die Zielfunktion die Form $c^Tx$. Mit
\begin{align*}
  A &=
  \begin{pmatrix}
    1&1&0&0\\
    1&0&1&1\\
    0&0&0&-1\\
    0&0&1&0
  \end{pmatrix} & b &=
  \begin{pmatrix}
    1300\\2000\\-800\\500
  \end{pmatrix}
\end{align*}
können wir die Nebenbedingungen in der Form $Ax\leq b$ schreiben. Dann
können wir das Optimierungsproblem in der Form $\min c^Tx$ unter den
Nebenbedingungen $Ax\leq b, x\geq 0$ schreiben.
\end{bsp}

\begin{bsp}\label{bsp:17}
  Eine Tierfarm kauft drei verschiedene Kornsorten, um daraus
  Futtermittel zu mischen.

  \begin{tabular}{l|r|r|r|r}
    & $S_1$ & $S_2$ & $S_3$ & Mindestbedarf\\
    \bottomrule
    Nährstoff A & 2 & 3 & 7 & 1250\\
    Nährstoff B & 1 & 1 & 0 & 250\\
    Nährstoff C & 5 & 3 & 0 & 900\\
    Nährstoff D & 0,6 & 0,25 & 1 & 232,5\\
    \bottomrule
    Kosten & 41 & 35 & 96 & \\
  \end{tabular}

Gesucht ist eine Mischung, die den Nährstoffbedarf deckt und dabei
möglichst billig herzustellen ist.

Wir bezeichnen mit $x_i, i=1,2,3$ diejenige Menge, die von der
Kornsorte $s_i$ eingekauft wird. Dabei müssen wir die Mengen so
bestimmen, dass die Gesamtkosten der Futtermischung minimiert wird,
d.\,h. die Funktion $41x_2+ 35x_2+ 96x_3$ soll minimiert
werden. Folgende Nebenbedingungen gelten:
\begin{itemize}
\item Sinnvollerweise ist $x_i\geq 0, i=1,2,3$
\item Um den Nährstoffbedarf zu decken, müssen wir fordern:
  \begin{align*}
    2x_1 &+ 3x_2 &+ 7x_3 &\geq 1250\\
    x_1 &+ x_2 & &\geq 250\\
    5x_1 &+ 3x_2 & &\geq 900\\
    0,6x_1 &+ 0,25x_2 &+ x_3 &\geq 232,5
  \end{align*}
\item Wir definieren weiter:
  \begin{align*}
    c =
    \begin{pmatrix}
      41\\35\\96
    \end{pmatrix}
    & A =
    \begin{pmatrix}
      2 & 3 & 7 \\
      1 & 1 & 0\\
      5 & 3 & 0\\
      0,6 & 0,25 & 1
    \end{pmatrix}
    & b =
    \begin{pmatrix}
      1250\\250\\900\\232,5
    \end{pmatrix}
  \end{align*}
\item Somit können wir das Problem in der Form $\min c^Tx$ unter den
  Nebenbedingungen $Ax\geq b, x\geq0$ schreiben.
\end{itemize}
\end{bsp}

\paragraph{Allgemein zur Produktplanung}

Es sollen bestimmte Güter aus Rohstoffen hergestellt werden. Durch die
Zielfunktion soll der Gewinn maximiert bzw. die Kosten minimiert
werden und als Nebenbedingung sollen keine negativen Mengen produziert
bzw. benutzt werden.

\section{Numerische Lösung von Optimierungsproblemen}

Allgemein sei das Problem 
\begin{gather*}
  \min f(x) \text{ mit } x\in \FF
\end{gather*}
mit der Zielfunktion $f\colon\R^n\rightarrow\R$ und der zulässigen Menge
$\FF \subset \R^n$ zu lösen.

Numerische Verfahren sind Iterationsverfahren. Sie berechnen ausgehend
von einem Startpunkt $x^{(0)}$ eine Folge $\{x^{(k)}\}\subset\R^n,
k=1,2,3, \ldots$ mit dem Ziel:
\begin{itemize}
\item nach endlich vielen Schritten $m$ ist $x^{(m)}$ eine Lösung oder,
  falls dies nicht möglich ist,
\item deren Grenzwert ist eine Lösung.
\end{itemize}

Diese numerische Verfahren berechnen nur eine Näherungslösung. Da die
Zielfunktion zu minimieren ist, versucht man, die Folge $x^{(k)}$ so
zu berechnen, dass $f(x^{(k+1)})< f(x^{(k)})$ ist. Dies nennt man
\highl{Abstiegsverfahren}. Prinzipiell geht man dabei wie folgt vor:
\begin{itemize}
\item  Man berechnet eine \highl{Abstiegsrichtung} $d^{(k)}$, d.\,h. eine
  Richtung $d^{(k)}$ mit $f(x^{(k)} + td^{(k)})< f(x^{(k)})$ für
  hinreichend kleines $t>0$.
\item Man berechnet eine geeignete Schrittweite $t_k$.
\end{itemize}

Ist $\FF\neq\R^n$, dann fordert man in der Regel
\begin{itemize}
\item $x^{(0)}\in\FF$
\item $x^{(k)}\in\FF, k=1,2,\ldots$; Dazu bestimmt man die zulässige
  Abstiegsrichtung $d^{(k)}$ mit der Eigenschaft, dass
  $x^{(k)}+td^{(k)} \in\FF$ für hinreichend kleines $t>0$.
\end{itemize}

Es gibt verschiedene Verfahren, die diese Vorgehensweise für bestimmte
Klassen von Optimierungsproblemen realisieren:
\begin{itemize}
\item Lineare Optimierungsprobleme: Simplexverfahren, Innere-Punkte-Verfahren
\item Differenzierbare Optimierungsprobleme: Gradientenverfahren
\end{itemize}
Es gibt auch allgemeine Verfahren, die keine speziellen
Voraussetzungen an die Zielfunktion und die Nebenbedingungen machen. 

\begin{bsp}[Mutations-Selektions-Verfahren]
  \begin{asparaenum}
  \item Wähle $x^{(0)}\in\FF$. Setze $k:=0$
  \item Mutation: Berechne einen neuen Punkt $v^{(k)}$ durch zufällige
    Änderung $x^{(k)}$.
  \item Selektion: Ist $v^{(k)}\in\FF$ und $f(v^{(k)})< f(x^{(k)})$,
    dann $x^{(k+1)}:= v^{(k)}$. Andernfalls $x^{(k+1)}:=x^{(k)}$.
  \item Setze $k:=k+1$ und gehe zu Punkt 2.
  \end{asparaenum}
Dieses Verfahren hat kein Abbruchkriterium. Eine häufig benutzte
Mutation ist die Berechnung von $v^{(k)}_i = x^{(k)}_i +t_k (d^{(k)}_i
-0,5)$. Dabei ist $d^{(k)}_i\in[0,1]$ zufällig erzeugt und $t_k$ muss
durch Probieren bestimmt werden.
\end{bsp}

\begin{bsp}
  Sei folgendes Optimierungsproblem gestellt:
  \begin{align*}
    \min -(2000x_1+ 1200x_2+ 1000x_3+ 1500x_4) &\\
    x_1+x_2+x_3+x_4 &\leq 16\\
    1200x_2 &\geq 4000\\
    1200x_2 - 1500x_4 &\leq 0\\
    2000x_1 &\leq 7000\\
    xi\geq 0 & i=1,\ldots, 4
  \end{align*}
Das Simplexverfahren berechnet die Lösung in zwei Schritten. Zuerst
wird $x^{(0)}\in\FF$ ermittelt. Dazu benötigt das Verfahren vier
Iterationen. Danach erfolgt die eigentliche Berechnung in zwei
Iterationen und es stoppt mit $x=
\begin{pmatrix}
  3,5\\3,33\\0\\9,16
\end{pmatrix}, f(x)=-24750$.

Das Mutations-Selektions-Verfahren wird mit einer Schrittweite
$t_0=0,8$ gestartet. Diese wird immer nach 5000 Schritten halbiert und
nach 100000 Iterationen wird abgebrochen. Das ermittelte Ergebnis ist $x=
\begin{pmatrix}
  3,5\\3,333\\1,6557\cdot 10^{-7}\\9,1666
\end{pmatrix}, f(x)=-24749,999985$.
\end{bsp}

\chapter{Grundlagen der linearen Optimierung}
Ein lineares Optimierungsproblem wird im englischen auch als linear
program bezeihnet. Daher rührt die oft benutzte Abkürzung LP her. Das
Problem besteht aus einer linearen Zielfunktion $f(x)=c^Tx$ und
Nebenbedingungen, die in Form von Gleichungen und Ungleichungen
definiert werden.

Es gibt verschiedene "`Standardformen"' von linearen
Optimierungsproblemen.

\section{Lineare Optimierungsprobleme}

\subsection{Standardformen von linearen Optimierungsproblemen}
\begin{defin}
  Ein lineares Optimierungsproblem in allgemeiner Form (LPA) ist die
  Aufgabe:
  \begin{align*}
    \min c^Tx \text{ mit}\\
    l_x\leq x\leq u_x\\
    l_A\leq A^{(1)}\leq u_A\\
    A^{(2)}(x)= b
  \end{align*}
  wobei gilt: $x\in\R^n, c\in\R^n, l_x, u_x\in\R^n, A^{(1)}$ ist eine
  $m_1\times n$-Matrix, $l_A, u_A\in\R^{m_1}, A^{(2)}$ ist eine
  $m_2\times n$-Matrix.
\end{defin}

\begin{bsp}
  Eine Supermarktkette stellt ein Billiggetränk auf Weinbasis
  her. Grundlage ist ein Landwein mit Kosten von 1~\euro{} pro
  Liter. Weitere Zutaten sind Zuckerlösung (1,20~\euro{} pro Liter) und
  Konservierungsmittel (1,80~\euro{} pro Liter). Das Ziel ist die
  Herstellung einer möglichst billigen Mischung. Dabei müssen folgende
  Restriktionen beachtet werden.
  \begin{asparaitem}
  \item mindestens ein Drittel Zuckerlösung
  \item mindestens halb soviel Wein wie Zuckerlösung
  \item Anteil des Konservierungsmittels mindestens halb so groß wie
    der Anteil der Zuckerlösung und höchstens so groß wie der Anteil
    der Zuckerlösung
  \item Anteil des Konservierungsmittels mindestens die Hälfte des
    Weinanteils
  \end{asparaitem}
  Wir legen folgende Optimierungsvariablen fest:
  \begin{description}
  \item[$x_1$] Anteil der Zuckerlösung
  \item[$x_2$] Anteil des Konservierungsmittels
  \item[$x_3$] Anteil des Weins
  \end{description}
  Die Kosten $1,2x_1+1,8x_2+x_3$ sind unter folgenden Restriktionen zu
  minimieren.
  \begin{align*}
    x_1, x_2, x_3 \geq 0 & x_1+x_2+x_3=1 & x_1\geq \frac{1}{3}\\
    \frac{1}{2}x_1\leq x_2 \leq x_1 & 2x_3\geq x_1 & x_2\geq
    \frac{1}{2} x_3
  \end{align*}
\end{bsp}

\begin{bem*}
  \begin{itemize}
  \item Ungleichungen zwischen Vektoren sind komponentenweise zu
    verstehen
  \item Sinnvollerweise gilt: $l_x\leq u_x, l_A\leq u_A$. Gilt für
    einen Index $i\in\{1, \ldots, n\}\colon l_{x_i}=u_{x_i}$, d.\,h. wir
    fordern $l_{x_i}=x_i=u_{x_i}$, dann ist $x_i$ fest
  \item Das lineare Funktional $\FF=\{x\in\R^n \colon l_x\leq x\leq u_x,
    l_A\leq A^{(1)}\leq u_A, A^{(2)}x=b\}$ heißt zulässige Menge
    (feasible set).
  \item Ein Punkt $\tilde{x}\in\FF$ (zulässiger Punkt) heißt
    \highl{optimal}, falls $\forall x\in\FF\colon f(x)\geq f(\tilde{x})$
    gilt, d.\,h. $\tilde{x}$ ist globales Minimum von $f$ auf $\FF$.
  \end{itemize}
\end{bem*}

\paragraph{Mögliche Transformationen}

\begin{itemize}
\item Sei $a\in\R^n, b\in\R$. Eine Ungleichung vom Typ $a^Tx\leq -b$
  ist äquivalent zu $-a^Tx\geq b$.
\item Eine Gleichung $a^Tx=b$ ist äquivalent zu $-a^Tx=-b$ und
  äquivalent zu $a^Tx\leq b$ und $a^Tx\geq b$.
\item Gibt es für $x_i$ keine Vorzeichenbedingung, dann kann man zwei
  neue Variablen $x^+_i, x^-_i$ mit $x_i=x^+_i-x^-_i$ mit $x^+_i,
  x^-_i\geq 0$ einführen.
\end{itemize}

\begin{bsp}
  Wir betrachten das Problem, $\min x_1-x_2$ unter der
  Nebenbedingungen $x_1\geq 0, x_2\leq 5$. Die Lösung ist mit $x_1=0,
  x_2=5$ eindeutig bestimmt. Um ein Problem mit Vorzeichenbedingungen
  für \emph{alle} Variablen zu erhalten, definieren wir $x^+_2, x^-_2$
  und erhalten das Problem $\min x_1-x^+_2+x^-_2$ mit den
  Nebenbedingungen $x_1\geq 0, x^+_2, x^-_2\geq 0, x^+_2-x^-_2\leq 5$.
\end{bsp}

\begin{defin}
  Ein lineares Optimierungsproblem in kanonischer Form (LPK) ist die Aufgabe:
  \begin{align*}
    \min c^Tx \text{ mit}\\
    Ax\geq b, x\geq0
  \end{align*}
  mit $c\in\R^n, A$ ist eine $m\times n$-Matrix, $b\in\R^m$. In diesem
  Fall ist die zulässige Menge $\FF= \{x\in\R\colon Ax\geq b, x\geq 0\}$.
\end{defin}

\begin{bsp}
  siehe \autoref{bsp:prodplan-16}
  \begin{align*}
    \max 1,5x_1+ x_2+ 2x_3+ 1,4x_4 && \\
    x_1+ x_2+ \leq 1300 &&\\
    x_1+ x_3+ x_4 \leq 2000&&\\
    x_4 \leq 800&&\\
    x_3 \leq 500&&\\
    c=-
    \begin{pmatrix}
      1,5\\1\\2\\1,4
    \end{pmatrix}&
    A=
    \begin{pmatrix}
      -1 & -1 & 0 & 0\\
      -1 & 0 & -1 &-1\\
      0 & 0 & 0 & 1\\
      0 & 0 & -1 & 0
    \end{pmatrix}&
    b=
    \begin{pmatrix}
      -1300\\-2000\\800\\-500
    \end{pmatrix}
  \end{align*}
\end{bsp}

\begin{defin}
  Ein lineares Optimierungsproblem in Normalform oder Standardform ist
  die Aufgabe
  \[\min c^{T} x\]
  unter den Nebenbedingungen
  \[Ax=b, x\geq 0\]
  mit $c\in\R^{n}, b\in\R^{m}$ und $A$ ist eine $m\times n$-Matrix.
  Die zulässige Menge ist $\FF=\set{x\in\R^{n}|Ax=b,x\geq 0 }$
\end{defin}

\begin{bem*}
  \begin{itemize}
   \item Wir können \obda $b\geq 0$ setzen.
   \item Bei $n$ Variablen und $m$ Gleichungsrestriktionen folgt, dass
    ein lineares Gleichungssystem vorliegt.
   \item In der Regel fordert man $m<n$.
  \end{itemize}
\end{bem*}

Wie kann man nun ein Problem in kanonischer Form in Standardform
umwandeln? Die Lösung ist hier die Einführung von
\highl{Schlupfvariablen}.

\begin{bsp}
  Sei folgende Aufgabe gegeben:\\
  Maximiere $x_{1}+x_{2}$ unter den Nebenbedingungen $x_{1}+
  3x_{2}\leq 13, 3x_{1}+x_{2}\leq 15, -x_{1}+x_{2}\leq 3, x_{i}\geq
  0$. Wir führen die \highl[Schlupfvariable]{Schlupfvariablen}
  $x_{3}, x_{4}, x_{5}$ ein und ersetzen die Ungleichung durch:
  \begin{align*}
    x_{1} + 3x_{2} + x_{3} &= 13\\
    3x_{1} + x_{2} + x_{4} &= 15\\
    -x_{1} + x_{2} + x_{5} &= 3\\
    x_{i} &\geq 0\\
  \end{align*}
\end{bsp}

Allgemein lässt sich der Sachverhalt so formulieren:
Sei ein lineares Problem in kanonischer Form gegeben. Um das Problem
in Standardform zu transformieren, führen wir die Schlupfvariablen
\[y=\begin{pmatrix}y_{1}\\\vdots\\y_{m}\end{pmatrix} =
\begin{pmatrix}x_{n+1}\\\vdots\\x_{n+m}\end{pmatrix}\in \R^{m}\]
ein und erhalten somit das Problem
\[\min_{x\in\R^{n}, y\in\R^{m}} c^{T}x =(c^{T}, 0^{T}_{m})
\begin{pmatrix}x\\y\end{pmatrix}\] unter den Nebenbedingungen $x,y\geq
0$, d.\,h.
\[Ax-y=b=(A-I_{m})\begin{pmatrix}x\\y\end{pmatrix}\]

\begin{bsp}
  Frannie verkauft jedes Jahr drei
  Festmeter\footnote{\url{http://de.wikipedia.org/wiki/Festmeter}}
  Holz. Zwei Kunden machen folgendes Angebot. Kunde1 bietet 90 \euro{}
  für einen halben Festmeter und Kunde2 bietet 150~\euro{} für einen
  Festmeter. Seien also $x_{1}$ die Anzahl der Festmeter an Kunde1 und
  $x_{2}$ die Anzahl der Festmeter an Kunde2. Die Einnahmen sollen
  maximiert werden. Damit ergibt sich folgendes Optimierungsproblem:
  \[\min -90x_{1} - 150x_{2}\]
  unter den Nebenbedingungen:
  \[0,5x_{1} + x_{2} \geq 3 \quad x_{i} \geq \]
  Wir betrachten die Geraden $x_{2}=-0,5x_{1} + 3$ und $-90x_{1}-150
  x_{2}=r$
\end{bsp}

\subsection{Grafische Lösung von linearen Optimierungsproblemen}
\label{sec:212-grafLsg}
Hierzu muss man sich auf $n=2$ beschränken und es sind nur
Ungleichungen als Nebenbedingungen zugelassen
\todo{Grafik einfügen}

Zur Bestimmung der Lösungsmenge werden die Randgeraden eingezeichnet
und die richtige Lösungshälfte bestimmt.

\subsection{Lösbarkeit von linearen Optimierungsproblemen}

Hier kann man drei Fälle unterscheiden:
\begin{enumerate}
 \item Lösung ist eindeutig bestimmt
 \item Es gibt unendlich viele Lösungen.
 \item Es gibt keine Lösung.
\end{enumerate}

\subsection{Software zur Lösung}

Zur Lösung von linearen Optimierungsproblemen kann man u.\,a. MATLAB
oder auch Scilab benutzen.

MATLAB ist eine kommerzielle Lösung der Firma The MathWorks, Inc. und
hat die Webseite \url{http://www.mathworks.de/}.

Scilab ist eine kostenlose Entwicklung aus Frankreich und hat die
Webseite \url{http://www.scilab.org/}.

\begin{bsp}
  \todo{Beispiel einfügen}
\end{bsp}

\begin{bsp}
  \todo{Beispiel einfügen}
\end{bsp}
\begin{bsp}
  \todo{Beispiel einfügen}
\end{bsp}
\begin{bsp}
  \todo{Beispiel einfügen}
\end{bsp}

\section{Konvexe Mengen}

\begin{defin}
  Ein Menge $C\subset\R^{n}$ heißt \highl{konvex}\index{Menge!konvexe},
  falls mit $x,y\in C$ und $\alpha\in(0,1)$ auch der Punkt
  $(1-\alpha)x+ \alpha y\in C$ ist.
\end{defin}

\begin{bem*}
  Seien $x,y\in C$. Dann heißt $[x,y] := \set{(1-\alpha)x + \alpha y|
  \alpha\in [0,1]}$ \highl{Verbindungsstrecke}
  von $x$ und $y$. Konvexität heißt $x,y\in C\Rightarrow[x,y]\in C$
\end{bem*}

\begin{bsp}
  Sei $n=1$. Dann sind die konvexen Mengen Intervalle. Für den Fall,
  dass $n$ beliebig ist, gilt:

  Sei $x\in\R^{n}$ und $r\in\R$. Wir betrachten $B(x, r)=\set{y\in\R^{n}|
  \lVert y-x\rVert<r}$ und behaupten, dass $B(x,r)$ konvex ist. Denn
  sei $y,z\in B(x,r)$ und $\alpha\in(0,1)$. Dann folgt:
  \begin{align*}
    \lVert r-x\rVert &= \lVert (1-\alpha)y+\alpha z -x\rVert = \lVert
       (1-\alpha)y+\alpha z -(1-\alpha)x-\alpha x\rVert\\
    &= \lVert (1-\alpha)(y-x)+\alpha(z-x)\rVert \leq \lVert
       (1-\alpha)(y-x)\rVert + \lVert \alpha(z-x)\rVert\\
    &\leq (1-\alpha) \underbrace{\lVert y-x\rVert}_{< r} + \alpha
       \underbrace{\lVert z-x\rVert}_{<r}\\
    &< (1-\alpha+\alpha)r=r
  \end{align*}
\end{bsp}

\begin{bsp}
  Sei $(s,r)\in\R^{n}\times\R, s\neq 0$, wobei $0$ der Nullvektor ist.
  Die Hyperbene $H_{s,r}=\set{x\in\R^{n}| \langle s,x\rangle=r}$ ist
  konvex. Denn sei $y,z\in H_{s,r}$ und $\alpha\in[0,1]$ beliebig.
  Also ist zu zeigen, $v=(1-\alpha)y+\alpha z\in H_{s,r}$, d.\,h.
  $\langle s,v\rangle =r= \langle s,(1-\alpha)y + \alpha z\rangle=
  (1-\alpha)\underbrace{\langle s,y\rangle}_{=r} +\alpha
  \underbrace{\langle s,z\rangle}_{=r} =r$. Gleiches gilt für den
  offenen Halbraum $\set{x\in\R^{n}| \langle s,x\rangle<r}$ und den
  abgeschlossenen Halbraum $\set{x\in\R^{n}| \langle s,x\rangle\leq r}$.
\end{bsp}

\begin{lemma}
  \label{lem:konvDurchsch}
  Ist $(C_{j})_{j\in J}$ eine beliebige Familie konvexer Mengen, dann
  ist auch der Durchschnitt dieser Mengen $C:=\bigcap_{j\in J} C_{j}$
  konvex.

  Denn seien $x,y\in C$ und $\alpha\in[0,1]$ beliebig. Dann folgt für
  alle $j\in J$, dass $x,y\in C_{j}$ und weiter ist $(1-\alpha)x +
  \alpha y\in C_{j}$. Damit gilt aber $(1-\alpha)x + \alpha y\in C$.
\end{lemma}

\begin{bsp}
  Sei $(a^{(i)}, b^{(i)})\in\R^{n}\times\R, i=1,\ldots,s,s+1,\ldots,
  m$. Weiter sei $K=\set{x\in\R^{n}| \langle a^{(i)}, x\rangle =
  b^{(i)}, i=1,\ldots,s, \langle a^{(i)}, x\rangle\leq b^{(i)}, i=s+1,
  \ldots, n}$ die Lösungsmenge eines Systems linearer Gleichungen und
  Ungleichungen. Man nennt $K$ auch \highl{Polyeder}. Es wird
  behauptet, dass $K$ konvex ist.

  Sei $C_{i}= \set{x\in\R^{n}| \langle a^{(i)}, x\rangle=b_{i}}$ mit
  $i=1,\ldots,s$ bzw. $C_{i}= \set{x\in\R^{n}| \langle a^{(i)},
  x\rangle\leq b_{i}}$ mit $i=s+1,\ldots,n$. Dann ist
  $K=\bigcap_{i=1}^{m} C_{i}$ konvex nach
  \autoref{lem:konvDurchsch}.

  Ein lineares Optimierungsproblem in allgemeiner, kanonischer oder
  Standardform ist vom Typ $\min c^{T}x$ \unb $x\in K$, wobei $K$ ein
  Polyeder ist.
\end{bsp}

\begin{satz}
  \label{satz:218-lokglobLsg}
  Ist $c\in\R^{n}$ und $K\subset \R^{n}$ nichtleer und konvex, dann
  ist jede lokale Lösung des Problems $\min c^{T}x$ \unb $x\in K$ auch
  globale Lösung. Weiterhin ist die Lösungsmenge konvex.
  \begin{proof}
    Sei $\tilde{x}\in K$ eine beliebige lokale Lösung. Dann existiert
    ein $r>0$ mit $f(x)\geq f(\tilde{x})$ für alle $x\in K\cap
    B(\tilde{x}, r)$. Sei weiter $y\in K, y\neq \tilde{x}$ beliebig.
    Wir zeigen: $f(y)\geq f(\tilde{x})$. Dazu nutzen wir die
    Konvexität aus. Da $K$ konvex ist, gilt $\tilde{x}+\alpha
    (y-\tilde{x})=(1-\alpha)\tilde{x}+\alpha y\in K$. Weiter gilt für
    hinreichend kleines $\alpha>0$ (speziell $\alpha<\frac{r}{\lVert
    y- \tilde{x}\rVert}$). Für $0<\alpha<\min\{1, \frac{r}{\lvert y-
    \tilde{x}\rVert}\}$ gilt also: $\tilde{x}+\alpha(y-\tilde{x})\in
    K\cap B(\tilde{x}, r)$. Somit folgt:
    $f(\tilde{x}+\alpha(x-\tilde{x})\geq f(\tilde{x})$. Dies ist
    gleichbedeutend mit $c^{T}(\tilde{x}+\alpha(y-\tilde{x}))\geq
    c^{T}\tilde{x}\Rightarrow c^{T}\tilde{x} +\alpha c^{T}x -\alpha
    c^{T} \tilde{x}\geq c^{T}\tilde{x} \Rightarrow \alpha c^{T} y \geq
    \alpha c^{T}\tilde{x} \Rightarrow c^{T}y \geq c^{T}\tilde{x}
    \Rightarrow f(y)\geq f(\tilde{x})$.

    Seien $\tilde{x}, \tilde{y}$ beliebige Lösungen. Dann sind diese
    auch globale Lösungen, d.\,h. $f(\tilde{x})=f(\tilde{y})$. Sei
    $\alpha\in[0,1]$. Nun ist zu zeigen, dass auch
    $(1-\alpha)\tilde{x} +\alpha \tilde{y}$ Lösung ist. Es gilt:
    $f((1-\alpha)\tilde{x}+\alpha\tilde{y})=c^{T} ((1-\alpha)\tilde{x}
    +\alpha\tilde{y})= (1-\alpha)c^{T}\tilde{x} +\alpha c^{T}\tilde{y}
    = (1-\alpha)f(\tilde{x}) + \alpha f(\tilde{y})=
    (1-\alpha)f(\tilde{x}) + \alpha f(\tilde{x}) =f(\tilde{x})$.
  \end{proof}
\end{satz}

\begin{bem*}
  Die Aussage von Satz~\autoref{satz:218-lokglobLsg} gilt allgemein
  für konvexe Zielfunktionen. Dabei heißt eine Funktion $f\colon
  \R^{n}\rightarrow \R$ konvex auf $K$, wenn gilt: $f((1-\alpha) x+
  \alpha y)\leq (1-\alpha)f(x)+\alpha f(y)$ für alle $x,y\in K$ und
  $\alpha\in[0,1]$. Im Spezialfall $f(x)=c^{T}x$ gilt sogar
  $f((1-\alpha) x+\alpha y)=(1-\alpha)x+\alpha y$.
\end{bem*}

\begin{defin}
  Eine \highl{Konvexkombination} von Punkten
  $x^{(1)},\ldots,x^{(k)}\in\R^{n}$ ist ein Punkt der Form
  $\sum_{i=1}^{k} \alpha_{i} x^{(i)}$ mit $\alpha_{i}\geq 0,
  i=1,\ldots,k$ und $\sum_{i=1}^{k} \alpha_{i}=1$.
\end{defin}

\begin{bem*}
  Für $k=2$ gilt: $\alpha_{1} x^{(1)} + \alpha_{2} x^{(2)}$ mit
  $\alpha_{1}, \alpha_{2}\geq 0,
  \alpha_{1}+\alpha_{2}=0\Leftrightarrow \alpha_{1} = 1-\alpha_{2}$. 
\end{bem*}

\begin{satz}
  Eine Menge $C\subset\R^{n}$ ist genau dann konvex, wenn sie alle
  Konvexkombinationen  von Punkten in $C$ enthält.
  \begin{proof}
    Dies stellt eine Beweisidee dar:

    In der Gegenrichtung zeigt man: Wenn $C$ alle Konvexkombinationen
    von Punkten in $C$ enthält, dann enthält sie alle
    Konvexkombinationen von zwei Punkten in $C$. Dann folgt, dass $C$
    konvex ist.

    In der Hinrichtung nimmt man an, dass $C$ konvex ist. Weiter sei
    $x^{(1)},\ldots,x^{k}\in C$ beliebig und
    $\alpha_{1},\ldots,\alpha_{k}\in\R$ mit $\alpha_{i}\geq 0,
    i=1,\ldots, k, \sum_{i=1}^{k} \alpha_{i}=1$. Dann ist zu zeigen,
    dass $\sum_{i=1}^{k} \alpha_{i} x^{(i)}\in C$. Dieser Beweis wird
    per Induktion geführt.
  \end{proof}
\end{satz}

\begin{defin}
  Sei $A\in\R^{n}$ beliebig. Die \highl[Hülle!konvexe]{konvexe
  Hülle} von $A$ ist die Menge
  \[\bigcap_{A\subset C} C\]
  Dies ist die kleinste konvexe Menge, die $A$ umfasst.
\end{defin}

\begin{lemma}
  Für eine beliebige Menge $A\subset\R^{n}$ ist $\co A= \set{x\in\R| x
  \text{ ist Konvexkombination von Punkten in } A} =: B$.
  \begin{proof}
    Für den Fall $\co A\subset B$ gilt, dass $B$ konvex ist und das
    $A\subset B$. Somit folgt, $\co A\subset B$.

    Für $B\subset\co A$ gilt: Sei $C$ eine beliebige konvexe Menge mit
    $C\supset A\Rightarrow C$ enthält alle Konvexkombinationen von
    Punkten in $A$, d.\,h. $C\supset B\Rightarrow B\subset
    \bigcap_{C\supset A} C \subset \co A$.
  \end{proof}
\end{lemma}

\section{Hauptsatz der linearen Optimierung}

Ziel dieses Abschnittes ist, zu zeigen, wenn ein lineares
Optimierungsproblem eine Lösung hat, existiert eine Ecke, die Lösung
ist.

\begin{defin}[geometrische Definition einer Ecke]
  Sei $\FF$ die zulässige Menge eines linearen Optimierungsproblems.
  Dann heißt ein Punkt $x\in\FF$ \highl{Ecke} von $\FF$,
  falls $x$ nicht als Konvexkombination $x=(1-\alpha)y+\alpha z$ mit
  $y,z\in\FF, y\neq z, \alpha\in(0,1)$ darstellbar ist.
\end{defin}

\begin{bem}
  \label{bem:224}
  Ist $\FF$ die zulässige Menge eines linearen Optimierungsproblems
  mit Vorzeichenbedingung $x\geq 0_{n}$ und ist der Nullpunkt
  $0_{n}\in\FF$, dann ist $0_{n}$ eine Ecke von $\FF$. Dann sind
  $y,z\in\FF$ mit $y\neq z, \alpha\in(0,1)$ mit $0_{n}= (1-\alpha)
  y+\alpha z\Rightarrow y=z=0_{n}$.
\end{bem}

\begin{bsp}
  siehe das lineare Optimierungsproblem aus
  \autoref{sec:212-grafLsg}
\end{bsp}

\begin{bsp}
  Sei $\FF\subset\R^{2}$ die zulässige Menge eines linearen
  Optimierungsproblems in kanonischer Form. Es ist definiert durch:
  $2x_{1}+3x_{2}\leq 6, x_{1}\leq 2, x_{i}\geq 0$. 
\end{bsp}

\begin{bsp}
  \todo{Beispiel einfügen}
\end{bsp}

Ab jetzt betrachten wir nur noch lineare Optimierungsprobleme in
Standardform, d.\,h. mit der zulässigen Menge $\FF=\set{x\in\R^{n}|
Ax=b, x\geq 0}$. Dabei ist $A$ eine $m\times n$-Matrix. Die
zulässigen Punkte $x\in\FF$ sind insbesondere Lösung des linearen
Gleichungssystems $Ax=b$.

Es gibt folgende Spezialfälle:
\begin{itemize}
 \item[$n=m$] Hat $A$ den vollen Rang, dann hat das System $Ax=b$
  genau eine Lösung: $x=A^{-1}b$. Gilt für $x$ auch noch $x\geq
  0_{n}\Rightarrow \FF=\{x\}$. Andernfalls ist $\FF=\emptyset$.
 \item[$m>n$] In diesem Falle gibt es mehr Gleichungen als Variablen
  (überbestimmtes System). Dies ist für die Optimierung uninteressant.
 \item[$m<n$] Dieses System hat weniger Gleichungen als Variablen. Die
  Menge $\set{x\in\R^{n}| Ax=b}$ ist dann ein affiner Unterraum.
  Spezialfall: Der Rang der Matrix $A$ ist gleich $m$.
\end{itemize}

\begin{satz}
  \label{satz:228}
  Sei $\FF=\set{x\in\R^{n}| Ax=b, x\geq 0_{n}}, A\in\R^{m\times n},
  b\in\R^{m}, m\leq n$ die zulässige Menge eines linearen
  Optimierungsproblems in Standardform. Dann gilt: $x\in\FF$ ist genau
  dann Ecke von $\FF$, wenn die zu positiven Komponenten von $x$
  gehörenden Spaltenvektoren linear unabhängig sind, d.\,h. wenn die
  Vektoren $A_{.i}$ linear unabhängig sind.
  \begin{proof}
    Ist $x=0_{n}$ (d.\,h. $I(x)=\emptyset$), dann ist $x$ Ecke von
    $\FF$ (\autoref{bem:224}). Sei jetzt $I(x)=\emptyset$ ($x$
    hat mindestens eine Komponente, die größer als 0 ist).
    \begin{itemize}
     \item["`$\Rightarrow$"'] Sei $x$ Ecke von $\FF$. Dann ist zu
      zeigen, dass die Vektoren $A_{.i}$ mit $i\in I(x)$ linear
      unabhängig sind. Angenommen die Vektoren $A_{.i}$ seien linear
      abhängig. Dann zeigen wir: $\exists y,z\in\FF$ mit $y\neq z$ und
      $x=0,5y+0,5z$. Da die Vektoren $A_{.i}$ linear abhängig,
      existieren Zahlen $\alpha_{i}$ mit $\sum_{i\in I(x)}$
      $\alpha_{i} A_{.i}$  und $\gamma := \max_{i\in I(x)} \lvert
      \alpha_{i}\rvert >0$. Sei $\beta:= \min_{i\in I(x)} x
      \Rightarrow \beta>0$. Wir definieren zwei Vektoren:
      \begin{align*}
	y_{i} &= \begin{cases}x_{i}-\frac{\beta}{\gamma}\alpha_{i} &
		   i\in I(x)\\
		   0 & \text{sonst}\end{cases} &
	   z_{i} &= \begin{cases}x_{i}+ \frac{\beta}{\gamma}\alpha_{i}
		      & i\in I(x)\\
		      0 & \text{sonst}\end{cases}
      \end{align*}
      Dann ist $x=0,5y+0,5z$ und weiter $y\neq z$. Nun ist noch zu
      zeigen, dass $y,z\in\FF$, d.\,h. $y,z\geq 0, Ay=b=Az$. Aus der
      Definition von $\beta$ und $\gamma$ erhalten wir:
      \[\left\lvert \frac{\beta}{\gamma} \alpha_{i} \right\rvert
      =\beta \underbrace{\frac{\lvert \alpha_{i}\rvert}{\gamma}}_{\leq 1}
      \leq \beta \Rightarrow y,z\geq 0_{n}\]
      Weiter ist $x_{i}=0$ für $i\notin I(x)$. Damit folgt,
      $Ay=\sum_{i=1}^{n} y_{i} A_{.i} = \sum_{i\in I(x)} y_{i} A_{.i}
      = \sum_{i\in I(x)} \left(x_{i}-\frac{\beta}{\gamma} \alpha_{i}
      \right) A_{.i}= \sum_{i\in I(x)} x_{i} A_{.i}-
      \frac{\beta}{\gamma} \underbrace{\sum_{i\in I(x)} \alpha_{i}
      A_{.i}}_{0_{n}} = \sum_{i=1}^{n} x_{i} A_{.i}=Ax$
      \emph{Widerspruch}
     \item["`$\Leftarrow$"'] Seien die $A_{.i}$ linear unabhängig.
      Dann ist zu zeigen, $x$ ist Ecke von $\FF$. Sei $x=(1-\lambda)
      x^{(1)} +\lambda x^{(2)}$ mit $x^{(1)}, x^{(2)}\in\FF,
      \lambda\in (0,1)$. Dann ist zu zeigen, dass $x^{(1)} = x^{(2)}$.
      Sei nun $j\notin I(x)\Rightarrow 0=x_{j} = (1-\lambda)
      x^{(1)}_{j} + \lambda x^{(2)}_{j} \Rightarrow x^{(1)}_{j} =
      x^{(2)}_{j} =0$. Es gilt, $Ax^{(1)}=b, Ax^{(2)}=b\Rightarrow
      0=b-b = Ax^{(1)}-Ax^{(2)}=A(x^{(1)}-x^{(2)})$ und weiter
      $0=\sum_{i=1}^{n} (x^{(1)}_{i} -x^{(2)}_{i}) A_{.i}= \sum_{i\in
      I(x)} (x^{(1)}_{i} -x^{(2)}_{i}) A_{.i}$. Da die Vektoren
      $A_{.i}$ linear unabhängig sind, folgt $x^{(1)}_{i} -x^{(2)}_{i}
      =0 \Rightarrow x^{(1)}_{i}=x^{(2)}_{i}$. Somit ist gezeigt, dass
      $x^{(1)}=x^{(2)}$.
    \end{itemize}
  \end{proof}
\end{satz}

\begin{bem*}
  Bei einer Auswahl von Spaltenvektoren von $A$ können höchstens $m$
  linear unabhängig sein, d.\,h. $I(x)$ hat höchtens $m$ Elemente und
  es folgt, dass eine Ecke höchtens $m$ positive Komponenten haben kann.
\end{bem*}

\begin{bsp}
  siehe die Beispiele 2.27 bzw. 2.28  \todo{Referenz einfügen}

  Die Ecken der zulässigen Mengen sind anschaulich die Punkte:
  \begin{align*}
    x^{(1)} &= \begin{pmatrix}0\\0\\3\end{pmatrix} & x^{(2)} &=
       \begin{pmatrix}6\\0\\0\end{pmatrix} & x^{(3)} &=
       \begin{pmatrix}0\\3\\0\end{pmatrix}
  \end{align*}
  Es gilt: $A=(\nicefrac{1}{2}, 1, 1), b=2, m=1, n=3$.
\end{bsp}

\begin{bsp}[siehe Beispiel 2.26\todo{Referenz}]
  Die zulässige Menge in kanonischer Form wird definiert durch:
  \begin{align*}
    A &= \begin{pmatrix}-2 & -3\\-1 & 0\end{pmatrix} & b &=
       \begin{pmatrix}-6\\-2\end{pmatrix}
  \end{align*}
  Um \autoref{satz:228} anzuwenden, müssen wir das Problem in
  Standardform bringen:
  \[\tilde{A} = \begin{pmatrix}-2 & -3 & -1 & 0\\
		  -1 & 0 & 0 -1\end{pmatrix}\]
  Kandidaten für die Ecken sind:
  \begin{align*}
    x^{(1)} &= \begin{pmatrix}0\\0\\6\\2\end{pmatrix} &
       x^{(2)} &= \begin{pmatrix}2\\0\\2\\0\end{pmatrix} &
       x^{(3)} &= \begin{pmatrix}0\\2\\0\\2\end{pmatrix} &
       x^{(4)} &= \begin{pmatrix}2\\\nicefrac{2}{3}\\0\\0\end{pmatrix}
  \end{align*}
  Sei $x\in\FF_{k}$ und $s=Ax-b\geq 0\Rightarrow
  \begin{pmatrix}x\\s\end{pmatrix}\in \FF$ zulässige Punkte für die
  Standardform.
\end{bsp}

\begin{bem}
  Sei $\FF_{k}=\set{x=(x_{1},\ldots,x_{n})^{T}\in\R^{n}| x\geq 0,
  Ax\geq b}$ die zulässige Menge eines linearen Optimierungsproblems
  in kanonischer Form. Transformiert man das Problem in Standardform,
  erhält man die zulässige Menge:
  \[\FF =\set{\tilde{x} =(x_{1},\ldots,x_{n}, \underbrace{x_{n+1},
  \ldots,x_{n+m}}_{=s})^{T}\in\R^{n\times m}| \tilde{x}\geq 0,
  (A-I_{m}) x=b}\]
  Dann gilt:
  \begin{itemize}
   \item $x\in\FF_{k}$ und $s=Ax-b\Leftrightarrow
    \begin{pmatrix}x\\s\end{pmatrix}\in\FF$
   \item $x\in\FF_{k}$ ist Ecke von $\FF_{k}\Leftrightarrow
    \begin{pmatrix}x\\s\end{pmatrix}$ mit $s=Ax-b$ ist Ecke von $\FF$.
  \end{itemize}
\end{bem}

\begin{satz}
  \label{satz:232}
  Die zulässige Menge $\FF=\set{x\in\R^{n}| x\geq 0, Ax=b}$ eines
  linearen Optimierungsproblems in Standardform sei nicht leer. Dann
  existiert mindestens eine und höchstens endlich viele Ecken von $\FF$.
  \begin{proof}
    \todo{Beweis einfügen}
  \end{proof}
\end{satz}

\begin{lemma}
  \label{lem:233}
  Ist $\FF=\set{x\in\R^{n}| Ax=b, x\geq 0}\neq\emptyset$ zulässige
  Menge eines linearen Optimierungsproblems in Standardform. Dann ist
  $\FF$ genau dann beschränkt, wenn es kein $d\in\R^{n}\setminus
  \{0_{n}\}$ gibt mit $Ad=0, d\geq 0$.
  \begin{proof}
    \todo{Beweis einfügen}
  \end{proof}
\end{lemma}

\begin{satz}
  Sei $\FF=\set{x\in\R^{n}| Ax=b, x\geq 0}\neq\emptyset$ und
  $v^{(1)},\ldots,v^{k}$ Ecken von $\FF$. Jeder Punkt $x\in\FF$ hat
  eine Darstellung:
  \begin{gather*}
    x=\sum_{i=1}^{k} \alpha_{i} v^{(i)}_{k} +d
  \end{gather*}
  mit $\alpha_{i}\geq 0, i=1,\ldots,k, \sum_{i=1}^{k} \alpha_{i}=1,
  d\geq 0, Ad=0$. Ist $\FF$ beschränkt, gilt:
  \begin{gather}
    \label{eq:234}
    \FF=\co\{v^{(1)},\ldots,v^{(k)}\}
  \end{gather}
  \begin{proof}
    Sei $p_{0}$ die Minimalzahl der positiven Komponenten von Vektoren
    $x\in\FF$. Der Beweis wird nach dem Prinzip der vollständigen
    Induktion geführt:
    \begin{itemize}
     \item[Induktionsanfang] $p=p_{0}$
     \item[Induktionsvoraussetzung] Es gelte \autoref{eq:234}
      für $x\in\FF$ und höchstens $n-1$ positiven Komponenten.
     \item[Induktionsbeweis] Sei $x\in\FF$ ein Vektor mit genau $p$
      positiven Komponenten ($I(x):= \set{1\leq i\leq n| x_{i}\geq
      0}$). Sei nun $x$ keine Ecke von $\FF$. Nach \autoref{satz:228}
      folgt, dass die $A_{.i}$ linear abhängig sind, d.\,h. $\exists
      w\in\R^{n}\setminus\{0\}$ mit $w_{i}=0$ für $i\notin I(x)$ und
      $\sum_{i=1}^{n} w_{i} A_{.i} =0$.
      \begin{enumerate}[1.~F{a}ll]
       \item Es gibt mindestens ein $w_{j}<0$ und $w_{k}>0$. Dann sind
	$x^{(1)}, x^{(2)}\in\FF$ und haben höchstens $p-1$ positive
	Komponenten. Nach der Induktionsvoraussetzung haben die
	$x^{(1)}, x^{(2)}$ eine Darstellung der Form:
			\begin{gather*}
			  x^{(i)} = \sum_{j=1}^{k} \alpha_{j}^{(i)}
			     v^{(j)} + d^{(i)} \qquad i=1,2
			\end{gather*}
	mit $\alpha_{j}^{(i)}\geq 0, \sum_{j=1}^{k}
	\alpha_{j}^{(i)}=1, d^{(i)} \geq 0, Ad^{(i)}=0$. Weiter gilt:
		\begin{align*}
		  x &= \underbrace{\frac{\lambda_{2}}{\lambda_{1} +
		     \lambda_{2}}}_{=(1-\mu)} x^{(1)} +
		     \underbrace{\frac{\lambda_{1}}{\lambda_{1} +
		     \lambda_{2}}}_{=\mu\in(0,1)} x^{(2)} (1-\mu)
		     x^{(1)} + \mu x^{(2)}\\
		  &= (1-\mu) \sum_{j=1}^{k} \alpha_{j}^{(i)} v^{(j)}
		     d^{(i)} +\mu \sum_{j=1}^{k} \alpha_{j}^{(i)}
		     v^{(j)} + d^{(i)}\\
		  &= \left(\sum_{j=1}^{k} \underbrace{(1-\mu)
		     \alpha_{j}^{(i)} + \mu \alpha_{j}^{(i)}}_{=:
		     \alpha_{j}}\right) v^{(j)} + (1-\mu) d^{(1)} +
		     \mu d^{(2)}\\
		  &\Rightarrow \alpha_{j}\geq 0, \sum_{j=1}^{k}
		     \alpha_{j}=1
		\end{align*}
	Für $d:= (1-\mu) d^{(1)} + \mu d^{(2)}\geq 0$ und $Ad=(1-\mu)A
	d^{(1)} +\mu Ad^{(2)}$.
       \item $w\geq 0$\\
	Dann definiert man $x^{(1)}$ und $\lambda_{1}$ wie oben. Dann
	ist $x^{(1)}\in\FF$ und hat höchstens $p-1$ positive
	Komponenten. Mit der Anwendung der Induktionsvoraussetzung auf
	$x^{(1)}$ folgt $x=x^{(1)}+\lambda_{1} w= \sum_{j=1}^{k}
	\alpha_{j}^{(1)} v^{(j)} + d^{(1)} + \lambda_{1} w$. Für $d:=
	d^{(1)} +\lambda_{1} w$ gilt $d\geq 0$ und $Ad^{(1)}
	+\lambda_{1} w=0$.
       \item $w\leq 0$\\
	Man definiert $x^{(2)}$ und $\lambda_{2}$ wie oben. Die
	Beweisführung erfolgt wie im zweiten Fall.
      \end{enumerate}
      Sei $\FF$ nun  beschränkt. Dann folgt nach \autoref{lem:233},
      dass es kein $d\in\R^{n}\setminus \{0\}$ mit $d\geq 0$ und
      $Ad=0$ gibt. Damit hat jedes $x\in\FF$ eine Darstellung wie in
      \autoref{eq:234} mit $d=0_{n}$, d.\,h. für alle $x\in\FF$
      ist Konvexkombination der Ecken $v^{(1)},\ldots,v^{k}\in\FF
      \Rightarrow \FF\subseteq \co \{v^{(1)},\ldots,v^{(k)}\}$. Da
      $\FF$ konvex und $v^{(1)},\ldots,v^{(k)}\in\FF\Rightarrow \FF
      \supseteq \co\{v^{(1)},\ldots,v^{(k)}\}$.
    \end{itemize}
  \end{proof}
\end{satz}

\begin{satz}[Hauptsatz der linearen Optimierung]
  Sei $A\in\R^{m\times n}, m\leq n, b\in\R^{m}$. Wir betrachten das
  lineare Optimierungsproblem in Standardform:
  \begin{gather*}
    \min c^{T}x \text{ \unb } x\in\FF=\set{x\in\R^{n}| Ax=b, x\geq 0}
  \end{gather*}
  Dann gilt:
  \begin{enumerate}[(a)]
   \item Entweder ist eine der endlich vielen Ecken des linearen
    Optimierungsproblems in Standardform Lösung oder die Zielfunktion
    ist auf $\FF$ nicht nach unten beschränkt.
   \item Ist $\FF$ beschränkt, dann hat das lineare
    Optimierungsproblem in Standardform mindestens eine Lösung. Ein
    $x\in\FF$ ist genau dann Lösung, wenn gilt: $x$ ist
    Konvexkombination von Ecken, die ebenfalls Lösung sind.
    \begin{proof}
      Es existiere ein $d\in\R^{n}$ mit $d\geq 0, Ad=0$ und
      $c^{T}d\leq 0$. Dann ist die Zielfunktion auf $\FF$ nicht nach
      unten beschränkt. Denn sei $x\in\FF$ beliebig. Dann folgt für
      alle $t\geq 0\colon x+td\in\FF\Rightarrow c^{T}(x+td) = c^{T}x
      +tc^{T}d\xrightarrow{t\rightarrow\infty} -\infty$.

      Betrachten jetzt $\forall d\in\R^{n}$ mit $d\geq 0, Ad=0\colon
      c^{T}d\geq 0$. Es ist zu zeigen, dass mindestens eine der Ecken
      von $\FF$ Lösung ist. Seien $v^{(1)},\ldots,v^{(k)}$ Ecken. Nach
      \autoref{satz:234} hat jedes $x\in\FF$ die Darstellung:
      \begin{align*}
	x &= \sum_{i=1}^{k} \alpha_{i} v^{(i)} +d\\
	\Rightarrow c^{T}x &= c^{T} \left(\sum_{i=1}^{k} \alpha_{i}
	   v^{(i)} +d \right) = \sum_{i=1}^{k}\alpha_{i} c^{T} v^{(i)}
	   + \underbrace{c^{T}d}_{\geq 0}
      \end{align*}
      Sei $\mu:= \min_{i=1,\ldots,k} (c^{T} v^{(i)})$. Dann gilt
      $c^{T}x \geq \sum_{i=1}^{k} \alpha_{i} c^{T} v^{(i)} \geq
      \sum_{i=1}^{k} \alpha_{i} \mu = \mu \sum_{i=1}^{k} \alpha_{i}
      \mu$. Somit ist jede Ecke $v^{(j)}$ von $\FF$ mit
      $c^{T}v^{(j)}$ Lösung und der erste Teil des Satzes ist gezeigt.

      Für den zweiten Teil des Satzes sei nun $\FF$ beschränkt. Da
      $\FF$ weiterhin auch abgeschlossen ist, folgt, dass $\FF$
      kompakt. Die Zielfunktion $f(x)=c^{T}x$ ist stetig. Nach dem
      Satz vom Weierstraß nimmt $f$ auf $\FF$ ihr Minimum an.

      Sei $x$ beliebige Lösung des linearen Optimierungsproblems in
      Standardform. Dann ist zu zeigen, dass das $x$ eine
      Konvexkombination ist. Nach \autoref{satz:234} gilt, $x=
      \sum_{i=1}^{k} \alpha_{i} v^{(i)}$ mit $\alpha_{i}\geq 0,
      \sum_{i=1}^{k} \alpha_{i}=1, i=1,\ldots,k$. Wir zeigen,
      $\alpha_{i}=0$ für alle nichtoptimalen Ecken. Wissen
      $\mu=c^{T}x= \sum_{i=1}^{k} \alpha_{i} c^{T} v^{(i)}=
      \min_{i=1,\ldots,k} c^{T} v^{(i)}$. Denn eine optimale Lösung
      existiert. Sei jetzt $j\in\{1,\ldots,k\}$ für den $v^{(j)}$
      nicht optimal ist. Dann ist $c^{T} v^{(j)} =\mu + \epsilon$ mit
      $\epsilon>0\Rightarrow \mu= \sum_{i=1}^{k} \alpha_{i}
      \underbrace{c^{T}v^{(i)}}_{\geq \mu} \geq \sum_{i=1}^{k}
      \alpha_{i} \mu +\alpha_{i} \epsilon=\mu+\alpha_{j}
      \epsilon\Rightarrow \alpha_{j}=0$. Es ist nun noch zu zeigen,
      dass, wenn $x$ keine Konvexkombination ist, $x$ dann optimal
      ist. Sei $x\leq \sum_{j=1}^{p} \alpha_{ij} v^{(ij)}, p\leq k$
      und optimale Ecken $v^{(ij)}, j=1,\ldots,p$ und $\alpha_{ij}\geq
      0, j=1,\ldots,p, \sum \alpha_{ij}=1$. Es gilt: $c^{T}v^{(ij)}
      =\mu\Rightarrow c^{T} x=\sum_{j=1}^{p} \alpha_{ij}
      \underbrace{c^{T}v^{(ij)}}_{=\mu} =\mu$.
    \end{proof}
  \end{enumerate}
\end{satz}

\section{Basislösungen}

Sei $J_{B}\subset \{1,\ldots,n\}, J_{N}=\{1,\ldots,n\}\setminus
J_{B}$. Somit ist $J_{B}\cup J_{N} =\{1,\ldots,n\}$ und $J_{B}\cap
J_{N}=\emptyset$. Für einen Vektor $x\in\R^{n}$ bezeichnen wir mit:
\begin{itemize}
 \item $x_{B}$ den Vektor mit den Komponenten $x_{i}, i\in J_{B}$
 \item $x_{N}$ den Vektor mit den Komponenten $x_{i}, i\in J_{N}$
\end{itemize}
Entsprechend bezeichnen wir für eine $m\times n$-Matrix $A$ mit:
\begin{itemize}
 \item $A_{B}$ die Matrix mit den Spaltenvektoren $A_{.i}, i\in J_{B}$
 \item $A_{N}$ die Matrix mit den Spaltenvektoren $A_{.i}, i\in J_{N}$
\end{itemize}
Sei $m\leq n$ und $A\in\R^{m\times n}$ mit vollem Zeilenrang $m,
b\in\R^{m}$. Weiter sei $J_{B}\subset\{1,\ldots,n\}$ mit genau $m$
Elementen, so dass die Matrix $A_{B}$ invertierbar ist. Wie oben sei
$J_{N}$. Wir definieren $B, N$ durch:
\begin{align*}
  B &:= \set{A_{.i}| i\in J_{B}}\\
  N &:= \set{A_{.i}| i\in J_{N}}
\end{align*}

\begin{bsp}
  \label{bsp:236}
  Sei:
  \begin{align*}
    (A|b) &= \begin{pmatrix}0 & 1 & 5 & 1 & | & 2\\
	       1 & 0 & -1 & 0 & | & 2\\
	       1 & 0 & 2 & 1 & | & 3\end{pmatrix} & m &= 3 & n &= 4
  \end{align*}
  Wir wählen $J_{B}=\{1,2,4\}\Rightarrow J_{N}=\{3\}$. Die Matrix
  $A_{B}= \begin{pmatrix}0&1&1\\ 1&0&0\\ 1&0&1\end{pmatrix}$ ist
  invertierbar.
  \begin{align*}
    A_{N} &= \begin{pmatrix}5\\-2\\1\end{pmatrix} & x_{B} &=
       \begin{pmatrix}x_{1}\\x_{2}\\x_{4}\end{pmatrix} & x_{N} &=
       (x_{3})
  \end{align*}
\end{bsp}

Sei $x\in\R^{n}$ eine Lösung des linearen Systems $Ax=b$. Dann gilt:
\begin{align}
  \label{eq:25}
  b &= Ax =(A_{B}| A_{N})\begin{pmatrix}x_{B}\\x_{N}\end{pmatrix} =
     A_{B} x_{B}+ A_{N} x_{N}\nonumber\\
  x_{B} &= A_{B}^{-1} (b-A_{N}x_{N})
\end{align}
Das bedeutet, dass man das System nach $x_{B}$ auflösen kann und es
folgt, dass die Lösungsmenge
\begin{gather*}
  \set{x\in\R^{n}| x_{N}\in\R^{n-m}, x_{B}=A_{B}^{-1}(b-A_{N}x_{N})}
\end{gather*}
ein Unterraum des $\R^{n}$ ist. Die Dimension des Lösungsraumes ist
$n-m$. In jedem $x_{N}\in\R^{n-m}$ existiert genau eine Lösung, wobei
$x_{B}$ durch die \autoref{eq:25} bestimmt ist.

Für die Optimierung ist die Lösung $x$ zu $x_{N}=0_{n-m}$ von
besonderem Interesse, d.\,h. $x_{B}=A_{B}^{-1}b\Leftrightarrow A_{B}
x_{B}=b$.

\begin{defin}[Basislösung, Basisvariablen, Nichtbasisvariablen]
  Sei $m\leq n, A\in\R^{m\times n}$, wobei der Rang der Matrix $A$
  gleich $m$ ist. Weiter sei $J_{B}\subset\{1,\ldots,n\}$ eine
  Indexmenge, so dass $A_{B}$ invertierbar ist. Der eindeutig
  bestimmte Vektor $x$ mit $A_{B} x_{B}=b, x_{N}=0$ heißt
  \highl{Basislösung} des Systems $Ax=b$ zur Basis
  $B$. Die Variablen $x_{B}$ heißen
  \highl{Basisvariablen}, $x_{N}$ heißen
  \highl{Nichtbasisvariablen}. Die
  Basislösung $x$ zur Basis $B$ heißt
  \highl[Basislösung!zulässige]{zulässige Basislösung}, falls $x\geq
  0$.
\end{defin}

\begin{bem*}
  Wegen $x_{N}=0$ hat eine Basislösung höchstens $m$ von Null
  verschiedene Komponenten. Für die Zulässigkeit eines Basislösung
  reicht es zu fordern, dass $x_{B}\geq 0_{n}$ ist.
\end{bem*}

\begin{bsp}
  Es sei ein lineares Optimierungsproblem in kanonischer Form mit den
  Nebenbedingungen $Ax\geq b$ gegeben. Zur Transformation auf die
  Standardform benutzen wir die Schlupfvariablen
  $x_{n+1},\ldots,x_{n+m}$:
  \begin{gather*}
    \underbrace{(A-I_{m})}_{=: \tilde{A}} \begin{pmatrix}x_{1}\\
					    \vdots \\
					    x_{n+m}\end{pmatrix} = b
  \end{gather*}
  Der Rang der Matrix $\tilde{A}$ ist $m$. Hier bietet es sich an,
  $J_{B}= \{n+1,\ldots,n+m\}, J_{N}=\{1,\ldots,n\}$ zu wählen, d.\,h.
  $A_{B}=-I_{m}, B=(-e^{1},\ldots,-e^{m})$, wobei $e_{i}$ der $i$-te
  Einheitsvektor ist.\\
  zulässige Basislösung:
  \begin{align*}
    x_{N} &= \begin{pmatrix}x_{1}\\\vdots\\x_{N}\end{pmatrix} = 0_{n}\\
    x_{B} &= A_{B}^{-1} b = -I_{m}^{-1} b= -I_{m}b=-b
  \end{align*}
  Das bedeutet, $x=(0,\ldots,0,-b_{1},\ldots,b_{m})^{T}$ und $x$ ist
  zulässig, wenn $x_{B}=-b\geq 0_{m}$, d.\,h. $b\leq 0_{m}$.
\end{bsp}

\begin{bsp}[siehe \autoref{bsp:236}]
    \begin{align*}
    (A|b) &= \begin{pmatrix}0 & 1 & 5 & 1 & | & 2\\
	       1 & 0 & -1 & 0 & | & 2\\
	       1 & 0 & 2 & 1 & | & 3\end{pmatrix} & m &= 3 & n &= 4
  \end{align*}
  Wir wählen $J_{B}=\{1,2,4\}, J_{N}=\{3\}$. Dann ist
  $A_{B}=\begin{pmatrix}0 & 1 & 1\\1 & 0 & 0\\1 & 0 & 1\end{pmatrix}$.
  Die eindeutig bestimmte Lösung zur Basis $B$ ist gegeben durch
  $x_{N}=(x_{3})=0, x_{B}=A_{B}^{-1}\Leftrightarrow A_{B} x_{B}=b$
\end{bsp}

\begin{satz}[allgemeine Charakterisierung einer Ecke]
  Sei $\FF= \set{x\in\R^{n}| Ax=b, x\geq 0}$ zulässige Menge eines
  linearen Optimierungsproblems in Standardform, $A\in\R^{m\times n},
  m\leq n, \rg(A)=m, b\in\R^{m}$. Ein Punkt $x\in\FF$ ist Ecke von
  $\FF$ genau dann, wenn $x$ zulässige Basislösung ist.
  \begin{proof}
    \begin{itemize}
     \item["`$\Rightarrow$"'] Sei $x$ Ecke von $\FF$ und $I(x)=
      \set{1\leq i\leq n| x_{i}>0}$. Nach der Voraussetzung ist $b=Ax=
      \sum_{i=1}^{n} x_{i} A_{.i} =\sum_{i\in I(x)} x_{i} A_{.i}$.
      Nach \autoref{satz:228} sind die Spaltenvektoren $A_{.i}$
      linear unabhängig. Es sei $\tilde{B}:= \set{A_{.i}| i\in I(x)}$
      und $p$ die Anzahl der Elemente von $\tilde{B}$. Falls $p=m$,
      dann isr $\tilde{B}$ Basis von $A$ und $x$ ist zugehörige
      Basislösung. Da $x\in\FF$, ist $x$ zulässig und es folgt, dass
      $x$ zulässige Basislösung ist. Ist nun $p<m$, dann kann man
      $\tilde{B}$ wegen $\rg(A)=m$ zu einer Basis von $A$ ergänzen und
      $x$ ist wie oben zulässige Basislösung.
     \item["`$\Leftarrow$"'] Sei $x$ zulässige Basislösung zu einer
      Basis $B$ von $A$. Dann ist $x_{N}=0_{n-m}\Rightarrow
      I(x)\subset J_{B}$. Die Spaltenvektoren $A_{.i}, i\in J_{B}$
      sind linear unabhängig. Damit sind auch die Spaltenvektoren
      $A_{.i}, i\in I(x)$ linear unabhängig und nach
      \autoref{satz:228} ist $x$ eine Ecke.
    \end{itemize}
  \end{proof}
\end{satz}

\begin{kor}
  Die Voraussetzungen seien wie oben. Dann existiert mindestens eine
  Ecke von $\FF$ und höchsten $\binom{n}{m}$ Ecken.
  \begin{proof}
    Die Existenz einer Ecke ergibt sich aus \autoref{satz:232}. Es
    existieren $\binom{n}{m}$ Möglichkeiten um Spaltenvektoren
    auszuwählen. Damit gibt es höchsten $\binom{n}{m}$ Basislösungen.
  \end{proof}
\end{kor}

\begin{bsp}
  \begin{align*}
    A &= (\nicefrac{1}{2}, 1, 1) & b &= (3) & m &= 1 & n &= 3
  \end{align*}
  Es gibt höchstens $\binom{n}{m}=3$ Basislösungen. Für
  $B=B_{1}=\{A_{.1}\}$ gilt $A_{B}^{-1} b=2\cdot 3=6\Rightarrow
  x^{(1)}=(6, 0, 0)^{T}$. Für $B=B_{2}=\{A_{.2}\}$ gilt $A_{B}^{-1}
  b=3\Rightarrow x^{(2)}=(0,3,0)^{T}$. Für $B=B_{3}=\{A_{3}\}$ gilt
  $A_{B}^{-1}b=3\Rightarrow x^{(3)}=(0,0,3)^{T}$. Es gilt $x^{i}\geq
  0, i=1,2,3\Rightarrow$ alle Basislösungen sind zulässig und Ecken.
\end{bsp}

\begin{bsp}
  \begin{align*}
    A &= \begin{pmatrix}-2 & -3 & -1 & 0\\-1 & 0 & 0 & -1\end{pmatrix}
       & b &= \begin{pmatrix}-6\\-2\end{pmatrix} & n &= 4 & m &= 2
  \end{align*}
  Damit gibt es höchstens sechs Basislösungen:
  \begin{align}
    B_{1} &= \{A_{.1}, A_{.2}\} &\colon A_{B}^{-1} b &=
       \begin{pmatrix}2\\\nicefrac{2}{3}\end{pmatrix} & x^{(1)} &=
       (2,\nicefrac{2}{3},0,0)^{T}\\
    B_{2} &= \{A_{.1}, A_{.3}\} &\colon A_{B}^{-1} b &=
       \begin{pmatrix}2\\2\end{pmatrix} & x^{(2)} &=
       (2,0,2,0)^{T}\\
    B_{3} &= \{A_{.1}, A_{.4}\} &\colon A_{B}^{-1} b &=
       \begin{pmatrix}3\\-1\end{pmatrix} & x^{(3)} &=
       (3,0,0,-1)^{T}\\
    B_{4} &= \{A_{.2}, A_{.3}\} &\colon A_{B}^{-1} b &
        &  &  \text{keine Basislösung}\\
    B_{5} &= \{A_{.2}, A_{.4}\} &\colon A_{B}^{-1} b &=
       \begin{pmatrix}2\\2\end{pmatrix} & x^{(5)} &=
       (0,2,0,2)^{T}\\
    B_{6} &= \{A_{.3}, A_{.4}\} &\colon A_{B}^{-1} b &=
       \begin{pmatrix}6\\2\end{pmatrix} & x^{(6)} &=
       (0,0,6,2)^{T}
  \end{align}
  Außer den Gleichungen 2.5 (negative Komponente) und 2.6 sind alle
  anderen Gleichungen Ecken.
\end{bsp}

Bei der Lösung eines linearen Optimierungsproblems können folgende
Situationen auftreten:
\begin{enumerate}
 \item $\FF=\emptyset$: Dann existiert keine zulässige Basislösung.
 \item $\FF\neq\emptyset$ und die Zielfunktion ist auf $\FF$ nach
  unten beschränkt: Dann existiert mindestens eine Ecke. die Lösung ist.
 \item $\FF\neq\emptyset$ und die Zielfunktion ist nicht nach unten
  beschränkt.
\end{enumerate}

Für den zweiten Fall kann man folgende "`naive"' Strategie anwenden:
\begin{itemize}
 \item Bestimme alle möglichen Basislösungen
 \item Teste auf Zulässigkeit und erhalte so alle Ecken
 \item Bestimme die Ecke mit dem minimalen Funktionswert
\end{itemize}

\chapter{Das Simplexverfahren}

Das Simplexverfahren wurde 1947 von George Dantzig entwickelt. Der
amerikanische Mathematiker lebte 1914 bis 2005 und war Mitbegründer
der Mathematical Programming Society.

Der Ausgangspunkt für das Simplexverfahren ist ein lineares
Optimierungsproblem in Standardform $\min c^{T}x$ \unb $Ax=b, x\geq 0$
mit $A\in\R^{m\times n}, c\in\R^{n}m b\in\R^{m}$. Als Voraussetzung
soll $A$ den vollen Zeilenrang besitzen und es ergibt sich die
folgende Vorgehensweise:
\begin{enumerate}
 \item Man bestimme die Ausgangsecke (Phase 1).
 \item Iteration: Ist die aktuelle Ecke optimal, dann stoppe.
 \item Falls die Ecke nicht optimal ist, berechne eine neue bessere
  Ecke. Dies wird solange wiederholt, bis die optimale Ecke gefunden
  wurde (Phase 2), wobei die Ecken zulässige Basislösungen sind.
\end{enumerate}

\section{Grundlagen des Simplexverfahren}

Ist $x^{*}$ Ecke zur Basis $B$, dann gilt: $x_{N}^{*}=0_{n-m},
x_{B}=A_{B}^{-1}(b-A_{N}x_{N}$. In der Phase zwei ergibt sich folgende
Vorgehensweise:
\begin{itemize}
 \item Test, ob Ecke $x^{*}$ optimal
 \item Falls $x^{*}$ nicht optimal, Test, ob Zielfunktion auf
  zulässiger Menge nach unten beschränkt ist. Falls nein, existiert
  keine Lösung und das Programm kann abbrechen.
 \item Berechnung einer neuen Ecke mit dem Ziel, dass der
  Zielfunktionswert in der neuen Ecke kleiner ist.
\end{itemize}

\subsection{Reduktion der Variablen}

Wir betrachen das System $Ax=b$ mit $B$ Basis von $A$. Dann können wir
das System nach $x_{B}$ auflösen:
\begin{gather}\label{eq:26}
  x_{B}= A_{B}^{-1}(b-A_{N}x_{N}) \qquad x_{N}\in\R^{n-m}
\end{gather}
Somit kann das Ausgangsproblem mit den $n$ Variablen auf ein Problem
in den Variablen $x_{N}\in\R^{n-m}$ reduziert werden.

Das Einsetzen in die Zielfunktion ergibt:
\begin{align*}
  c^{T}x &= c_{B}^{T} x_{B} c_{N}^{T} x_{N} = c_{B}^{T} A_{B}^{-1}
     (b-A_{N}x_{N}) + c_{N}^{T}x_{N}\\
  &= \underbrace{c_{B}^{T} A_{B}^{-1} b}_{=: z_{B}} +
     (\underbrace{c_{N}^{T} - c_{B}^{T} A_{B}^{-1} A_{N}}_{ =:
     s_{N}^{T}})x_{N}
\end{align*}
Dabei ist $z_{B}$ konstant und $s_{N}$ heißt Vektor der reduzierten
Kosten. Mit:
\begin{gather}\label{eq:27}
  s_{N}^{T} = c_{N}^{T} - c_{B}^{T} A_{B}^{-1} A_{N} \in\R^{n-m}
\end{gather}
gilt
\begin{gather}\label{eq:33}
  c^{T}x = z_{B} + s_{N}^{T} x_{N}
\end{gather}

Die Vorzeichenbedingung $x\geq 0$ ist äquivalent zu $x_{B}\geq 0,
x_{N}\geq 0\Leftrightarrow A_{B}^{-1} (b-A_{N}x_{N})\geq 0
\Leftrightarrow A_{B}^{-1} A_{N} x_{N}\leq A_{B}^{-1}b$. Damit
erhalten wir das zum linearen Problem äquivalente, reduzierte Problem
\begin{gather}\label{eq:29}
  \min s_{N}^{T} v + z_{B}\\
  \text{ \unb} v\in\FF_{B}=\set{v\in\R^{n-m}| v\geq 0, A_{B}^{-1}
     A_{N}v \leq A_{B}^{-1}b}
\end{gather}
Die Konstante $z_{B}$ wird in der Regel weggelassen.

\begin{lemma}
  \label{lem:31}
  Die Probleme seien wie oben definiert. Dann gilt:
  \begin{enumerate}[(i)]
   \item Ist $v\in\FF_{B}$ und ist $x\in\R^{n}$ definiert durch
    $x_{N}=v, x_{B}=A_{B}^{-1} (b-A_{N}v)$. Dann ist $x\in\FF$. Ist
    nun $x\in\FF$ und definiert man $v=x_{N}$, dann ist $v\in\FF_{B}$.

    Somit hat man eine eineindeutige Zuordnung: $\FF_{B}\ni
    v\Leftrightarrow x\in\FF$ mit $x_{N}=v, x_{B}=A_{B}^{-1} (b-A_{N}
    x_{N})$.
    \begin{proof}
      klar
    \end{proof}
   \item Ist $v^{*}\in\FF_{B}$ Lösung des reduzierten Problems und ist
    $x^{*}$, definiert durch $x_{N}^{*}=v^{*}, x^{*}_{B} = A_{B}^{-1}
    (b-a_{N}v^{*})$, Lösung des linearen Optimierungsproblems.
    \todo{Satzbau verbessern, nachschlagen}
    \begin{proof}
      Sei $v^{*}\in\FF_{B}$ Lösung von \autoref{eq:29}, $x^{*}$ definiert
      durch $x^{*}_{N}=v, x_{B}^{*}= A_{B}^{-1}(b-A_{N} x_{N})$. Nach
      dem ersten Punkt ist $x^{*}\in\FF$. Sei $x\in\FF$ beliebig. Dann
      gilt nach \autoref{eq:33}: $c^{T}x=z_{B}+s_{N}^{T} x_{N}
      \geq z_{B} +s_{N}^{T} v^{*}=c^{T}x^{*}$, d.\,h. $x^{*}$ ist
      Lösung des linearen Problems.

      Sei umgekehrt $x^{*}$ Lösung des linearen Problems und
      $v^{*}=x_{N}^{*}$. Nach dem ersten Punkt ist $v^{*}\in\FF_{B}$.
      Sei $v\in\FF_{B}$ und $x$ definiert durch $x_{N}=v, x_{B} =
      A_{B}^{-1} (b-A_{N}x_{N}$. Dann ergibt sich $z_{B} + s_{N}^{T}
      v= z_{B}+s_{N}^{T} x_{N}= c^{T}x\geq c^{T}x^{*}=z_{B}+ s_{N}^{T}
      x_{N}^{*}= z_{B}+ s_{N}^{T}v^{*}$, d.\,h. $s_{N}^{T} v \geq
      s_{N}^{T} v^{*}$ für alle $v\in\FF_{B}$.
    \end{proof}
   \item Ist $x^{*}$ zulässige Basislösung des linearen Problems zur
      Basis $B$, dann ist $x^{*}$ genau dann Lösung, wenn
      $x^{*}0_{n-m}$ Lösung des \autoref{eq:29} ist.
    \begin{proof}
      Sei $x^{*}$ Basislösung zur Basis $B$. Dann ist
      $x_{N}^{*}=0_{n-m}$. Nach Punkt (ii) ist $x^{*}$ genau dann
      Lösung des linearen Problems, wenn $v^{*}=x_{N}^{*}= 0_{n-m}$
      Lösung von \autoref{eq:29} ist.
    \end{proof}
  \end{enumerate}
\end{lemma}

Damit folgt der Optimalitätstest\index{Optimalitätstest}: Sei
$x^{*}\in\FF$ zulässige Basislösung zur Basis $B$. Man stellt
(theoretisch) das reduzierte \autoref{eq:29} auf und prüft, ob
$v^{*} =0_{n-m}$ Lösung von \autoref{eq:29} ist. Falls ja, ist $x^{*}$
Lösung des linearen Problems.

\subsection{Ein Optimalitätskriterium}

Gesucht ist eine hinreichende Bedingung dafür, dass $v^{*}=0_{n-m}$
Lösung von \autoref{eq:29} ist.

Es gilt $s_{N}^{T} v^{*}=0$, d.\,h. $v^{*}$ ist Lösung, wenn gilt
$s_{N}^{T} v\geq s_{N}^{T} v^{*}=0$. Für $v\in\FF_{B}$ gilt $v\geq 0$
und es folgt, wenn
\begin{gather}\label{eq:34}
  s_{N} \geq 0
\end{gather}
erfüllt ist, gilt $s_{N}^{T} v\geq 0=s_{N}^{T} v^{*}$


\begin{lemma}
  Ist unter den obigen Voraussetzungen $s_{N}\geq 0$, dann ist die
  aktuelle Ecke $x^{*}$ optimal. Wenn $s_{N}>0$, dann ist $x^{*}$
  eindeutig bestimmt.
  \begin{proof}
    Der erste Teil ist klar. Für den zweiten Teil sei $x\in\FF$. Dann
    ist $x_{N}\geq 0, x_{B}= A_{B}^{-1}(b-A_{N} x_{N})\geq 0$. Ist
    $x\neq x^{*}\Rightarrow x_{N}\neq x_{N}^{*}=v^{*}=0_{n-m}$, d.\,h.
    für mindestens eine Komponente je $J_{N}$ gilt $x_{j}>0\Rightarrow
    S_{N}^{T} x_{N} >0 \Rightarrow c^{T}x =z_{B} +
    \underbrace{s_{N}^{T} x_{N}}_{>0} >z_{B} + \underbrace{s_{N}^{T}
    v^{*}}_{=0} =c^{T}x^{*}$, d.\,h. $c^{T}x> c^{T}x^{*}$ für alle
    $x\in\FF, x\neq x^{*}$. Somit ist $x$ eindeutig bestimmte Lösung.
  \end{proof}
\end{lemma}

Für den Optimalitätstest ergibt sich nun: Sei $x^{*}$ die aktuelle
Ecke eine zulässige Basislösung zur Basis $B$. Berechne $s_{N}$ nach
\autoref{eq:27}, prüfe, ob $s_{N}\geq 0$. Falls ja, ist $x^{*}$
optimal.

\begin{bsp}
  \label{bsp:33}
  Sei folgendes Problem gegeben:
  \begin{align*}
    \min 2x_{1} &+ 6x_{2} &-  7x_{3} &+ 2x_{4} &+ 4x_{5} &\\
         4x_{1} &- 3x_{2} &+  8x_{3} &-  x_{4} &         &= 12\\
                &-  x_{2} &+ 12x_{3} &- 3x_{4} &+ 4x_{5} &= 20\\
                &         &          &         & x_{i}   &\geq 0
  \end{align*}
  Damit folgt:
  \begin{align*}
    A &= \begin{pmatrix}4 & -3 & 8 & -1 & 0\\
	   0 & -1 & 12 & -3 & 4\end{pmatrix} & b &=
       \begin{pmatrix}12\\ 20\end{pmatrix} & c &= (2,6,-7,2,4)^{T}
  \end{align*}
  Wir wählen $J_{B}=\{1,5\}, J_{N}=\{2,3,4\}, B=\{A_{.1}, A_{.5}\}$.
  \begin{align*}
    A_{B} &= \begin{pmatrix}4&0\\0&4\end{pmatrix} & A_{B}^{-1} &=
       \begin{pmatrix}\nicefrac{1}{4} & 0\\0 &
	 \nicefrac{1}{4}\end{pmatrix}\\
    c_{B} &= (2,4)^{T} & A_{B}^{-1}b &= \begin{pmatrix}3\\
					  5\end{pmatrix}\\
    A_{B}^{-1} A_{N} &= \begin{pmatrix}-\nicefrac{3}{4} & 2 &
			  -\nicefrac{1}{4}\\
			  -\nicefrac{1}{4} & 3 &
			  -\nicefrac{3}{4}\end{pmatrix} &
       s_{N}^{T} &= c_{N}^{T}- c_{B}^{T} A_{B}^{-1}A_{N} =
       (\nicefrac{17}{2}, -23, \nicefrac{11}{2})^{T}
  \end{align*}
  Somit ist das Optimalitätskriterium nicht erfüllt.
\end{bsp}

Nunmehr stellt sich die Frage, ob die Bedingung $s_{N}^{T}\geq 0$ auch
notwendig ist. Dies ist mit ja zu beantworten, falls die Ecke $x^{*}$
nicht entartet ist.

\begin{defin}
  Sei $A\in\R^{m\times n}$ mit $\rg(A)=m\leq n, b\in\R^{m}, \FF=
  \set{x\in\R^{n}| Ax=b, x\geq 0}, B$ Basis von $A$. Eine zulässige
  Basislösung $x\in\FF$ heißt \highl{nicht entartet}, wenn $x_{B}>0$
\end{defin}

\begin{bem*}
  Zu jeder Basis $B$ existiert genau eine Basislösung $x$ mit
  $x_{N}=0_{n-m}, x_{B}= A_{B}^{-1}(b-A_{N}x_{N})$, d.\,h. zu jeder
  Basis existiert höchstens eine Ecke.

  Ist $x\in\FF$ Ecke, dann kann $x$ zu verschiedenen Basen die
  zugehörige Ecke sein.
\end{bem*}

\begin{bsp}
  \begin{align*}
    A &= (I_{m}, I_{m}) & b &= (0,1,\ldots,1)^{T}\in\R^{m} & x &=
       \begin{pmatrix}b\\0_{m}\end{pmatrix}
  \end{align*}
  Dann ist $x$ zulässige Basislösung zur Basis mit den Indizes $J_{B}
  = \{1,\ldots,n\}$, aber auch zu $J_{B} = \{2,\ldots,m+1\}$
\end{bsp}

\begin{lemma}
  Sei $x\in\FF$ eine nicht entartete Ecke. Ist $x^{*}\in\FF$ Lösung
  des linearen Problems, dann ist $s_{N}\geq 0$.
  \begin{proof}
    Nach \autoref{lem:31} gilt $v^{*}=x_{N}^{*}=0_{n-m}$ ist Lösung
    des reduzierten Problems. Angenommen, es existiert ein $j\in
    \{1,\ldots,n-m\}$ mit $s_{N,j}<0$. Wir definieren die Richtung
    $d\in\R^{n-m}$ durch $d_{j}=1, d_{i}=0$ für $i\in \{1,\ldots,n-m\}
    \setminus \{j\}$ ($j$-ter Einheitsvektor von $\R^{n-m}$) und
    zeigen, $d$ ist eine zulässige Abstiegsrichtung im Punkt
    $v^{*}=0_{n-m}$. Sei $v(t) := v^{*}+td=td\geq 0$ für $t\geq 0$
    sowie $\widehat{A}:= A_{B}^{-1}A_{N}$ und $\widehat{A_{.j}}$ der zu obigen
    Index $j$ gehörige Spaltenvektor. Da $x^{*}_{B}$ nicht entartet
    ist, gilt, $0< x_{B}^{*}=A_{B}^{-1}b$. Für hinreichend kleines
    $t>0$ gilt:
    \begin{gather*}
      t\widehat{A_{.j}} <A_{B}^{-1}b\Rightarrow \exists \sigma>0\colon
	 A_{B}^{-1} A_{N} v(t)=t\widehat{A_{.j}}\leq A_{B}^{-1}b \qquad
	 \forall t\in[0,\sigma]
    \end{gather*}
    Damit ist $v(t)\in\FF_{B}$ (zulässig für reduziertes Problem).
    Speziell gilt: $v(\sigma)\in\FF_{B}$ und $s_{N}^{T} v(\sigma)=
    \sigma s_{N}^{T}d= \sigma s_{N,j}<0$. Dies ist jedoch eine
    Widerspruch zur Optimalität von $v^{*}$.
  \end{proof}
\end{lemma}

\subsection{Ein Unbeschränkheitskriterium}

Sei wieder $x^{*}$ Ecke und damit zulässige Basislösung zur Basis $B$
von $A$. Das Optimalitätskriterium $s_{N}\geq 0$ sei nicht erfüllt,
d.\,h. es existiert ein $j\in\{1,\ldots,n-m\}\colon s_{N,j} <0$. Wir
definieren wie oben $d\in\R^{n-m}$ durch $d_{j}=1, d_{i}=0$ für
$i\in\{1,\ldots,n-m\}\setminus \{j\}$. Dann gilt wieder $v(t):= v^{*}
+td=td\geq 0$ für alle $t\geq 0$. Weiter sei $\widehat{A}:= A_{B}^{-1}
A_{N}$. Für den $j$-ten Spaltenvektor gelte $\widehat{A_{.j}}\leq 0$. Dann
folgt für $t\geq 0$:
\begin{gather*}
  A_{B}^{-1}A_{N} v(t) =t\widehat{A_{.j}}\leq 0\leq x_{B}^{*}=A_{B}^{-1} b
\end{gather*}
d.\,h. $v(t)\in\FF_{B}$ und $\FF_{B}$ ist unbeschränkheit. Nach
\autoref{lem:31} ist $x(t)\in\FF$ mit $x_{N}(t)=v(t), x_{B}(t)=
A_{B}^{-1} (b-A_{N}v(t))$. Damit ist auch $\FF$ beschränkt.

Zusammen mit $s_{N,j}< 0$ folgt, $c^{T}x(t)= z_{B}+s_{N}^{T} v(t)=
z_{B}+ ts_{N,j}\xrightarrow{t\rightarrow+\infty} -\infty$, d.\,h. die
Zielfunktion des linearen Problems ist auf $\FF$ nicht nach unten
beschränkt. Also existiert keine Lösung.

\begin{lemma}
  Existiert ein Index $j\in\{1,\ldots,n-m\}$ mit
  \begin{gather}
    \label{eq:35}
    s_{N,j}<0 \text{ und }\widehat{A_{.j}} \leq 0
  \end{gather}
  dann ist $\FF$ nicht nach unten beschränkt und die Zielfunktion ist
  auf $\FF$ nicht nach unten beschränkt.
\end{lemma}

\begin{bsp}[Fortsetzung von \autoref{bsp:33}]
  Im obigen Beispiel hatten wir den Vektor
  $s_{N}^{T}=(\nicefrac{17}{2}, -23, \nicefrac{11}{2})^{T}$ berechnet.
  Für $j=2$ ist $\widehat{A_{.j}}=\begin{pmatrix}2\\3\end{pmatrix}>0$,
  d.\,h. die \autoref{eq:35} ist nicht erfüllt.  
\end{bsp}

\subsection{Basiswechsel}

Sei $x^{*}\in\FF$ Ecke mit Basis $B$ von $A$. Das
Optimalitätskriterium und das Unbeschränkheitskriterium seien nicht
erfüllt. Das Ziel ist nun, eine neue Basis $B^{+}$ zu konstruieren, so
dass für $x^{+}$ gilt:
\begin{gather*}
  c^{T}x^{+} \leq c^{T} x^{*}
\end{gather*}

Dazu definieren wir $d$ wie oben. Dann gilt $v(t):= v^{*}+td
\in\FF_{B}$ mit $t\geq 0$ nicht zu groß. Bestimme das maximale
$\sigma$, so dass $v(\sigma)\in\FF_{B}$. Sei $x(\sigma)$ definiert
durch $x_{N}(\sigma) =v(\sigma), x_{B}(\sigma)= A_{B}^{-1}(b-A_{N}
v(\sigma))$. Dann kann man $x^{+}=x(\sigma)$ wählen.

Bezeichnungen: $J_{B}= \{b(1),\ldots,b(m)\}, J_{N}=\{n(1),\ldots,n(m)\}$

Dann ist $B=\{A_{.,b(1)},\ldots,A_{.,b(m)}\},
N=\{A_{.,n(1)},\ldots,A_{.,n(m)}\}, x_{B,i}= x_{b(i)}, i=1,\ldots,m,
x_{N}=x_{n(i)}, i=1,\ldots,n-m, \overline{A}:= A_{B}^{-1}, \widehat{A}=
A_{B}^{-1}A_{N}$.

Ist die \autoref{eq:34} nicht erfült, dann existiert ein
\begin{gather}\label{eq:36}
  s_{N,j} <0 \qquad j\in\{1,\ldots,n-m\}
\end{gather}
Wir definieren:
\begin{align}\label{eq:37}
  d_{j} &=1 & d_{i} &= 0 \quad j\in\{1,\ldots,n-m\}\setminus\{j\}
\end{align}
Damit ist $d$ eine zulässige Abstiegsrichtung für das reduzierte
Problem, falls $x^{*}$ nicht entartet ist. Weiter definieren wir:
\begin{gather*}
  v(t):= \underbrace{v^{*}}_{0_{n-m}} +td=td \qquad \forall t\geq 0
\end{gather*}

Ist das Unbeschränkheitskriterium erfüllt, dann gilt $v(t)\in\FF_{B}$.
Sei jetzt das Unbeschränkheitskriterium nicht erfüllt. Wir wollen das
$\max \sigma\geq 0$ mit $v(t)\in\FF_{B}$ für alle $t\in[0,\sigma]$
bestimmen. Wegen $v(t)\geq 0$ ist die Vorzeichenbedingung immer
erfüllt. Wir müssen nur noch die Restriktion
\begin{gather}\label{eq:38}
  \underbrace{A_{B}^{-1}A_{N}}_{=\widehat{A}} v(t)\leq A_{B}^{-1}b
\end{gather}
einhalten. Diese Bedingung ist äquivalent zu:
\begin{gather}\label{eq:39}
  t\widehat{A_{.j}}\leq x_{B}^{*}
\end{gather}
Sei $\nu\in\{1,\ldots,m\}$ ein Index mit $\widehat{A_{\nu, j}}\leq
0\Rightarrow t\widehat{A_{\nu,j}}\leq 0 \leq x_{B,\nu}^{*}$, d.\,h.
entscheidend sind nur die Indizes $\nu\in\{1,\ldots,m\}$ mit
$\widehat{A_{\nu,j}}>0$. Für einen solchen Index gilt:
\begin{gather*}
  t\widehat{A_{\nu,j}}\leq x_{B,\nu}^{*}\Leftrightarrow t\leq
     \frac{x_{B,\nu}}{\widehat{A_{\nu,j}}}
\end{gather*}

Damit folgt:
\begin{gather}\label{eq:310}
  \sigma= \min\Set{\frac{x_{B,\nu}^{*}}{\widehat{A_{\nu,j}}}| \nu\in
     \{1,\ldots,m\} \text{ mit } \widehat{A_{\nu,j}}>0}
\end{gather}

Der Punkt $v(\sigma)$ ist \highl{Randpunkt} von $\FF_{B}$. Zunächst
gilt, $x^{+}\in\FF$ wegen des \autoref{lem:31}. Wir müssen noch eine
Basis $B^{+}$ von $A$ finden, so dass $x^{+}$ Basislösung von $B^{+}$
ist.

Sei $j\in\{1,\ldots,n-m\}$ der Index mit $s_{N}<0$ nach \autoref{eq:36} und
weiter $l\in\{1,\ldots,m\}$ ein Index mit $\widehat{A_{l,j}}>0$ und
\begin{gather}\label{eq:312}
  \sigma=\frac{x_{B,l}^{*}}{\widehat{A_{l,j}}}
\end{gather}
Die neue Basis $B^{+}$ wird dann definiert durch:
\begin{align*}
  J_{B^{+}} &:= (J_{B}\setminus \{b(l)\})\cup \{n(j)\} & B^{+} &=
     \set{A_{.i}| i\in J_{B^{+}}}\\
  J_{N^{+}} &:= (J_{N}\setminus \{n(j)\})\cup \{b(l)\} & N^{+} &=
     \set{A_{.i}| i\in J_{N^{+}}}
\end{align*}

Damit $x^{+}$ Basislösung zu $B^{+}$ ist, muss gelten, $x_{j}^{+}=0$,
d.\,h. wir müssen zeigen, $x_{N,\mu}^{+}=0, \forall \mu\in\{1,\ldots,
n-m\}\setminus \{j\}$ und $x_{B,l}^{+}=0, \forall j\in J_{N^{+}}$.
Wegen $v(\sigma)=\sigma d=x_{N}^{+}$ ist $x_{N,\mu}^{+}=0$ und nach
der Wahl von $l$ gilt: $\sigma\widehat{A_{l,j}}=
x_{B,l}^{*}\Rightarrow x_{B,l}^{+} =
(\underbrace{A_{B}^{-1}b}_{=x_{B}^{*}} - \underbrace{A_{B}^{-1}A_{N}
v(\sigma)}_{=\sigma \widehat{A_{l,j}}}= x_{B,l}^{*}- \sigma
\widehat{A_{l,j}}=0$. Damit ist $x_{N^{+}}=0_{n-m}$ und es ist noch zu
zeigen, dass $A_{B^{+}}x_{B^{+}}^{+}=b$.

Wir wissen, dass $x\in \FF$. Es gilt, $b=Ax^{+}=\sum_{i=1}^{n} A_{.i}
x_{i}^{+}= \sum_{i\in J_{B^{+}}} x_{i}^{+} A_{.i} = A_{B^{+}}
x_{B^{+}}^{+}$. Somit bleibt noch zu zeigen, dass $B^{+}$ eine Basis
von $A$ ist.

\begin{lemma}
\label{lem:39}
  Sei $B=\{v^{(2)},\ldots,v^{(m)}\}\subseteq \R^{m}$ eine Basis des
  $\R^{m}$, $l\in\{1,\ldots,m\}$ ein Index, $w\in\R^{m}$ ein Vektor
  sowie $V$ eine $m\times m$-Matrix mit den Spaltenvektoren
  $v^{(1)},\ldots,v^{(m)}$. Bildet man $B^{+}$ aus $B$ durch
  Basistausch des Vektors $v^{(l)}$ gegen $w$, also $B^{+}=
  \{v^{(1)},\ldots,v^{(l-1)},w, v^{(l+1)},\ldots,v^{(m)}\}$, dann
  gilt, dass $B^{+}$ genau dann Basis des $\R^{m}$ ist, wenn
  $\hat{w_{l}} \neq 0$ mit $\hat{w_{l}}:= V^{-1}w$.
  \begin{proof}
    Sei $V^{+}$ die Matrix mit den Spaltenvektoren
    $v^{(1)},\ldots,v^{(l-1)}, w, v^{(l+1)},\ldots,v^{(m)}$. Wir
    zeigen, dass $V^{+}$ genau dann invertierbar ist, wenn
    $\hat{w_{l}} \neq 0$ gilt. Wir wissen, $V^{+}$ ist genau dann
    invertierbar, wenn $V^{-1}V^{+}$ invertierbar ist und es gilt,
    $V^{-1}V^{+}=(e^{(1)},\ldots,e^{(l-1)}, \hat{w_{l}},
    e^{(l+1)},\ldots,e^{(m)})$ \todo{Beweis prüfen und weiter
    vervollständigen}
  \end{proof}
\end{lemma}

\begin{bem*}
  Die obigen Betrachtungen zeigen, $x_{n(j)}^{*}=0$, da
  $x_{N}^{*}=0_{n-m}, x_{n(j)}^{+}=\sigma$. Für $\sigma$ gilt:
  $\sigma>0$, falls die Ecke $x^{*}$ nicht entartet ist und
  andernfalls ist $\sigma=0$, d.\,h. in diesem Fall ist es möglich,
  dass nur der Basiswechsel stattfindet. Die aktuelle Ecke ändert sich
  dann nicht.

  Wie berechnet sich nun der Wert der Zielfunktion in der neuen Ecke
  $x^{+}$? Nach der \autoref{eq:33} gilt:
  \begin{gather}\label{eq:313}
    c^{T}x^{+} = z_{B} +s_{N}^{T} x_{N}^{+} = z_{B} + s_{N}^{T}
       v(\sigma)= \underbrace{z_{B}}_{=c^{T}x^{*}} + \sigma s_{N,j}
  \end{gather}
  Somit folgt, $c^{T}x^{+}\leq z_{B}=c^{T}x^{*}$ und, falls $x^{*}$
  nicht entartet ist: $c^{T}x^{+}<z_{B}=c^{T}x^{*}$
\end{bem*}

\begin{satz}
  \label{satz:310}
  Sei $x^{*}$ Ecke von $\FF$ zur Basis $B$ von $A$. Ist $x^{*}$ nicht
  optimal und ist das Unbeschränkheitskriterium nichterfüllt, dann ist
  die zur neuen Basis gehörige Basislösung $x^{+}$ zulässig und damit
  Ecke von $\FF$ mit $c^{T}x^{+}\leq c^{T}x^{*}$.
\end{satz}

\section{Durchführung des Simplexverfahrens}
\subsection{Verfahrenskonzept}

Für die Phase~2 des Simplexverfahrens lässt sich folgendes Verfahren
beschreiben: Gegeben seien eine Basis $B=B^{(0)}$ von $A$, die
Matrizen $\overline{A}=A_{B}^{-1}, \widehat{A_{B}^{-1}}=A_{B}^{-1}
A_{N}$, der Testvektor $x_{B}^{(0)}$ und die Indexmengen $J_{B},
J_{N}$. Berechne den Testvektor $s_{N}^{T}= c_{N}^{T}-c_{B}^{T}
A_{B}^{-1} A_{N}$ und den Zielfunktionswert $z_{B}=c_{B}^{T}
x_{B}^{(0)}$. Setze $k:=0$. Danach werden folgende Iterationsschritte
ausgeführt:
\begin{enumerate}
 \item Optimalitätskriterium: Gilt $s_{N}\geq 0$, dann ist die
  aktuelle Ecke $x^{*}$ optimal. Das Programm endet hier.
 \item Unbeschränkheitskriterium: Wähle einen Index
  $j\in\{1,\ldots,n-m \}$ mit $s_{N,j}<0$. Gilt nun $\widehat{A.j}<0$,
  ist die Zielfunktion nich nach unten beschränkt und das Programm
  endet.
 \item Setze $B^{(k+1)}:= B:= B^{+}$. Berechne mit der neuen Basis
  $\overline{A}=A_{B}^{-1}, \widehat{A_{B}^{-1}}A_{N}$, den Vektor
  $s_{N}$ und $z_{B}$.
 \item Setze $k:=k+1$ und gehe zum ersten Punkt.
\end{enumerate}

\subsection{Konvergenz des Simplexverfahrens}

\begin{satz}
  Sind alle Ecken $x^{(k)}$ zu den berechneten Basen $B^{(k)}$ nicht
  entartet, dann stoppt das Verfahren nach endlich vielen Schritten
  mit einer optimalen Basis oder das Unbeschränkheitskriterium ist
  erfüllt.
  \begin{proof}
    Sei $x^{(k)}$ die in der $k$-ten Iteration berechnete Ecke zur
    Basis $B^{(k)}$. Dabei sei weder das Optimalitätskriterium noch
    das Unbeschränkheitskriterium erfüllt. Nach dem
    \autoref{satz:310} folgt: Das Verfahren berechnet eine neue Basis
    $B^{+}= B^{(k+1)}$, so dass für die zugehörige Ecke $x^{(k+1)}$
    gilt:
    \begin{gather*}
      c^{T}x^{(k+1)} < c^{T} x^{(k)}
    \end{gather*}
    d.\,h. $x^{(k+1)}$ ist nicht entartet. Da es nur endlich viele
    Ecken gibt, kann der Fall nur endlich oft auftreten.
  \end{proof}
\end{satz}

\subsection{Tableaudarstellung}

\begin{tabular}{c|c|c|c}
  & $J_{N}$ & & $J_{B}$\\
  \toprule
  $J_{B}$ & $\hat{A}=A_{B}^{-1}A_{N}$ & $x_{B}^{*}= A_{B}^{-1}b$ &
     $A_{B}^{-1}$\\
  \bottomrule
  & $s^{T}_{N}$ & $z_{B}$ & $c^{T}_{B} A^{-1}_{B}$
\end{tabular}

\begin{bsp}
  \label{bsp:313}
  \begin{tabular}{c|ccc|c|cc}
    & 2 & 3 & 4& & 1 &5\\
    \toprule
    1 & $-\nicefrac{3}{4}$ & 2 & $-\nicefrac{1}{4}$ & 3 &
       $\nicefrac{3}{4}$ & 0\\
    5 & $-\nicefrac{1}{4}$ & 3 & $-\nicefrac{3}{4}$ & 5 & 0 &
       $\nicefrac{1}{4}$\\
    \bottomrule
    & $\nicefrac{17}{2}$ & -8 & $\nicefrac{11}{2}$ & 26 &
       $\nicefrac{1}{2}$ &1
  \end{tabular}

  Im Beispiel gilt $x^{*}_{B}>0$, d.\,h. die aktuelle Ecke ist nicht
  entartet. Allerdings ist wegen $s_{N,2}<0$ das Optimalitätskriterium
  nicht erfüllt. Also müssen wir im vorliegenden Fall $j=2$ wählen.
  Für den Basistausch muss der Index $l$ so gewählt werden, dass gilt:
  \todo{Vergleich untenstehendes A-dach mit den obigen}
  \begin{align*}
    \frac{x_{B,l}}{\hat{A}_{l,j}} &= \sigma =
       \min\Set{\frac{x^{*}}{\hat{A}_{\nu,j}}|\nu\in\{1,\dotsc,m\}
       \text{ mit } \hat{A}_{\nu,j}>0}\\
    &= \min\left\{\frac{x^{*}_{B,1}}{\hat{A}_{1,j}},
       \frac{x^{*}_{B,2}}{\hat{A}_{2,j}}\right\}
       =\frac{3}{2}\Rightarrow l=1
  \end{align*}
\end{bsp}

\begin{bsp}
  Führt man den Basistausch mit $j=2, l=1$ durch, so erhält man:
    \begin{tabular}{c|ccc|c|cc}
    & 2 & 1 & 4& & 3 &5\\
    \toprule
    3 & $-\nicefrac{3}{8}$ & $\nicefrac{1}{2}$ & $-\nicefrac{1}{8}$ &
	 $\nicefrac{3}{2}$ & $\nicefrac{1}{8}$ & 0\\
    5 & $\nicefrac{7}{8}$ & $-\nicefrac{3}{2}$ & $-\nicefrac{3}{8}$ &
	 $\nicefrac{1}{2}$ & $-\nicefrac{3}{8}$ & $\nicefrac{1}{4}$\\
    \bottomrule
    & $\nicefrac{1}{8}$ & $\nicefrac{23}{2}$ & $\nicefrac{21}{8}$ &
	 $-\nicefrac{17}{2}$ & $-\nicefrac{19}{8}$ &1
  \end{tabular}
  Auch hier gilt, $x^{(1)}$ ist nicht entartet, da $x^{*}_{B}>0$. Das
  Optimalitätskriterium ist nicht erfüllt, da $s_{N,1}<0$ und auch das
  Unbeschränkheitskriterium ist nicht erfüllt, da nicht die gesamte
  Spalte kleiner oder gleich Null ist.

  Aus einem erneuten Basistausch mit $j=1, l=2$ resultiert das
  folgende Tableau:
  \begin{tabular}{c|ccc|c|cc}
    & 2 & 1 & 4& & 3 &2\\
    \toprule
    3 & $-\nicefrac{3}{7}$ & $-\nicefrac{1}{7}$ & $-\nicefrac{2}{7}$ &
	 $\nicefrac{12}{7}$ & $-\nicefrac{1}{28}$ & $\nicefrac{3}{28}$\\
    2 & $\nicefrac{8}{7}$ & $-\nicefrac{12}{7}$ & $-\nicefrac{3}{7}$ &
	 $\nicefrac{4}{7}$ & $-\nicefrac{3}{7}$ & $\nicefrac{2}{7}$\\
    \bottomrule
    & $\nicefrac{1}{7}$ & $\nicefrac{79}{7}$ & $-\nicefrac{60}{7}$ &
	 $-\nicefrac{60}{7}$ & $-\nicefrac{65}{28}$ &$\nicefrac{27}{28}$
  \end{tabular}
  Das Optimalitätskriterium ist erfüllt. Somit ist die aktuelle Ecke
  optimal.
  \begin{align*}
    x^{*}_{B} &= \begin{pmatrix}x^{*}_{2}\\x^{*}_{3}\end{pmatrix} =
       \begin{pmatrix}\nicefrac{4}{7}\\\nicefrac{12}{7}\end{pmatrix} &
       x^{*}_{N} &= \begin{pmatrix}x^{*}_{1}\\x^{*}_{4}
          \\x^{*}_{5}\end{pmatrix} = \begin{pmatrix}0\\0\\0\end{pmatrix}
  \end{align*}
\end{bsp}

Wir führen folgende, neue Bezeichnungen ein:
\begin{align*}
  \hat{A} &= A^{-1}_{B}A_{N} & \overline{A} &= A^{-1}_{B} &
     \overline{x} &= x^{*}_{B}\\
  \hat{s} &= s_{N} & \overline{z} &= z_{B} & \overline{y} &=
     A^{-T}_{B} c_{B}
\end{align*}

Mit den neuen Bezeichnungen wählen wir zum Basistausch einen Index
$\{1,\dotsc,n-m\}$ mit $\hat{s}_{j}<0$ und einen Index
$l\in\{1,\dotsc,m\}$ mit $\frac{\overline{x}_{l}}{\hat{A}_{l,j}} =\min
\Set{\frac{\overline{x}_{\nu}}{\hat{A}_{\nu,j}}| \nu\in\{1,\dotsc,m\}
\text{ mit } \hat{A}_{\nu,j}>0}$.

\section{Der Austauschschritt}
\label{sec:der-austauschschritt}

Die Spalten des Simplextableaus sind von der Form $A_{B}^{-1}v$. Zum
Update des Tableaus müssen wir die entsprechenden Vektoren
$A_{B^+}^{-1}v$ berechnen. Betrachten wir die allgemeine Situation von
\autoref{lem:39}:

Sei $B=\{v^{(1)},\ldots, v^{(m)}\}$ eine Basis des $\R^m$. Weiter sei
$l\in \{1,\ldots,m\}$ ein gegebener Index, $w\in\R^m$ und $V=[v^{(1)},
\ldots, v^{(m)}]$. Die Koordinaten $\hat{w}=V^{-1}w$ von $w$ bezüglich
Basis $B$ seien bekannt. Weiter sei $\hat{w}_l\neq 0$. Tauscht man
$v^{(l)}$ gegen $w$, d.\,h. definiert man
\[B^{+}=\{v^{(1)},\dotsc, v^{(l-1)},w,v^{(l+1)},\dotsc, v^{(m)}\}\]
nach \autoref{lem:39} ist $B^{+}$ wieder eine Basis.

Sei $V^{+}=[v^{(1)},\dotsc, v^{(l-1)}, w, v^{(l+1)},\dotsc, v^{(m)}]$.
Für beliebige $v\in\R^{m}$ sei $\hat{v}=V^{-1}v$ und
$v^{+}=(V^{+})^{-1}v$. Berechnen $w^{+}=(V^{+})^{-1}v^{(l)}$. Wir
kennen $\hat{w}= V^{-1}w\Leftrightarrow V\hat{w}=w\Leftrightarrow w=
\sum_{i=1}^{m} \hat{w}_{i}v^{(i)}\Rightarrow \hat{w}_{l}v^{(l)} = w-
\sum_{i=1, i\neq l}^{m} \hat{w}_{i}v^{(i)}$. Da $\hat{w}_{l}\neq 0$ folgt,
$v^{(l)}= \frac{w}{\hat{w}_{l}}- \sum_{i=1,i\neq l}^{m}
\frac{\hat{w}_{i}}{\hat{w}_{i}} v^{(i)}$. Dann folgt:
\begin{gather}\label{eq:314}
  v^{(l)} = V^{+}w^{+} \text{ mit } w^{+}=
     \begin{cases}
       \frac{1}{\hat{w}_{l}} & i=l\\
       -\frac{\hat{w}_{i}}{\hat{w}_{l}} & i\in\{1,\dotsc,m\}\setminus
       \{l\}
       \end{cases}
\end{gather}

Nachdem man $w^{+}$ entsprechend \autoref{eq:314} berechnet hat, kann
man für beliebigen Vektor $v\in\R^{n}$ jetzt aus $\hat{v}=V^{-1}v$
einfach $v^{+}=(V^{+})^{-1}v$ berechnen. Es gilt:
\[\hat{v}=V^{-1}v\Leftrightarrow v=V\hat{v}\Leftrightarrow
v=\sum_{i=1}^{m}\hat{V}_{i}v^{(i)}\]
Für $v^{(l)}$ setzen wir die Darstellung $v^{(l)}=V^{+}w^{+}$ ein:
\begin{align*}
  v &= \sum_{i=1, i\neq l}^{m} \hat{v}_{i} v^{(i)}+ \hat{v}_{l}v^{(l)}=
     \sum_{i=1, i\neq l}^{m} \hat{v}_{i}v^{(i)}+ \hat{v}_{l}V^{+}w^{+}\\
  &= \sum_{i=1, i\neq l}^{m} \hat{v}_{i}v^{(i)}+ \hat{v}_{l}
     \sum_{i=1, i\neq l}^{m} w_{i}^{+}v^{(i)} + \hat{v}_{l}w_{l}^{+}w\\
  &= \sum_{i=1, i\neq l}^{m} (\hat{v}_{i}+\hat{v}_{l}w_{i}^{+})v^{(i)}
     + \hat{v}_{l}w_{l}^{+}w
\end{align*}

\todo{zwei Tabellen einfügen}

Allgemeine Situation beim Basistausch:\\
aktuelle Basis: $B=\{v^{(1)},\dotsc, v^{(l-1)}, v^{(l)},
v^{(l+1)},\dotsc, v^{(m)}\}$\\
neue Basis: $B^{+}=\{v^{(1)},\dotsc,v^{(l-1)}, w, v^{(l+1)},\dotsc,
v^{(m)}\}$ mit $w\in\R^{m}, \hat{w}_{l}\neq 0$, wobei $\hat{w}=V^{-1}w$

Seien $w^{+}:= (V^{+})^{-1}v^{(l)}$ Koordinaten von $v^{(l)}$
(des aus der aktuellen Basis gestrichenen Vektors) bezüglich der neuen
Basis $B^{+}$

\begin{gather}\label{eq:315}
  w_{i}^{+}=
     \begin{cases}
       \frac{1}{\hat{w}_{l}} & i=l\\
       -\frac{\hat{w}_{i}}{\hat{w}_{l}} & i\neq l
     \end{cases}
\end{gather}
Sei allgemein $v\in\R^{m}$ ein beliebiger Vektor, $\hat{v}:=
A_{B}^{-1}v$ (Koordinaten von $v$ bezüglich der aktuellen Basis $B$),
$v^{+}:= A_{B^{+}}^{-1}v$ (Koordinaten von $v$ bezüglich der neuen
Basis $B^{+}$).
\begin{gather}\label{eq:316}
  v_{i}^{+}=
     \begin{cases}
       \hat{v}_{l}\hat{w}_{l} & i=l\\
       \hat{v}_{i}+\hat{v}_{l}\hat{w}_{i} & i\neq l
     \end{cases}
\end{gather}

Anwendung auf das Simplextableau:
\begin{itemize}
 \item Umrechnung der Pivotspalte nach \autoref{eq:315}
 \item Umrechnung der restlichen Spalten von $\hat{A}$ nach
  \autoref{eq:316}
 \item Umrechnung von $\overline{x}$ unf $\overline{A}$ nach
  \autoref{eq:316}
 \item Umrechnung von $\hat{s}, \overline{z}, \overline{y}$
\end{itemize}

\subsection{Neuberechnung des Simplextableaus}

Indextausch: $b^{+}(l)= n(j), n^{+}(j)=b(l)$, restliche Indizes
bleiben unverändert

Neuberechnung der Pivotspalte: Der Vektor der aktuellen Pivotspalte
ist $\hat{w} =\hat{A}_{.j}$ und $\hat{w}$ wird durch den Vektor
$w^{+}=\hat{A}_{.j}^{+}$ ersetzt (Koordinaten des aus der Basis
entfernten Vektors $v^{(l)}=A_{b(l)}$ bezüglich der neuen Basis
$B^{+}$), d.\,h. wir können \autoref{eq:315} anwenden mit
$\hat{w}=\hat{A}_{.j}$ (aktuelle Pivotspalte) und wir erhalten
\begin{gather}\label{eq:317}
  \hat{A}_{i,j}^{+}=
     \begin{cases}
       \frac{1}{\hat{A}_{i,j}} & i=l\\
       -\frac{\hat{A}_{i,j}}{\hat{A}_{l,j}} & i\neq l
     \end{cases}
\end{gather}

\begin{bsp}[Problem aus \autoref{bsp:313}]
  \label{bsp:315}
  Mit \autoref{eq:317} erhalten wir als neue Spalte:
  \begin{align*}
    \hat{A}_{1,2}^{+} &= \frac{1}{\hat{A}_{1,2}} = \frac{1}{2} &
       \hat{A}_{2,2}^{+} &= -\frac{\hat{A}_{2,2}}{\hat{A}_{1,2}} =
       -\frac{3}{2}
  \end{align*}
\end{bsp}

\minisec{Berechnung von $\hat{A}^{+}$ (ohne Pivotspalte)}
Wir benutzen \autoref{eq:316}, wobei $w^{+}=\hat{A}_{.j}^{+}$ die
bereits neu berechnete Pivotspalte ist. Weiter ist
$\hat{v}=\hat{A}_{.\nu}, \nu\in\{1,\dotsc,n-m\}, \nu\neq j$.
\begin{gather}\label{eq:318}
  \hat{A}_{i,\nu}^{+}=
     \begin{cases}
       \hat{A}_{l,\nu}\hat{A}_{l,j}^{+} & i=l\\
       \hat{A}_{i,\nu}+\hat{A}_{l,\nu}\hat{A}^{+}_{i,j}& i\neq l
     \end{cases}
\end{gather}

\begin{bsp}[Fortsetzung von \autoref{bsp:315}]
  \label{bsp:316}
  Zur Neuberechnung der Spalten 1 und 3 von $\hat{A}$ erhalten wir aus
  \autoref{eq:318} mit $\nu=1,3$:
  \begin{align*}
    \hat{A}_{1,1}^{+} &= \hat{A}_{1,1}\hat{A}_{1,2}^{+}=
       -\frac{3}{4}\cdot \frac{1}{2}= -\frac{3}{8}\\
    \hat{A}_{1,3}^{+} &= \hat{A}_{1,3}\hat{A}_{1,2}^{+}= -\frac{1}{8}\\
    \hat{A}_{2,1}^{+} &= \hat{A}_{2,1}+\hat{A}_{1,1}\hat{A}_{2,2}^{+}=
       \frac{7}{8}\\
    \hat{A}_{2,3}^{+} &= -\frac{3}{8}
  \end{align*}

  neues Tableau: \todo{Tableau einfügen}
\end{bsp}

Berechnung von $\overline{x}^{+}$: Anwendung von \autoref{eq:317} mit
$w^{+}:= \hat{A}_{.j}^{+}$ und $\hat{v}=\overline{x}$
\begin{gather}\label{eq:319}
  \overline{x}_{i}^{+} =
     \begin{cases}
       \overline{x}_{l}\hat{A}_{l,j}^{+} & i=l\\
       \overline{x}_{i}+\overline{x}_{l}\hat{A}_{i,j}^{+} & i\neq l
     \end{cases}
\end{gather}

\begin{bem}
  Zusammen mit \autoref{eq:316} und \autoref{eq:312} erhalten wir:
  \[x_{l}^{+} = \overline{x}_{l} \hat{A}_{l,j}^{+}=
  \frac{\overline{x}_{l}}{\hat{A}_{l,j}}=\sigma\]
  Für $i\neq l$ gilt:
  \[x_{i}^{+} =\overline{x}_{i}+\overline{x}_{l}\hat{A}_{i,j}^{+}=
  \overline{x}_{i} -\overline{x}_{l}
  \frac{\hat{A}_{i,j}}{\hat{A}_{l,j}} = \overline{x}_{i}- \sigma
  \hat{A}_{i,j}\]
\end{bem}

\begin{bsp}[Fortsetzung von \autoref{eq:316}]
  \label{bsp:317}
  $\overline{x}_{1}^{+}= \overline{x}_{1}
  \hat{A}_{1,2}^{+}=\nicefrac{3}{2}, \overline{x}_{2}^{+}=
  \overline{x}_{2}+ \overline{x}_{1}\hat{A}_{2,2}^{+}=\nicefrac{1}{2}$
\end{bsp}

Berechnung von $\overline{A}^{+}$: Analog zur Berechnung von
$\overline{x}^{+}$ und $\hat{A}^{+}$ erhält man für $\mu=1,\dotsc,m$:
\begin{gather}\label{eq:320}
  \overline{A}_{i,\mu}^{+}=
     \begin{cases}
       \overline{A}_{l,\mu} \hat{A}_{l,j}^{+} & i=l\\
       \overline{A}_{i,\mu} +\overline{A}_{l,\mu} \hat{A}_{i,j}^{+} &
       i\neq l
     \end{cases}
\end{gather}

\begin{bsp}[Fortsetzung von \autoref{bsp:317}]
  \label{bsp:318}
  siehe voriges Tableau, Zahlen sind dort mit ergänzt.
\end{bsp}

Berechnung von $\overline{y}^{+}$: Nach der Definition ist
$\overline{y}^{+} =((A_{B^{+}})^{-1})^{+}c_{B^{+}}=
(\overline{A}^{+})^{-1} c_{B^{+}}$. Mit \autoref{eq:320} erhalten wir
für $\mu= 1,\dotsc,m$:
\begin{align*}
  \overline{y}_{\mu}^{+} &= \sum_{i=1, i\neq l}^{m} c_{b(i)}
     \overline{A}_{i,\mu}^{+} + c_{n(j)} \overline{A}_{l,\mu}^{+}\\
  &= \sum_{i=1, i\neq l}^{m} c_{b(i)} (\overline{A}_{i,\mu}+
     \overline{A}_{l,\mu} \hat{A}_{i,j}^{+}) +c_{n(j)}
     \overline{A}_{l,\mu} \hat{A}_{l,j}^{+}\\
  &= \sum_{i=1, i\neq l}^{m} c_{b(i)} \overline{A}_{i,\mu}+ c_{b(l)}
     \overline{A}_{l,\mu} + \sum_{i=1, i\neq l}^{m} c_{b(i)}
     \overline{A}_{l,\mu} \hat{A}_{i,j}^{+} - c_{b(l)}
     \overline{A}_{l,\mu} + c_{n(j)} \overline{A}_{l,\mu}
     \hat{A}_{l,j}^{+}
\end{align*}
Zusammen mit $\sum_{i=1}^{m} c_{b(i)} \overline{A}_{i,\mu}=
\overline{y}_{\mu}$ und Einsetzen von \autoref{eq:317} folgt:
\begin{align*}
  \overline{y}_{\mu}^{+} &= \overline{y}_{\mu} - \sum_{i=1}^{m}
     c_{b(i)} \overline{A}_{l,\mu} \frac{\hat{A}_{i,j}}{\hat{A}_{l,j}}
     + c_{n(j)} \overline{A}_{l,\mu} \frac{1}{\hat{A}_{l,j}}\\
  &= \overline{y}_{\mu} + \frac{\overline{A}_{l,\mu}}{\hat{A}_{l,j}}
     \underbrace{\left(c_{n(j)} - \sum_{i=1}^{m} c_{b(i)}
     \hat{A}_{i,j}\right)}_{\hat{s}_{j}}
\end{align*}
Daraus folgt:
\begin{align}\label{eq:321}
  \overline{y}_{\mu}^{+} &= \overline{y}_{\mu} +\hat{s}_{j}
     \frac{\overline{A}_{l,\mu}}{\hat{A}_{l,j}}= \overline{y}_{\mu} +
     \hat{s}_{j}\overline{A}_{l,\mu}\hat{A}_{l,j}\\
  \label{eq:322}
     \hat{s}_{\nu}^{+} &=
     \begin{cases}
       -\hat{s}_{j}\hat{A}_{l,j}^{+} & \nu =j\\
       \hat{s}_{\nu}-\hat{s}_{j}\hat{A}_{l,j}^{+} \hat{A}_{l,\nu} &
       \nu\neq j
     \end{cases}\\
  \label{eq:323}
     \overline{z}^{+} &= \overline{z}+ \hat{s}_{j} \overline{x}_{l}^{+}
\end{align}

Wir definieren $Q$ durch $Q:=(\overline{x}, \overline{A})$ und $q:=
\begin{pmatrix}\overline{z}\\\overline{y}\end{pmatrix}$.

\begin{bsp}
  In \autoref{bsp:313} ist $Q=\begin{pmatrix}3 & | & \nicefrac{1}{4} &
				0\\
				5 & | & 0 &
				\nicefrac{1}{4}\end{pmatrix}$. Die
  Zeilenvektoren von $Q$ sind lexikalisch positiv. Mit der neuen Basis
  $B^{+}$ nach dem Basistausch sei $Q^{+}:= (\overline{x}^{+},
  \overline{A}^{+})$ und $q^{+}=\begin{pmatrix}\overline{z}^{+}
				  \\\overline{y}^{+}\end{pmatrix}$.
\end{bsp}

\begin{lemma}
  \label{lem:324}
  Wird $l$ für den Basistausch nach der zusatzregel und sind die
  Zeilenvektoren von $Q$ lexikalisch positiv, dann gilt:
  \begin{enumerate}[a)]
   \item Zeilenvektoren von $Q^{+}$ sind lexikalisch positiv
   \item $q^{+}\prec q$
  \end{enumerate}
  \begin{proof}
    Nach der Voraussetzung sind die Zeilenvektoren $Q$ lexikalisch
    positiv:
    \begin{gather*}
      \begin{pmatrix}
	\overline{x}_{i}\\\overline{A}_{i}
      \end{pmatrix}\prec 0\qquad i=1,\dotsc,m
    \end{gather*}
    Für $i=l$ gilt nach \autoref{eq:319} und \autoref{eq:320}:
    \begin{gather*}
      \begin{pmatrix}
	\overline{x}_{l}^{+}\\\overline{A}_{l}^{+}
      \end{pmatrix}=\underbrace{\frac{1}{\hat{A}_{l,j}}}_{>0}
	 	 \begin{pmatrix}
		   \overline{x}_{l}\\\overline{A}_{l}
		 \end{pmatrix}\prec 0
    \end{gather*}
    Für $i\in\{1,\dotsc,n-m\}\setminus\{l\}$ gilt
    \begin{gather*}
      \begin{pmatrix}
	\overline{x}_{i}^{+}\\\overline{A}_{i}^{+}
      \end{pmatrix}=
	 	 \begin{pmatrix}
		   \overline{x}_{i}\\\overline{A}_{i}
		 \end{pmatrix}+\hat{A}_{i,j}^{+}
	 	 \begin{pmatrix}
		   \overline{x}_{l}\\\overline{A}_{l}
		 \end{pmatrix}=
	 	 \begin{pmatrix}
		   \overline{x}_{i}\\\overline{A}_{i}
		 \end{pmatrix}-\frac{\hat{A}_{i,j}}{\hat{A}_{l,j}}
	 	 \begin{pmatrix}
		   \overline{x}_{l}\\\overline{A}_{l}
		 \end{pmatrix}
    \end{gather*}
    Für $i\notin\overline{J}$ gilt $\hat{A}_{i,j}\leq 0\Rightarrow
    \begin{pmatrix}\overline{x}_{i}^{+}
      \\\overline{A}_{i}^{+}\end{pmatrix} \prec 0$ und für
    $i\in\overline{J}$ schreiben wir die obige Formel um:
    \begin{gather*}
      \begin{pmatrix}
	\overline{x}_{i}^{+}\\\overline{A}_{i}^{+}
      \end{pmatrix}\hat{A}_{i,j} \left( \frac{1}{\hat{A}_{i,j}}
	 \begin{pmatrix}\overline{x}_{i}\\
	   \overline{A}_{i}\end{pmatrix} -\frac{1}{\hat{A}_{l,j}}
	 	 \begin{pmatrix}
		   \overline{x}_{l}\\\overline{A}_{l}
		 \end{pmatrix}\right)\prec 0
    \end{gather*}

    Der Punkt b) folgt aus \autoref{eq:323} und \autoref{eq:321}:
    $q^{+}=q+ \hat{s}_{j} \hat{A}_{l,j}
    \begin{pmatrix}\overline{x}_{l}\\ \overline{A}_{l}\end{pmatrix}
    \Rightarrow q^{+}\prec q$ 
  \end{proof}
\end{lemma}

\begin{satz}
  Die Zeilenvektoren der zur Basis $B$ gehörenden Matrix $Q$ seien
  lexikalisch positiv. Wird $l$ für den Basistausch in jedem
  Iterationsschritt nach der Zusatzregel gewonnen, dann stoppt das
  Verfahren nach endlich vielen Schritten, wobei das
  Optimalitätskriterium oder das Unbeschränkheitskriterium erfüllt sind.
  \begin{proof}
    Seien $B^{(k)}$ Basen für $k=1,2,3,\dotsc$ und $Q^{k}, q^{(k)}$
    die zugehörigen Matrizen $Q$ und Vektoren $q$. Dann gilt nach
    \autoref{lem:324} $q^{(k+1)}\prec q^{(k)}$ für alle $k$. Nehmen
    wir an, es gelte $B^{(k+\nu)}=B^{(k)}$ mit einem $\nu>0$. Nach der
    Definition von $q$ bzw. $\overline{z}, \overline{y}$ müsste dann
    gelten: $q^{(k+\nu)}=q^{(k)}$. Dies ist allerdings ein Widersprch.
  \end{proof}
\end{satz}

\begin{bem}
  Sei ein lineares Programm $\min c^{T}x$ \unb $Ax\leq b, x\geq 0$ mit
  $b\geq 0$ gegeben. Durch die Einführung von Schlupfvariablen
  $y=\begin{pmatrix}y_{1}\\\vdots\\y_{m}\end{pmatrix}$ erhalten wir
  $\min (c^{T}, 0_{m})\begin{pmatrix}x\\y\end{pmatrix}$ \unb
  $\underbrace{(A,I_{m})}_{\tilde{A}} \begin{pmatrix}x\\y\end{pmatrix}=b,
  \begin{pmatrix}x\\y\end{pmatrix} \geq 0$. Startbasis sind die
  letzten $m$ Spaltenvektoren von $\tilde{A}$. Dann ist
  $\overline{x}=b$ und $Q= \begin{pmatrix}b_{1}& | & 1 & \hdots & 0\\
			     \vdots & | & \vdots & \ddots & \vdots\\
			     b_{m} & | & 0 & \hdots & 1\end{pmatrix}$.
\end{bem}

\begin{bsp}
  \todo{Tabelle einfügen}
\end{bsp}

\subsection{Implemtierung des Austauschschritts}

Sei ein Tableau mit den Daten $b, n, \hat{A}, \overline{A},
\overline{x}, \hat{s}, \overline{y}, \overline{z}$ gegeben. Weiterhin
seien $j\in\{1,\dotsc,n-m\}$ und $l\in\{1,\dotsc,m\}$ die Indizes für
den Basistausch.
\begin{enumerate}
 \item Neuberechnung der Pivotspalte nach \autoref{eq:317}
 \item Neuberechnung von $\overline{x}$ nach \autoref{eq:319}
 \item Neuberechnung von $\overline{z}$ mit \autoref{eq:323},
  $\overline{y}$ nach \autoref{eq:321} und $\hat{s}$ nach
  \autoref{eq:322}
 \item Neuberechnung der restlichen Spalten von $\hat{A}$ nach
  \autoref{eq:318}
 \item Neuberechnung von $\overline{A}$ nach \autoref{eq:320}
 \item Indextausch
\end{enumerate}

\subsection{Die lexikografische Zusatzregel}

\begin{defin}
  Ein Vektor $x\in\R^{n}\setminus\{0_{n}\}$ heißt
  \highl{lexikografisch positiv}, wofür wir $x\prec 0_{n}$ schreiben,
  wenn die erste von Null verschieden Komponente größer als Null
  ist.\\
  Sind $x,y\in\R^{n}$, dann heißt $x$ lexikografisch größer
  $y$, falls $x-y>0$ ist.
\end{defin}

\begin{bsp}
  Die Vektoren $x=(0,1,2)^{T}, y=(0,2,-1)^{T}$ sind lexikografisch
  positiv. Weiter gilt $x-y= (0,-1,3)^{T}\prec 0, y-x= (0,1,-3)^{T}
  \succ 0\Rightarrow y\prec x$.
\end{bsp}

\begin{bem}
  Zwei Vektoren $x,y\in\R^{n}$ sind bezüglich der lexikografischen
  Ordnung immer vergleichbar, d.\,h. $x\neq y\Rightarrow x\succ y
  \wedge x\prec y$.

  Sei $y\in\R^{n}$ fest und $K=\Set{x\in\R| x\succeq y}$. Dann ist $K$
  nicht immer abgeschlossen. Dazu betrachten wir einen
  Iterationsschritt des Simplexverfahrens. Sei $B$ eine aktuelle
  Basis, $j\in \{1,\dotsc,n-m\}$ mit $s_{N,j}<0, \sigma=\min\Set{
  \frac{\overline{x}_{\nu}}{\hat{A}_{\nu,l}}| \nu\in\{1,\dotsc,m\}
  \text{ mit } \hat{A}_{\nu,l}>0}, J=\Set{\mu\in \{1,\dotsc,m\}|
  \hat{A}_{\mu,j}>0, \sigma=\frac{\overline{x}_{\mu}}{\hat{A}_{\mu,j}}}$

  Mit $\overline{A}_{i}$ bezeichnen wir den $i$-ten Zeilenvektor von
  $\overline{A}=A_{B}^{-1}$. Für die Bestimmung von $l$ benützen wir
  die lexikografische Zusatzregel. Wähle $l\in J$ so, dass
  $\frac{1}{\hat{A}_{l,j}} \overline{A}_{l}\prec
  \frac{1}{\hat{A}_{\mu,l}} \overline{A}_{\mu}$ für alle $\mu\in
  J\setminus \{l\}$.
\end{bem}

\begin{bem}
  Hat $J$ mehr als ein Element, dann sind die Vektoren
  $\frac{\overline{A}_{\mu}}{\hat{A}_{\mu,j}}, \mu\in J$ paarweise
  verschieden. Damit ist $l$ durch die lexikografische Zusatzregel
  eindeutig bestimmt.

  Definieren $\overline{J}= \Set{\nu\in \{1,\dotsc,m\}|
  \hat{A}_{\nu,j}>0}\Rightarrow J\leq \overline{J}$. Nach der
  Definition von $J$ und $\overline{J}$ gilt für $\mu\in J\colon
  \sigma= \frac{\overline{x}_{\mu}}{\hat{A}_{\mu,j}}$. Betrachten
  $\frac{\overline{x}_{\nu}}{\hat{A}_{\nu,j}}$ für alle $\nu\in
  \overline{J}\setminus J\Rightarrow \sigma<
  \frac{\overline{x}_{\nu}}{\hat{A}_{\nu,j}}$. Damit erhält man die
  äquivalente Form der lexikografischen Zusatzregel: Wähle
  $l\in\overline{J}$ so, dass gilt:
  \begin{gather*}
    \frac{1}{\hat{A}_{l,j}}\begin{pmatrix}\overline{x}_{l} \\
			     \hat{A}_{l}\end{pmatrix} \prec \frac{1}{\hat{A}_{\nu,j}}
       \begin{pmatrix}
	 \overline{x}_{\nu}\\ \overline{A}_{\nu}
       \end{pmatrix}\qquad \nu\in\overline{J}\setminus\{l\}
  \end{gather*}
\end{bem}

\begin{bsp}
  \label{bsp:327}
  \todo{Tabelle einfügen}
  Mit der Zusatzregel ergibt sich $\frac{1}{4}
  \begin{pmatrix}0\\1\\0\end{pmatrix} \prec \frac{1}{4}
  \begin{pmatrix}0\\0\\1\end{pmatrix}\Rightarrow l=3$.
\end{bsp}

\section{Phase 1 des Simplexverfahrens}

Ausgangspunkt ist ein lineares Programm in Standardform. Die Phase 1
löst das Hilfsproblem (HP) und man erhält aus der Lösung des
Hilfsproblems
\begin{itemize}
 \item die Startbasis für die Phase 2 oder
 \item die Information, dass die zulässige Menge nichtleer ist oder
 \item die Information, dass der Rang von $A$ kleiner als $m$ ist
  (redundante Gleichungen)
\end{itemize}

\minisec{Das Hilfsproblem}
Wir bezeichnen den Einheitsvektor mit $e=(1,1,\dotsc,1)^{T}$ und die
Variablen des Hilfsproblems mit $z= (z_{1},\dotsc, z_{n},
z_{n+1},\dotsc, z_{n+m})^{T} = (x_{1},\dotsc, x_{n}, y_{1},\dotsc,
y_{m})^{T}$. Das Hilfsproblem lautet $\min e^{T}y= \sum_{i=1}^{m}
y_{i}$ \unb $\begin{pmatrix}x\\y\end{pmatrix} \in\tilde{\FF} :=
\Set{z=\begin{pmatrix}x\\y\end{pmatrix} \in\R^{n+m}|
  \begin{pmatrix}x\\y\end{pmatrix} \geq 0, (A, I_{m})
  \begin{pmatrix}x\\y\end{pmatrix}=b}=
\Set{z=\begin{pmatrix}x\\y\end{pmatrix}| x,y\geq 0, Ax+y=b}$.

Ist $\begin{pmatrix}x\\y\end{pmatrix} \in\tilde{\FF}$ und
$y=0_{m}\Rightarrow Ax=b$, d.\,h. $x\in\FF$. Wir schreiben das
Hilfsproblem als lineares Programm in Standardform: Definieren
$\tilde{c}= \begin{pmatrix}0_{n}\\e\end{pmatrix}, \tilde{A}=(A,I_{m})$
und es ist $\min \tilde{c}^{T} z$ \unb $z\geq 0, \tilde{A}z=b$. Sei
o.\,B.\,d.\,A. $b\geq0$. Es gilt, $\FF\neq \emptyset$. Die
Zielfunktion ist durch 0 nach unten beschränkt. Nach dem Hauptsatz der
linearen Optimierung existiert eine Ecke, die Lösung ist.

Nun wenden wir die Phase 2 des Simplexverfahrens auf das Hilfsproblem
an. Wir wählen die letzten $m$ Spalten von $\tilde{A}$ als Basis. Dann
ist:
\begin{align*}
  J_{B} &= \{n+1,\dotsc,n+m\} & J_{N} &= \{1,\dotsc,n\} &
     \tilde{A}_{B} &= I_{m} & \tilde{A}_{N} &=A
\end{align*}

Die zugehörige Basislösung ist
$z_{B}=\underbrace{\tilde{A}_{B}^{-1}}_{=I_{m}} b=b, z_{N}=0_{n}$.
Wegen $b\geq 0$ ist $\tilde{z}_{B}\geq0\Rightarrow$ zulässige
Basislösung. Einträge für das Starttableau:
\begin{align*}
  \tilde{A}_{B}^{-1} &= I_{m}\\
  \tilde{A}_{B}^{-1} \tilde{A}_{N} &= \tilde{A}_{N}=A\\
  \hat{s}^{T} &= \tilde{c}_{N}^{T} - \tilde{c}_{B}^{T}
     \tilde{A}_{B}^{-1} \tilde{A}_{N} = 0_{n}-e^{T}I_{m}A=-e^{T}A\\
  \tilde{c}^{T}z &= \tilde{c}_{B}^{T} z_{b} = e^{T}b\\
  y^{T} &= \tilde{c}_{B}^{T} \tilde{A}_{B}^{-1}=e^{T}
\end{align*}

\todo{Tableau einfügen}

Damit folgt, dass das Simplexverfahren mit der Zusatzregel eine Ecke
nach endlich vielen Schritten berechnet.

\begin{lemma}
  Sei $\tilde{z}=\begin{pmatrix}\tilde{x}\\\tilde{y}\end{pmatrix}$
  eine Lösung des Hilfsproblems. Ist der Optimalwert $\tilde{c}^{T}
  \tilde{z}=0$, dann erfüllt $\tilde{x}\in\FF$, also $\FF\neq 0$ und
  $\tilde{y}=0_{m}$. Ist der Optimalwert $\tilde{c}^{T}\tilde{z}>0$,
  dann ist $\FF=\emptyset$.
  \begin{proof}
    Sei $\tilde{c}^{T} \tilde{z}=\sum_{i=1}^{m} \tilde{y}_{i}=0$.
    Wegen $\tilde{y}\geq 0\Rightarrow \tilde{y}=0_{m}\Rightarrow
    b=\tilde{A}\tilde{z}= (A, I_{m}) \begin{pmatrix}\tilde{x}\\
				       \tilde{y}\end{pmatrix} =
    A\tilde{x} +\tilde{y}=A\tilde{x}, \tilde{x}\geq 0 \Rightarrow
    \tilde{x}\in\FF$, d.\,h. $\FF\neq\emptyset$.

    Sei $\FF\neq\emptyset$ und $x\in\FF$. Dann ist $z=
    \begin{pmatrix}x\\ 0_{m}\end{pmatrix}\in \tilde{\FF}$. Denn $z\geq
    0$ und $\tilde{A}z= Ax+0_{m}=b$. Damit ist der Optimalwert Null.
  \end{proof}
\end{lemma}

Das Lemma sagt uns: Nach Anwendung der Phase~2 des Simplexverfahrens
auf das Hilfsproblem gibt es zwei Möglichkeiten. Für die berechnete
Lösung gilt:
\begin{enumerate}
 \item $\tilde{c}^{T} \tilde{z}>0$: \emph{Stoppe} das Verfahren, dann
  $\FF0=$.
 \item $\tilde{c}^{T}\tilde{z}=0$: (Damit ist auch $\tilde{y}=0_{m}$).
  Dies ist noch genauer zu untersuchen.
\end{enumerate}

\begin{bsp}
  \begin{gather*}
    \min 2x_{1} + 6x_{2} -7x_{3} +2x_{4} +4x_{5}
  \end{gather*}
  \unb
  \begin{gather*}
    4x_{1}-3x_{2} +8x_{3} -x_{4} = 12\\
    -x_{2} +12x_{3} -3x_{4}+4x_{5} = 20\\
    x_{i} \geq 0, i=1,\dotsc,5
  \end{gather*}
  Das Hilfsproblem lautet wie folgt:
  \begin{gather*}
    \min y_{1}+y_{2}
  \end{gather*}
  \unb
  \begin{gather*}
    4x_{1} -3x_{2} +8x_{3} -x_{4} =12\\
    -x_{2} +12x_{3} -3x_{4} 4x_{5} =20\\
    x_{i}\geq 0, i=1,\dotsc,5, y_{1}, y_{2}\geq 0
  \end{gather*}
  \todo{Tableau einfügen}
  Die Lösung ist:
  \begin{align*}
    \tilde{z} &= \begin{pmatrix}\\\\\tilde{x}\\\\\\\tilde{y}\\\end{pmatrix}
       &= \begin{pmatrix}0\\0\\\nicefrac{3}{2}\\ 0\\ \nicefrac{1}{2} \\ 0\\0
	  \end{pmatrix} \Rightarrow \tilde{y}=0_{m}
  \end{align*}
  Es gilt: $\tilde{x}\in\FF$. Alle Basisindizes sind kleiner als
  $n=5$. Damit ist $\tilde{x}$ Basislösung für das Anfangsproblem
  ($\tilde{x}$ ist Ecke für das Anfangsproblem.).

  Sei $\tilde{z}=\begin{pmatrix}\tilde{x}\\\tilde{y}\end{pmatrix}$ die
  berechnete Lösung des Hilfsproblems. Es gelte: $\tilde{c}^{T}
  \tilde{z}=0, \tilde{y}=0_{m}$. Weiter ist:
  \begin{align*}
    B=\{\tilde{A}_{.b(1)},\dotsc, \tilde{A}_{.b(n)}\} & N =
       \{\tilde{A}_{.n(1)},\dotsc, \tilde{A}_{.n(n)}\}
  \end{align*}
  Letztes Tableau der Phase~1: \todo{Tableau einfügen}
  Dann ist $x\in\FF$.

  Im schönen Fall gilt, $b(i)\leq n, i=1,\dotsc,m$., d.\,h. zur Basis
  gehören nur Spaltenvektoren von $A$. Dann folgt, dass auch $B$ Basis
  von $A$ ist und $\tilde{x}$ Basislösung zur Basis $B$ ist. Somit
  kann $B$ auch als Startbasis für die Phase~2 genutzt werden.

  Sei $l\in\{1,\dotsc,m\}$ ein Index mit $b(l)=\max_{1\leq i\leq m}
  b(i)$. Wir unterscheiden zwei Fälle:
  \begin{enumerate}
   \item Es ist $b(l)\leq n$, d.\,h. alle Basisindizes sind $\leq
    n\Rightarrow B\subset \{A_{.1},\dotsc, A_{.n}\} \Rightarrow
    \tilde{x}$ ist auch Basislösung zu $Ax=b$. Somit ist $\tilde{x}$
    zulässige Basislösung für das Ausgangsproblem. Daher können wir
    mit der Basis $B$ von $A$ die Phase~2 starten. Der obere Teil des
    Starttableaus entsteht aus dem Endtableau der Phase~1 durch das
    Streichen derjenigen Spalten, die zu Indizes $>n$ gehören.
   \item Es ist $b(l)>n$, d.\,h. einige Hilfsvariablen sind
    Basisvariablen.
    \begin{enumerate}[a)]
     \item Es gibt einen Index $j\in\{1,\dotsc,n\}$ mit
      $n(j)\in\{1,\dotsc,n\}$ (d.\,h. die Variable $x_{n(j)}$ ist
      keine Basisvariable)
      \begin{gather*}
	\tilde{z}=\begin{pmatrix}\tilde{x}\\\tilde{y}\end{pmatrix} =
	   (\tilde{x}_{1},\dotsc, \tilde{x}_{n}, \tilde{y}_{1},\dotsc,
	   \tilde{y}_{m})^{T}
      \end{gather*}
      $b(l)>n\Rightarrow \tilde{y}_{b(l)-n}$ ist Basisvariable und für
      den $j$-ten Spaltenvektor der Matrix $\tilde{A}:=
      \tilde{A}_{B}^{-1}\tilde{A}_{N}$ gilt: $\hat{A}_{l,j}\neq 0$.

      In dem Fall können wir nach \autoref{lem:39} einen Basistausch $b(l)
      \Leftrightarrow n(j)$ durchführen und wir erhalten eine neue
      Basis. Nach endlich vielen Schritten sind keine Hilfsvariablen
      mehr Basisvariablen. Dann gehen wir analog zu Fall 1 vor.
     \item Für alle Indizes $j\in\{1,\dotsc,n\}$ mit
      $n(j)\in\{1,\dotsc,n\}$ ist $\hat{A}_{l,j}=0$. Wegen $b(l)>n$
      gibt es ein $i\in\{1,\dotsc,m\}$ mit $b(l)=n+1\Leftrightarrow
      i=b(l)-n$. Dann kann man zeigen: Die $i$-te Zeile von $A$ ist
      Linearkombination der restlichen Zahlen, d.\,h. $\rg(A)<m$ bzw.
      die $i$-te Gleichung ist redundant und kann daher gestrichen
      werden.
    \end{enumerate}
  \end{enumerate}
\end{bsp}

\begin{bsp}
  \begin{gather*}
    \min 2x_{1} +3x_{2}+ x_{3}
  \end{gather*}
  \unb
  \begin{gather*}
    -x_{1} + 2x_{2} + 3x_{4} = 2\\
    4x_{1} + 2x_{2} + 4x_{4} = 2\\
    x_{1} + x_{3} + 4x_{4} = 2\\
    x_{i} \geq 0
  \end{gather*}
  Hilfsproblem für Phase 1\\
  Das Problem aus \autoref{bsp:327} war: \todo{Tabelle einfügen}

  Wir tauschen $x_{5}=y_{1}$ gegen $x_{4}$: \todo{Tabelle einfügen}

  Das Streichen ergibt das obere Starttableau für die Phase 2. Die
  letzte Zeile muss neu berechnet werden.
\end{bsp}

\begin{verf}[Zusammenfassung der Phase 1]
  \begin{enumerate}
   \item Berechne eine Lösung
    $\tilde{e}=\begin{pmatrix}\tilde{x}\\\tilde{y}\end{pmatrix}$ vom
    Hilfsproblem mit der Phase 2. Ist $\tilde{x}^{T}\tilde{z} > 0
    \Rightarrow \FF =\emptyset$ und das Verfahren stoppt.
   \item Berechne $l\in\{1,\dotsc,m\}$ mit $b(l)=\max_{1\leq i\leq m}
    b(i=)$.
   \item Ist $b(l)\leq n$, dann streiche die zu den Hilfsvariablen
    gehörigen Spalten im linken Tableauteil, berechne die letzte
    Tableauzeile neu und starte Phase 2.
   \item Ist $b(l)>n$, dann führe den Basistausch $b(l)\Leftrightarrow
    n(j)$ durch, wenn es ein $j\in\{1,\dotsc,n\}$ mit
    $n(j)\in\{1,\dotsc,n\}$ und $\hat{A}_{l,j}\neq\emptyset$ gibt.
   \item Falls $\rg(A)<m$, berechne $i=b(l)-n$ und streiche die $i$-te
    Gleichungsrestriktion.xs
  \end{enumerate}
\end{verf}

\begin{bem}
  Gegeben sei ein lineares Problem vom Typ $\min c^{T}x$ \unb $Ax\leq
  b, x\geq 0$. Wenn wir das in Standardform transformieren, erhalten
  wir $\min (c^{T}, 0,\dotsc,0) \begin{pmatrix}x\\y\end{pmatrix}$ \unb
  $(A, I_{m}) \begin{pmatrix}x\\y\end{pmatrix}=b, x,y\geq 0$.
  Naheliegend ist es, $J_{B}=\{n+1,\dotsc,n+m\}$ zu wählen. Dies geht
  nur, wenn $b\geq 0$ ist.
\end{bem}

\section{Das revidierte Simplex-Verfahren}

Dieses Verfahren benutzt anstelle von $\hat{A}$ nur die Pivotspalte
$\hat{w}$ und spart damit Speicherplatz. Es erhebt sich die Frage, wie
man dann den Basiswechsel durchführen kann.

Wir wählen einen Index $j\in\{1,\dotsc,n-m\}$ mit $\hat{s}_{j}<0$. Wir
berechnen die Pivotspalte $\hat{w} = A^{-1}_{B} A_{.n(j)}$. Falls
$\hat{w} \leq0$ ist das Unbeschränkheitskriterium erfüllt und das
Verfahren stoppt.

Als nächsten wählen wir einen Index $l\in\{1,\dotsc,m\}$ mit $\sigma =
\frac{\overline{x}_{l}}{\hat{w}_{l}} =
\min\{\frac{\overline{x}_{\mu}}{\hat{w}_{\mu}}\colon 1\leq \mu \leq m,
\hat{w}_{\mu}>0\}$. Benutzt man beim reduzierten Problem die
Suchrichtung $d$ mit $d_{j}=1, d_{i}=0, i\neq j$, dann ist $\delta$
die maximale Schrittweite mit $v(t)=td\in\FF_{B}$.

Die neue Ecke wird definiert durch
\begin{align*}
  x^{+}_{N} &= v^{+}=\delta d\\
  x^{+}_{B} &= x^{*}_{B}-\delta\hat{w}
\end{align*}

Somit erhalten wir für $x^{+}$ die einfache Updateformel:
\begin{align*}
  x^{+}_{N} &= x^{*}_{B} -\delta\hat{w}\\
  x^{+}_{n(j)} &= \delta
\end{align*}
Für den Vektor $\overline{x}=x^{*}_{B}$ bedeutet das:
\begin{gather}\label{eq:324}
  \overline{x}^{+}_{i}=
     \begin{cases}
       \overline{l}- \delta \hat{w}_{i} & i\neq l\\
       \delta & i=l
     \end{cases}
\end{gather}

Fasst man die Updateformel (\autoref{eq:317}) für die Pivotspalte und
\autoref{eq:320} für $\overline{A}$ zusammen, folgt:
\begin{gather}\label{eq:325}
  \overline{A}^{+}_{l,\mu} = \frac{\overline{A}_{l,\mu}}{\hat{w}_{l}}
     \qquad \mu=1,\dotsc,m\\
\end{gather}
und für $i\leq l$:
\begin{gather}\label{eq:326}
  \overline{A}^{+}_{i,\mu} = \overline{A}_{i,\mu}- \frac{\hat{w}_{i}
     \overline{A}_{l,\mu}}{\hat{w}_{l}} \qquad \mu= 1,\dotsc,m
\end{gather}

\begin{verf}[revidiertes Simplex-Verfahren, Phase 2]
  Gegeben seien eine Basis $B=B^{(0)}$ von $A$, die Matrix
  $\overline{A}=A^{-1}_{B}$ und $\overline{x}=x_{B}$. Setze $k\colon=
  0$.
  \begin{enumerate}
   \item Berechne $\overline{y}=A^{T} c_{B}, \hat{s}$ mit den
    Komponenten $\hat{s}_{r}=c_{n(r)}-\overline{y} A_{.n(r)}$.
   \item Optimalitätskriterium: Gilt $\hat{s}\geq0$, dann ist die
    aktuelle Ecke optimal und das Verfahren stoppt.
   \item Wähle $j\in\{1,\dotsc,n-m\}$ mit $\hat{s}_{j}<0$ und berechne
    $\hat{w} =\overline{A}A_{.n(j)}$.
   \item Unbeschränkheitskriterium: $\hat{w}\leq 0\Rightarrow 0$
   \item Wähle $l\in\{1,\dotsc,m\}$ und $\delta$ wie oben.
   \item Update von $\overline{x}$ und $\overline{A}$ nach
    \autoref{eq:324}, \autoref{eq:325} und \autoref{eq:326}.
   \item Indextausch: $b(l)\Leftrightarrow n(j)$
   \item Setze $k\colon= k+1$ und gehe zum ersten Punkt.
  \end{enumerate}
\end{verf}

\chapter{Optimalitätsbedingungen und Dualität}

Wir erinnern uns an die Analysisvorlesung. Seien $f\colon \R^{n}
\rightarrow \R, g\colon \R^{n}\rightarrow \R^{m}$ stetig
differenzierbar. Nun betrachten wir die Aufgabe $\min f(x)$ \unb
$g(x)=0$.

Falls $\tilde{x}$ Lösung des linearen Problems ist und $g(\tilde{x})$
vollen Rang hat, dann gibt es ein $\lambda\in\R^{m}$ mit
$f'(\tilde{x}) +\lambda^{T}g'(\tilde{x})=0$. Das $\lambda$ wird als
\highl{Lagrangemultiplikator} bezeichnet. Für den Spezialfall
$A\in\R^{m\times n}, b\in\R^{m}, g(x)=Ax-b$ gilt:
\begin{gather*}
  g(x)=0 \Leftrightarrow Ax=b\\
  f'(\tilde{x})+\lambda^{T} A=0 \Leftrightarrow \nabla f(\tilde{x}) +
     A^{T}\lambda=0
\end{gather*}

Nutzen dieser Bedigungen:
\begin{enumerate}
 \item $\tilde{x}$ Lösung $\Rightarrow\exists \lambda\colon \nabla
  f(\tilde{x}) + A^{T}\lambda=0$ ($n$ Gleichungen)
 \item $\tilde{x}$ ist Lösung $\Rightarrow \tilde{x}$ ist zulässig
  $\Rightarrow A\tilde{x}=b$ ($m$ Gleichungen)
\end{enumerate}
Damit haben wir $n+m$ Gleichungen für die $n+m$ Unbekannten
$\tilde{\alpha}\in \R^{n}, \lambda\in\R^{m}$.

\section{Optimalitätsbedingungen}

Sei $c\in\R^{n}, A\in\R^{m\times n}, B\in\R^{m}, m\leq n$ sowie das
lineare Problem $\min c^{T}x$ \unb $x\in\FF=\{x\in\R^{n}\colon x\geq
0, Ax=b\}$.

\begin{satz}[Kuhn-Tucker]\label{satz:41}
  Ein Punkt $\tilde{x}\in\FF$ ist genau dann Lösung des linearen
  Problem, wenn ein $\tilde{y}\in\R^{m}, \tilde{s}\in\R^{n}$ existiert mit:
  \begin{gather}\label{eq:41}
    A^{T}\tilde{y}+\tilde{s}=c\qquad \tilde{x}^{T}\delta =\sigma\qquad
    \tilde{s}\geq0
  \end{gather}
  \begin{proof}
    \begin{itemize}
     \item["`$\Rightarrow$"'] Sei $x\in\FF$ die Lösung des linearen
      Problems. Wir betrachten zunächst den Fall $\rg(A)=m$. Da das
      Problem eine Lösung hat, berechnet das Simplex-Verfahren (nach
      \autoref{satz:325}) in endlich vielen Schritten eine optimale
      Ecke $x$ zu einer Basis $B$ von $A$.

      Weiter ist die Optimalitätsbedingung $s_{N}\geq 0$ erfüllt und
      $s_{N}^{T} = c_{N}^{T} - c_{B}^{T} A^{-1}_{B} A_{N}$. mit
      $\tilde{y}^{T}\colon= c^{T}_{B} A^{-1}_{B}$ gilt daher:
      \begin{align*}
	s_{N} = c_{N} - A^{T}_{N}\tilde{y} &\Leftrightarrow A^{T}_{N}
	   \tilde{y}\leq c_{N}\\
	&\Rightarrow \begin{pmatrix}A^{T}_{B}\\A^{T}_{N}\end{pmatrix}
	   \tilde{y} =
	   \begin{pmatrix}c_{B}\\A^{T}_{N}\tilde{y}\end{pmatrix} \leq
	   \begin{pmatrix}c_{B}\\c_{N}\end{pmatrix}\\
	&\Leftrightarrow A^{T}\tilde{y}\leq c
      \end{align*}
      Wir definieren $\tilde{s}\colon= c-A^{T} \tilde{y}$. Dann gilt
      $\tilde{s}\geq0, A^{T}\tilde{y}+\tilde{s}=c$ und weiter $b^{T}
      \tilde{y} = y^{T}b= c^{T}_{B} \underbrace{A^{-1}_{B}b}_{=x_{B}}
      = c^{T}_{B} x_{B} =^{x_{N}=0} c^{T}x=c^{T}\tilde{x} \Rightarrow
      \tilde{x}^{T} \tilde{s} = \tilde{x}^{T} (c-A^{T} \tilde{y})=
      c^{T}x - (\underbrace{A\tilde{x}}_{=b})\tilde{y}= c^{T}
      \tilde{x} - b\tilde{y}= c^{T}\tilde{x}-c^{T} \tilde{x}=0$.
      \begin{gather}\label{eq:41.1}
	\tilde{x}^{T}\tilde{s}=0
      \end{gather}

      Ist $rg(A)=m_{1}<m$, dann können wir $m-m_{1}$ redundante
      Gleichungen streichen. Damit erhält man ein äquivalentes Problem
      zum (LP) und man kann den obigen Beweis anwenden. Man erhöht
      \autoref{eq:41} für ein $\tilde{y}\in\R^{m}$. Setzt man für eine
      redundante Gleichung mit Index $i\colon \tilde{y}_{i}=0$, dann
      erhält man \autoref{eq:41} für das Ausgangsproblem.
      \item["`$\Leftarrow$"'] Seien $\tilde{x}\in\FF$ und
      $\tilde{y}\in\R^{m}, \tilde{s}\in\R^{n}$ mit \autoref{eq:41}
      gegeben. Dann gilt \autoref{eq:41.1} und es folgt:
      \begin{gather}\label{eq:41.2}
	c^{T}\tilde{x} = b^{T}y	
      \end{gather}
      Sei $x\in\FF$ beliebig. Wegen \autoref{eq:41} ist
      $A^{T}\tilde{y} =c-\tilde{s}$. Wir zeigen $c^{T}x\geq c^{T}
      \tilde{x}$:
      \begin{gather*}
	b^{T} \tilde{y} = (Ax)^{T}\tilde{y} = x^{T}A^{T}\tilde{y} =
	   x^{T} (c-\tilde{s}) = x^{T}c-x^{T}\tilde{s} = c^{T}x -
	   \underbrace{x^{T}\tilde{s}}_{\geq 0}
      \end{gather*}
      Weiter ist $x\geq 0, \tilde{s}\geq0 \Rightarrow
      x^{T}\tilde{s}\geq 0 \Rightarrow c^{T}\tilde{x} = c^{T}x - x^{T}
      \tilde{s} \leq c^{T}x$.
    \end{itemize}
  \end{proof}
\end{satz}

Die Vektoren $\tilde{y}, \tilde{s}$ heißen
\highl{Lagrangemultiplikatoren}. Der Beweis des obigen Satzes zeigt:
\begin{gather}\label{eq:42}
  \tilde{x}^{T}\tilde{s}=0 \Leftrightarrow c^{T} \tilde{x} = b^{T}
     \tilde{y}
\end{gather}
Aus $\tilde{x}^{T}\tilde{s}= \sum_{i=1}^{n} \tilde{x}_{i} s_{i}=0$ und
$\tilde{x}\geq0, \tilde{s}\geq 0\Leftrightarrow$:
\begin{gather}\label{eq:43}
  \tilde{x}_{i} \tilde{s}_{i} = 0 \qquad \forall i=1,\dotsc,n
\end{gather}
d.\,h. ist $\tilde{x}_{i}>0\Rightarrow \tilde{s}_{i}=0$ und
$\tilde{s}_{i}>0\Rightarrow \tilde{x}_{i}=0$. Die \autoref{eq:43}
heißen \highl{Komplementaritätsbedingungen}.

Somit folgt aus \autoref{satz:41}, dass ein Punkt $\tilde{x}\in\FF$
genau dann Lösung des linearen Problems ist, wenn
$\exists\tilde{y}\in\R^{m}, \exists\tilde{s}\in\R^{n}\colon (x,y,s)=
(\tilde{x}, \tilde{y}, \tilde{s})$ Lösung des Systems:
\begin{gather}\label{eq:44}
  Ax=b\qquad x \geq 0\\
  A^{T}y+s=0 \qquad s\geq 0\nonumber\\
  x_{i}s_{i}=0 \qquad \forall i=1,\dotsc,n \nonumber
\end{gather}
ist. Dabei heißt $x$ \highl[Variable!primale]{primale Variable} und
$y,s$ \highl[Variable!duale]{duale Variablen}.

\begin{bem}
  Der Multiplikator $\tilde{y}$ war formal definiert durch
  $\tilde{y}=(c^{T}_{B} A^{-1}_{B})^{T}=(A^{-1}_{B})^{T} c_{B}$. Der
  Vektor $\tilde{y}$ wird vom Simplex-Verfahren mit berechnet. Es ist
  der untere rechte Tabelleneintrag.

  Der Vektor $\tilde{s}$ war definiert durch:
  \begin{align*}
    \tilde{s}=c-A^{T}\tilde{y} \Leftrightarrow &\tilde{s}_{B}=
       c_{B}-A^{T}_{B}\tilde{y}= c_{B}-c_{B}=0\\
       &\tilde{s}_{N}=c_{N}-A^{T}_{N}\tilde{y}=c_{N}-c^{T}_{B}A^{-1}_{B}A_{N}
  \end{align*}
  d.\,h. $\tilde{s}_{B}=0_{m}$ und $\tilde{s}_{n}$ ist der vom
  Simplex-Verfahren berechnete Testvektor. Für eine optimale
  Basislösung $\tilde{x}$ zur Basis $B$ gilt daher:
  \begin{gather*}
    \tilde{x}^{T}\tilde{s}= \tilde{x}^{T}_{B}\tilde{s}_{B} +
       \tilde{x}^{T}_{N} \tilde{s}_{N}=0
  \end{gather*}
\end{bem}

\section{Duale Programme}

Wegen \autoref{eq:42} ist das System aus \autoref{eq:44} äquivalent zu:
\begin{align}\label{eq:45}
  Ax &= b \qquad x\geq 0\\
  A^{T}y+s &= c \qquad s\geq 0\nonumber\\
  c^{T}x &= b^{T}y \Leftrightarrow x^{T}s=0
\end{align}

\begin{bsp}
  Wir betrachten das bereits bekannte Mischungsproblem
  (\autoref{bsp:17}). Gegeben sind
  drei Kornsorten $s_{1}, s_{2}, s_{3}$ mit den Nährstoffen $A,B,C,D$.
  \begin{tabular}{c|rrr|r}
    & $s_{1}$ & $s_{2}$ & $s_{3}$ & Mindestbedarf\\
    \toprule
    A & 2 & 3 & 7 & 1250\\
    B & 1 & 1 & 0 & 250\\
    C & 5 & 3 & 0 & 900\\
    D & 0,6 & 0,25 & 1 & 232,5\\
    \bottomrule
    Preis & 41 & 25 & 96 &
  \end{tabular}
  Das Ausgangs- oder primale Problem war die Herstellung einer
  Mischung, die den Nährstoffbedarf deckt und möglichst billig ist.
  Als lineares Problem:
  \begin{align*}
    \min 41x_{1} + 35x_{2} + 96x_{3} &\\
    \text{ \unb}&\\
    2x_{1} + 3x_{2} + 7x_{3} &\geq 1250\\
    x_{1}  +  x_{2}          &\geq 250\\
    5x_{1} + 3x_{2}          &\geq 900\\
    0,6x_{1} + 0,25x_{2} + x_{3} &\geq 232,5\\
    x_{i} &\geq 0
  \end{align*}
  Ein zweiter Händler benutzt andere Kornsorten und möchte ein
  Konkurrenzangebot machen. Die fiktiven Preise sind $y_{1}$ für $A$,
  $y_{2}$ für $B$, $y_{3}$ für $C$ und $y_{4}$ für $D$. Das Ziel des
  zweiten Händlers ist, die fiktiven Preise so zu berechnen, dass der
  Gewinn möglichst groß ist und der Preis pro Mengeneinheit höchstens
  so groß ist, wie der des ersten Händlers. Die Zielfunktion ist:
  \begin{gather*}
    1250y_{1}+250y_{2}+900y_{3}+232,5y_{4}=b^{T}y
  \end{gather*}
  Als Nebenbedingungen gilt, dass das Angebot konkurrenzfähig sein muss:
  \begin{align*}
    s_{1}&\colon 2y_{1}+y_{2}+5y_{3}+0,6y_{4} \leq 41\\
    s_{2}&\colon 3y_{1}+y_{2}+3y_{3}+0,25y_{4} \leq 35\\
    s_{3}&\colon 7y_{1}+y_{4}\leq 96
  \end{align*}
  Sinnvollerweise müssen die $y_{i}\geq 0$ sein. Das duale Problem
  lautet: $\max b^{T}y$ \unb $A^{T}\leq c, y\geq0$.

  Die obige Tabelle kann auf Standardform transformiert werden. Damit
  folgt, $-y_{i}\leq 0, i=1,\dotsc,4 \Leftrightarrow y_{i}\geq 0$. Sei
  $\tilde{y} =(\tilde{y}_{1},\dotsc,\tilde{y}_{4})^{T}$ Lösung des
  dualen Problems. Das Angebot des Konkurrenten sei beispielsweise:
  \begin{tabular}{c|rrr}
    & $s_{1}'$ & $s_{2}'$ & $s_{3}'$\\
    \hline
    A & 3 & 2 & 3\\
    B & 0,5 & 4 & 1\\
    C & 6 & 4 & 1\\
    D & 1 & 0 & 0
  \end{tabular}
  Dann sind die optimalen Preise für die Kornsorten:
  \begin{align*}
    s_{1}' &\colon 3\tilde{y}_{1}+ 0,5\tilde{y}_{2}+ 6\tilde{y}_{3} +
       \tilde{y}_{4}\\
    s_{2}' &\colon 2\tilde{y}_{1}+ 4\tilde{y}_{2}+ 4\tilde{y}_{3}\\
    s_{3}' &\colon 3\tilde{y}_{1}+ \tilde{y}_{2}+ \tilde{y}_{3}
  \end{align*}
\end{bsp}

Allgemeiner kann man ausgehend vom primalen Problem folgendes
formulieren:
\begin{gather}
  \min c^{T}x \tag{LP}\\
  \text{ \unb} Ax=b, x\geq 0\nonumber\\
  \FF_{P}=\{x\in\R^{n}\colon Ax=b, x\geq0\}\nonumber\\
  \max b^{T}y \tag{DS}\nonumber\\
  \text{ \unb} \begin{pmatrix}y\\s\end{pmatrix} \in\FF_{D}=
     \{y\in\R^{m}, s\in\R^{n}\colon A^{T}y+s=c, s\geq0\}\nonumber\\
  \text{\emph{Äquivalente Formulierung}}\nonumber\\
  \max b^{T}y \tag{DP}\\
  \text{ \unb} y\in\tilde{\FF}_{D}= \{y\in\R^{m}\colon A^{T}y\leq c\}
\end{gather}

\begin{bsp}
  zu Beispiel \todo{richtigen Verweis finden}
  Dort ist die zuläsige Menge von (DP) angegeben: $\tilde{\FF}_{D}=
  \{y\in\R^{n}\colon \tilde{A}^{T}y\leq \tilde{c}\}$ mit $\tilde{A}=
  (A, -I_{m}), \tilde{c}=
  \begin{pmatrix}c\\0_{m}\end{pmatrix}\Rightarrow \tilde{\FF}_{D}=
  \{y\in\R^{n}\colon A^{T}y\leq c, -y\leq 0_{n}\}= \{y\in\R^{n}\colon
  A^{T}y\leq c, y\geq 0\}$. Anwendung des Satzes von Kuhn-Tucker auf
  (DS) bzw. (DP).
\end{bsp}

\begin{satz}\label{satz:45}
  \begin{enumerate}
   \item Ein Punkt $(\tilde{y}, \tilde{s})\in\FF_{D}$ ist genau dann
    Lösung von (DS), wenn es ein $\tilde{x}\in\R^{m}$ gibt:
    \begin{gather}\label{eq:46}
      A\tilde{x}=b \qquad \tilde{x}\geq0 \qquad \tilde{x}s=0
    \end{gather}
   \item Ein Punkt $\tilde{y}\in\tilde{\FF}_{D}$ ist genau dann Lösung
    von (DP), wenn es ein $\tilde{x}\in\R^{n}$ gibt, so dass mit
    $\tilde{s}=c-A^{T}\tilde{y}$ die Bedingung oben (\autoref{eq:46})
    erfüllt ist.
  \end{enumerate}
  \begin{proof}
    Wir müssen (DS) auf ein Problem in Standardform transformieren,
    indem wir die Variablen $y^{-}, y^{+}$ mit $y=y^{+}-y^{-}, y^{+},
    y^{-}\geq 0$ einführen. (Variablen des transformieren Problems:
    $s, y^{+}, y^{-}$) Der Zielfunktionsvektor ist $(-b, b,
    0_{n})^{T}$. Das (DS) ist $\max b^{T}y \Leftrightarrow \min
    -b^{T}y \Leftrightarrow \min -b^{T}(y^{+}-y^{-})+0_{n}^{T}s$ und
    $A^{T}y+s=c\Leftrightarrow A^{T}(y^{+}-y^{-})+I_{s}=c = (A^{T},
    -A^{T}, I_{n})(y^{+}, y^{-}, s)^{T}$. Somit ist die Nebenbedingung
    $(A^{T}, -A^{T}, I_{n})(y^{+}, y^{-}, s)^{T}=c, y^{+}, y^{-},
    s\geq0$. Der Satz von Kuhn-Tucker ergibt, dass ein zulässiger
    Punkt $(\tilde{y}^{+}, \tilde{y}^{-}, \tilde{s})$ genau dann
    Lösung von $\min (-b^{T}, b^{T}, 0^{T}_{n})(y^{+}, y^{-}, s)^{T}$
    \unb $(A^{T}, -A^{T}, I_{n})(y^{+}, y^{-}, s)^{T}=c$, wenn es
    Vektoren (lagrangemultiplikatoren) $\xi\in\R^{n}, \sigma_{+},
    \sigma_{-}\in\R^{m}, \sigma\in\R^{n}$ gibt mit:
    \begin{gather}
      \begin{pmatrix}
	A\\-A\\I_{n}
      \end{pmatrix}\xi +
	 \begin{pmatrix}
	   \sigma_{+}\\\sigma_{-}\\\sigma
	 \end{pmatrix}=
	 \begin{pmatrix}
	   -b\\b\\0_{n}
	 \end{pmatrix}\label{eq:47.1}\\
      \sigma_{+}, \sigma_{-}, \sigma\geq 0\label{eq:47.2}\\
      (\tilde{y}^{+})^{T} \sigma_{+} + (\tilde{y}^{-})^{T}\sigma_{-} +
	 \tilde{s}^{T}\sigma=0\label{eq:47.3}
    \end{gather}
    Für die dritte Gleichung in \autoref{eq:47.1} ergibt sich
    $\xi+\sigma= 0\Leftrightarrow \xi=-\sigma$ weiter folgt nach der
    Addition der ersten beiden Gleichungen $A\xi-A\xi+\sigma_{+}+
    \sigma_{-}=-b+b=0 \Leftrightarrow \sigma_{+}+\sigma_{-}=0
    \Leftrightarrow^{\text{\autoref{eq:47.2}}} \sigma_{+}=0=
    \sigma_{-}$. Sei $\tilde{x}\colon=\sigma=-\xi$. Dann gilt
    $A\tilde{x}=b$ (wegen der zweiten Gleichung in \autoref{eq:47.1}),
    $\tilde{x}\geq0$ und $\tilde{s}^{T}\tilde{x}=0$ wegen
    \autoref{eq:47.3}.
  \end{proof}
\end{satz}

\begin{bem}
  Die Aussagen von \autoref{satz:41} und \autoref{satz:45}:
  \begin{itemize}
   \item Ein primal zuläsiger Punkt $\tilde{x}\in\FF_{P}$ ist genau
    dann Lösung von (LP), wenn ein dual zulässiger Punkt $(\tilde{y},
    \tilde{s})\in\FF_{D}$ mit $c^{T}\tilde{x}=b^{T}\tilde{y}$ existiert.
   \item Ein dual zulässiger Punkt $(\tilde{y}, \tilde{s})\in\FF_{D}$
    ist genau dann Lösung von (DS), wenn ein primal zulässiger Punkt
    $\tilde{x}\in\FF_{P}$ mit $b^{T}\tilde{y}=c^{T}\tilde{x}$ existiert. 
  \end{itemize}
\end{bem}

\begin{satz}[Schwacher Dualitätssatz]\label{satz:47}
  Gegeben seien das Problem (LP) und das dazu duale Problem (DS). Dann
  gilt:
  \begin{enumerate}
   \item Sind $x\in\FF_{P}$ und $(y,s)\in\FF_{D}$, dann ist $b^{T}y
    \leq c^{T}x$.
   \item Sind $\tilde{x}\in\FF_{P}$ und $(\tilde{y}, \tilde{s})\in
    \FF_{D}$ mit $b^{T}\tilde{y}=c^{T}\tilde{x}$, dann ist $\tilde{x}$
    Lösung von (LP) und $\tilde{y}$ Lösung von (DS).
  \end{enumerate}
  \begin{proof}
    \begin{enumerate}
     \item Seien $x\in\FF_{P}$ und $(y,s)\in\FF_{D}$ beliebig. Dann
      gilt $b^{T}y=(Ax)^{T}y= \underbrace{x^{T}}_{\geq 0}
      \underbrace{A^{T}y}_{\leq c} \leq x^{T}c=c^{T}x$.
     \item Seien $\tilde{x}\in\FF_{P}$ und $(\tilde{y},
      \tilde{s})\in\FF_{D}$ mit $c^{T}\tilde{x}= b^{T}\tilde{y}$.
      Daraus folgt, dass für einen beliebigen Punkt $x\in\FF_{P}$ und
      $(y,s)\in\FF_{D}$ gilt:
      \begin{gather*}
	b^{T}y\leq c^{T}\tilde{x} = b^{T}\tilde{y}\leq c^{T}x \qquad
	   \forall x\in\FF_{P}\,\forall (y,s)\in\FF_{D}
      \end{gather*}
      Somit ist $\tilde{x}$ Lösung von (LP) und $(\tilde{y},
      \tilde{s})$ Lösung von (DS).
    \end{enumerate}
  \end{proof}
\end{satz}

\begin{bsp}
  Wir wenden die Erkenntnisse auf das Mischungsproblem an. Der Vektor
  $x$ (primal zulässig) definiere eine zulässige Mischung für den
  ersten Händler. Der Vektor $y$ (dual zulässig) definiere ein
  zulässiges (fiktives) Angebot des zweiten Händlers. Dann gilt nach
  dem obigen Satz $b^{T}y\leq c^{T}x$, d.\,h. ein beliebiges Angebot
  des Konkurrenten muss billiger als jede zulässige Mischung des
  ersten Händlers sein. Ist $\tilde{x}$ eine zulässige Mischung des
  ersten Händlers und $\tilde{y}$ ein zulässiges Angebot des
  Konkurrenten mit $c^{T}\tilde{x}=b^{T}\tilde{y}$ (gleiche Kosten),
  dann ist $\tilde{x}$ optimal für den ersten Händler und $\tilde{y}$
  optimal für den zweiten Händler.
\end{bsp}

\begin{bsp}[zum praktischen Nutzen der Dualität]
  Sei $A\in\R^{m\times n}, b\in\R^{m}$. Diskrete lineare
  Optimierungsaufgabe:
  \begin{gather*}
    \min_{x\in\R^{n}} f(x)=\norm{Ax-b} \qquad
       \norm{y}=\max_{i=1,\dotsc,n} \norm{y_{i}}
  \end{gather*}
  Dies ist äquivalent zu dem (LP): $\min \delta$ \unb $\delta\geq 0,
  -\delta e\leq ax-b\leq \delta e$. Dabei ist $e$ der Einheitsvektor.
  Vergleiche:
  \begin{gather*}
    \min(-b^{T}, b^{T}, 0)y \text{ \unb}\\
    \begin{pmatrix}
      A^{T} & -A^{T} & 0_{n}\\
      e^{T} & e^{T} & 1
    \end{pmatrix}y=
       \begin{pmatrix}
	 0_{n}\\1
       \end{pmatrix}, y\geq 0
  \end{gather*}
\end{bsp}

\begin{bem}\label{bem:410}
  Es gelte $\FF_{P}\neq\emptyset$ und $\FF_{D}\neq\emptyset$. Dann
  gilt nach \autoref{satz:47} für beliebige $x\in\FF_{P},
  (y,s)\in\FF_{D}\colon b^{T}y\leq c^{T}x$ und es folgt, dass die
  Zielfunktion von (LP) ist auf $\FF_{P}$ nach unten beschränkt.
  Weiterhin folgt nach dem Hauptsatz der linearen Optimierung, dass
  das (LP) eine Lösung hat. Dies ist analog, dass (DS) eine Lösung hat.
\end{bem}

\begin{satz}[Starker Dualitätssatz]
  Gegeben seien das lineare Problem (LP) und das duale Problem (DS).
  Ist $\FF_{P}\neq\emptyset$ und $\FF_{D}\neq\emptyset$, dann besitzen
  beide Probleme eine Lösung und die Optimalwerte sind gleich.
  \begin{proof}
    Zur Existenz der Lösung vergleiche \autoref{bem:410}. Sei
    $\tilde{x}\in\FF_{P}$ Lösung von (LP). Nach dem \autoref{satz:41}
    gibt es $(\tilde{y}, \tilde{s})\in\FF_{P}$ mit
    $\tilde{x}^{T}\tilde{s}=0\Leftrightarrow
    b^{T}\tilde{y}=c^{T}\tilde{x}$. Somit folgt nach dem schwachen
    Dualitätssatz, dass $(\tilde{y}, \tilde{s})$ Lösung von (DS) ist.
  \end{proof}
\end{satz}

\section{Sensitivitätsanalyse}

\begin{bsp}[Produktionsplanungssystem]
  \label{bsp:412}
  Aus drei Betriebsmitteln $B_{1}, B_{2}, B_{3}$ sollen vier Produkte
  $P_{1}, P_{2}, P_{3}, P_{4}$ hergestellt werden:
  \begin{tabular}{l|cccc|r}
    & $P_{1}$ & $P_{2}$ & $P_{3}$ & $P_{4}$ & Vorrat\\
    \toprule
    $B_{1}$ & 1 & 1 & 1 & 1 & 15\\
    $B_{2}$ & 7 & 5 & 3 & 2 & 120\\
    $B_{3}$ & 3 & 5 & 10 & 15 & 100\\
    \bottomrule
    Gewinn & 4 & 5 & 9 & 11 &\\
  \end{tabular}
  Als Gleichung $\max 4x_{1} + 5x_{2} + 9x_{3} + 11x_{4}$ \unb
  \begin{align*}
    x_{1} + x_{2} + x_{3} + x_{4} &\leq 15\\
    7x_{1}+ 5x_{2}+ 3x_{3}+ 2x_{4}&\leq 120\\
    3x_{1}+ 5x_{2}+ 10x_{3}+15x_{4}&\leq 100\\
    x_{i} &\geq 0, i=1,\dotsc,4
  \end{align*}
  Die Standardform ist gegeben durch:
  \begin{gather*}
    c = (-4, -5, -9, -11, 0,0,0)^{T}\\
    A=
       \begin{pmatrix}
	 1 & 1 & 1 & 1 & 1 & 0 & 0\\
	 7 & 5 & 3 & 2 & 0 & 1 & 0\\
	 3 & 5 &10 &15 & 0 & 0 & 1
       \end{pmatrix}\qquad
       b=
       \begin{pmatrix}
	 15\\120\\100
       \end{pmatrix}
  \end{gather*}
  Das Simplex-Verfahren berechnet:
  \begin{tabular}{c|cccc|c|ccc}
      & 5 & 2 & 7 & 4 &   & 1 & 6 & 3\\
    1 & \nicefrac{10}{7} & \nicefrac{5}{7} & -\nicefrac{1}{7} &
       -\nicefrac{5}{7} & \nicefrac{50}{7} & \nicefrac{10}{7} & 0 &
       -\nicefrac{1}{7}\\
    6 & -\nicefrac{61}{7} & -\nicefrac{6}{7} & \nicefrac{4}{7} &
       \nicefrac{13}{7} & \nicefrac{325}{7} & -\nicefrac{61}{7} & 1 &
       \nicefrac{4}{7}\\
    3 & -\nicefrac{3}{7} & \nicefrac{2}{7} & \nicefrac{1}{7} &
       \nicefrac{12}{7} & \nicefrac{55}{7} & -\nicefrac{3}{7} & 0 &
       \nicefrac{1}{7}\\
      & \nicefrac{13}{7} & \nicefrac{3}{7} & \nicefrac{5}{7} &
       \nicefrac{11}{7} & -\nicefrac{695}{7} & -\nicefrac{13}{7} & 0 &
       -\nicefrac{5}{7}
  \end{tabular}
  Die optimale primale Lösung ist $\tilde{x}=(\nicefrac{50}{7}, 0,
  \nicefrac{55}{7}, 0, 0, \nicefrac{325}{7}, 0)^{T}$ und die optimale
  duale Lösung ist $\tilde{y}=(-\nicefrac{13}{7}, 0, -\nicefrac{5}{7})$.
\end{bsp}

Für den Anwender ergeben sich also folgende Schritte:
\begin{enumerate}[1. Schr{i}tt]
 \item Berechnung einer Optimallösung
 \item Sensitivitätsanalyse, d.\,h. Untersuchung der Abhängigkeit der Lösung
  und des Optimalwertes von Parametern.
\end{enumerate}

Allgemein betrachten wir die folgende Situation:
\begin{gather}
  \min c^{T}x\text{ \unb} \tag{(LP)\textsubscript{v}}\\
  x\in\FF(v)=\{x\in\R^{n}\colon x\geq0, Ax=b+v\}\nonumber\\
\end{gather}
Frage: Wie hängen Lösung und Optimalwert von $V\in\R^{n}$ ab? Das duale
Problem lautet:
\begin{gather}
  \max (b+t)^{T}y\text{ \unb} \tag{(LP)\textsubscript{v}}\\
  y\in\tilde{\FF}_{D}=\{y\in\R^{n}\colon A^{T}y\leq c\}\nonumber\\
\end{gather}
Sei $x$ eine Lösung des linearen Problems für $v=0_{m}$ zur Basis $B$ und
$\tilde{y}$ der zugehörige Multiplikator. Dann ist:
\begin{gather*}
  \tilde{x}_{B} = A^{-1}_{B}b\qquad \tilde{y}= A^{-1}_{B}c_{B}
\end{gather*}
Für $v\in\R^{m}$ definieren wir:
\begin{gather*}
  x_{B}(v)= A^{-1}_{B}(b+v)=\tilde{x}_{B}+A^{-1}_{B}v \qquad x_{N}(v)= 0_{n-m}
\end{gather*}

\begin{satz}
  Unter obiger Voraussetzung gelte $x_{B}(v)\geq0$ und damit auch
  $x(v)\geq0$). dann ist $x(v)$ Lösung von $(LP)_{v}$. Für den Optimalwert
  gilt:
  \begin{gather}\label{eq:47}
    c^{T}x(v)= c^{T}\tilde{x}+ \tilde{y}v
  \end{gather}
  Weiter ist auch $\tilde{y}$ Lösung von $(DP)_{v}$.
  \begin{proof}
    Nach der Definition von $x(v)$ gilt: $Ax(v)=A_{B}x_{B}(v)= b+v$. Nach der
    Voraussetzung ist $x(v)\geq0\Rightarrow x(v)\in\FF_{v}$. Weiter ist
    $\tilde{y}\in\tilde{\FF}_{D}$ und es gilt nach dem starken Dualitätssatz
    $c^{T}\tilde{x}= b^{T}\tilde{y}$ und $\tilde{y}=(A^{-1}_{B})^{T} c_{B}
    \Rightarrow c^{T}x(v)= c^{T}_{B} x_{B}(v)=\underbrace{c^{T}_{B}
    \tilde{x}_{B}}_{=c^{T} \tilde{x}=b^{T}\tilde{y}} + c^{T}_{B} A^{-1}_{B} v=
    (b+t)^{T}\tilde{y}$. Damit gilt $x(v)\in\FF_{v},
    \tilde{y}\in\tilde{\FF}_{D}$ und $c^{T}x(v)=(b+v)^{T}\tilde{y}$ und es
    folgt die Behauptung.
  \end{proof}
\end{satz}

\begin{bem}
  Bleibt $x(v)\geq0\Rightarrow x(v)$ ist Lösung von $(LP)_{v}$ zur Basis $B$.
  Die \autoref{eq:47} zeigt: Der Multiplikator $\tilde{y}$ (duale Lösung)
  bestimmt, wie sich der Optimalwert in Abhängigkeit von $v$ ändert. Die
  $\tilde{y}_{i}$ mit $i=1,\dotsc,n$ heißen \highl{Spaltenpreise}.

  Folgendes ist für die obige Theorie wichtig:\\
  Die Referenzlösung für $v=0_{m}$ ist eine Basislösung an der Basis $B$ von
  $A$ und damit Ecke der zulässigen Menge. Benutzt man das Simplex-Verfahren
  zur Lösung des Referenzproblems, so erhält man immer eine optimale
  Basislösung.
\end{bem}

\begin{bsp}[Fortsetzung von \autoref{bsp:412}]
  Die Kapazität (Vorrat) $b_{1}$ von $B_{1}$ soll geändert werden. Dazu
  definieren wir $v_{\epsilon}=\epsilon\begin{pmatrix}1\\0\\0\end{pmatrix}$.
  Somit gilt:
  \begin{gather*}
    x_{B}(v_{\epsilon})=\tilde{x}_{B}+\epsilon
       A^{-1}_{B}\begin{pmatrix}1\\0\\0\end{pmatrix} =
      \begin{pmatrix}
	\nicefrac{50}{7}\\
	\nicefrac{325}{7}\\
	\nicefrac{55}{7}
	\end{pmatrix}+ \epsilon
       \begin{pmatrix}
	 \nicefrac{10}{7}\\
	 -\nicefrac{61}{7}\\
	 -\nicefrac{3}{7}
       \end{pmatrix}
  \end{gather*}
  Für $-5\leq\epsilon\leq\nicefrac{325}{61}$ ist $x_{B}(v_{\epsilon})\geq 0$
  und damit also Lösung von $(LP)_{v_{\epsilon}}$. Der Optimalwert ist
  $c^{T}\tilde{x}+\tilde{y}^{T}v_{\epsilon}= c^{T}\tilde{x}+\epsilon
  \tilde{y}_{1}= -\nicefrac{695}{7}-\nicefrac{13}{7}\epsilon$.
\end{bsp}

\chapter{Affine Skalierung}

(einfachstes Innere-Punkte-Verfahren)

\section{Einführung}

Das Simplex-Verfahren wurde gegen Ende der 50er Jahre von George B.
Dantzig entwickelt. Sei $S(n,m)$ die Anzahl der Iterationen des
Simplex-Verfahrens. Dann gilt $s(n,m)\leq \binom{n}{m}$. Im
schlechtesten Fall werden alle Ecken durchlaufen.

\begin{bsp}[Klee-Minty-Würfel]
  Wir betrachten das lineare Problem $\min \sum_{j=1}^{n} 10^{n-j}
  x_{j}$ \unb $x_{1}\leq 1, 2\sum_{j=1}^{i-1} 10^{i-j} x_{j} +x_{i}
  \leq 100^{i-1}$ für alle $i=2,\dotsc,n$. Die zulässige Menge ist
  dabei ein verzerrter Würfel. Hierbei wächst die Anzahl
  der Iterationen im schlechtesten Fall exponentiell. Dies wurde 1972
  von Klee und Minty herausgefunden. Später fand man auch ähnliche
  Beispiele für Zeilen- und Spaltenauswahlregeln.
\end{bsp}

\section{Das Konzept des Verfahrens}

Wir betrachten das lineare Problem:
\begin{gather}
  \min c^{T}x \text{ \unb}\tag{(LP)}\\
  x\in\FF_{P}=\{x\in\R^{n}\colon Ax=b, x\geq0\}\nonumber
\end{gather}

\begin{defin}
  Ein Vektor $x\in\R^{n}$ heißt \highl[Punkt!innerer
  zulässiger]{innerer zulässiger Punkt} von (LP), wenn $x\in\FF_{P}$
  und $x>0$ ist.
\end{defin}

\begin{bem}
  Das Innere-Punkt-Verfahren startet mit einem inneren zulässigen
  Punkt $x^{(0)}$ und berechnet eine Folge $\{x^{(k)}\}_{k\in\N}$
  innerer zulässiger Punkte. Wir bezeichnen die Menge der inneren
  zulässigen Punkte mit $\FF_{P}^{0}$. Sei nun $x^{(k)}\in\FF_{P}^{0}$
  nicht optimal. Dann stellt sich die Frage, wann man ausgehend von
  $x^{(k)}$ einen besseren Punkt $x^{(k+1)}$ berechnen kann.
\end{bem}

\begin{defin}
  Eine Richtung $d\in\R^{n}$ heißt
  \highl[Richtung!zulässige]{zulässige Richtung} von $x^{(k)}$, wenn
  ein $\overline{\sigma}>0$ existiert, so dass:
  \begin{gather}\label{eq:51}
    x^{(k)}+\sigma d\in\FF_{P} \qquad \forall
       \sigma\in[0,\overline{\sigma}]
  \end{gather}
\end{defin}

\begin{bem}
  Wenn $x^{(k)}>0$ ist, dann gilt $x^{(k)}+\sigma d>0$, falls
  $\overline{\sigma}$ hinreichend klein ist.

  Um \autoref{eq:51} sicher zu stellen, muss noch folgendes gelten:
  \begin{gather}\label{eq:52}
    A(x^{(k)}+\sigma d)=b= Ax^{(k)}+\sigma Ad\Leftrightarrow Ad=0
  \end{gather}
  Somit sind alle Richtungen $d\in\R^{n}$ mit $Ad=0$ zulässige
  Richtungen.
\end{bem}

\begin{defin}
  Eine Richtung $d\in\R^{n}$ heißt
  \highl[Abstiegsrichtung!zulässige]{zulässige Abstiegsrichtung}, wenn
  $d$ zulässige Richtung ist und es gilt:
  \begin{gather*}
    c^{T}(x^{(k)}+\sigma d)<c^{T} x^{(k)} \qquad \forall \sigma\in[0,
       \overline{\sigma}]
  \end{gather*}
\end{defin}

Um eine zulässige Abstiegsrichtung zu berechnen, betrachten wir das
Problem:
\begin{gather}
  \min c^{T}d \text{ \unb}\tag{(AS)}\\
  Ad=0, \norm{d}\leq1\nonumber
\end{gather}
Die zulässige Menge von (AS) ist nicht leer und kompakt. Damit folgt
nach dem Satz von Weierstraß, dass (AS) eine Lösung hat.

\begin{lemma}\label{lem:53}
  Sei $\overline{x}\in\FF_{P}^{0}$ keine Lösung von (LP). Weiter sei
  $\overline{d}$ eine Lösung von (AS). Dann ist $\overline{d}$
  zulässige Abstiegsrichtung in $\overline{x}$.
  \begin{proof}
    Sei $A\overline{d}=0\Rightarrow\overline{d}$ ist zulässige
    Richtung. Nun ist noch zu zeigen, dass $\overline{d}$ eine
    Abstiegsrichtung ist. Wir nehmen an, $c^{T}\overline{d}\geq0$. Sei
    $x\in\FF_{P}^{0}$ beliebig und
    d:=$\frac{x-\overline{x}}{\norm{x-\overline{x}}}$. Dann gilt
    $\norm{d}=1, Ad=\frac{1}{\norm{x-\overline{x}}}
    (\underbrace{Ax}_{=b} -\underbrace{A\overline{x}}_{=b})=0$. Dann
    folgt, dass $d$ zulässige Lösung für (AS) $\Rightarrow c^{T}d\geq
    c^{T}\overline{d}\Rightarrow
    c^{T}d=\frac{d}{\norm{x-\overline{x}}}
    (c^{T}x-c^{T}\overline{x})\geq 0 \Rightarrow c^{T}x\geq
    c^{T}\overline{x}\Rightarrow\overline{x}$ ist Lösung des (LP).
    Dies steht aber in Widerspruch zur Voraussetzung.
  \end{proof}
\end{lemma}

Im $k$-ten Iterationsschritt wollen wir $\norm{\cdot}_{b}$ als Norm
wählen.

\begin{defin}
  Sei $Q\in\R^{n\times n}$ mit $\rg(Q)=n, x\in\R^{n}, \rho\prec 0$.
  Dann heißt die durch
  \begin{gather*}
    E(Q, x, \rho)=\{y\in\R^{n}\colon (y-x)^{T} (Q^{-1})^{T}
       Q(y-x)\leq\rho^{2}\}
  \end{gather*}
  definierte Menge \highl{Ellipsoid} mit MIttelpunkt $x$ und Radius
  $\rho$.
\end{defin}

\begin{bem}
  Ist $Q=I_{n}\Rightarrow E(I_{n},x,\rho)=\{y\in\R^{n}\colon
  (y-x)^{T}(y-x)\leq\rho^{2}\}= \{y\in\R^{n}\colon
  \norm{y-x}^{2}\leq\rho^{2}\}\Rightarrow E(I_{n}, x, \rho)$ ist die
  abgeschlossene Kugel um $x$ mit Radius $\rho$.

  Sei allgemein $Q\in\R^{n\times n}\colon\rg(Q)=n\Rightarrow
  \rg(Q^{-1})=n$. Die Matrix $(Q^{-1})^{T}Q$ ist symmetrisch und
  positiv definit. Man kann auf dem $\R^{n}$ eine Norm
  $\norm{\cdot}_{Q}$ definieren:
  \begin{gather*}
    \norm{z}_{Q}\colon= \sqrt{z^{T} (Q^{-1})^{T}Q^{-1}z}= \norm{Q^{-1}z}
  \end{gather*}
  Mit dieser Norm ist $E(Q,x,\rho)=\{y\in\R^{n}\colon
  \norm{y-x}_{Q}\leq \rho\}$, d.\,h. ein Ellipsoid ist eine
  abgeschlossene Kugel bezüglich der durch $Q$ definierten Norm.

  Sei $x^{(k)}\in\FF_{P}^{0}\Rightarrow x^{(k)}\prec0$. Damit folgt,
  dass die Matrix mit den Diagonalelementen $x^{(k)}$ invertierbar ist
  ($X_{k}=\diag(x^{(k)})$) und
  $X_{k}^{-1} = \diag(\nicefrac{1}{x^{(k)}_{1}}, \dotsc,
  \nicefrac{1}{x^{(k)}_{n}})$.

  Das Verfahren der affinen Skalierung benutzt im $k$-ten
  Iterationsschritt beim Problem (AS) die Norm $\norm{\cdot}_{X_{k}}$,
  d.\,h. zur Berechnung einer zulässigen Abstiegsrichtung lösen wir
  das Problem:
  \begin{gather}
    \min_{d\in\R^{n}} c^{T}d \text{ \unb}\tag{(AS)\textsubscript{k}}\\
    Ad=0, \norm{d}_{X_{k}}<1\nonumber
  \end{gather}
  Um (AS)\textsubscript{k} einfach lösen zu können, führen wir eine
  "`affine Skalierung"' der Variablen durch:
  \begin{gather*}
    u=X^{-1}_{k} d\Leftrightarrow d=X_{k}u
  \end{gather*}
  Dann gilt:
  \begin{align}\label{eq:53}
    \norm{d}^{2}_{X_{k}} &=\norm{X_{k}u}^{2}_{X_{k}}= (X_{k}u)^{T}
       X^{-1}_{k} X^{-1}_{k} X_{k}u\\
    &= u^{T}u=\norm{u}^{2}\nonumber\\
    \Rightarrow \norm{d}_{X_{k}} &= \norm{u}\nonumber
  \end{align}
  Damit erhalten wir aus (AS)\textsubscript{k} das Problem:
  \begin{gather}
    \min_{u\in\R^{n}} (X_{k}c)^{T} u \text{
       \unb}\tag{(AS\textsubscript{u})\textsubscript{k}}\\
    AX_{k}u=0, \norm{u}\leq1\nonumber
  \end{gather}
\end{bem}

\begin{bsp}[Frannie]
  $x^{(k)}=(3,6, 0,2, 1)^{T}$
\end{bsp}

\section{Berechnung der Suchrichtung}

\begin{satz}\label{satz:56}
  Sei $x^{(k)}\in\FF_{P}^{0}$ und $\rg(A)=m<n$. Wir definieren:
  \begin{gather}\label{eq:54}
    y^{(k)}\colon=(AX_{k}^{2} A^{T})^{-1} AX_{k}^{2}c\qquad
       s^{(k)}\colon= x_{k} (c-A^{T}y^{(k)})
  \end{gather}
  Wenn $s^{(k)}=0_{n}$ ist, dann ist $x^{(k)}$ Lösung von (LP).
  Andernfalls ist:
  \begin{gather}\label{eq:55}
    u^{(k)}\colon=-\frac{s^{(k)}}{\norm{s^{(k)}}}
  \end{gather}
  eindeutig bestimmte Lösung von (AS\textsubscript{u})\textsubscript{k}.
  \begin{proof}
    Die Matrix $\underbrace{AX_{k}^{2} A^{T}}_{=AX_{k}(AX_{k})^{T}}\in
    \R^{n\times m}$ ist invertierbar und $A$ ist positiv definit.
    Somit ist die Formel für $y^{(k)}$ sinnvoll. Weiterhin ist
    $AX_{k}(AX_{k})^{T}$ positiv definit. Denn $y^{T}
    (AX_{k})(AX_{k})^{T} y= (y^{T}AX_{k})(y^{T}AX_{k})^{T} =
    \norm{(AX_{k})^{T}}^{2}>0$.

    Sei $s^{(k)}=0_{n}$. Dann gilt nach Definition $A^{T}y^{(k)}=c$.
    Wegen $x^{(k)}\in\FF_{P}^{0}$ folgt, $Ax^{(k)}=b, x^{(k)}\geq0
    \Rightarrow Ay^{(k)}+s^{(k)}=c$ mit $s^{(k)}\geq 0$. Somit folgt,
    dass $(x,y,s)=(x^{(k)}, y^{(k)}, s^{(k)})$ eine Lösung des Systems
    \autoref{eq:41} ist und nach dem Satz von Kuhn-Tucker ist
    $x^{(k)}$ eine Lösung von (LP).

    \todo{Beweis zu Ende bringen}
  \end{proof}
\end{satz}

Die Lösungsformel fpr $y^{(k)}$ ist eine theoretische Formel. Zur
praktischen Berechnung von $y^{(k)}$ löst man das lineare
Gleichungssystem
\begin{gather}\label{eq:56}
  \underbrace{AX_{k}^{2} A^{T}y}_{=(AX_{k})(AX_{k})^{T}} = AX_{k}^{2}c
\end{gather}
Dies kann entweder durch das Choleskyverfahren geschehen. Eine weitere
Lösungsmöglichkeit erhält man durch folgende Betrachtungen. Die Form
$(AX_{k})(AX_{k})^{T}y=AX_{k}^{2}c$ des Systems \autoref{eq:56}
definiert die Normalengleichung für das überbestimmte Gleichungssystem
$(AX_{k})^{T} y=X_{k}c$. Definiere als Lösung einen Vektor
$\tilde{y}$, der die Zielfunktion $\norm{(AX_{k})^{T}y-X_{k}c}$
minimiert.

\begin{satz}
  Sei $x^{(k)}\in\FF_{P}^{0}$ und es gelte $\rg(A)=m<n$. Weiterhin
  seien $y^{(k)}, s^{(k)}$ durch \autoref{eq:54} definiert. Ist
  $s^{(k)}=0$, dann ist $x^{(k)}$ Lösung von (LP). Andernfalls ist
  \begin{gather}\label{eq:58}
    d^{(k)}=-\frac{X_{k}s^{(k)}}{\norm{s^{(k)}}}
  \end{gather}
  die eindeutige Lösung von (AS)\textsubscript{k}.
  \begin{proof}
    Für $s^{(k)}=0$ siehe oben. Sei nun $s^{(k)}\neq 0$. Dann ist $d$
    genau dann Lösung von (AS)\textsubscript{k}, wenn $u=X^{-1}_{k}d$
    Lösung von (AS\textsubscript{u})\textsubscript{k} ist, d.\,h.
    $d^{(k)}=X_{k}u^{(k)}$.
  \end{proof}
\end{satz}

\section{Wahl der Schrittweite}

Für die Wahl der Schrittweite existieren zwei Varianten:
\begin{itemize}
 \item Short-Step-Variante: Man benutzt in jedem Schritt eine feste
  Schrittweite $0<\sigma<1$.
 \item Long-Step-Variante: Man benutzt in jedem Schritt die
  größtmögliche Schrittweite.
\end{itemize}

\subsection{Die Short-Step-Variante}

\begin{lemma}
  Sei $\rg(A)=m$. Ist $x^{(k)}\in\FF_{P}^{0}, s^{(k)}\neq 0, d^{(k)}$
  durch \autoref{eq:58} definiert und $0<\sigma<1$. Dann ist
  $x=x^{(k)}+\sigma d^{(k)}\in\FF_{P}^{0}$.
  \begin{proof}
    Es gilt $Ax^{(k)}=b, Ad^{(k)}=0\Rightarrow Ax=Ax^{(k)}+\sigma
    Ad^{(k)}=b$. Nun bleibt zu zeigen, dass $x\geq0$. Für einen
    beliebigen Index $i\in\{1,\dotsc,n\}$ gilt: \todo{Summen richtig
    einfügen}
    \begin{align*}
      &= \sigma^{2} (d^{(k)})^{T} X_{k}^{2} d^{(k)} = \sigma^{2}
	 (X_{k}^{-1}d^{(k)})^{T} (X_{k}^{-1}d^{(k)})\\
      &= \sigma^{2} \norm{X_{k}^{-1} d^{(k)}}^{2} = \sigma^{2}
	 \norm{d^{(k)}}^{2}_{X_{k}} \leq\sigma^{2}
    \end{align*}
    Wegen $x^{(k)}<0$ und $0<\sigma<1$ folgt:
    \begin{align*}
      \abs{x_{i}x_{i}^{(k)}} &\leq\sigma x_{i}^{(k)}<x_{i}^{(k)}
	 \qquad 1\leq i\leq n\\
      x_{i}<x_{i}^{(k)} &\Rightarrow \abs{x_{i}-x_{i}^{(k)}} -
	 x_{i}^{(k)} -x_{i}< x_{i}^{(k)}\Rightarrow x_{i}>0\\
      x_{i}\geq x_{i}^{(k)} &\Rightarrow x_{i}\geq x_{i}^{(k)}>0
    \end{align*}
  \end{proof}
\end{lemma}

\subsection{Die Long-Step-Variante}

Man wählt in jedem Iterationsschritt möglichst große Schrittweite.

\begin{lemma}
  Es gelte $\rg(A)=m$ und  $x^{(k)}\in\FF_{P}^{0}$ sei keine Lösung
  von (LP). Dann ist $s^{(k)}\neq =0$ und $d^{(k)}$ sei durch
  \autoref{eq:58} definiert. Ist $d^{(k)}>0$\footnote{genau dann, wenn
  $s^{(k)}\leq0$}, dann ist die Zielfunktion von (LP) auf der
  zulässigen Menge $\FF_{P}$ nicht nach unten beschränkt.
  \begin{proof}
    Für $\sigma>0$ sei $x(\sigma)\colon= x^{(k)}+\sigma d^{(k)}$. Dann
    gilt $x(\sigma)\geq0$ für alle $\sigma\geq0$ und
    $Ax(\sigma)=\underbrace{Ax^{(k)}}_{=b} + \sigma
    \underbrace{Ad^{(k)}}_{=0}=b$ für alle $\sigma\geq0$, d.\,h.
    $x(\sigma)\in\FF_{P}^{0}$. Nach \autoref{lem:53} ist $d^{(k)}$ als
    Lösung von (AS)\textsubscript{k} zulässige Abstiegsrichtung,
    d.\,h. $c^{T}d^{(k)}<0 \Rightarrow c^{T}x(\sigma)=c^{T}x^{(k)}+
    \sigma c^{T}c^{(k)}\xrightarrow{\sigma\rightarrow\infty} -\infty$.

    Ist der aktuelle Iterationspunkt $x^{(k)}$ keine Lösung von (LP),
    dann gibt es zwei Möglichkeiten:
    \begin{enumerate}
     \item $d^{(k)}\geq0$ bzw. $s^{(k)}\leq0\Rightarrow$ Das lineare
      Problem hat keine Lösung.
     \item $\exists j\in\{1,\dotsc,n\}\colon d_{j}^{(k)}<0$
    \end{enumerate}
    \todo{Fall zwei zu Ende führen}
  \end{proof}
\end{lemma}

\section{Abbruchkriterium}

Nach \autoref{satz:56} ist $x^{(k)}$ Lösung von (LP), wenn
$s^{(k)}\geq0$. Es gilt aber $x^{(k)}>0$. In einer Lösung $\tilde{x}$
von (LP) gilt in der Regel $\tilde{x}_{i}=0$ für mindestens einen
Index $i\in\{1,\dotsc,n\}$, d.\,h. die Bedingung $s^{(k)}=0_{n}$ ist
in der Regel nicht erfüllt.

Sei nun $\epsilon\geq0$. Ein Punkt $\overline{x}\in\FF_{P}$ heißt
\highl{$\epsilon$-Minimum} von (LP), wenn $c^{T}\overline{x}\leq
c^{T}x+\epsilon$ für alle $x\in\FF_{P}$ gilt. Hat (LP) eine Lösung
$\tilde{x}$, dann muss $c^{T}\overline{x}\leq c^{T} \tilde{x}\epsilon$ gelten.
Entsprechend heißt ein Punkt $\overline{y}\in\FF_{D}$
$\epsilon$-Maximum von (DP), wenn $b^{T}\overline{y}\geq
b^{T}y-\epsilon$ für alle $y\in\FF_{D}$ gilt. Hat (DP) eine Lösung
$\tilde{y}$, muss gelten $b^{T}\overline{y}\geq
b^{T}\tilde{y}-\epsilon$.

Unser Ziel ist nun, dass Verfahren der affinen Skalierung abzubrechen,
wenn $x^{(k)}$ ein $\epsilon$-Minimum von (LP) ist.


%% Anhang
\appendix
\chapter{Klausuraufgaben}

\begin{enumerate}
 \item Gegeben sei das Problem $\max c^{T}x$ unter den Nebenbedigungen
  $Ax\leq b, x\in\R^{n}$.
  \begin{enumerate}
   \item Transformieren Sie es in Standardform mit der zulässigen
    Menge $\FF$.
   \item Zeigen Sie, dass für beschränktes $\FF$ das Problem mind.
    eine Lösung besitzt.
   \item Welche wichtige Eigenschaft besitzt die Menge $\FF$? Was
    versteht man unter einer "`Ecke"' der Menge $\FF$? Geben Sie
    darauf aufbauend eine Charakterisierung jedes einzelnen Punktes
    aus $\FF$ an.
   \item Sei nun $\FF^{*}= \{x\in\FF | b_{l}\leq x\leq b_{u}\}$ mit
    $b_{l}, b_{u}\in\R^{n}$ mit $b_{l}<b_{u}$ und komponentenweise
    geltenden Relationen. Welche Inklusion(en) ($\subset, \subseteq,
    \subsetneq, \supset, \supseteq, \supsetneq$) gilt bzw. gelten
    zwischen $\FF$ und $\FF^{*}$.
  \end{enumerate}
 \item Die Niveaumenge einer Funktion $f\colon D(f)\rightarrow \R$ zum
  Niveau $\alpha\in\R$ ist gegeben durch
  \begin{gather*}
    \mathcal{N}(f, \alpha)=\{x\in D(f)| f(x)\leq a\}
  \end{gather*}
  Zeigen Sie, dass die Niveaumenge einer \emph{konvexen Funktion}
  ebenfalls konvex ist, vorausgesetzt sie ist nichtleer. Definieren
  Sie dafür zunächst den Begriff einer \emph{konvexen Menge} und einer
  \emph{konvexen Funktion}.
 \item Betrachten Sie das Problem
  \begin{align*}
    \min &c^{T}x\\
    \text{Nb.} &Ax=b, \quad 0\leq x\in\R^{n}
  \end{align*}
  oder äquivalent
  \begin{align*}
    \min &c^{T}x\\
    \text{Nb.} &\sum_{i=1}^{n} A_{.i} x_{i}=b, \quad 0\leq x\in\R^{n}
  \end{align*}
  wobei $A_{.i}$ die $i$-te Spalte der Matrix $A$ ist.

  Was versteht man unter einer Basis und einer Basislösung? Wann ist
  eine Basislösung zulässig?
 \item Lineare Optimierungsprobleme spielen bei der Auswertung von
  Messungen eine wichtige Rolle. Wir betrachten hierzu die folgende
  Aufgabenstellung. Es seien Messpunkte $(x_{i}, y_{i})\in\R^{2},
  i=1,\dotsc,n$, gegeben. Zwischen den $x$- und den $y$-Werten wird
  ein linearer Zusammenhang vermutet. Daher soll zu den Messpunkten
  eine "`optimale"' Gerade bestimmt werden. Benutzen Sie als
  Optimalitätskriterium die Summe der Abweichung an den Messpunkten.
  Definiert man eine Gerade $g(x)=ax+b$ durch die beiden Parameter $a$
  und $b$, dann ist eine Paar $(a,b)\in\R^{2}$ gesucht, so dass
  \begin{gather*}
    \sum_{i=1}^{n} \lvert ax_{i} + b-y_{i}\rvert
  \end{gather*}
  minimal wird, d.\,h. es ist das Optimierungsproblem
  \begin{gather*}
    \min_{(a,b)\in\R^{2}} \sum_{i=1}^{n} \lvert ax_{i}+b-y_{i}\rvert
  \end{gather*}
  zu lösen.
  \begin{enumerate}
   \item Formulieren Sie diese Aufgabe als lineares Problem in
    kanonischer Form.
   \item Transformieren Sie das Problem in Standardform.
   \item Welcher Zusammenhang besteht zwischen den zulässigen Mengen
    bzw. den Ecken der Mengen der beiden Probleme.
   \item Geben Sie das duale Problem an.
   \item Stellen Sie das Hilfsproblem für die Phase~1 des
    Simplex-Verfahrens auf.
  \end{enumerate}
 \item Gegeben sei mit einem $\epsilon\in\R\setminus\{0\}$ das lineare
  Optimierungsproblem
  \begin{align*}
    \max      4x_{1} &+ 5x_{2} + 9x_{3} + 18x_{4}        &\\
    \text{Nb.} x_{1} &+  x_{2} +  x_{3} + \epsilon x_{4} &= 15\\
              7x_{1} &+ 5x_{2} + 3x_{3}                  &\leq 120\\
              3x_{1} &+ 5x_{2} +10x_{3}                  &\leq 100\\
              x_{i}  &\geq 0                             &i= 1,\dotsc,4
  \end{align*}
  \begin{enumerate}
   \item Transformieren Sie das Problem in Standardform!
   \item Bestimmen Sie eine Basis. Für welche $\epsilon$ ist die
    zugehörige Basislösung zulässig? Berechnen Sie in diesem Fall das
    Starttableau zu dieser Basis.
   \item Bestimmen Sie alle $\epsilon$, für welche die Basislösung
    optimal ist.
  \end{enumerate}
\end{enumerate}

\clearpage
\pdfbookmark[0]{Index}{index}
\printindex

\end{document}
