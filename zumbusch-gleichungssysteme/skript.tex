\RequirePackage[l2tabu,orthodox]{nag}
\documentclass[halfparskip,pointednumbers,smallheadings,headsepline,ngerman]{scrreprt}
\usepackage{babel}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{amsmath,amsthm,amssymb,xspace,url,svn,color,fixmath,pifont,booktabs,scrpage2}
\usepackage[colorlinks,breaklinks]{hyperref}
\usepackage[	top=30mm,
					bottom=20mm,
					inner=30mm,
					outer=20mm]{geometry}
\clearscrheadfoot
\automark{chapter}
\ihead[]{\normalfont\sffamily\headmark}
\ohead[]{\normalfont\textsf{\textbf{\thepage}}}
\renewcommand*{\chapterpagestyle}{scrheadings}

\usepackage{tikz}
\usetikzlibrary{calc}

\theoremstyle{definition}

\newtheorem{defi}{Definition}
\theoremstyle{remark}
\newtheorem*{bsp}{Beispiel}

\newcommand*{\bR}{\ensuremath{\mathbb{R}}\xspace}
\newcommand*{\bN}{\ensuremath{\mathbb{N}}\xspace}
\newcommand*{\cO}{\ensuremath{\mathcal{O}}\xspace}

\let\epsilon\varepsilon
\let\phi\varphi
\let\rho\varrho
\let\kappa\varkappa

\let\emptyset\varnothing
\let\subset\subseteq
\let\subsetneq\subsetneqq

\newcommand*{\pmat}[1]{\ensuremath{\begin{pmatrix} #1 \end{pmatrix}}\xspace}
\newcommand*{\bmat}[1]{\ensuremath{\begin{bmatrix} #1 \end{bmatrix}}\xspace}
\newcommand*{\vmat}[1]{\ensuremath{\begin{vmatrix} #1 \end{vmatrix}}\xspace}
\newcommand*{\Bmat}[1]{\ensuremath{\begin{Bmatrix} #1 \end{Bmatrix}}\xspace}
\newcommand*{\Vmat}[1]{\ensuremath{\begin{Vmatrix} #1 \end{Vmatrix}}\xspace}
\newcommand*{\kpmat}[1]{\ensuremath{\left(\begin{smallmatrix} #1 \end{smallmatrix}\right)}\xspace}
\newcommand*{\kbmat}[1]{\ensuremath{\left[\begin{smallmatrix} #1 \end{smallmatrix}\right]}\xspace}
\newcommand*{\kBmat}[1]{\ensuremath{\left\{\begin{smallmatrix} #1 \end{smallmatrix}\right\}}\xspace}
\newcommand*{\kvmat}[1]{\ensuremath{\left|\begin{smallmatrix} #1 \end{smallmatrix}\right|}\xspace}
\newcommand*{\kVmat}[1]{\ensuremath{\left\|\begin{smallmatrix} #1 \end{smallmatrix}\right\|}\xspace}

\SVN $LastChangedRevision$
\SVN $LastChangedDate$

\begin{document}
\pagestyle{scrheadings}
\title{Schnelle iterative Löser großer schwach besetzter linearer Gleichungssysteme}
\author{Professor\,Dr.\,G. W. Zumbusch\\FSU Jena}
\date{}
\maketitle

\chapter*{Vorwort}

  Dieses Dokument wurde als Skript für die auf der Titelseite genannte
  Vorlesung erstellt und wird jetzt im Rahmen des Projekts "`Vorlesungsskripte
  der Fakultät für Mathematik und Informatik"' weiter betreut. Das Dokument
  wurde nach bestem Wissen und Gewissen angefertigt. Dennoch garantiert weder
  der auf der Titelseite genannte Dozent, die Personen, die an dem Dokument
  mitgewirkt haben, noch die Mitglieder des Projekts für dessen
  Fehlerfreiheit. Für etwaige Fehler und dessen Folgen wird von keiner der
  genannten Personen eine Haftung übernommen. Es steht jeder Person frei,
  dieses Dokument zu lesen, zu verändern oder auf anderen Medien verfügbar zu
  machen, solange ein Verweis auf die Internetadresse des Projekts
  \url{http://uni-skripte.lug-jena.de/} enthalten ist.

  Diese Ausgabe trägt die Versionsnummer~\SVNLastChangedRevision{} und ist vom
  \SVNDate{}. Eine neue Ausgabe könnte auf der Webseite des Projekts
  verfügbar sein.

  Jeder ist dazu aufgerufen, Verbesserungen, Erweiterungen und
  Fehlerkorrekturen für das Skript einzureichen bzw. zu melden oder diese
  selbst einzupflegen -- einfach eine E-Mail an die Mailingliste
  \texttt{<uni-skripte@lug-jena.de>} senden. Weitere Informationen sind
  unter der oben genannten Internetadresse verfügbar.

  Hiermit möchten wir allen Personen, die an diesem Skript mitgewirkt haben,
  vielmals danken:
  \begin{itemize}
   \item Ivo Hedtke \texttt{<hedtke@math.uni-jena.de>} (2008/09)
  \end{itemize}


\tableofcontents

\chapter*{Literatur}

\begin{enumerate}
    \item \textsc{W. Hackbusch}: Iterative Lösung großer schwachbesetzter Gleichungssysteme. 1991.
	\item \textsc{A. Meister}: Numerik linearer Gleichungssysteme. Eine Einführung in moderne Verfahren. 2007.
\end{enumerate}

\chapter{Grundbegriffe}

Parallele Algorithmen und Parallelisierung von Iterationsverfahren

\paragraph{Warum?} \begin{itemize}
    \item Verfügbarkeit von Parallelrechnern (Dual-Core, \dots)
\item Physikalische Grenzen
\item Parallelität in vielen Anwendungen
\end{itemize}

\section{Modellproblem: Skalarprodukt von Vektoren}

Seien $x,y \in \bR^n$. Wir wollen berechnen: \[S = \langle x,y \rangle = \sum_{i=0}^{n-1}x_i y_i.\]

\paragraph{Parallelität?}
\begin{enumerate}
    \item Berechnung der $x_i y_i$ $\forall i$ unabhängig
\item Sei $P$ die Anzahl der Prozessoren ($n \gg P$). Die Indizes $\{0,\dots, n-1\}$ werden auf die Prozessoren verteilt: \[I_p \subset \{0,\dots,n-1\}.\] Jeder Prozessor berechnet eine Teilsumme: \[S_p = \sum_{i \in I_p} x_i y_i.\]
\item Gesamtsumme:
\begin{enumerate}
    \item sequentiell: $S = \sum_{p=0}^{P-1} S_p$ oder
\item parallel: (Beispiel $P=8)$\begin{align*}
S &= \underbrace{S_0 + S_1}_{S_{01}} + \underbrace{S_2 + S_3}_{S_{23}} + \underbrace{S_4 + S_5}_{S_{45}} + \underbrace{S_6 + S_7}_{S_{67}}\\
&= \underbrace{S_{01} + S_{23}}_{S_{0123}} + \underbrace{S_{45} + S_{67}}_{S_{4567}}\\
&= \underbrace{S_{0123} + S_{4567}}_{S_{01234567}}\\
&= S_{01234567} = S
\end{align*}
\end{enumerate}
Statt $7$ Schritten im sequentiellen Verfahren werden hier nur $3$ benötigt.
\end{enumerate}

Gesamtaufwand sequentiell: $\cO(n)$.

Aufwand in jedem parallelen Schritt:
\begin{enumerate}
    \item $\frac{n}{P}$
\item $\frac{n}{P}$
\item $\log P$
\end{enumerate}

Gesamtaufwand im parallelen Verfahren: $\cO\left(\frac{n}{P} + \log P\right)$. Für die Extremfälle haben wir:
\begin{itemize}
    \item $n \to \infty$: $\cO\left(\frac{n}{P}\right)$, eine Beschleunigung um den Faktor $P$
\item $n \approx P$: $\cO(\log N)$
\end{itemize}

\section{Kommunizierende sequentielle Prozesse}

\begin{defi}[Sequentieller Prozess]
Abstraktion, Ausführen eines sequentiellen Programms. Jederzeit klarer Zustand. Genau ein Befehlszähler (PC, Program Counter) und Variablen (Speicher, Register).
\end{defi}

\begin{defi}[Paralleles Programm]
Interagierende sequentielle Prozesse. Sinnvoll auf mehreren Prozessoren. Gegebenenfalls zyklisches Umschalten.
\end{defi}

\minisec{Vereinfachtes Muster eines parallelen Programms}

\begin{verbatim}
{
   globale Variablen

   thread<name>[Parameter]{
      lokale Variablen
      Anweisungen
   }

   thread<name>[...]{
      ...
   }

   ...
}
\end{verbatim}

\minisec{Modell mit statischen Prozessen}

Starte alle.  Paralleles Programm terminiert, wenn alle Prozesse terminieren.

Üblicherweise unterscheidet man zwischen:
\begin{itemize}
    \item Prozessen: Eigener Adressraum
\item Threads: Gemeinsamer Adressraum, billiger
\end{itemize}

\minisec{Darstellung des Skalarproduktes in verschiedenen Modellen}

\begin{bsp} Skalarprodukt mit $2$ Prozessen\\
\verb|{ N=8; real X[N], Y[N], s=0;|\\
\verb|   thread| $\pi_\mathtt{1}$ \verb|{ real t=0;|\\
\verb|      for i=0 ...| $\mathtt{\left(\frac{N}{2} -1\right)}$\\
\verb|         t = t + X[i]*Y[i];|\\
\verb|      s = s + t;}|\\
\verb|   thread| $\pi_\mathtt{2}$ \verb|{ real t=0;|\\
\verb|      for i=| $\mathtt{\frac{N}{2}}$ \verb|... N|\\
\verb|         t = t + X[i]*Y[i];|\\
\verb|      s = s + t;}|\\
\verb|}|

Es fehlt die Eingabe von \texttt{X} und \texttt{Y}. Ein Thema hier ist \textit{paralleles Lesen}. Fehler können beim \textit{parallelen Schreiben} auftreten. Was ist das Ergebnis beim parallelen Schreiben?

Problem ist die Zeile "`\verb|s=s+t|"'.  
\end{bsp}

\section{Der kritische Abschnitt (Detailbetrachtung von \texttt{s=s+t})}

(Die Zahlen in den Kreisen kennzeichnen Programmzeilen, auf die wir uns später wieder beziehen werden.)

\begin{tabbing}
\ding{202}\verb|  |\= \verb|ADDIERE  R1, R2 NACH R3| XXXXXXXXXXX \= \ding{202}\verb|  |\= \verb|ADDIERE  R1, R2 NACH R3|\kill
\verb|THREAD| $\pi_\mathtt{1}$ \> \> \verb|THREAD| $\pi_\mathtt{2}$\\
\ding{202}\>\verb|LADE     s      IN   R1| \> \ding{204}\>\verb|LADE     s      IN   R1|\\
\>\verb|LADE     t      IN   R2| \> \>\verb|LADE     t      IN   R2|\\
\>\verb|ADDIERE  R1, R2 NACH R3| \>\>\verb|ADDIERE  R1, R2 NACH R3|\\
\ding{203}\>\verb|SCHREIBE R3     NACH s| \> \ding{205}\>\verb|SCHREIBE R3     NACH s|\\
\end{tabbing}





Innerhalb eines Prozesses wird die Reihenfolge der Programmzeilen eingehalten.

CSP: Hintereinanderausführung in jedem Thread.
\begin{itemize}
    \item In Thread $\pi_1$ wird Zeile 1 zuerst und Zeile 4 zuletzt ausgeführt
\item Analog in Thread $\pi_2$
\end{itemize}

Aber wie sieht die relative Reihenfolge aus, wenn die Threads nicht Zeile für Zeile parallel laufen?

Beispiele:
\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Reihenfolge} & \textbf{Ergebnis}\\ \midrule
\ding{202} \ding{203} \ding{204} \ding{205} & $s=t_1 + t_2$\\
\ding{204} \ding{205} \ding{202} \ding{203} & $s= t_2 + t_2$ auch korrekt\\
\ding{202} \ding{204} \ding{203} \ding{205} & $s = t_2$ falsch\\
\ding{202} \ding{204} \ding{205} \ding{203} & $s=t_1$ falsch\\
\ding{204} \ding{202} \ding{203} \ding{205} & $s=t_2$ falsch\\
\ding{204} \ding{202} \ding{205} \ding{203} & $s=t_1$ falsch\\\bottomrule
\end{tabular}
\end{center}


Benötigen einen Mechanismus, sodass die beiden Prozessoren aufeinander Rücksicht nehmen.

\minisec{Lösung des Problems "`Kritische Abschnitte"' (critical region)}
Mit gegenseitigem Ausschluss (mutual exclusion = "`MUTEX"')
Schreibweise in CSP: Anweisung in eckige Klammern setzen: \verb|[s=s+t]|. Die Reihenfolge (auch im Konfliktfall) ist noch immer unbestimmt, diese brauchen wir auch nicht. Aber im Konfliktfall wird diese Operation nacheinander abgearbeitet.

Es wird so ausgeführt: [\ding{202} \ding{203}] [\ding{204} \ding{205}] oder [\ding{204} \ding{205}] [\ding{202} \ding{203}]

\section{Parametrisierte Form (single program -- multiple data, = "`SPMD"')}

Häufig eine variable Zahl von Prozessen. Wir wollen eine Aufteilung gleichartiger Operationen. Wir wollen ein Programm für alle.

\begin{bsp}
Skalarprodukt mit $P=$\texttt{Pr} Prozessoren

\verb|{ int N, Pr; real X[N], Y[N], s=0;|\\
\verb|   thread| $\pi$ \verb|(int p) {|\\
\verb|      real t = 0;|\\
\verb|      for i=| $\mathtt{N\cdot \frac{p}{Pr}}$ \verb|...| $\mathtt{\left( N \cdot \frac{p+1}{Pr} - 1 \right)}$\\
\verb|         t = t + X[i]*Y[i];|\\
\verb|      [s = s + t;]}   // Kritischer Abschnitt|\\
\verb|}|

Jeder Prozess wird meinem anderen $p$ gestartet. Das Programm macht daraus die Verteilung der Operationen.

Für diese Implementierung haben einen Aufwand von $\cO \left( \frac{N}{P} + P\right)$
\end{bsp}

\section{Verbesserung der Effizienz}

Der kritische Abschnitt läuft sequentiell ab.

Ziel ist es, den kritischen Abschnitt baumartig auszuführen, damit ein logarithmischer Aufwand entsteht. Auch ein Ziel ist es, dass der kritische Abschnitt nicht mehr kritisch ist.

\begin{figure}[t!]
\centering \begin{tikzpicture}
\draw[->] (-2,0) -- (-1,0) (-1.5,0) -- (-1.5,-6.5);
\draw (-1.5,-3) node[rotate=90,above] {Schritte};
\draw (-1.5,-1) node[right] {0.};
\draw (-1.5,-3) node[right] {1.};
\draw (-1.5,-5) node[right] {2.};
\draw (0,0) node[right] (a) {$S_0$};
\draw (1,0) node[right] (b) {$S_1$};
\draw (2,0) node[right] (c) {$S_2$};
\draw (3,0) node[right] (d) {$S_3$};
\draw (4,0) node[right] (e) {$S_4$};
\draw (5,0) node[right] (f) {$S_5$};
\draw (6,0) node[right] (g) {$S_6$};
\draw (7,0) node[right] (h) {$S_7$};
\draw (0,-2) node[right] (aa) {$S_0 + S_1$};
\draw (2,-2) node[right] (bb) {$S_2 + S_3$};
\draw (4,-2) node[right] (cc) {$S_4 + S_5$};
\draw (6,-2) node[right] (dd) {$S_6 + S_7$};
\draw (0,-4) node[right] (aaa) {$S_0 + S_1 + S_2 + S_3$};
\draw (4,-4) node[right] (bbb) {$S_4 + S_5 + S_6 + S_7$};
\draw (0,-6) node[right] (aaaa) {$S_0 + S_1 + S_2 + S_3+S_4 + S_5 + S_6 + S_7$};
\draw[->] (a.south west) -- (aa.north west);
\draw[->] (b.south west) -- ($(aa.north west) + (0.1,0)$);
\draw[->] (c.south west) -- (bb.north west);
\draw[->] (d.south west) -- ($(bb.north west)+(0.1,0)$);
\draw[->] (e.south west) -- (cc.north west);
\draw[->] (f.south west) -- ($(cc.north west)+(0.1,0)$);
\draw[->] (g.south west) -- (dd.north west);
\draw[->] (h.south west) -- ($(dd.north west)+(0.1,0)$);
\draw[->] (aa.south west) -- (aaa.north west);
\draw[->] (bb.south west) -- ($(aaa.north west)+(0.1,0)$);
\draw[->] (cc.south west) -- (bbb.north west);
\draw[->] (dd.south west) -- ($(bbb.north west)+(0.1,0)$);
\draw[->] (aaa.south west) -- (aaaa.north west);
\draw[->] (bbb.south west) -- ($(aaaa.north west)+(0.1,0)$);
\draw (0,-.5) node[right,fill=black!20] {\tiny \sffamily 000};
\draw (1,-.5) node[right,fill=black!20] {\tiny \sffamily 001};
\draw (2,-.5) node[right,fill=black!20] {\tiny \sffamily 010};
\draw (3,-.5) node[right,fill=black!20] {\tiny \sffamily 011};
\draw (4,-.5) node[right,fill=black!20] {\tiny \sffamily 100};
\draw (5,-.5) node[right,fill=black!20] {\tiny \sffamily 101};
\draw (6,-.5) node[right,fill=black!20] {\tiny \sffamily 110};
\draw (7,-.5) node[right,fill=black!20] {\tiny \sffamily 110};
\draw (0,-2.5) node[right,fill=black!20] {\tiny \sffamily 000};
\draw (2,-2.5) node[right,fill=black!20] {\tiny \sffamily 010};
\draw (4,-2.5) node[right,fill=black!20] {\tiny \sffamily 100};
\draw (6,-2.5) node[right,fill=black!20] {\tiny \sffamily 110};
\draw (0,-4.5) node[right,fill=black!20] {\tiny \sffamily 000};
\draw (4,-4.5) node[right,fill=black!20] {\tiny \sffamily 100};
\draw (0,-6.5) node[right,fill=black!20] {\tiny \sffamily 000};
\end{tikzpicture}
\caption{Schema zu "`Skalarprodukt Baumkombination"'. \newline \tiny Die Pfeile kennzeichnen Warteabhängigkeiten zwischen Threads. Die Zahlen in den grauen Kästen sind die Binärdarstellungen der Zahl $p$ des aktiven Threads $p$.}

\vspace{5mm}

\hrule
\end{figure}

Wir benötigen eine globale Variable für Zwischenergebnisse \verb|S[p]| und für die Terminierung $\verb|flag[p]|$.

Sei \texttt{Pr}$=P= 2^d$. Im Schritt $i \in \{0, \dots, d-1\}$ addiert
\begin{center}
Thread "`$l=\dots \underbrace{0\dots\dots 0}_{i+1 \text{ Nullen} }$"'
\end{center}
mit
\begin{center}
Thread "`$r=\dots 1 \underbrace{0\dots\dots 0}_{i \text{ Nullen}}$"'
\end{center}
(die vorderen Zifferen -- die "`\dots"' -- müssen gleich sein), der danach terminiert.

\newpage\minisec{Skalarprodukt Baumkombination}
\verb|{ int N, d, Pr=| $\mathtt{2^d}$ \verb|; real X[N], y[N], s[Pr]=0;|\\
\verb|  boolean flag[Pr] = false; // Initialisierung vor Start des|\\
\verb|                           // ersten Threads|\\
\verb|   thread | $\pi$ \verb|(int p){|\\
\verb|      for i=| $\mathtt{N\cdot\frac{p}{Pr}}$ \verb|...| $\mathtt{\left( N\cdot\frac{p+1}{Pr} - 1\right)}$\\
\verb|         S[p] = S[p] + X[i]*Y[i];|\\
\verb|      for i = 0,...,d-1|\\
\verb|         l = (p >> (i+1)) << (i+1); // lösche (i+1) Bits|\\
\verb|         r = l +| $\mathtt{2^i}$ \verb|;|\\
\verb|         if (p==r)|\\
\verb|            flag[p]=true; end;|\\
\verb|         if (p==l)|\\
\verb|            while (flag[r] == false) // BUSY WAIT|\\
\verb|               S[p] = S[p] + S[r];}|\\
\verb|}|

Ergebnis liegt am Ende in \verb|S[0]|. Alternativ zum Busy Wait kann man Synchronisation des OS verwenden.

\newpage\section{Daten verteilen / lokalisieren}

Der Zugriff auf lokale Daten ist meist schneller als auf globale Daten. Zum Teil ist der Zugriff auf globale Daten auch nicht möglich, z.\,B. bei mehreren PCs ohne gemeinsamen Speicher.

Beispiel:

\verb|Skalarprodukt lokal {|\\
\verb|   int Pr, N; real S=0; // globale Variable f. d. Summe|\\
\verb|                       // keine glob. Variablen für die Vektoren X u. Y|\\
\verb|   thread| $\pi$ \verb|(int p) {|\\
\verb|      real X| $\mathtt{\left[\frac{N}{Pr}\right]}$ \texttt{, Y} $\mathtt{\left[\frac{N}{Pr}\right]}$\texttt{;}\\
\verb|      real t = 0;|\\
\verb|      for i = 0 ... |$\mathtt{\left(\frac{N}{Pr} - 1\right)}$\\
\verb|         t = t + X[i]*Y[i];|        // Teilsumme\\
\verb|      [ s = s + t ;] }|             // krit. Abschnitt\\
\verb|}|

Folgendes Diagramm veranschaulicht die Lokalisierung der Daten:
\begin{center}
\begin{tabular}{|lr|lr|lr|}
\hline
\multicolumn{3}{|l}{$0$} & \multicolumn{3}{r|}{$N$}\\
\hline
$0$ & \qquad $\frac{N}{P}$ & $0$ & \qquad $\frac{N}{P}$ & $0$ & \qquad $\frac{N}{P}$\\
\hline
\end{tabular}
\begin{tabular}{l}
\quad lokal\\
\quad global
\end{tabular}
\end{center}

In diesem Beispiel fehlt das Einlesen der Daten $X$ und $Y$, z.\,B. mit einem parallelen Dateisystem. Die Umrechnung der Indizes kann auf verschiedene Arten erfolgen:
\begin{enumerate}
\item $i_\text{global} = i_\text{lokal} + N \cdot \frac{p}{P}$ oder
\item $i_\text{global} = i_\text{lokal} \cdot P + p$.
\end{enumerate}

Die Erweiterung auf $\frac{N}{P} \notin \bN$ erfolgt durch Rundung. Es besteht aber immer noch das Problem der globalen Variable $S$.

\section{Nachrichten austauschen (message passing)}

Es liegt ein verteilter Speicher vor, also sind keine globalen Variablen mehr möglich. Konstanten können aber beispielsweise global sein (im Programmcode). Die Kommunikation geschieht über einen Nachrichtenaustausch (siehe \autoref{fig:block_senden}).

\begin{figure}[t!]
\begin{center}
\begin{tikzpicture}
\draw[->] (0,0) -- (0,-3);
\draw (-.5,-2.5) node {$t$};
\draw (2,0) node {$\pi_1$};
\draw (8,0) node {$\pi_2$};
\draw (2,-.5) node[rectangle,draw=black] (a) {\tt send};
\draw (8,-.5) node[rectangle,draw=black] {\tt recv};
\draw (2,-1.2) node[rectangle,draw=black] (d) {warte};
\draw (8,-1.2) node[rectangle,draw=black] {warte};
\draw (8,-3) node[rectangle,draw=black] (b) {~~~~~};
\draw[->] (a) -- (b);
\draw (2,-3) node (c) {};
\draw[->] (b) -- (c);
\draw[->] (d) -- (c);
\draw (5,-2.7) node {Bestätigung};
\draw (5,-1.5) node[rotate=-25] {Daten};
\end{tikzpicture}
\end{center}
\caption{Blockierendes Senden / Empfangen. \newline \tiny Schematische Darstellung der Übertragung einer Nachricht von Prozess $\pi_1$ nach $\pi_2$ (z.\,B. auf unterschiedlichen Rechnern).}

\vspace{5mm}

\hrule \label{fig:block_senden}
\end{figure}

\newpage Beispiel: Sei \texttt{Pr=}$P=2^d$. $N$ ein Vielfaches von $P$.

\verb|Skalarprodukt message-passing {|\\
\verb|   int N, d, Pr=|$\mathtt{2^d}$\\
\verb|   process | $\pi$ \verb|(int p) {|\\
\verb|      real X|$\mathtt{\left[\frac{N}{Pr}\right]}$ \texttt{, Y} $\mathtt{\left[\frac{N}{Pr}\right]}$\texttt{;}\\
\verb|      real s, t=0;|\\
\verb|      for i = 0 ... |$\mathtt{\left(\frac{N}{Pr} - 1\right)}$\\
\verb|         t 0 t + X[i]*Y[i];|\\
\verb|      for i = 0 ... d-1|\\
\verb|         l = (p >> i+1) << (i+1);|\verb+ r = (l | (1 << i);+\\
\verb|         if (p == r)|\\
\verb|            send(l,t); // Sende t an Prozess l|\\
\verb|            terminate;|\\
\verb|         else|\\
\verb|            recv(r,s); // Empfange s von Prozess r|\\
\verb|            t = t + s;}|\\
\verb|}|

Das Endergebnis liegt bei Prozess $0$ in der Variable $t$ vor. Die Synchronisation geschieht über \texttt{send} und \texttt{recv}. Ein verteilter Speicher ist im Allgemeinen schwierig zu programmieren und zu debuggen. Aber er ist skalierbar für große $P$.

\section{Leistungsbewertung und Komplexität}

Wir führen folgende Bezeichnungen ein:
\begin{align*}
T(N,P) &:= \text{Programmlaufzeit mit $N$ Daten und $P$ Prozessoren}\\
T_\text{SEQ}(N) &:= \text{Laufzeit des seq. Programms}
\end{align*}

\begin{defi}[Paralleles Speedup] Wir definieren den Beschleunigungsfaktor $S$ durch:
\[S(N,P) := \frac{T_\text{SEQ}(N)}{T(N,P)}.\]
\end{defi}

Es gilt $0 < S(N,P) \leq P.$

Für unser Beispiel des Skalarproduktes gilt nun:

\begin{description}
\item[sequentiell:] Es ist $T_\text{SEQ} = 2 \cdot N \cdot t_a$, wobei $N$ die Anzahl der Optionen bezeichnet und $t_a$ die Zeit pro arithmetischer Operation (Add. / Mult.).

\item[parallel:] Es ist $T(N,P) = \underbrace{2 \tfrac{N}{P} t_a}_\text{Teilsumme} + \underbrace{\left( \log_2 P \right) \left(t_a + t_m \right)}_\text{Baum}$, wobei $t_m$ die Zeit für das Versenden eines Wortes bezeichnet.

Somit ist
\[S(N,P) = \frac{2 N t_a}{2\frac{N}{P}t_a + \left( \log_2 P \right) \left(t_a + t_m \right)} = \frac{P}{1 + \frac{P}{N} \left( \log_2 P \right) \left(\frac{t_a + t_m}{2t_a} \right) }< P.\]

Für die Extremfälle gilt:
\begin{itemize}
\item $N=P$ $\Rightarrow$ $S(N,P) \approx \frac{P}{\left( \log_2 P \right) \left(\frac{t_a + t_m}{2t_a} \right)}$ 
\item $N \gg P$ $\Rightarrow$ $S(N,P) \approx P$
\end{itemize}
\end{description}

\end{document}
