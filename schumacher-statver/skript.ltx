% Einige zusätzliche Informationen für rubber
%  rubber erkennt nicht, dass die Datei weg kann, daher sagen wir es ihm
% rubber: clean $base.thm
%  rubber soll nach Änderungen an der Datei nochmal bauen
% rubber: watch $base.thm
% rubber: index.tool      xindy
% rubber: index.language  german-din
%
% scrreprt trifft am Besten die Bedürfnisse eines Skripts, das ganze wird
% zweiseitig (twoside), d.h. es wird zwischen linker und rechter Seite
% unterschieden, und wir verwenden zwischen den Absätzen einen Abstand
% von einer halben Zeile (halfparskip) und dafür keinen Absatzeinzug,
% wobei die letzte Zeile eines Absatzes zu min. 1/4 leer ist.

\RequirePackage[l2tabu,orthodox]{nag}  % nag überprüft den Text auf veraltete
                   % Befehle oder solche, die man nicht in LaTeX verwenden
                   % soll -- l2tabu-Checker in LaTeX

\RequirePackage[ngerman=ngerman-x-latest]{hyphsubst} % einbinden der neuen
                   % Trennmuster, diese korrigieren einige Fehler der alten
                   % und bieten mehr Trennstellen

\documentclass[ngerman,draft,parskip=half*,twoside]{scrreprt}

\usepackage{ifthen}
\usepackage{index}
% \usepackage[final]{graphicx}  % Für Grafiken
\usepackage{xcolor}
\usepackage[draft=false,colorlinks,bookmarksnumbered,linkcolor=blue,breaklinks]{hyperref}

\usepackage[utf8]{inputenc}
\usepackage{babel}
% \usepackage{nicefrac}
% \usepackage{xfrac}           % xfrac erfüllt einen ähnlichen Zweck wie
			       % nicefrac. Jedoch macht nicefrac bei einigen
			       % Schriften Probleme. Dies behebt xfrac. Daher
			       % sollte eher das Paket verwendet werden.
                               % xfrac muss vor mathtools geladen werden!
% \usepackage{tabularx}

\usepackage{lmodern}		% Latin Modern
% \usepackage{type1ec}           % cm-super
\usepackage[T1]{fontenc}        % T1-Schriften notwendig für PDFs
\usepackage{textcomp}           % wird benötigt, damit der \textbullet
                                % für itemize in lmodern gefunden wird.

\usepackage[intlimits,leqno]{amsmath}
\usepackage[all,warning]{onlyamsmath}  % warnt bei Verwendung von nicht
                                       % amsmath-Umgebungen z.\,B. $$...$$
\usepackage{amssymb}     % wird für \R, \C,... gebraucht
\usepackage{fixmath}     % ISO-konforme griech. Buchstaben
\usepackage[euro]{isonums} % definiert Komma als Dezimaltrennzeichen

\usepackage[amsmath,thmmarks,hyperref]{ntheorem} % für die Theorem-Umgebungen
                                                 % (satz, defini, bemerk)
\usepackage{xspace}      % wird weiter unten gebraucht
\usepackage{slashbox}    % für schräge Striche links oben in der
                         % Tabelle; s. texdoc slashbox

\usepackage{paralist}    % besseres enumerate und itemize und neue
                         % compactenum/compactitem; s. texdoc paralist

\usepackage{svn}         % Zum Auswerten und ordentlichen Darstellen der
                         % SVN-Schlüsselwörter (s. vor \begin{document})
                         % dafür muss in SVN noch das Flag svn:keywords
                         % auf "LastChangedRevision LastChangedDate"
                         % gesetzt werden
% \usepackage{ifpdf}       % Erkennung, ob PDF generiert wird; nützlich zur
                         % Unterscheidung bei Grafiken \input{XYZ.pdf_t}
\usepackage{ellipsis}    % Korrektur für \dots
\usepackage{fixltx2e}
\usepackage[final,babel]{microtype} % Verbesserung der Typographie
\usepackage{mathtools}   % Zur Definition von \abs und \norm
\usepackage{todonotes}   % definiert den Befehl \todo{} um sich leicht
                         % Markierungen für offene Aufgaben zu setzen; wird
                         % auch für \help (s.u.) verwendet
% \usepackage[text]{esdiff} % Zum Setzen von (partiellen) Ableitungen
                            % (df/dx). Das d wird korrekt in roman
                            % gesetzt. Verwendung: \diff{f}{x} oder
                            % \diffp{f}{x} für partielle Ableitungen
% \usepackage{tikz-cd}  % Einfaches Zeichnen von kommutativen
                        % Diagrammen. Durch die Benutzung von TikZ ist
                        % das mächtiger als xypic.
% Damit auch die Zeichen im Mathemode in Überschriften fett sind
% <news:lzfyyvx3pt.fsf@tfkp12.physik.uni-erlangen.de>
\usepackage{braket}
\usepackage[text]{esdiff}
\usepackage{mathabx}
\usepackage{nicefrac}
\addtokomafont{sectioning}{\boldmath}

% nach dem Theoremkopf wird ein Zeilenumbruch eingefügt, die Schrift des
% Körpers ist normal und der Kopf wird fett gesetzt
\theoremstyle{break}
\theoremnumbering{arabic}
\theorembodyfont{\normalfont}
\theoremheaderfont{\normalfont\bfseries}

% Das Ende von Umgebungen, für die kein Beweis erbracht wurde, soll mit einer
% leeren Box gekennzeichnet werden. Wenn jedoch ein Beweis erbracht wurde,
% soll kein Zeichen ausgegeben werden (die ausgefüllte Box vom proof wird
% verwendet); man beachte die spezielle Definition von \theoremheaderfont für
% die Umgebung proof
% \newboolean{hasproof}
% \theoremheaderfont{\global\hasprooffalse\normalfont\bfseries}
% \theoremsymbol{\ifthenelse{\boolean{hasproof}}{}{\ensuremath{_\Box}}}

% Die folgenden Umgebungen werden einzeln nummeriert und am Ende jedes
% Kapitels zurückgesetzt
\newtheorem{satz}{Satz}[chapter]
\newtheorem{bemerk}{Bemerkung}[chapter]
\newtheorem{defini}{Definition}[chapter]
\newtheorem{bsp}{Beispiel}[chapter]
\newtheorem{festl}{Festlegung}[chapter]

% Die folgenden Theoremumgebungen bekommen keine Nummer
\theoremstyle{nonumberbreak}
\newtheorem{fakt}{Fakt}

% \theoremheaderfont{\global\hasprooftrue\scshape}
\theoremheaderfont{\scshape}
\theorembodyfont{\normalfont}
% Das Zeichen am Ende eines Beweises
\theoremsymbol{\ensuremath{_\blacksquare}}
% \theoremsymbol{q.\,e.\,d.}
\newtheorem{proof}{Beweis:}

% Hier die Definition, wie \autoref die Umgebungen nennen soll, die mit
% \newtheorem definiert wurden
\newcommand*{\satzautorefname}{Satz}
\newcommand*{\bemerkautorefname}{Bemerkung}
\newcommand*{\definiautorefname}{Definition}
\newcommand*{\bspautorefname}{Beispiel}
\newcommand*{\festlautorefname}{Festlegung}
% Zwischen Unter- und Unterunterabschnitten sollte nicht unterschieden
% werden.
\renewcommand*{\subsectionautorefname}{Abschnitt}
\renewcommand*{\subsubsectionautorefname}{Abschnitt}

\pagestyle{headings}

\newcommand*{\R}{\mathbb{R}}      % reelle Zahlen
\newcommand*{\C}{\mathbb{C}}      % komplexe Zahlen
\newcommand*{\N}{\mathbb{N}}      % natürliche Zahlen
\newcommand*{\Q}{\mathbb{Q}}      % gebrochene Zahlen
\newcommand*{\Z}{\mathbb{Z}}      % ganze Zahlen

% Wenn irgendwo Unklarheiten zum Inhalt im Skript auftreten, können sie
% einfach mit \help{Ich verstehe das nicht} hervorgehoben werden. Dies
% macht es leichter sie alle zu finden und auch ganz einfach
% auszublenden, indem man den Befehl einfach leer definiert
\newcommand*{\help}[1]{\todo[color=green!40]{#1}}

% Um wichtige Begriffe im Text überall gleich vorzuheben (gleiches
% Markup), sollte dieser Befehl verwendet werden. Das Argument wird
% automatisch als Indexeintrag verwendet. Dieser kann aber auch als
% optionales Argument selbst bestimmt werden.
\newcommand*{\highl}[2][]{\textbf{\boldmath{#2}}%
  \ifthenelse{\equal{#1}{}}{\index{#2}}{\index{#1}}%
}

% Befehl für die Darstellung der Gliederungsüberschriften im Index
\newcommand*{\lettergroup}[1]{\minisec{#1}}

% Für Leute, die nicht gern o.\,B.\,d.\,A. jedesmal eintippen wollen
\newcommand*{\obda}{o.\,B.\,d.\,A.\xspace}

% Diese Befehle sind dafür gedacht, dass die Symbole für "genau dann wenn"
% im ganzen Dokument gleich aussehen. Außerdem erlaubt es eine schnelle
% Veränderung aller Stellen, falls der Prof. doch nicht mehr gdw nimmt,
% sondern \Leftrightarrow.
\newcommand*{\gdw}{\ifthenelse{\boolean{mmode}}%
			       {\mspace{8mu}gdw\mspace{8mu}}%
			       {$gdw$\xspace}}
\newcommand*{\gdwdef}{\ifthenelse{\boolean{mmode}}%
			       {\mspace{8mu}gdw_{def}\mspace{8mu}}%
			       {$gdw_{def}$\xspace}}

% Um sicherzustellen, dass jeder Betrag/jede Norm links und rechts die
% Striche bekommt, sind diese Befehle da. Damit kann man nicht die
% rechten Striche vergessen und es wird etwas übersichtlicher. Aus
% mathtools.pdf, z. B. \abs[\big]{\abs{a}-\abs{b}} \leq \abs{a+b}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

% Für die Gaußklammer empfiehlt sich ebenso eine Definition mit
% Benutzung von mathtools.
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% Das original Epsilon sieht nicht so toll aus
\renewcommand*{\epsilon}{\varepsilon}
% ... und mancheinem gefällt auch das Phi nicht
\renewcommand*{\phi}{\varphi}

\newcommand*{\CA}{\mathcal{A}}
\newcommand*{\CL}{\mathcal{L}}
\newcommand*{\CM}{\mathcal{M}}

\newcommand*{\EE}{\mathbb{E}}

\makeindex

\SVN $LastChangedRevision$
\SVN $LastChangedDate$

\begin{document}

\title{Statistische Verfahren}
\author{Jens Schumacher}
\date{Semester:  WS 2012/13}
\maketitle

\clearpage
\chapter*{Vorwort}

{\itshape
  Dieses Dokument wurde als Skript für die auf der
  Titelseite genannte Vorlesung erstellt und wird jetzt im Rahmen des
  Projekts
  "`\href{http://uni-skripte.lug-jena.de/}
  {Vorlesungsskripte der Fakultät für Mathematik}
  \href{http://uni-skripte.lug-jena.de/}{und Informatik}"'
  weiter betreut. Das
  Dokument wurde nach bestem Wissen und Gewissen angefertigt. Dennoch
  garantiert weder der auf der Titelseite genannte Dozent, die Personen,
  die an dem Dokument mitgewirkt haben, noch die
  Mitglieder des Projekts für dessen Fehlerfreiheit. Für etwaige Fehler
  und dessen Folgen wird von keiner der genannten Personen eine Haftung
  übernommen. Es steht jeder Person frei, dieses Dokument zu lesen, zu
  verändern oder auf anderen Medien verfügbar zu machen, solange ein
  Verweis auf die Internetadresse des Projekts
  \url{http://uni-skripte.lug-jena.de/}
  enthalten ist.

  Diese Ausgabe trägt die Versionsnummer~\SVNLastChangedRevision{} und ist vom
  \SVNDate{}. Eine neue Ausgabe könnte auf der Webseite des Projekts verfügbar
  sein.

  Jeder ist dazu aufgerufen, Verbesserungen, Erweiterungen und
  Fehlerkorrekturen für das Skript einzureichen bzw. zu melden oder diese
  selbst einzupflegen -- einfach eine E-Mail an die
  \href{mailto:uni-skripte@lug-jena.de}{Mailingliste
  \nolinkurl{<uni-skripte@lug-jena.de>}} senden. Weitere Informationen
  sind unter der oben genannten Internetadresse verfügbar.

  Hiermit möchten wir allen Personen, die an diesem Skript mitgewirkt
  haben, vielmals danken:
  \begin{itemize}
   \item \href{mailto:jens@kubieziel.de}{Jens Kubieziel
    \nolinkurl{<jens@kubieziel.de>}} (2012/3)
  \end{itemize}
}

\clearpage
\pdfbookmark[0]{Inhaltsverzeichnis}{inhaltsverzeichnis}
\tableofcontents

\clearpage
\pdfbookmark[0]{Auflistung der Sätze}{theoremlist}
\chapter*{Auflistung der Theoreme}

\pdfbookmark[1]{Sätze}{satzlist}
\section*{Sätze}
\theoremlisttype{optname}
\listtheorems{satz}

\pdfbookmark[1]{Definitionen und Festlegungen}{definilist}
\section*{Definitionen und Festlegungen}
% \theoremlisttype{all}
\listtheorems{defini,festl}

\chapter{Einführung}

In der Wahrscheinlichkeitstheorie gibt es einen Raum $(\Omega,\CA,P)$. Dann
hat man eine Zufallsvariable~$Y$. Diese bildet die folgende Abbildung
$(\Omega,\CA,P)\xrightarrow{Y} (\Omega_{Y}, \CA_{Y}, P^{Y})$. Für das
Wahrscheinlichkeitsmaß gilt: $P^{Y}\colon \CA_{Y}\rightarrow[0,1]$.

\begin{bsp}
  Es kann $P^{Y}$ das Lebesgue-Maß auf $[0,1]$ sein. Dies entspricht der
  Dichtefunktion. Falls eine Dichtefunktion existiert, so gilt:
  \begin{gather*}
    P(\Set{\omega | Y(\omega)\in[a,b]})= \int_{a}^{b} f(y)\,dy\\
    P(a\leq Y\leq b)=\int_{a}^{b} f(y)\,dy
  \end{gather*}

  Für die Normalverteilung schreibt man $Y\sim N(\mu,\sigma^{2})$. Die
  Dichtefunktion ist
  \begin{gather*}
    f(y)=
       \frac{1}{\sqrt{2\pi\sigma^{2}}}\exp{-\frac{(y-\mu)^{2}}{2\sigma^{2}}}=
       \phi_{\mu,\sigma^{2}}(y)
  \end{gather*}

  Die Poissonverteilung hat keine Dichtefunktion. Es ist $\Omega_{N}=\N$.
  \begin{gather*}
    P(\Set{\omega | Y(\omega)=k})=P(Y=k)=P^{Y}(\{k\})= \frac{\lambda^{k}}{k!}
       e^{-\lambda}\\
    P(Y\in[a,b])= \sum_{a\leq k\leq b} \frac{\lambda^{k}}{k!}e^{-\lambda}
  \end{gather*}
\end{bsp}

Wenn wir uns andererseits mit Statistik beschäftigen, ist nicht die »ideale
Situation« gegeben. Typischerweise hat man ein (statistisches) Experiment. Wie
schon oben bei der Wahrscheinlichkeitstheorie haben wir
$(\Omega,\CA,P)\xrightarrow{Y}(\Omega_{Y},\CA_{Y},\Set{P_{\theta} | \theta
\in\Theta})$.
Letzteres ist eine Familie von Wahrscheinlichkeitsmaßen, wobei der
Parameter~$\theta$ unbekannt ist.

\begin{defini}[Statistisches Experiment]
  $(Y,\Omega,\CA,\Set{P_{\theta} | \theta \in\Theta})$
  heißt \highl[Experiment!statistisches]{statistisches Experiment}.
\end{defini}

\begin{defini}[Schätzer]
  Ein \highl{Schätzer} ist eine messbare Abbildung $T\colon
  \Omega_{Y}\rightarrow \Theta$.
\end{defini}

\begin{bemerk}
  Vorschrift zur Berechnung eines Schätzwertes für unbekannte Parameter.
\end{bemerk}

\begin{defini}[Schätzung]
  Für eine konkrete Realisierung $y=Y(\omega)$ heißt $T(y)$ \highl{Schätzung}
  für den unbekannten Parameter~$\theta$.
\end{defini}

Es ist sinnvoll, die Definition des zentralen Grenzwertsatzes sowie die das
Poissonschen Grenzwertsatzes zu wiederholen.

\chapter{Konstruktion von Schätzern}

Zur Konstruktion von Schätzern verwenden wir in dieser Vorlesung zwei Methoden:
\begin{itemize}
 \item Momentenmethode
 \item Maximum"=Likelihood"=Methode
\end{itemize}

\begin{bsp}
  Poissonverteilung: Wir haben Beobachtungen $y_{1},\dotsc, y_{n}$. Diese
  werden als Realisierung von unabhängigen poissonverteilten Zufallsgrößen mit
  dem Parameter~$\lambda$  aufgefasst. Wir nennen die $Y_{1},\dotsc, Y_{n}$,
  d.\,h. das statistische Experiment sieht wie folgt aus:
  $(\underline{Y}=(Y_{1},\dotsc, Y_{n}), \Omega, \CA,
  \text{Poisson}(\lambda))$ mit $\lambda>0$. Eigentlich ist das Poissonmaß ein
  Produktmaß, wird also $n$"~mal multipliziert. Wenn der unbekannte
  Parameter~$\lambda$ bekannt wäre, könnte man alle interessierenden
  Wahrscheinlichkeiten ausrechnen: $P(Y_{1}=y_{1},\dotsc, Y_{n}=y_{n})$ ist
  wegen der Unabhängigkeit gleich dem Ausdruck $P(Y_{1}=y_{1})\cdot\dotso
  \cdot P(Y_{n}=y_{n})$. Dies ist aber:
  \begin{gather*}
    \frac{\lambda^{y_{1}}}{y_{1}!}e^{-\lambda}\cdot\dotso
       \cdot\frac{\lambda^{y_{n}}}{y_{n}!}e^{-\lambda}=
       \frac{\lambda^{y_{1}+\dotsb+ y_{n}}}{y_{1}!\cdot\dotso \cdot y_{n}!}
       e^{-n\lambda}
  \end{gather*}
\end{bsp}

Die Grundidee der Maximum"=Likelihood"=Methode ist es, als Schätzer für den
unbekannten Parameter denjenigen Wert zu wählen, der die Wahrscheinlichkeit
der beobachteten Daten maximiert.

Likelihood"=Funktion: Betrachte für konkrete Realisierungen (Beobachtungen)
$y_{1},\dotsc, y_{n}$ die Wahrscheinlichkeit $P(Y_{1}=y_{1},\dotsc,
Y_{n}=y_{n})$ als Funktion des unbekannten Parameters:
$\CL(\lambda|(y_{1},\dotsc, y_{n}))= \frac{\lambda^{y_{1}+\dotsb+
y_{n}}}{y_{1}!\cdot\dotso \cdot y_{n}!} e^{-n\lambda}$. Vor dem Ableiten und
Nullsetzen erfolgt der Übergang zur
Log"~Likelihood"=Funktion~$l(\lambda|(y_{1},\dotsc, y_{n}))=
\log(\CL(\lambda|(y_{1},\dotsc, y_{n})))= (y_{1}+\dotsb+
y_{n})\cdot\log\lambda-n\lambda -\log(y_{1}!\cdot\dotso y_{n}!)$. Die
Ableitung ist $\diffp{}{\lambda} l(\lambda|(y_{1},\dotsc,
y_{n}))= \frac{y_{1}+\dotsb+ y_{n}}{\lambda} -n\neq0\Rightarrow\hat{\lambda}
=\frac{y_{1}+\dotsb+ y_{n}}{n}$. Das ist die Maximum"=Likelihood"=Schätzung
für den unbekannten Parameter.

Welche Art von Modellen werden wir behandeln?
\begin{itemize}
 \item unabhängige Beobachtungen
 \item nicht notwendig identische Verteilung der Beobachtungen
\end{itemize}

\begin{bsp}[Körpergewicht eines Säuglings im ersten Lebensjahr]
  Beobachtungen: $y_{i}$ Körpergewicht und $x_{i}$ Alter für $i=1,\dotsc,n$;
  Insgesamt also $(y_{i}, x_{i})$\\
  Statistisches Modell: Wir sehen $x_{i}$ als fest an. Das $y_{i}$ ist die
  Realisierung einer Zufallsgröße~$Y_{i}$.\\
  deterministischer Teil: $\EE Y_{i}= f(x_{i})=
  \beta_{1}+\beta_{2}x_{i}=\colon\mu_{i}$\\
  stochastischer Teil: $Y_{i}\sim N(\mu_{i},\sigma^{2})$\\
  Dies ist ein einfaches lineares Regressionsmodell.

  % VL vom 16.10.2012
  Wir wollen uns überlegen, warum Maximum"=Likelihood"=Schätzung der Parameter
  $\beta_{1}$ und $\beta_{2}$ zum Erfolg führt. Die Likelihood"=Funktion,
  die entsprechende Log"~Likelihood"=Funktion sowie die partiellen Ableitungen
  sind:
  \begin{gather*}
    \CL(\beta_{1},\beta_{2}| y_{1},\dotsc, y_{n})= \prod_{i=1}^{n}
  \frac{1}{\sqrt{2\pi\sigma^{2}}} \exp\left(-\frac{(y_{i}- (\beta_{1}+
  \beta_{2}x_{i}))^{2}}{2\sigma^{2}}\right)\\
    l(\beta_{1},\beta_{2}| y_{1},\dotsc, y_{n})= \sum_{i=1}^{n} -\frac{1}{2}
       \log(2\pi\sigma^{2}) -\frac{(y_{i}- (\beta_{1}+
       \beta_{2}x_{i}))^{2}}{2\sigma^{2}}\\
    \diffp{l(\beta_{1},\beta_{2})}{{\beta_{1}}}= \sum_{i=1}^{n} -\frac{2(y_{i}-
       (\beta_{1}+ \beta_{2}x_{i}))}{2\sigma^{2}}\cdot(-1)=
       \frac{1}{\sigma^{2}} \sum_{i=1}^{n} y_{i}-
       (\beta_{1}+\beta_{2}x_{i})= 0\\
    \diffp{l(\beta_{1},\beta_{2})}{{\beta_{2}}}= \sum_{i=1}^{n} -\frac{2(y_{i}+
       (\beta_{1}+\beta_{2}x_{i}))}{2\sigma^{2}} \cdot(-x_{i})=
       \frac{1}{\sigma^{2}} \sum_{i=1}^{n}
       x_{i}y_{i}-(\beta_{1}+\beta_{2}x_{i})x_{i}=0
  \end{gather*}

  Weiterhin haben wir:
  \begin{align*}
    \sum_{i=1}^{n} (y_{i}-\beta_{1}-\beta_{2}x_{i}) &= 0 &
    \sum_{i=1}^{n} y_{i}-n\beta_{1}-\beta_{2}\sum_{i=1}^{n}x_{i} &= 0\\
    \frac{1}{n} \sum_{i=1}^{n} y_{i}- \beta_{2}\frac{1}{n} \sum_{i=1}^{n}
       x_{i} &= \beta_{1} &
    \overline{y_{n}}-\beta_{2}\overline{x_{n}} &= \beta_{1}\\
    \intertext{Einsetzen ergibt}\\
    \sum_{i=1}^{n} (x_{i}y_{i}-( \overline{y_{n}} -\beta_{2}\overline{x_{n}}
       +\beta_{2}x_{i})x_{i}) &=0 &
    \sum_{i=1}^{n} x_{i}y_{i} -\overline{y_{n}} \sum_{i=1}^{n} x_{i}+
       \beta_{2} \overline{x_{n}} \sum_{i=1}^{n} x_{i}-\beta_{2}\sum_{i=1}^{n}
       x_{i}^{2} &= 0
   \end{align*}
  \begin{align*}
  \hat{\beta_{2}} &= \frac{\sum_{i=1}^{n} x_{i}y_{i} -\overline{y_{n}}
       \sum_{i=1}^{n} x_{i}}{\sum_{i=1}^{n} x_{i}^{2}- \overline{x_{n}}
       \sum_{i=1}^{n} x_{i}}= \frac{\frac{1}{n} \sum_{i=1}^{n} x_{i}y_{i} -\overline{x_{n}}
       \overline{y_{n}}}{\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2}-
       \overline{x_{n}}^{2}}\\
    \hat{\beta_{1}} &= \overline{y_{n}}- \hat{\beta_{2}} \overline{x_{n}}
  \end{align*}
  Die $(\hat{\beta_{1}}, \hat{\beta_{2}})$ stellen das Maximum der
  Likelihood"=Funktion dar. Dazu müssen wir die zweite Ableitung untersuchen.
  Weiterhin ist $\hat{\beta_{2}}$ genau dann nicht eindeutig bestimmt, wenn
  $x_{1}= \dotsb= x_{n}= \overline{x_{n}}$.
\end{bsp}

\begin{bemerk}
  Die Maximum"=Likelihood"=Schätzung maximiert:
  \begin{gather*}
    \sum_{i=1}^{n} -(y_{i}+(\beta_{1}+\beta_{2}x_{i}))^{2}
  \end{gather*}
  also minimiert die Summe $\sum_{i=1}^{n}
  (y_{i}-(\beta_{1}+\beta_{2}x_{i}))^{2}$, d.\,h. Summe der quadratischen
  Abweichungen zwischen beobachteten Werten und vom Modell vorhergesagten
  Werten. Also ist der Maximum"=Likelihood"=Schätzer gleich dem Schätzer nach
  der Methode der kleinsten Quadrate.
\end{bemerk}

\begin{bsp}[Artenzahlen auf den Galapagos"=Inseln]
  \begin{itemize}
   \item nichtlinearer Zusammenhang
   \item Normalverteilung nicht sinnvoll, da
    \begin{itemize}
     \item diskrete Zielgröße
     \item nichtnegative Zielgröße
    \end{itemize}
   \item Variabilität der Zielgröße wächst mit steigender Inselgröße
  \end{itemize}

  Für den stochastischen Teil nehmen wir an, dass eine Poissonverteilung
  vorliegt: $Y_{i}\sim\text{Poisson}(\mu_{i})$. Im deterministischen Teil
  haben wir: $\mu_{i}= \EE Y_{i}= \beta_{1}+ \beta_{2}x_{i}$. Der lineare
  Ansatz kann zu unmöglichen Erwartungswerten führen.  Daher betrachten wir
  $\log(\mu_{i})= \log(\EE Y_{i})= \beta_{1}+\beta_{2}x_{i}$. Dies heißt dann
  \highl[Poissonregression!einfache]{einfache Poissonregression}.
  Die Varianz von $Y_{i}$ ist $\mu_{i}= \exp{\beta_{1}+\beta_{2}x_{i}}$.

  Kann man mit der Maximum"=Likelihood"=Schätzer der Parameter zum Erfolg
  kommen? Die Likelihood"=Funktion ist
  \begin{gather*}
    \CL(\beta_{1},\beta_{2}| y_{1},\dotsc, y_{n})= \prod_{i=1}^{n}
       \frac{\mu_{i}^{y_{i}}}{y_{i}!} e^{-\mu_{i}}
  \end{gather*}
  Die Log"~Likelihood"=Funktion ist
  \begin{gather*}
    l(\beta_{1},\beta_{2}| y_{1},\dotsc, y_{n})= \sum_{i=1}^{n} y_{i} \log
       \mu_{i}- \log y_{i}!- \mu_{i}= \sum_{i=1}^{n}
       y_{i}(\beta_{1}+\beta_{2}x_{i}) -\log y_{i}!
       -\exp{\beta_{1}+\beta_{2}x_{i}}
  \end{gather*}
  Die partiellen Ableitungen sind:
  \begin{align*}
    \diffp{l(\beta_{1},\beta_{2})}{{\beta_{1}}} &= \sum_{i=1}^{n} (y_{i}-
       \exp{\beta_{1}+\beta_{2}x_{i}})=0)\\
    \diffp{l(\beta_{1},\beta_{2})}{{\beta_{2}}} &= \sum_{i=1}^{n} (y_{i}x_{i}
       -\exp{\beta_{1}+\beta_{2}x_{i}}\cdot x_{i})=0
  \end{align*}
  Dies ist ein nichtlineares Gleichungssystem in $\beta_{1}, \beta_{2}$. Zur
  Lösung müssen wir laos numerische Lösungsverfahren des Gleichungssystems
  anwenden.
\end{bsp}

\begin{bsp}[Schwefeldioxidbelastung der Luft in amerikanischen Städten]
  Das $y_{i}$ stellt den Schwefeldioxidgehalt der Luft dar. Hier gibt es
  mehrere Einflussgrößen: $x_{i2}$ ist die mittlere Jahrestemperatur, $x_{i3}$
  ist die Anzahl der Betriebe mit mehr als 20~Beschäftigten, $x_{i4}$ die
  Einwohnerzahl, $x_{i5}$ mittlerer Jahresniederschlag, $x_{i6}$ mittlere
  Windgeschwindigkeit, $x_{i7}$ mittlere Anzahl der Regentage.

  Wenn wir ein Modell dafür bauen wollen, so soll es nicht zu kompliziert
  sein. Für den deterministischen Teil sagen wir, $y_{i}$ eine Realisierung
  einer Zufallsgröße $Y_{i}$ und $\EE Y_{i}= \beta_{1}+\beta_{2}x_{i2}+\dotsb+
  \beta_{7}x_{i7}=\mu_{i}$. Für den stochastischen Teil nehmen wir an, dass
  $Y_{i}\sim N(\mu_{i},\sigma^{2})$. Dies ist ein
  \highl[Regressionsmodell!multiples]{multiples lineares Regressionsmodell}. 
\end{bsp}

\begin{bsp}[Geburtsgewicht von Säuglingen]
  Das $x_{i}$ ist die Dauer der Schwangerschaft und $y_{i}$ ist das
  Geburtsgewicht. Letzteres fassen wir als Realisierung einer Zufallsgröße
  $Y_{i}$ auf. Im deterministischen Teil ist
  \begin{gather*}
    \EE Y_{i}= \mu_{i}=
       \begin{cases}
	 \beta_{11}+\beta_{21}x_{i} & \boy\\
	 \beta_{12}+\beta_{22}x_{i} & \girl
       \end{cases}
  \end{gather*}
  Eine interessante Hypothese wäre, dass die beiden Parameter
  $\beta_{21}=\beta_{22}$ gegen die Alternativhypothese
  $\beta_{21}\neq\beta_{22}$.

  Man kann ein gemeinsames Modell für Mädchen und Jungen mit Hilfe von
  \highl{Indikatorvariablen} (oder \highl{Dummyvariablen}). Es ist:
  \begin{gather*}
    \mathbb{I}_{i}=
       \begin{cases}
	 1 & \girl\\
	 0 & \boy
       \end{cases}\\
    \EE Y_{i}= \mu_{i}= \beta_{1}+ \beta_{2}\mathbb{I}_{i}+ \beta_{3}x_{i}+
       \beta_{4}x_{i}\mathbb{I}_{i}= \beta_{1}+\beta_{2}x_{i2}+
       \beta_{3}x_{i3}+ \beta_{4}x_{i4}
  \end{gather*}
  Dies ist ein multiples lineares Regressionsmodell mit sehr speziellen
  Einflussgrößen. Es ist
  \begin{gather*}
    \EE Y_{i}=
       \begin{cases}
	 \beta_{1}+\beta_{3}x_{i}& \boy\\
	 (\beta_{1}+\beta_{2})+ (\beta_{3}+\beta_{4})x_{i} & \girl
       \end{cases}
  \end{gather*}
  Die ursprüngliche Hypothese entspricht $H_{0}\colon\beta_{4}=0$.

  Für den stochastischen nehmen wir wieder an, dass $Y_{i}\sim
  N(\mu_{i},\sigma^{2})$. Aus historischen Gründen wird hier von einem
  \highl{Kovarianz-Analyse-Modell} oder \highl{Kovarianzanalyse} gesprochen.
\end{bsp}

\begin{bsp}[Beregnungsexperiment]\label{bsp:5}
  Die Einflussgröße sind vier unterschiedliche Beregnungsverfahren. Die
  Zielgröße ist der Ertrag einer Kartoffelsorte. Beim $Y_{ij}$ steht das $i$
  für ein Beregnungsverfahren und das $j$ für eine Parzelle. Es ist $\EE
  Y_{ij}= \mu_{i}$. Der Erwartungswert des Ertrags hängt vom
  Beregnungsverfahren ab. Der stochastische Teil ist $Y_{ij}\sim
  N(\mu_{i},\sigma^{2})$. Wir verwenden wieder Dummyvariablen
  \begin{gather*}
    \mathbb{I}_{ij2}=
       \begin{cases}
	 1 & i=2\\
	 0 &
       \end{cases}\\
       \mathbb{I}_{ij3}=
       \begin{cases}
	 1 & i=3\\
	 0
       \end{cases}\\
    \mathbb{I}_{ij4}=
       \begin{cases}
	 1& i=4\\
	 0
       \end{cases}\\
    \EE Y_{ij}= \beta_{1}+\beta_{2}\mathbb{I}_{ij2}+
       \beta_{3}\mathbb{I}_{ij3}+ \beta_{4}\mathbb{I}_{ij4}\\
    =
       \begin{cases}
	 \beta_{1} & i=1\\
	 \beta_{1}+\beta_{2} & i=2\\
	 \beta_{1}+\beta_{3} &i=3\\
	 \beta_{1}+\beta_{4}& i=4
       \end{cases}
  \end{gather*}
  Interessante Hypothese: $H_{0}\colon \mu_{1}=\mu_{2}=\mu_{3}=\mu_{4}$ bzw.
  $H_{0}\colon \beta_{2}=\beta_{3}=\beta_{4}$. Bei nur qualitativen
  Einflussgrößen spricht man von einer \highl{Varianzanalyse}.
\end{bsp}

% Vorlesung vom 22.10.2012
\begin{bsp}
  Wir haben nun zwei Einflussgrößen:
  \begin{itemize}
   \item Beregnungsverfahren
   \item Sorte
  \end{itemize}
  Die Zielgröße ist der Ertrag. Es ist naheliegend, dass man bei beobachteten
  Daten eine Mehrfachindizierung verwendet: $Y_{ijk}$. Dabei steht das $i$ für
  das Beregnungsverfahren, das $j$ für die Sorte und $k$ für die Wiederholung.
  Es sind $i=1,\dotsc,4$, $j=1,2$ und $k=1,\dotsc,n_{ij}$. Ds $y_{ijk}$ ist
  eine Realisierung einer Zufallsgröße $Y_{ijk}$. Der Erwartungswert $\EE
  Y_{ijk}= \mu_{ij}$ und es ist $Y_{ijk}\sim N(\mu_{ij}, \sigma^{2})$. Unser
  Ziel ist es, eine lineare Funktion von Einflussgrößen hinzuschreiben. Dazu
  müssen wir das \autoref{bsp:5} erweitern und eine zusätzliche Dummyvariable
  einführen.
  \begin{gather*}
    \mathbb{I}_{ijk,2}=
       \begin{cases}
	 1& \text{Beregnungsverfahren 2}\\
	 0
       \end{cases}\\
    \mathbb{I}_{ijk,l}=\delta_{il} \qquad l=2,3,4\\
    \mathbb{I}_{ijk},5=
       \begin{cases}
	 1& j=2\\
	 0
       \end{cases}
  \end{gather*}
  Im Modell haben wir $\mu_{ij}= \EE Y_{ijk}= \beta_{1}+
  \beta_{2}\mathbb{I}_{ijk,2}+ \beta_{3}\mathbb{I}_{ijk,3}+
  \beta_{4}\mathbb{I}_{ijk,4}+ \beta_{5}\mathbb{I}_{ijk,5}$. Aufgesplittet
  ergibt das:
  \begin{gather*}
    \mu_{ij}=
       \begin{cases}
	 \beta_{1}& b=1, s=1\\
	 \beta_{1}+\beta_{2} & b=2,s=1\\
	 \beta_{1}+\beta_{3} & b=3,s=1\\
	 \beta_{1}+\beta_{4} & b=4, s=1\\
	 \beta_{1}+ \beta_{5} & b=1, s=2\\
	 \beta_{1}+\beta_{2}+\beta_{5}& b=2,s=2\\
	 \beta_{1}+\beta_{3}+\beta_{5} & b=3,s=2\\
	 \beta_{1}+\beta_{4}+\beta_{5} & b=4,s=2
       \end{cases}
  \end{gather*}

  Wir können aber auch ein Modell mit Wechselwirkungen betrachten, d.\,h. der
  Effekt der Beregnungsverfahren ist nicht bei allen Sorten gleich. Formell
  multiplizieren wir die Indikatorvariablen für die Beregnung und Sorte. Das
  ergibt $\mu_{ij}= \beta_{1}+
  \beta_{2}\mathbb{I}_{ijk,2}+ \beta_{3}\mathbb{I}_{ijk,3}+
  \beta_{4}\mathbb{I}_{ijk,4}+ \beta_{5}\mathbb{I}_{ijk,5}+
  \beta_{6}\underbrace{\mathbb{I}_{ijk,2}\mathbb{I}_{ijk,5}}_{x_{ijk,6}}+
  \beta_{7}\underbrace{\mathbb{I}_{ijk,3}\mathbb{I}_{ijk,5}}_{x_{ijk,7}}+
  \beta_{8}\underbrace{\mathbb{I}_{ijk,4}\mathbb{I}_{ijk,5}}_{x_{ijk,8}}$.
  Eine interessante Hypothese wäre $H_{0}\colon\beta_{6}=\beta_{7}=\beta_{8}$
  (\emph{keine} Wechselwirkung). Allgemeine Anmerkungen zur Parameterzahl: $1+
  (a-1)+(b-1)+(a-1)(b-1)= 1+a-1+b-1+ab-a-b+1= ab$.

  Das Modell wird als \highl[Varianzanalyse!zweifache]{zweifache
  Varianzanalyse} mit oder ohne Wechselwirkung bezeichnet.
\end{bsp}

\begin{bsp}[Vorkommen der blauflügligen Ödlandschrecke]
  Die Einflussgröße ist der Offenbodenanteil. Die Zielgröße ist entweder $0$
  oder $1$, also Vorkommen bzw. Nichtvorkommen. Im stochastischen Teil ist die
  Normalverteilung nicht so sinnvoll. Die einzige Verteilung, die wir kennen,
  ist die Bernoulliverteilung. Diese können wir als Spezialfall der
  Binomialverteilung interpretieren. In dieser Form schreiben wir das auf:
  $Y_{i}\sim\text{Bin}(1,p_{i}=\mu_{i})$. Im deterministischen Teil geht es
  darum, diesen Erwartungswert der Zielgröße zu modellieren: $\EE Y_{i}=
  p_{i}=\mu_{i}= \beta_{1}+\beta_{2}x_{i}$. Der Ansatz ist nicht unbedingt
  sinnvoll, da man in Bereiche kommt, die kleiner als $0$ oder größer als $1$
  sind. Dies ist für relative Häufigkeiten nicht sinnvoll. Um dies zu beheben,
  fügen wir die \highl{Responsefunktion} $h$ ein. An die Funktion~$h$ stellen
  folgende Forderungen:
  \begin{itemize}
   \item Wertebereich $(0,1)$
   \item stetig differenzierbar
   \item bijektiv
   \item streng monoton steigend
  \end{itemize}
  Diese Forderungen werden von einer Verteilungsfunktion einer stetigen
  Zufallsgröße erfüllt. Hier bietet sich an, Verteilungsfunktion der
  Standardnormalverteilung zu benutzen: $h(x)=\Phi(x)$. Also haben wir $\EE
  Y_{i}= \Phi(\beta_{1}+\beta_{2}x_{i})$. Dieses Modell heißt
  \highl{Probitmodell}.

  Ein anderer Ansatz ist $h(x)=\frac{1}{1+e^{-x}}$. Diese Funktion ist die
  Verteilungsfunktion der logistischen Verteilung. Wir erhalten dann $\EE
  Y_{i}= \frac{1}{1+\exp(-(\beta_{1}+\beta_{2}x_{i}))}$, das
  \highl{Logitmodell}.
\end{bsp}

Gemeinsamkeiten aller Beispiele:
\begin{itemize}
 \item Linearkombination von Einflussgrößen. Wir werden dies
  \highl[Prädiktor!linearen]{linearen Prädiktor} nennen.
 \item Verteilungsannahme aus einer »schönen« Klasse von
  Wahrscheinlichkeitsverteilungen. Diese heißen
  \highl{Exponential-Dispersionsfamilien}.
 \item linearer oder nichtlinearer Zusammenhang zwischen linearem Prädiktor
  und dem Erwartungswert der Zielgröße. Dies nennen wir später Responsefunktion.
 \item eine Zielgröße
\end{itemize}
\begin{gather*}
  \EE Y_{i}= h(\beta_{1}+\beta_{2}x_{i2}+\dotsb+ \beta_{k}x_{ik})=
     h(\underline{\beta}^{T}\underline{x})
\end{gather*}

Das nennt sich \highl[Modell!verallgemeinertes lineares]{verallgemeinerte
lineare Modelle}.

\chapter{Exponentialfamilien}

Das Ziel ist es, eine flexible Klasse von Wahrscheinlichkeitsverteilungen zu
haben. Diese sollen einheitlich theoretisch behandelt werden.

\begin{table}[htb]
  \centering
  \begin{tabular}{l|l}
    Normalverteilung $N(\mu,\sigma^{2})$ &
    Poissonverteilung$\text{Poisson}(\mu)$\\
    Gleichverteilung auf $[0,\theta]$ &  diskrete Verteilung auf
    $\{0,\dotsc,k\}$\\
    Exponentialverteilung $\text{Exp}(\lambda)$ &  Binomialverteilung
    $\text{Bin}(n,p)$\\
    $\chi^{2}$"~Verteilung $\chi^{2}(n)$ &  geometrische Verteilung
    $\text{geom}(p)$\\
    Gammaverteilung & hypergeometrische Verteilung
    $\text{hypergeo}(N,M,n)$\\
    Student"=Verteilung ($t$"~Verteilung)\\
    logistische Verteilung\\
    Fishersche $F$"~Verteilung
  \end{tabular}
  \caption{Verschiedene Verteilungen}
  \label{tab:verteilungen}
\end{table}

\begin{defini}[Natürliche oder lineare Exponentialfamilie]
  Eine Zufallsgröße~$Y$ besitzt eine Verteilung aus einer
  \highl[Exponentialfamilie!natürliche]{natürlichen Exponentialfamilie}, falls
  sich ihre Dichte bzw. Wahrscheinlichkeitsfunktion in der Gestalt $f_{Y}(y)=
  c(y) \exp(\theta y-A(\theta))$ für $\theta\in\Omega\subseteq\R$ darstellen
  lässt. Der Parameter~$\theta$ heißt
  \highl[Parameter!natürlicher]{natürlicher Parameter} oder
  \highl[Parameter!kanonischer]{kanonischer Parameter}.
\end{defini}

\begin{bemerk}
  Die Parametrisierung ist nicht eindeutig und die Menge
  $\Omega^{\ast}=\Set{\theta\in\R | \int_{-\infty}^{\infty} c(y)\exp(\theta
  y)\,dy<\infty}$ heißt \highl[Parameterraum!natürlicher]{natürlicher
  Parameterraum}.
\end{bemerk}

\begin{defini}
  Ein Zufallsvektor $\underline{Y}= (Y_{1},\dotsc, Y_{n})$ besitzt eine
  Verteilung aus einer linearen Exponentialfamilie mit dem natürlichen
  Parameter $\underline{\theta}=(\theta_{1},\dotsc,\theta_{n})$, falls die
  Dichte die Gestalt $f_{Y}(y)= c(y)
  \exp(\underline{\theta}^{T}\underline{y}-A(\underline{\theta}))$ besitzt.
\end{defini}

Wir gehen aus von $Y_{1},\dotsc, Y_{n}$. Das sind unabhängige Zufallsgrößen
aus Exponentialfamilie mit Parameter~$\theta_{1},\dotsc,\theta_{n}$. Wir
betrachten die gemeinsame Dichte, also die Dichtefunktion von $Y_{1},\dotsc,
Y_{n}$: $f_{Y_{1},\dotsc, Y_{n}}(y_{1},\dotsc, y_{n})= \prod_{i=1}^{n}
f_{Y_{i}}(y_{i})= \prod_{i=1}^{n} c(y_{i})
\exp(\theta_{i}y_{i}-A(\theta_{i}))= \prod_{i=1}^{n} c(y_{i})
\exp(\sum_{i=1}^{n} \theta_{i}y_{i}- \sum_{i=1}^{n} A(\theta_{i}))=
c^{*}(\underline{y}) \exp(\underline{\theta}^{T}\underline{y}-
A^{*}(\underline{\theta}))$.

\begin{bsp}[Poissonverteilung]
  Die Poissonverteilung hat die Wahrscheinlichkeitsfunktion $f_{Y}(y)=
  \frac{\mu^{y}}{y!} e^{-\mu}= \frac{1}{y!}\exp(y\log\mu-\mu)$ mit
  $y=0,1,2,\dotsc$. Dabei ist $\log\mu=\theta$ der natürliche Parameter und
  $A(\theta)= \mu=\exp(\theta)$.
\end{bsp}

\begin{bsp}[Exponentialverteilung]
  $f_{Y}(y)= \lambda e^{-\lambda y}\mathbb{I}_{[0,\infty)}(y)=
  \mathbb{I}_{[0,\infty)}(y) \exp(-\lambda y+\log\lambda)$. Hier ist $c(y)=
  \mathbb{I}_{[0,\infty)}(y)$, der natürliche Parameter ist $\theta=-\lambda$
  und $A(\theta)= -\log\lambda= -\log(-\theta)$.

  Die Exponentialverteilung ist eine spezielle Exponentialfamilie.
\end{bsp}

%Vorlesung vom 23.10.2012
\begin{bsp}[Gleichverteilung]
  Wir reden von der stetigen Gleichverteilung, d.\,h. $Y$ ist gleichverteilt
  auf dem Intervall $(0,\theta)$. Die Dichtefunktion hat die Gestalt:
  $f_{Y}(y)= \frac{1}{\theta}\mathbb{I}_{[0,\theta]}(y)$. Das $y$ und $\theta$
  sind untrennbar verbunden. Damit haben wir keine Chance eine Darstellung in
  der Exponentialform hinzukriegen.

  Allgemein lässt sich sagen, falls der Wertebereich (Träger) einer Verteilung
  vom Parameter~$\theta$ abhängt, dann liegt keine Exponentialfamilie vor.
\end{bsp}

\section{Konstruktion von Exponentialfamilien}

Wir betrachten Zufallsgröße~$Y$ mit der Dichtefunktion $h(y)$. Es existiere
der $\EE e^{\theta y}= \int_{-\infty}^{\infty} e^{\theta y}h(y)\,dy=
m(\theta)$. Die Funktion $m(\theta)$ heißt
\highl[Funktion!momentenerzeugende]{momentenerzeugende
  Funktion}\index{momentenerzeugend} der Zufallsgröße~$Y$.

\begin{defini}[Moment]
  Das $k$"~te \highl{Moment} einer Zufallsgröße ist $\EE Y^{k}=
  \int_{-\infty}^{\infty} y^{k}h(y)\,dy$. Das $k$"~te zentrale Moment ist
  $\EE(Y-\EE Y)^{k}= \int_{-\infty}^{\infty} (y-\EE Y)^{k} h(y)\,dy$. 
\end{defini}

\begin{satz}[Eigenschaften der momentenerzeugenden Funktion]
  Es sei $Y$ eine Zufallsgröße mit momentenerzeugender Funktion $m(\theta)$.
  Existiert ein $\theta_{0}>0$ derart, dass $m(\theta_{0})<\infty$ und
  $m(-\theta_{0})<\infty$, dann besitzt $m(\theta)$ eine Taylorentwicklung um
  $\theta=0$. Genauer gilt:
  \begin{gather*}
    m(\theta)=\sum_{k=0}^{\infty} \frac{\theta^{k}}{k!} \EE
       Y^{k}\qquad\abs{\theta}\leq\theta_{0}
  \end{gather*}
  Insbesondere folgt die Existenz sämtlicher Momente und
  \begin{gather*}
    \EE Y^{k}= m^{(k)}(0)\qquad\forall k\geq0
  \end{gather*}
\end{satz}

Aus $m(\theta)=\int_{-\infty}^{\infty} e^{\theta y}h(y)\,dy$ bekommen wir
$1=\int_{-\infty}^{\infty} h(y)e^{\theta y}\frac{1}{m(\theta)}\,dy=
\int_{-\infty}^{\infty} h(y) \exp(\theta y- \log m(\theta))\,dy$. Diese
Exponentialfamilie heißt \highl[Exponentialfamilie!konjugierte]{konjugierte
Exponentialfamilie}.

\section{Momente einer natürlichen Exponentialfamilie}

Wir haben eine Zufallsgröße $Y$ mit einer momentenerzeugenden Funktion
$m(\theta)$ und $Z$ aus der konjugierten Exponentialfamilie mit der Dichte
$f_{Z}(z)= h(z)\exp(\theta z-A(\theta))$. Die momentenerzeugende Funktion von
$Z$ ist $m_{Z}(t)= \EE e^{tZ}= \int_{-\infty}^{\infty} e^{tz}h(z)\exp(\theta
z- A(\theta))\,dz= \exp(- A(\theta))\underbrace{\int_{-\infty}^{\infty}
h(z)\exp((t+\theta)z)}_{\EE e^{(t+\theta)z}= m_{y}(t+\theta)}\,dz=
\frac{m_{y}(t+\theta)}{m_{y}(\theta)}$.

$\EE_{\theta}Z=\diff{}{t} \frac{m(t+\theta)}{m(\theta)}|_{t=0}=\diff{}{t}
\frac{e^{A(t+\theta)}}{e^{A(\theta)}}|_{t=0}= \frac{e^{A(t+\theta)
A'(t+\theta)}}{e^{A(\theta)}}|_{t=0}$

Das zweite Moment ist $\EE_{\theta}Z^{2}=
\frac{e^{A(t+\theta)}(A'(t+\theta))^{2} +e^{A(t+\theta)}
  A''(t+\theta)}{e^{A(\theta)}}= A'(\theta)^{2}+A''(\theta)$.

Die Varianz ist $\EE_{\theta}(Z-\EE_{\theta}Z)^{2}=
\EE_{\theta}Z^{2}-(\EE_{\theta}Z)^{2}= A''(\theta)$. Die
Normierungskonstante $A(\theta)$ liefert auf einfache Weise Erwartungswert und
Varianz der Exponentialfamilie. Wir bezeichnen den Erwartungswert mit
$\mu=\mu(\theta)= \EE_{\theta}Z=A'(\theta)$ und die Varianz mit
$\sigma^{2}=\sigma^{2}(\theta)= \EE_{\theta}(Z-\EE_{\theta}Z)^{2}= A''(\theta)$.

Weiterhin ist die Varianz von $Z$ größer als $0$. Damit folgt, dass die zweite
Ableitung $A''(\theta)>0$ und dass $A(\theta)$ eine konvexe Funktion ist.
Daneben ist $\mu(\theta)= A'(\theta)=\tau(\theta)$ eine streng monoton
wachsende stetige Funktion. Also ist $\tau$ eine bijektive Abbildung des
Parameterraums~$\Omega$ in eine Teilmenge $\CM\subseteq\R$ (mean value space)
mit $\tau\colon\Theta\rightarrow\R$. Die Exponentialfamilie kann durch den
Erwartungswert parametrisiert werden: $\theta=\tau^{-1}(\mu)$. Insbesondere
kann die Varianz als Funktion des Erwartungswertes dargestellt werden
(\autoref{def:1-varfunkt}).

\begin{defini}[Varianzfunktion]\label{def:1-varfunkt}
  Die Funktion
  \begin{gather*}
    v(\mu)= A''(\tau^{-1}(\mu))
  \end{gather*}
  heißt \highl{Varianzfunktion} der Exponentialfamilie.
\end{defini}

Linkfunktionen im verallgemeinerten linearen Modell:
\begin{gather*}
  g(\EE Y_{i})= g(\beta_{1}+\beta_{2}x_{i2}+\dotsb+ \beta_{k}x_{ik})
\end{gather*}
Wir wählen $g(\mu)= \tau^{-1}(\mu)=
\tau^{-1}(\tau(\theta))=\theta=\underline{x}^{T}\underline{\beta}$, d.\,h. für
diese Wahl von $g$ ist der natürliche Parameter $\theta$ der
Exponentialfamilie eine Linearkombination der unbekannten Parameter. Man
bezeichnet $g(\mu)=\tau^{-1}(\mu)$ als \highl[Linkfunktion!kanonische]{kanonische Linkfunktion} oder
\highl[Linkfunktion!natürliche]{natürliche Linkfunktion}.

\begin{bsp}
  Exponentialverteilung $Y\sim\text{Exp}(\lambda)$ und $f_{Y}(y)=
  \mathbb{I}_{[0,\infty)}(y) \exp(-\lambda y+\log\lambda)= c(y)\exp(\theta
  y-(-\log(-\theta)))$. Die erste Ableitung ist $A'(\theta)=
  -\frac{1}{-\theta}\cdot(-1)=\frac{-1}{\theta}=\EE_{\theta} Y= \mu(\theta)$.
  Die zweite Ableitung ist $A''(\theta)= \frac{1}{\theta^{2}}$. Die
  Umkehrfunktion ist $\theta=\tau^{-1}(\mu)= -\frac{1}{\mu}$ und $v(\mu)=
  A''(\tau^{-1}(\mu))= \mu^{2}$.
\end{bsp}

\section{Exponential-Dispersionsfamilien}

\begin{bsp}[Normalverteilung mit bekanntem $\sigma^{2}$]
  $Y\sim N(\mu,\sigma^{2})$ und $f_{Y}(y)= \frac{1}{\sqrt{2\pi\sigma^{2}}}
  \exp(-\frac{(y-\mu)^{2}}{2\sigma^{2}})= \frac{1}{\sqrt{2\pi\sigma^{2}}}
  \exp(-\frac{y^{2}}{2\sigma^{2}} +\frac{y\mu}{2\sigma^{2}}-
  \frac{\mu^{2}}{2\sigma^{2}})= \frac{1}{\sqrt{2\pi\sigma^{2}}}
  \exp(-\frac{y^{2}}{2\sigma^{2}}) \exp(y\frac{\mu}{\sigma^{2}}-
  \frac{1}{2}\frac{\mu^{2}}{\sigma^{2}})$.

  Die erste Ableitung ist $A'(\theta)= \theta\sigma^{2}=
  \frac{\mu}{\sigma^{2}}=\sigma^{2}=\mu$ und die zweite
  $A''(\theta)=\sigma^{2}$. Für die Varianzfunktion haben wir
  $v(\mu)=\sigma^{2}$. Diese ist unabhängig von $\mu$.
\end{bsp}

Wir benötigen eine Verallgemeinerung der Exponentialfamilie, um einen
flexiblen Zusammenhang zwischen Erwartungswert und Varianz zu ermöglichen:
$\text{Var}(Y)= \phi\cdot v(\mu)$. Dies kann man durch eine entsprechende
Skalierung hinbekommen.
Dabei sei $Y$ eine Zufallsgröße mit der Dichte $f_{Y}(y)= c(y)\exp(\theta y-
A^{*}(\theta))$ und wir betrachten die skalierte Zufallsgröße $Z=\phi Y$ mit
$\phi\in(0,\infty)$. Wir rechnen die Dichtetransformation aus $P(Z\in B)=
P(\phi Y\in B)= P(Y\in\frac{B}{\phi})= \int_{\nicefrac{B}{\phi}} c(y)
\exp(\theta y- A^{*}(\theta))\,dy$. Wir ersetzen das $y$ durch
$\frac{z}{\phi}$ und erhalten $\int_{B} c(\frac{z}{\phi})\exp(\frac{\theta
z}{\phi}- A^{*}(\theta))\frac{1}{\theta}\,dz= \int_{B}
\underbrace{\frac{1}{\phi}c(\frac{z}{\phi})}_{c(z,\phi)} \underbrace{\exp(\frac{\theta z-\phi
A^{*}(\theta)}{\phi})}_{\phi A^{*}(\theta)=A(\theta)}\,dz$. Die Dichte von $Z$
ist $f_{Z}(z)= c(z,\phi)\exp(\frac{\theta z-A(\theta)}{\phi})$.

\begin{defini}
  Eine Zufallsgröße $Z$ besitzt eine Verteilung aus einer
  Exponential"=Dispersionsfamilie, falls die Dichtefunktion die Gestalt
  $f_{Z}(z)= c(\frac{z}{\phi}) \exp(\frac{\theta z-A(\theta)}{\phi})$ besitzt.
\end{defini}

\begin{bemerk}[Momente]
  Die momentenerzeugende Funktion ist $m(t)= \EE e^{tY}=
  \int_{-\infty}^{\infty} e^{ty} c(y,\phi) \exp(\frac{\theta
  y-A(\theta)}{\phi})\,dy= \int_{-\infty}^{\infty} c(y,\phi)
  \exp(\frac{y(\theta+\phi t)-A(\theta+\phi t)+A(\theta+\phi
  t)-A(\theta)}{\phi})\,dy= \exp(\frac{A(\theta+\phi t)-A(\theta)}{\phi})
  \int_{-\infty}^{\infty} c(y,\phi) \exp(\frac{y(\theta+\phi t)-A(\theta+\phi
  t)}{\phi})\,dy$, also Integral ist die Dichtefunktion einer
  Exponential"=Dispersionsfamilie mit Parameter $\theta+\phi t$. Die erste
  Ableitung ist $m'(t)= \exp(\frac{A(\theta+\phi
  t)-A(\theta)}{\phi})\cdot\frac{A'(cmq+\phi t)}{\phi}\phi$ und
  $m'(0)=A'(\theta)=\mu$. Die zweite Ableitung ist $m''(t)=
  \exp(\frac{A(\theta+\phi t)-A(\theta)}{\phi})\cdot(A'(\theta+\phi t))^{2}+
  \exp(\frac{A(\theta+\phi t)-A(\theta)}{\phi})\cdot A''(\theta+\phi t)\phi$
  und $m''(0)= A'(\theta)^{2}+A''(\theta)\phi$. Schließlich ist die Varianz
  von $Y$ gleich $\phi A''(\theta)=\phi v(\mu)$.
\end{bemerk}

\clearpage
% \appendix
% \begin{thebibliography}{99}
%  \bibitem{shmathguide} Short Math Guide for \LaTeX{},\\
%   \url{ftp://ftp.ams.org/pub/tex/doc/amsmath/short-math-guide.pdf}
% \end{thebibliography}

\clearpage
\pdfbookmark[0]{Index}{index}
% Behebt das Problem der vielen "`overfull \hbox"' im Index
% <news:3419172.ueajl5DJLB@mjk.komascript.de>
\setlength{\parfillskip}{0pt plus 1fil}
\printindex

\end{document}
