% Einige zusätzliche Informationen für rubber
%  rubber erkennt nicht, dass die Datei weg kann, daher sagen wir es ihm
% rubber: clean $base.thm
%  rubber soll nach Änderungen an der Datei nochmal bauen
% rubber: watch $base.thm

\RequirePackage[l2tabu,orthodox]{nag}  % nag überprüft den Text auf veraltete
                   % Befehle oder solche, die man nicht in LaTeX verwenden
                   % soll -- l2tabu-Checker in LaTeX

\documentclass[ngerman,draft,parskip=half,twoside]{scrartcl}

\usepackage{xcolor}
\usepackage[draft=false,colorlinks,bookmarksnumbered,linkcolor=blue,breaklinks]{hyperref}

\usepackage[utf8]{inputenc}
\usepackage{babel}

\usepackage[T1]{fontenc}        % T1-Schriften notwendig für PDFs
\usepackage{lmodern}		% Latin Modern
\usepackage{textcomp}           % wird benötigt, damit der \textbullet
                                % für itemize in lmodern gefunden wird.

\usepackage[intlimits]{amsmath}
\usepackage[all,warning]{onlyamsmath}  % warnt bei Verwendung von nicht
                                       % amsmath-Umgebungen z.\,B. $$...$$
\usepackage{amssymb}     % wird für \R, \C,... gebraucht
\usepackage{fixmath}     % ISO-konforme griech. Buchstaben
\usepackage[euro]{isonums} % definiert Komma als Dezimaltrennzeichen

\usepackage[amsmath,thmmarks,hyperref]{ntheorem} % für die Theorem-Umgebungen
                                                 % (satz, defini, bemerk)
\usepackage{svn}         % Zum Auswerten und ordentlichen Darstellen der
                         % SVN-Schlüsselwörter (s. vor \begin{document})
                         % dafür muss in SVN noch das Flag svn:keywords
                         % auf "LastChangedRevision LastChangedDate"
                         % gesetzt werden
\usepackage{ellipsis}    % Korrektur für \dots
\usepackage{fixltx2e}
\usepackage[final,babel]{microtype} % Verbesserung der Typographie
\usepackage[babel,german=guillemets]{csquotes} % für Anführungszeichen
\usepackage{enumitem}

% Damit auch die Zeichen im Mathemode in Überschriften fett sind
% <news:lzfyyvx3pt.fsf@tfkp12.physik.uni-erlangen.de>
\addtokomafont{sectioning}{\boldmath}

\newtheorem{thm}{Satz}[section]

% Hier die Definition, wie \autoref die Umgebungen nennen soll, die mit
% \newtheorem definiert wurden
\newcommand*{\thmautorefname}{Satz}
% Zwischen Unter- und Unterunterabschnitten sollte nicht unterschieden
% werden.
\renewcommand*{\subsectionautorefname}{Abschnitt}
\renewcommand*{\subsubsectionautorefname}{Abschnitt}

\pagestyle{headings}

\newcommand*{\R}{\mathbb{R}}      % reelle Zahlen
\newcommand*{\C}{\mathbb{C}}      % komplexe Zahlen
\newcommand*{\N}{\mathbb{N}}      % natürliche Zahlen
\newcommand*{\Q}{\mathbb{Q}}      % gebrochene Zahlen
\newcommand*{\Z}{\mathbb{Z}}      % ganze Zahlen

\newcommand*{\Algeb}{\mathcal{A}}   % Algebra
\newcommand*{\BorelM}{\mathcal{B}}  % Borelmenge
\newcommand*{\PotM}{\mathcal{P}}    % Potenzmenge
\newcommand*{\E}{\mathbb{E}}        % Erwartungswert
\newcommand*{\V}{\mathbb{V}}        % Varianz
\newcommand*{\WKM}{\mathbb{P}}      % Wahrscheinlichkeitsmaß

\SVN $LastChangedRevision$
\SVN $LastChangedDate$

\setlist[enumerate,1]{label=\arabic*.}
\setlist[enumerate,2]{label=\theenumi\arabic*}

\begin{document}

\title{Eine Auswahl wichtiger Definitionen und Aussagen
 zur Vorlesung
  \enquote{Stochastik f"ur Informatiker und Regelschullehrer}, WS 08/09}

\author{Werner Linde}
\maketitle

\section{Wahrscheinlichkeiten}
\subsection{Wahrscheinlichkeitsr"aume}

\subsubsection{Grundraum}
Der \textbf{Grundraum} (meist mit $\Omega$ bezeichnet) ist eine Menge,
die mindestens alle bei einem stochastischen Versuch oder Vorgang
auftretenden Ergebnisse enth"alt. Die Teilmengen von $\Omega$ hei"sen \textbf{Ereignisse},
die einpunktigen Teilmengen nennt man \textbf{Elementarereignisse}.

\subsubsection{Eintreten eines Ereignisses}
Ein Ereignis $A\subseteq \Omega$ \textbf{tritt ein}, wenn das beim Versuch oder dem Vorgang
beobachtete zuf"allige Ergebnis in der Menge
$A$ liegt.

\subsubsection{$\sigma$-Algebra}
Auf dem Grundraum $\Omega$ wird ein System $\Algeb\subseteq \PotM(\Omega)$  von Ereignissen
ausgezeichnet, denen man in sinnvoller Weise die
Wahrscheinlichkeit ihres Eintretens zuordnen kann. Aus naheliegenden Gr"unden fordert man,
dass $\Algeb$ eine $\mathbf \sigma$-\textbf{Algebra} bildet, d.\,h.~$\Algeb$ erf"ullt
folgende Eigenschaften:
\begin{eqnarray*}
&(i)&\quad\emptyset\in\Algeb\;.\\
&(ii)&\quad\mbox{Aus}\quad A\in\Algeb\quad\mbox{folgt}\quad A^c\in \Algeb\;.\\
&(iii)&\quad
A_1,A_2,\ldots\in \Algeb \quad \mbox{impliziert}\quad \bigcup_{j=1}^\infty A_j\in\Algeb\;.
\end{eqnarray*}
Ist $\Omega$ h"ochstens abz"ahlbar unendlich, so kann man als $\sigma$-Algebra stets die
Potenzmenge $\PotM(\Omega)$ von $\Omega$ nehmen.
\subsubsection{Wahrscheinlichkeitsma"s}
Ein \textbf{Wahrscheinlichkeitsma"s} (oder eine \textbf{Wahrscheinlichkeitsverteilung})
 $\WKM$ ist eine Abbildung von
$\Algeb$ nach $[0,1]$, die jedem Ereignis $A\in \Algeb$ die Wahrscheinlichkeit seines
Eintretens zuordnet und folgende Eigenschaften besitzt:
\begin{eqnarray*}
&(i)&\quad\mbox{Es gilt}\quad \WKM(\emptyset)=0\quad\mbox{und}\quad \WKM(\Omega)=1\;.\\
&(ii)&\quad
\WKM\;\mbox{ist $\sigma$-additiv, d.\,h.~f"ur disjunkte}\;A_j\in\Algeb\;\mbox{folgt}\;
\WKM\Big(\bigcup_{j=1}^\infty A_j\Big)=\sum_{j=1}^\infty\WKM(A_j)\;.
\end{eqnarray*}
\subsubsection{Wahrscheinlichkeitsraum}
Das Tripel $(\Omega,\Algeb,\WKM)$ hei"st \textbf{Wahrscheinlichkeitsraum}. Zuf"allige
Experimente werden durch geeignete Wahrscheinlichkeitsr"aume  beschrieben.
\subsubsection{Eigenschaften von Wahrscheinlichkeitsma"sen}
%Wahrscheinlichkeitma"se besitzen
%folgende wichtigen Eigenschaften:
\begin{enumerate}
\item[\rm(i)]
Jedes Wahrscheinlichkeitsma"s ist auch \textbf{endlich additiv}, d.\,h.~sind
$A_1,\ldots,A_n$ aus $\Algeb$ disjunkt, so folgt
 \begin{gather*}
   \WKM\Big(\bigcup_{j=1}^n A_j\Big)=\sum_{j=1}^n\WKM(A_j)\;.
 \end{gather*}
\item[\rm(ii)]
Wahrscheinlichkeitsma"se sind \textbf{monoton}, d.\,h.~gilt f"ur $A,B\in\Algeb$ die Inklusion
$A\subseteq B$, so impliziert dies $\WKM(A)\le\WKM(B)$.
\item[\rm(iii)]
F"ur $A,B\in\Algeb$ mit $A\subseteq B$ folgt $\WKM(B\setminus A)=\WKM(B)-\WKM(A)$. Insbesondere ergibt sich hieraus
$\WKM(A^c)=\WKM(\Omega\setminus A)=1-\WKM(A)$ f"ur $A\in\Algeb$.
\item[\rm(iv)]
Wahrscheinlichkeitsma"se sind \textbf{stetig von oben}, d.\,h.~gilt f"ur $A_j\in\Algeb$
die Aussage $A_1\supseteq A_2\supseteq\cdots$,
so folgt
 \begin{gather*}
   \WKM\Big(\bigcap_{j=1}^\infty A_j\Big)=\lim_{j\to\infty}\WKM(A_j)\;.
 \end{gather*}
\item[\rm(v)]
Wahrscheinlichkeitsma"se sind auch \textbf{stetig von unten}, d.\,h.~gilt f"ur $A_j\in\Algeb$
die Aussage $A_1\subseteq A_2\subseteq\cdots$,
so folgt
  \begin{gather*}
    \WKM\Big(\bigcup_{j=1}^\infty A_j\Big)=\lim_{j\to\infty}\WKM(A_j)\;.
  \end{gather*}
\end{enumerate}
\subsection{Typen von Wahrscheinlichkeitsma"sen}
\subsubsection{Wahrscheinlichkeitsma"se auf h"ochstens abz"ahlbar unendlichen Grundr"aumen}
Bei einem Experiment seien h"ochstens abz"ahlbar unendlich viele Versuchsergebnisse
m"oglich. Dann kann man entweder $\Omega=\{\omega_1,\ldots,\omega_N\}$ oder
aber $\Omega=\{\omega_1,\omega_2,\ldots\}$ w"ahlen. Als $\sigma$-Algebra nimmt man in diesen
F"allen stets die Potenzmenge $\PotM(\Omega)$. Setzt man
\begin{equation}
\label{zu}
p_i :=\WKM(\{\omega_i\})\,,\quad 1\le i\le N\quad\mbox{bzw.}\quad i=1,2,\ldots,
\end{equation}
dann erh"alt man Zahlen mit den Eigenschaften
\begin{eqnarray}
\label{p1}
&&p_i\ge 0\quad \mbox{und}\\
\label{p2}
&&\sum_{i=1}^N p_i =1\quad \mbox{bzw.}\quad \sum_{i=1}^\infty p_i =1\;.
\end{eqnarray}
F"ur eine Menge $A\subseteq \Omega$ folgt dann
\begin{equation}
\label{diskret}
\WKM(A):= \sum_{\{i\, : \,\omega_i\in A\}} p_i\;.
\end{equation}
Umgekehrt, gibt man eine Folge $(p_i)_{i\ge 1}$ reeller Zahlen mit \eqref{p1} und \eqref{p2} vor,
so wird durch (\ref{diskret}) ein Wahrscheinlichkeitsma"s $\WKM$ auf $\PotM(\Omega)$ definiert. F"ur
endliche oder abz"ahlbar unendliche Grundr"aume $\Omega$ hat man also folgende "Aquivalenz:
\begin{gather*}
  \{\WKM\,:\,\WKM\;\mbox{Wahrscheinlichkeitsma"s auf}\;\PotM(\Omega)\}
     \Longleftrightarrow\{(p_i)_{i\ge 1}\,:\, (p_i)_{i\ge 1}\;\mbox{erf"ullen (\ref{p1}) und (\ref{p2})}\}
\end{gather*}
Die Zuordnung erfolgt "uber (\ref{zu}) bzw.~(\ref{diskret}).

\subsubsection{Diskrete Wahrscheinlichkeitsma"se}
Sei nunmehr $\Omega$ ein beliebiger Grundraum (nicht notwendig endlich
oder abz"ahlbar unendlich). Ein Wahrscheinlichkeitsma"s $\WKM$ auf $(\Omega,\PotM(\Omega))$
hei"st \textbf{diskret}, wenn es eine h"ochstens abz"ahlbar unendliche Teilmenge $D\subseteq \Omega$
mit $\WKM(D)=1$ gibt. Mit $D=\{\omega_1,\omega_2,\ldots\}$ gilt dann f"ur $A\subseteq \Omega$ wie zuvor
\begin{gather*}
  \WKM(A):= \sum_{\{i\, : \,\omega_i\in A\}} p_i\;,
\end{gather*}
wobei $p_i:=\WKM(\{\omega_i\})$. Auf h"ochstens abz"ahlbar unendlichen Grundr"aumen ist somit \textbf{jedes}
Wahrscheinlichkeitsma"s diskret.
\subsubsection{Wahrscheinlichkeitsdichten}
Eine st"uckweise stetige Funktion $p\colon\R\mapsto\R$ hei"st
\textbf{Wahrscheinlichkeitsdichte}, wenn
\begin{eqnarray*}
&(i)&\quad p(x)\ge 0\quad\mbox{f"ur}\quad x\in\R\quad \mbox{und}\\
&(ii)&\quad\int_{-\infty}^\infty p(x)\,\mathrm d x=1
\end{eqnarray*}
gelten.
\subsubsection{Borel-$\sigma$-Algebra}
Mit $\BorelM(\R)$ bezeichnet man die kleinste $\sigma$-Algebra von Mengen aus $\R$, die
die halboffenen Intervalle enth"alt. Man nennt $\BorelM(\R)$ die $\sigma$-Algebra der
\textbf{Borelmengen}. Elemente von $\BorelM(\R)$ sind z.\,B. alle offenen oder abgeschlossenen Mengen,
deren abz"ahlbaren Vereinigungen und Durchschnitte usw.
\subsubsection{Stetige Wahrscheinlichkeitsma"se}
Gegeben sei eine Wahrscheinlichkeitsdichte $p$. Dann existiert ein eindeutig bestimmtes
Wahrscheinlichkeitsma"s $\WKM \colon\BorelM(\R)\mapsto [0,1]$ mit
\begin{gather*}
  \WKM([\alpha,\beta])=\WKM((\alpha,\beta])=\int_\alpha^\beta\,p(x)\,\mathrm dx
\end{gather*}
f"ur alle reelle Zahlen $\alpha<\beta$. Das so erzeugte Wahrscheinlichkeitsma"s $\WKM$ hei"st \textbf{stetig}
und $p$ nennt man die \textbf{Dichte} von $\WKM$. Stetige Wahrscheinlichkeitsma"se beschreiben Vorg"ange,
bei denen "uberabz"ahlbar viele reelle Zahlen als Ergebnis auftreten k"onnen (z.\,B. Lebenszeiten etc.).
\subsection{Die wichtigsten diskreten Wahrscheinlichkeitsverteilungen}

\subsubsection{Einpunktverteilung}
Gegeben sei ein $\omega_0\in \Omega$, fest aber beliebig. Dann wird
durch
\begin{gather*}
  \delta_{\omega_0}(A):=\left\{
     \begin{array}{rcl}
       1 &:& \omega_0\in A\\
       0 &:& \omega_0\notin A
     \end{array}
     \right.
\end{gather*}
die \textbf{Einpunktverteilung} in $\omega_0$ (oder das \textbf{Diracsche $\mathbf \delta$-Ma"s} in
$\omega_0$) definiert. Der Wahrscheinlichkeitsraum $(\Omega,\PotM(\Omega),\delta_{\omega_0})$
beschreibt Vorg"ange, bei denen mit Wahrscheinlichkeit $1$ genau $\omega_0$ eintritt (deterministische
Vorg"ange).


\subsubsection{Gleichverteilung auf $N$ Punkten}
Gegeben seien $N$ Punkte $\omega_1,\ldots,\omega_N\in\Omega$.
Das Ma"s $\WKM$ auf $\PotM(\Omega)$ mit
\begin{gather*}
  \WKM:=\frac{1}{N}\sum_{i=1}^N \delta_{\omega_i}
\end{gather*}
hei"st \textbf{Gleichverteilung} auf $\{\omega_1,\ldots,\omega_N\}$. F"ur ein Ereignis $A$ gilt dann
\begin{gather*}
  \WKM(A)=\frac{\mathrm{card}\{ i\le N : \omega_i\in A\}}{N}=
     \frac{\mbox{Anzahl der g"unstigen F"alle f"ur A}}{\mbox{Anzahl der m"oglichen F"alle}}\;.
\end{gather*}
\subsubsection{Binomialverteilung}
Sei $\Omega=\{0,\ldots,n\}$ und sei $p\in[0,1]$ vorgegeben. Dann wird durch
\begin{gather*}
  B_{n,p}(\{k\}):={n\choose k} p^k(1-p)^{n-k},\quad k=0,\ldots,n\;,
\end{gather*}
ein Wahrscheinlichkeitsma"s $B_{n,p}$ auf $\PotM(\Omega)$ definiert. Man nennt $B_{n,p}$
\textbf{Binomialverteilung} mit den Parametern $n$ und $p$. Die Zahl
$B_{n,p}(\{k\})$ gibt die Wahrscheinlichkeit an, dass man bei $n$ unabh"angigen Versuchen genau $k$-mal
Erfolg hat. Dabei ist die Erfolgswahrscheinlichkeit in jedem einzelnen Versuch $p$, die f"ur
Misserfolg $1-p$ .
\subsubsection{Hypergeometrische Verteilung}
Gegeben seien Zahlen $M,N,n\in\N_0$ mit $M,n\le N$. Dann wird durch
\begin{gather*}
  H_{N, M ,n}(\{m\}) :=\frac{{M\choose m}{N-M\choose n-m}}{{N\choose n}}
\end{gather*}
ein Wahrscheinlichkeitsma"s $H_{N,M,n}$ auf $\PotM(\{0,\ldots,n\})$
definiert. Man nennt $H_{N,M,n}$  \textbf{hypergeometrische Verteilung} mit den Parametern $N,M$ und $n$.
Sind in einer Lieferung von $N$ Ger"aten $M$ St"uck defekt, so beschreibt $H_{N,M,n}(\{m\})$
die Wahrscheinlichkeit, dass man in einer zuf"allig entnommenen Stichprobe vom Umfang $n$ genau
$m$ defekte Ger"ate beobachtet.
\subsubsection{Poissonverteilung}
Es sei $\Omega=\N_0=\{0,1,2,\ldots\}$. F"ur eine Zahl $\lambda>0$ definiert
man die \textbf{Poissonverteilung} mit Parameter $\lambda$ durch
\begin{gather*}
  P_\lambda(\{k\}):= \frac{\lambda^k}{k !}\,\mathrm e^{-\lambda}\quad\mbox{f"ur}\quad k\in\N_0\;.
\end{gather*}
Die Bedeutung der Poissonverteilung ergibt sich aus folgendem Satz:
\begin{thm}
Gegeben sei eine Zahl $\lambda>0$. F"ur $n\in\N$ setze man
$p_n:=\lambda/n$. Dann folgt f"ur alle $k\in\N_0$
stets
  \begin{gather*}
    \lim_{n\to\infty} B_{n,p_n}(\{k\})= P_\lambda(\{k\})\;.
  \end{gather*}
\end{thm}
Inhaltlich bedeutet dies: F"uhrt man sehr viele unabh"angige Versuche durch ($n$ St"uck), bei denen jeweils
nur mit sehr kleiner
Wahrscheinlichkeit $p$ Erfolg eintreten kann, so ist die Anzahl der insgesamt beobachteten Erfolge
approximativ gem"a"s $P_\lambda$ verteilt, wobei $\lambda= n\cdot p$.
\subsubsection{Geometrische Verteilung}
Bei einem einzelnen Versuch trete Erfolg wieder mit Wahrscheinlichkeit $p$ und Misserfolg
mit Wahrscheinlichkeit $1-p$ auf. Man f"uhrt nun so lange unabh"angige Versuche durch, bis
man erstmals Erfolg beobachtet. Die Wahrscheinlichkeit, dass dies im $(k+1)$-ten Versuch mit
$k\in\N_0$ geschieht, wird durch die \textbf{geometrische Verteilung} mit Parameter $p\in(0,1]$
beschrieben:
\begin{gather*}
  \WKM(\{k\}):= p\cdot(1-p)^k\,,\quad k\in\N_0\;.
\end{gather*}
\subsection{Die wichtigsten stetigen Wahrscheinlichkeitsverteilungen}

\subsubsection{Gleichverteilung auf einem Intervall}
Es sei $[a,b]$ ein endliches Intervall. Durch
\begin{gather*}
  p(x):=\left\{
     \begin{array}{ccl}
       \frac{1}{b-a} &:& x\in [a,b]\\
       0 &:& x\notin [a,b]
     \end{array}
     \right.
\end{gather*}
wird eine Wahrscheinlichkeitsdichte auf $\R$ definiert. Damit berechnet sich die Wahrscheinlichkeit
eines Intervalls $[\alpha,\beta]$ durch
\begin{equation}
\label{gleich}
\WKM([\alpha,\beta])=\int_\alpha^\beta p(x)\,\mathrm dx= \frac{\mbox{L"ange von}\;([\alpha,\beta]\cap[a,b])}
{b-a}\;.
\end{equation}
Insbesondere ergibt sich im Fall $[\alpha,\beta]\subseteq [a,b]$ die Formel
\begin{gather*}
  \WKM([\alpha,\beta])= \frac{\beta-\alpha}{b-a}\;,
\end{gather*}
d.\,h.~die Wahrscheinlichkeit des Eintretens von $[\alpha,\beta]\subseteq[a,b]$
 h"angt nur von seiner L"ange, nicht aber von
seiner speziellen Lage innerhalb $[a,b]$ , ab.
Das durch (\ref{gleich}) erzeugte Wahrscheinlichkeitsma"s hei"st \textbf{Gleichverteilung} auf dem
Intervall $[a,b]$ .
\subsubsection{Exponentialverteilung}
Gegeben sei eine Zahl $\lambda>0$. Man definiert die
\textbf{Exponentialverteilung} $E_\lambda$ mit Parameter $\lambda>0$ durch
ihre Dichte
\begin{gather*}
  p(x):=\left\{
     \begin{array}{ccr}
       \lambda\,\mathrm e^{-\lambda x} &:& x>0\\
       0 &:& x\le 0
     \end{array}
     \right.\;.
\end{gather*}
F"ur ein Intervall $[\alpha,\beta]\subseteq [0,\infty)$ berechnet sich damit
die Wahrscheinlichkeit seines Eintretens durch
\begin{gather*}
  E_\lambda([\alpha,\beta])=\lambda \int_\alpha^\beta \mathrm e^{-\lambda x}\,\mathrm dx
     = \mathrm e^{-\lambda\alpha}- \mathrm e^{-\lambda\beta}\;.
\end{gather*}
\subsubsection{Normalverteilung}
Gegeben seien Zahlen $\mu\in\R$ und $\sigma>0$. Die
Funktion
\begin{gather*}
  p_{\mu,\sigma^2}(x):= \frac{1}{\sqrt{2\pi}\sigma}\,\mathrm e^{-(x-\mu)^2/2\sigma^2}\;,
     \quad x\in\R\;,
\end{gather*}
erzeugt ein Wahrscheinlichkeitsma"s $\mathcal N(\mu,\sigma^2)$, das
man \textbf{Normalverteilung} mit Mittelwert $\mu$ und Varianz $\sigma^2$ nennt. Es gilt dann
\begin{gather*}
  \mathcal N(\mu,\sigma^2)([\alpha,\beta])=\frac{1}{\sqrt{2\pi}\sigma}
     \int_\alpha^\beta \mathrm e^{-(x-\mu)^2/2\sigma^2}\mathrm dx\;.
\end{gather*}
Im Fall $\mu=0$ und $\sigma=1$ erh"alt man die \textbf{Standardnormalverteilung}
$\mathcal N(0,1)$ . Wahrscheinlichkeiten des Eintretens von Intervallen berechnen sich in diesem Fall durch
\begin{gather*}
  \mathcal N(0,1)([\alpha,\beta])=\frac{1}{\sqrt{2\pi}}
     \int_\alpha^\beta \mathrm e^{-x^2/2}\mathrm dx\;.
\end{gather*}

\subsubsection{Gleichverteilung auf einer Menge im $\R^n$}
Es sei $E\subseteq \R^n$ eine beschr"ankte und abgeschlossene
Teilmenge, deren $n$-dimensionales Volumen $\mathrm{vol}_n(E)$ man
berechnen kann. Man definiert die \textbf{Gleichverteilung auf $E$} durch den Ansatz
\begin{gather*}
  \WKM(A)=\frac{\mathrm{vol}_n(A\cap E)}{\mathrm{vol}_n(E)}\;.
\end{gather*}
Insbesondere ergibt sich f"ur $A\subseteq E$ die Aussage
\begin{gather*}
  \WKM(A)=\frac{\mathrm{vol}_n(A)}{\mathrm{vol}_n(E)}\;,
\end{gather*}
d.\,h., wie im eindimensionalen Fall h"angt die Wahrscheinlichkeit des Eintretens einer Menge $A\subseteq E$
nur von deren Volumen ab, nicht aber von deren Lage innerhalb $E$ noch von ihrer Gestalt.
\subsection{Verteilungsfunktion}
\subsubsection{Definition}
F"ur ein Wahrscheinlichkeitsma"s $\WKM$ auf $(\R,\BorelM(\R))$ wird die
\textbf{Verteilungsfunktion} $F\colon\R\mapsto\R$ durch
\begin{equation}
\label{F0}
F(t):=\WKM((-\infty,t])\,,\quad t\in\R  \;,
\end{equation}
definiert.\\
\textit{Hinweis:} Ist $\WKM$ ein diskretes Wahrscheinlichkeitsma"s auf $(\Omega,\PotM(\Omega))$ mit $\Omega\subseteq\R$,
so modifiziert sich die Definition zu
\begin{gather*}
  F(t):=\WKM((-\infty,t]\cap\Omega)\,,\quad t\in\R  \;.
\end{gather*}
\subsubsection{Eigenschaften der Verteilungsfunktion}
\begin{thm}
\label{VF}
Die Verteilungsfunktion $F$ eines Wahrscheinlichkeitsma"ses besitzt folgende Eigenschaften:
\begin{eqnarray*}
&(i)&\quad\lim_{t\to -\infty} F(t)=0\quad\mbox{\rm und}\quad\lim_{t\to\infty} F(t)=1\;,\\
&(ii)&\quad\mbox{\rm die Funktion}\; F\;\mbox{\rm ist nichtfallend und}\\
&(iii)&\quad\mbox{\rm die Funktion}\;
F\;\mbox{\rm ist rechtsseitig stetig}\;.
\end{eqnarray*}
\end{thm}
\subsubsection{Weitere Eigenschaften von Verteilungsfunktionen}
<<<<<<< HEAD:linde-stochastik-zusfa/definf.tex
\begin{enumerate}[label=(\alph*)]
 \item
F"ur jedes halboffene Intervall $(\alpha,\beta]$ gilt
  \begin{gather*}
    \WKM((\alpha,\beta])= F(\alpha)-F(\beta)\;.
  \end{gather*}
 \item
Die Funktion $F$ besitzt in einem Punkt $t_0\in\R$ genau dann einen Sprung der H"ohe
$h>0$ (man hat $F(t_0)-F(t_0-0)=h$) , wenn $\WKM(\{t_0\})=h$ gilt. Insbesondere hat die
Verteilungsfunktion eines diskreten Ma"ses Spr"unge in den Punkten, wo die Masse
des Ma"ses konzentriert ist. Dazwischen ist sie konstant.
 \item
Ist $F$ Verteilungsfunktion eines stetigen Wahrscheinlichkeitsma"ses $\WKM$ mit Dichte $p$, so berechnet
sich $F$ aus
  \begin{gather*}
    F(t)=\int_{-\infty}^t\,p(x)\,\mathrm d x\,,\quad t\in\R\;.
  \end{gather*}
Insbesondere gilt f"ur alle $t\in\R$, in denen $p$ stetig ist, die Gleichung
  \begin{gather*}
    F'(t)=\left(\frac{\mathrm d F}{\mathrm d t} \right)(t)= p(t)\;.
  \end{gather*}
\end{enumerate}
\subsection{Bedingte Verteilungen}
\subsubsection{Definition}
Es sei $(\Omega,\Algeb,\WKM)$ ein Wahrscheinlichkeitsraum. Dann wird
f"ur $B\in \Algeb$ mit $\WKM(B)>0$ die \textbf{bedingte Wahrscheinlichkeit} $\WKM(\,\cdot\,|B)$
(oder die Wahrscheinlichkeit von $A$ unter der Bedingung $B$)
durch
\begin{equation}
\label{Bed}
 \WKM(A|B):=\frac{\WKM(A\cap B)}{\WKM(B)}\quad\mbox{f"ur}\;\;A\in\Algeb
\end{equation}
definiert. Sie gibt die Wahrscheinlichkeit daf"ur an, dass $A$
eintritt, unter der Bedingung, dass $B$ bereits eingetreten ist. H"aufig verwendet man
Formel (\ref{Bed}) auch in der Form
\begin{gather*}
  \WKM(A\cap B)= \WKM(B)\,\WKM(A|B)\;.
\end{gather*}
\subsubsection{Eigenschaften}
\begin{thm}
Die Abbildung
  \begin{gather*}
    A\mapsto \WKM(A|B)
  \end{gather*}
von $\Algeb$ nach $[0,1]$ ist ein Wahrscheinlichkeitsma"s mit den zus"atzlichen Eigenschaften
  \begin{gather*}
    \WKM(B|B)=1\quad\mbox{und}\quad\WKM(B^c|B)=0\;.
  \end{gather*}
\end{thm}
\subsubsection{Formel "uber die totale Wahrscheinlichkeit}
\begin{thm}
\label{total}
Gegeben seien disjunkte Mengen $B_1,\ldots,B_n$ in $\Algeb$ mit
$\WKM(B_j)>0$. Dann gilt f"ur $A\in \Algeb$ mit $A\subseteq\bigcup_{j=1}^n B_j$
die Aussage
  \begin{gather*}
    \WKM(A)=\sum_{j=1}^n\WKM(B_j)\cdot\WKM(A|B_j)\;.
  \end{gather*}
\end{thm}
\textit{Bemerkung:} Insbesondere gilt der Satz im Fall $\bigcup_{j=1}^n B_j=\Omega$
f"ur alle $A\in\Algeb$.
\subsubsection{Formel von Bayes}
Zur Berechnung von a posteriori Wahrscheinlichkeiten ist die Formel von
Bayes wichtig. Sie besagt das folgende:
\begin{thm}
Unter den Voraussetzungen aus Satz~\ref{total} an $B_1,\ldots,B_n$ und $A$ folgt
f"ur
$\WKM(A)>0$ die Identit"at
\begin{equation}
\label{Bayes}
\WKM(B_k|A)=\frac{\WKM(B_k)\cdot\WKM(A|B_k)}{\sum_{j=1}^n
\WKM(B_j)\cdot\WKM(A|B_j)}\quad\mbox{f"ur}\;\;k=1,\ldots,n\;.
\end{equation}
\end{thm}
\textit{Bemerkung:} Den Nenner in Formel (\ref{Bayes}) kann man (falls bekannt) durch $\WKM(A)$
ersetzen.
\subsection{Unabh"angigkeit von Ereignissen}
\subsubsection{Unabh"angigkeit von zwei Ereignissen}
Gegeben seien zwei Ereignisse $A,B$ aus einem Wahrscheinlichkeitsraum $(\Omega,\Algeb,\WKM)$.
Dann hei"sen $A$ und $B$ (stochastisch) \textbf{unabh"angig}, wenn
\begin{gather*}
  \WKM(A\cap B)=\WKM(A)\,\WKM(B)
\end{gather*}
gilt.
\subsubsection{Eigenschaften}
Die $\emptyset$ und $\Omega$ sind von jeder Menge $A\in\Algeb$ unah"angig. Sind
$A$ und $B$ unah"angig, dann gilt dies auch f"ur die Paare $A$ und $B^c$ bzw.~$A^c$ und $B^c$.
\subsubsection{Unabh"angigkeit von $n$ Ereignissen}
Die Ereignisse $A_1,\ldots,A_n$ aus $\Algeb$ hei"sen (stochastisch) \textbf{unabh"angig},
wenn f"ur alle Teilmengen $I\subseteq\{1,\ldots,n\}$ stets
\begin{equation}
\label{unab1}
\WKM\Bigl(\bigcap_{i\in I} A_i\Bigr)= \prod_{i\in I}\WKM(A_i)
\end{equation}
gilt. Man kann dies auch wie folgt formulieren: F"ur alle $m\ge 2$ und alle
$1\le i_1<\cdots<i_m\le n$ hat man
\begin{equation}
\label{unab2}
\WKM(A_{i_1}\cap\cdots\cap A_{i_m})= \WKM(A_{i_1})\cdots\WKM(A_{i_m})\;.
\end{equation}
Die Ereignisse $A_1,\ldots,A_n$ aus $\Algeb$ hei"sen \textbf{paarweise unabh"angig}, wenn
jeweils zwei Ereignisse aus $A_1,\ldots,A_n$ unabh"angig sind, d.\,h.~die Gleichungen
(\ref{unab1}) bzw.~(\ref{unab2}) m"ussen nur f"ur $\mathrm{card}(I)=2$ bzw.~f"ur $m=2$ erf"ullt sein.
\subsubsection{Eigenschaften}
Unabh"angige Mengen $A_1,\ldots,A_n$ sind auch paarweise unabh"angig. Die Umkehrung ist
i.a.~falsch. Ebenso falsch ist, dass aus
\begin{gather*}
  \WKM(A_{1}\cap\cdots\cap A_{n})= \WKM(A_{1})\cdots\WKM(A_{n})
\end{gather*}
stets die Unabh"angigkeit der $A_j$ folgt.

Sind $A_1,\ldots A_n$ unabh"angig, so gilt dies auch f"ur $(A_j)_{j\in J}$  mit
$J\subseteq \{1,\ldots,n\}$.

\section{Zufallsvariable}
\subsection{Definition und Verteilungsgesetz}

\subsubsection{Das vollst"andige Urbild}
F"ur eine Abbildung $X\colon\Omega\mapsto\R$ und eine Teilmenge $B\subseteq\R$ wird das
\textbf{vollst"andige Urbild} von $B$ unter $X$ durch
\begin{gather*}
  X^{-1}(B):=\{\omega\in\Omega : X(\omega)\in B\}
\end{gather*}
definiert. Verk"urzend schreibt man auch $X^{-1}(B)=\;\{X\in B\}$.
\subsubsection{Zuf"allige Gr"o"sen}
Sei $\Omega$ eine Menge, die mit einer $\sigma$-Algebra $\Algeb$ versehen ist. Eine Abbildung
$X\colon\Omega\mapsto\R$ hei"st \textbf{zuf"allige Gr"o"se} oder \textbf{reellwertige Zufallsvariable}
oder \textbf{zuf"allige reelle Zahl}, wenn f"ur jedes $t\in\R$ die Menge $\{\omega\in\Omega: X(\omega)\le t\}$
zur $\sigma$-Algebra $\Algeb$ geh"ort.\\
\textit{Bemerkung:} In diesem Fall gilt dann auch $X^{-1}(B)\in\Algeb$ f"ur jede Borelmenge $B\subseteq \R$.

\subsubsection{Verteilungsgesetz einer zuf"alligen Gr"o"se}
Sei $(\Omega,\Algeb,\WKM)$ ein Wahrscheinlichkeitsraum.
F"ur eine zuf"allige Gr"o"se $X\colon\Omega\mapsto\R$ ist die Abbildung
$\WKM_X \colon \BorelM(\R)\mapsto [0,1]$ mit
\begin{gather*}
  \WKM_X(B)=\WKM\big(X^{-1}(B)\big)=\WKM\{\omega\in\Omega : X(\omega)\in B\}=\WKM(\{X\in B\})= \WKM(X\in B)
\end{gather*}
sinnvoll definiert.
\begin{thm}
Die Abbildung $\WKM_X$ ist ein Wahrscheinlichkeitsma"s auf $(\R,\BorelM(\R))$.
\end{thm}
Man nennt $\WKM_X$ das \textbf{Verteilungsgesetz} von $X$ (bzgl.~$\WKM$).
\subsubsection{Typen von zuf"alligen Gr"o"sen}
Eine zuf"allige Gr"o"se $X$ hei"st \textbf{diskret}, wenn $\WKM_X$ ein diskretes
Wahrscheinlichkeitsma"s ist. Damit hat $\WKM_X$ die Gestalt
\begin{gather*}
  \WKM_X(B)=\sum_{\{i\, : \,x_i\in  B\}} p_i
\end{gather*}
mit geeigneten $x_i\in\R$ und $p_i\ge 0$. Die $x_i$ sind die m"oglichen Werte von $X$, d.\,h.~es
gilt\\ $\WKM(X\in\{x_1,x_2,\ldots\})=1$,
und
\begin{gather*}
  p_i=\WKM\{\omega\in\Omega : X(\omega)=x_i\}\;.
\end{gather*}
Eine zuf"allige Gr"o"se $X$ hei"st \textbf{stetig}, falls $\WKM_X$ ein stetiges Wahrscheinlichkeitsma"s
ist. Das gilt genau dann, wenn mit einer Wahrscheinlichkeitsdichte $p$ f"ur alle $\alpha<\beta$
die Gleichung
\begin{gather*}
  \WKM_X([\alpha,\beta])=\WKM\{\omega\in\Omega : \alpha\le X(\omega)\le\beta\}=\int_\alpha^\beta\,p(x)\,
     \mathrm d x
\end{gather*}
erf"ullt ist. Die Funktion $p$ nennt man auch \textbf{Verteilungsdichte} (oder einfach
\textbf{Dichte}) von $X$.
\subsubsection{Speziell verteilte diskrete zuf"allige Gr"o"sen}
Eine zuf"allige Gr"o"se $X$ hei"st \textbf{gleichverteilt}
 auf einer endlichen Menge oder \textbf{binomialverteilt}
oder \textbf{Poissonverteilt} etc., wenn $\WKM_X$ von diesem Typ ist. In allen diesen F"allen
ist $X$ diskret. Zum Beispiel ist $X$ gem"a"s $B_{n,p}$ verteilt (man schreibt auch
$X\sim B_{n,p}$), falls f"ur $0\le k\le n$ stets
\begin{gather*}
  \WKM_X(\{k\})=\WKM\{\omega\in\Omega : X(\omega)=k\} = {n \choose k} p^k(1-p)^{n-k}
\end{gather*}
gilt. Analog ist $X$ gem"a"s $P_\lambda$ verteilt, sofern f"ur $k\in\N_0$
\begin{gather*}
  \WKM_X(\{k\})=\WKM\{\omega\in\Omega : X(\omega)=k\}=\frac{\lambda^k}{k!}\mathrm e^{-\lambda}\;.
\end{gather*}
\subsubsection{Speziell verteilte stetige zuf"allige Gr"o"sen}
Eine zuf"allige Gr"o"se $X$ hei"st \textbf{gleichverteilt} auf einem Intervall,
oder \textbf{exponentialverteilt} oder \textbf{normalverteilt} etc., wenn $\WKM_X$ von diesem Typ
ist.
Alle diese zuf"alligen Gr"o"sen sind stetig.
Zum Beispiel ist $X$ gleichverteilt auf $[a,b]$, falls f"ur alle $\alpha<\beta$
stets
\begin{gather*}
  \WKM_X([\alpha,\beta])=\WKM\{\omega\in \Omega : \alpha\le X(\omega)\le \beta\}
     =\frac{\mbox{L"ange von}\, ([\alpha,\beta]\cap[a,b])}{b-a}
\end{gather*}
gilt. Oder $X$ ist $\mathcal N(\mu,\sigma^2)$-verteilt (man schreibt $X\sim \mathcal N(\mu,\sigma^2)$),
sofern
\begin{gather*}
  \WKM_X([\alpha,\beta])=\WKM\{\omega\in \Omega : \alpha\le X(\omega)\le \beta\}=
     \frac{1}{\sqrt{2\pi}\sigma}\int_\alpha^\beta \mathrm e^{-(x-\mu)^2/2\sigma^2}\mathrm d x\;.
\end{gather*}
\subsubsection{Identisch verteilte zuf"allige Gr"o"sen}
Zwei zuf"allige Gr"o"sen $X$ und $Y$ sind \textbf{identisch verteilt}, wenn $\WKM_X=\WKM_Y$ gilt,
d.\,h.~f"ur alle $B\in\BorelM(\R)$ hat man
\begin{gather*}
  \WKM\{\omega\in \Omega : X(\omega)\in B\}= \WKM\{\omega\in \Omega : Y(\omega)\in B\}\;.
\end{gather*}
Man schreibt dann $X\stackrel{d}{=}Y$.
\subsubsection{Verteilungsfunktion einer zuf"alligen Gr"o"se}
Die \textbf{Verteilungsfunktion} $F_X$ einer zuf"alligen Gr"o"se ist die Verteilungsfunktion
ihres Verteilungsgesetzes, d.\,h., es gilt
\begin{gather*}
  F_X(t)=\WKM_X\big((-\infty,t]\big)=\WKM\{\omega\in\Omega : X(\omega)\le t\}\,,\quad t\in \R\;.
\end{gather*}
F"ur zwei zuf"allige Gr"o"sen $X$ und $Y$ gilt genau dann $X\stackrel{d}{=}Y$, wenn man
$F_X=F_Y$ hat.

Die Funktion $F_X$ besitzt die Eigenschaften aus Satz \ref{VF}.

\subsection{Zuf"allige Vektoren und Unabh"angigkeit zuf"alliger Gr"o"sen}
\subsubsection{Zuf"allige Vektoren}
Sei $\Omega$ eine Menge mit einer $\sigma$-Algebra $\Algeb$. Eine Abbildung $\vec X \colon \Omega\mapsto\R^n$
hei"st ($n$-dimensionaler) \textbf{zuf"alliger Vektor}, wenn seine Koordinatenabbildungen
$X_j \colon\Omega\mapsto \R$ alle zuf"allige Gr"o"sen sind. Dabei sind wie "ublich die $X_j$ durch
\begin{gather*}
  \vec X(\omega)=(X_1(\omega),\ldots,X_n(\omega))\;,\quad \omega\in\Omega\,,
\end{gather*}
definiert.

\subsubsection{Gemeinsames Verteilungsgesetz}
Sei $(\Omega,\Algeb,\WKM)$ ein Wahrscheinlichkeitsraum. Dann definiert
man wie im eindimensionalen Fall das Verteilungsgesetz $\WKM_{\vec X}$ von $\vec X$
durch
\begin{gather*}
  \WKM_{\vec X}(B):=\WKM\big(\vec X^{-1}(B)\big)=\WKM\{\omega\in\Omega : (X_1(\omega),\ldots,X_n(\omega))\in B\}\;.
\end{gather*}
Im Spezialfall $B=B_1\times\cdots\times B_n$ f"ur Borelmengen $B_j\subseteq\R$ folgt
\begin{gather*}
  \WKM_{\vec X}(B)=\WKM(X_1\in B_1,\ldots, X_n\in B_n)\;.
\end{gather*}
Deshalb nennt man $\WKM_{\vec X}$ auch \textbf{gemeinsames Verteilungsgesetz} der zuf"alligen
Gr"o"sen $X_1,\ldots,X_n$.
\subsubsection{Randverteilungen}
F"ur einen zuf"alligen Vektor $\vec X$ nennt man die Verteilungsgesetze $\WKM_{X_j}$, $1\le j\le n$,
die \textbf{Randverteilungen} von $\vec X$. Hierbei sind wie zuvor die zuf"alligen Gr"o"sen $X_j$ die
zugeh"origen Koordinatenabbildungen.
\begin{thm}
Die Randverteilungen berechnen sich aus der gemeinsamen Verteilung durch
  \begin{gather*}
    \WKM_{X_j}(B)= \WKM_{\vec X}(\R\times\cdots\times \underbrace{B}_j\times\cdots\times\R)\;,\quad B\in\BorelM(\R)\;.
  \end{gather*}
Damit bestimmt die gemeinsame Verteilung die zugeh"origen Randverteilungen.
\end{thm}
\textit{Bemerkung:} Die Umkehrung der obigen Aussage ist i.a.~falsch, d.\,h.~es existieren zuf"allige
Vektoren $\vec X=(X_1,\ldots,X_n)$ und $\vec Y=(Y_1,\ldots, Y_n)$ mit $\WKM_{X_j}=\WKM_{Y_j}$,
$1\le j\le n$, aber mit $\WKM_{\vec X}\not=\WKM_{\vec Y}$.
\subsubsection{Randverteilungen diskreter Vektoren}
\label{disk}
Wir betrachten hier nur den Fall $n=2$. Ein zuf"alliger $2$-dimensionaler Vektor hat die Gestalt
$(X,Y)$ mit vorgegebenen zuf"alligen Gr"o"sen $X$ und $Y$. Weiterhin seien $X$ und $Y$ diskret und die
Folgen
$(x_i)_{i\ge 1}$ bzw.~$(y_j)_{j\ge 1}$ von reellen Zahlen bezeichnen  die m"oglichen Werte von $X$ bzw.~$Y$.
Dann nimmt der Vektor $(X,Y)$ die Werte $(x_i,y_j)_{i,j\ge 1}$ an und f"ur das Verteilungsgesetz
von $\WKM_{(X,Y)}$, d.\,h.~die gemeinsame Verteilung von $X$ und $Y$, gilt
\begin{gather*}
  \WKM_{(X,Y)}(B)=\sum_{\{(i,j)\,:\, (x_i,y_j)\in B\}} p_{ij}\,,\quad B\in\PotM(\R^2)\,,
\end{gather*}
wobei
\begin{gather*}
  p_{ij}= \WKM_{(X,Y)}(\{(x_i,y_j)\})=\WKM(X=x_i,Y=y_j)\;.
\end{gather*}
F"ur die Randverteilungen ergibt sich dann
\begin{gather*}
  \WKM_X(B)=\sum_{\{i\,:\,x_i\in B\}} q_i \qquad\mbox{und}\qquad
     \WKM_Y(B)=\sum_{\{j\,:\,y_j\in B\}} r_j\,,\quad B\in\PotM(\R)\,,
\end{gather*}
mit
\begin{gather*}
  q_i=\sum_{j=1}^\infty p_{ij}\qquad\mbox{und}\qquad r_j=\sum_{i=1}^\infty p_{ij}\;\;\;.
\end{gather*}
\subsubsection{Randverteilungen stetiger Vektoren}
\label{stet}
Zur besseren "Ubersichtlichkeit betrachten wir auch hier nur den Fall $n=2$. Der
$2$-dimensionale Vektor $(X,Y)$ sei wie oben definiert. Diesmal nehmen wir aber an, dass
$\WKM_{(X,Y)}$ eine Dichte hat, es also eine Funktion $p \colon\R^2\mapsto\R$ gibt, so
dass f"ur alle $\alpha<\beta$ und $\gamma<\delta$ stets
\begin{gather*}
  \WKM_{(X,Y)}\big([\alpha,\beta]\times[\gamma,\delta]\big)
     =\WKM\{\omega\in \Omega : \alpha\le X(\omega)\le\beta,\,\gamma\le Y(\omega)\le\delta\}
     =\int_\alpha^\beta\int_\gamma^\delta p(x,y)\,\mathrm d y \,\mathrm d x
\end{gather*}
gilt. Dann haben $X$ bzw.~$Y$ Verteilungsdichten $q$ und $r$ mit
\begin{gather*}
  q(x):=\int_{-\infty}^\infty p(x,y)\,\mathrm d y\qquad\mbox{und}\qquad
     r(y):=\int_{-\infty}^\infty p(x,y)\,\mathrm d x\;.
\end{gather*}
\subsubsection{Unabh"angigkeit von zuf"alligen Gr"o"sen}
Gegeben seien $n$ zuf"allige Gr"o"sen $X_1,\ldots,X_n$ auf $(\Omega,\Algeb,\WKM)$. Gilt f"ur beliebige
Borelmengen $B_1,\ldots,B_n\in\BorelM(\R)$ stets
\begin{equation}
\label{unab}
\WKM(X_1\in B_1,\ldots,X_n\in B_n)=\WKM(X_1\in B_1)\cdots\WKM(X_n\in B_n)\;,
\end{equation}
so hei"sen $X_1,\ldots,X_n$ \textbf{unabh"angig}.\\
\textit{Bemerkung 1:} Die Unabh"angigkeit der $X_j$ ist "aquivalent zu folgender Aussage: F"ur
beliebige Borelmengen $B_j\in\BorelM(\R)$ sind die Ereignisse $\left(X_j^{-1}(B_j)\right)_{j=1}^n$
unabh"angig. Das folgt aus der Tatsache, dass man in Gleichung (\ref{unab}) f"ur gewisse vorgegebene $B_j$ auch die
reellen Zahlen $\R$ einsetzen kann.\\
\textit{Bemerkung 2:} Es reicht aus, wenn Gleichung (\ref{unab}) mit Intervallen $B_j$ der Form
$(-\infty,t_j]$ f"ur alle $t_j\in\R$ gilt. Die zuf"alligen Gr"o"sen $X_1,\ldots,X_n$ sind also
dann und nur dann unabh"angig, wenn f"ur alle $t_j\in\R$ stets
\begin{gather*}
  \WKM(X_1\le t_1,\ldots,X_n\le t_n)=\WKM(X_1\le t_1)\cdots\WKM(X_n\le t_n)
\end{gather*}
folgt.\\
\textit{Bemerkung 3:} Aufgrund von (\ref{unab}) ist die gemeinsame Verteilung von $X_1,\ldots,X_n$
im Fall der Unabh"angigkeit eindeutig durch
ihre Randverteilungen $P_{X_j},$ $1\le j\le n$, bestimmt.
\subsubsection{Spezialf"alle}
Besitzen $X$ und $Y$ die Eigenschaften aus \ref{disk}, so sind $X$ und $Y$ dann und nur  dann
unabh"angig, wenn
\begin{gather*}
  p_{ij}=q_i\cdot r_j\,,\quad 1\le i,j<\infty\;.
\end{gather*}
Im stetigen Fall \ref{stet} sind $X$ und $Y$ genau dann unabh"angig, wenn
\begin{gather*}
  p(x,y)=q(x)\cdot r(y)\,,\quad x,y\in\R\;.
\end{gather*}
\subsection{Rechnen mit zuf"alligen Gr"o"sen}
\subsubsection{Transformationen}
Eine Abbildung $f\colon\R\mapsto\R$ hei"st \textbf{messbar}, wenn f"ur jedes $t\in\R$ die
Menge $\{x\in \R : f(x)\le t\}$ eine Borelmenge ist. Stetige Funktionen, Grenzwerte stetiger
Funktionen oder auch monotone Funktionen besitzen diese Eigenschaft.
\begin{thm}
Sei $X$ eine zuf"allige Gr"o"se und sei $f\colon\R\mapsto\R$ messbar. Dann ist $Y:=f(X)$ ebenfalls
eine zuf"allige Gr"o"se.
\end{thm}
\textit{Allgemeine Aufgabe:} Man bestimme $\WKM_Y$ mit Hilfe von $\WKM_X$ und $f$.
Folgendes Beispiel illustriere die Situation: Sei $U$ gleichverteilt auf $[0,1]$, so ist mit
$f(s):=1-s$ auch $Y:=f(U)=1-U$ gleichverteilt auf $[0,1]$.
\subsubsection{Simulation stetiger zuf"alliger Gr"o"sen}
Sei $X$ eine stetige zuf"allige Gr"o"se mit Verteilungsfunktion $F_X$. Wir nehmen an,
dass mit zwei Zahlen
$-\infty\le a<b\le\infty$ die Verteilungsfunktion $F_X(a)=0$, $F_X(b)=1$ erf"ulle  und
auf $(a,b)$ streng wachsend sei. Dann
existiert die inverse Funktion von $F_X$, die mit $F_X^{-1}$ bezeichnet wird, und es
gilt $F_X^{-1}\colon(0,1)\mapsto (a,b)$.
\begin{thm}
Sei $U$ eine auf $[0,1]$ gleichverteilte zuf"allige Gr"o"se. Unter den
obigen Voraussetzungen gilt dann f"ur $Y:=F_X^{-1}(U)$ die Aussage
$X\stackrel{d}{=} Y$.
\end{thm}
\textit{Anwendung:} Sind $u_1,\ldots,u_n$ unabh"angig erzeugte reelle Zahlen, die gem"a"s der
Gleichverteilung aus $[0,1]$ gew"ahlt wurden, so sind die Zahlen $x_j:= F_X^{-1}(u_j)$ ebenfalls unabh"angig
und gem"a"s $\WKM_X$ verteilt.
\subsubsection{Lineare Transformationen}
F"ur reelle Zahlen $a\not=0$ und $b\in\R$ betrachte man die lineare Transformation
\begin{gather*}
  Y:=a\,X+ b
\end{gather*}
einer zuf"alligen Gr"o"se $X$.
\begin{thm}
Im Fall $a>0$ folgt
  \begin{gather*}
    F_Y(t)=F_X\left(\frac{t-b}{a}\right)\;.
  \end{gather*}
Ist $a<0$, so ergibt sich
  \begin{gather*}
    F_Y(t)=1-\WKM\left(X<\frac{t-b}{a}\right)\;,
  \end{gather*}
also
  \begin{gather*}
    F_Y(t)=1-F_X\left(\frac{t-b}{a}\right)
  \end{gather*}
im Fall stetiger $X$.
\end{thm}
\textit{Folgerung:} Besitzt $X$ die Verteilungsdichte $p$, so hat $Y=a\,X+b$ eine Dichte $q$,
die sich aus $p$ durch
\begin{gather*}
  q(t)=\frac{1}{|a|}\;p\left(\frac{t-b}{a}\right)\,,\quad t\in\R\,,
\end{gather*}
ergibt.
\subsubsection{Addition zuf"alliger Gr"o"sen}
F"ur zwei zuf"allige Gr"o"sen $X$  und $Y$ wird ihre Summe $X+Y$ durch
\begin{gather*}
  (X+Y)(\omega):=X(\omega)+Y(\omega)\;,\qquad \omega\in\Omega\,,
\end{gather*}
definiert.
\begin{thm}
Sind $X$ und $Y$ zuf"allige Gr"o"sen, so gilt dies auch f"ur $X+Y$ .
\end{thm}
Das Verteilungsgesetz der Summe $X+Y$ kann man f"ur unabh"angige zuf"allige Gr"o"sen in einigen
F"allen in einfacher Form angeben.
\begin{thm}~
Es seien $X$ und $Y$ unabh"angige zuf"allige Gr"o"sen.
\begin{enumerate}
\item
Nehmen $X$ und $Y$ Werte in den ganzen Zahlen $\Z$ an, so folgt
  \begin{gather*}
    \WKM(X+Y=k)=\sum_{i=-\infty}^\infty\WKM(X=i)\,\WKM(Y=k-i)\;,\quad k\in \Z\;.
  \end{gather*}
\item
Besitzen $X$ und $Y$ Werte in $\N_0$, so ergibt sich
  \begin{gather*}
    \WKM(X+Y=k)=\sum_{i=0}^k\WKM(X=i)\,\WKM(Y=k-i)\;,\quad k\in \N_0\;.
  \end{gather*}
\end{enumerate}
\end{thm}
Im Fall stetiger zuf"alliger Gr"o"sen gilt folgender Satz:
\begin{thm}
Seien $X$ und $Y$ unabh"angig mit Verteilungsdichten $p$ und $q$. Dann besitzt $X+Y$
die Verteilungsdichte $r$  mit
  \begin{gather*}
    r(x)=\int_{-\infty}^\infty p(x-y)\,q(y)\,\mathrm d y = \int_{-\infty}^\infty p(y)\,q(x-y)\,\mathrm d y \;.
  \end{gather*}
\end{thm}
Man nennt $r$ die \textbf{Faltung} von $p$ und $q$ und schreibt $r=p*q$.
\subsubsection{Addition speziell verteilter zuf"alliger Gr"o"sen}
Im folgenden seien $X$ und $Y$ stets als unabh"angig vorausgesetzt. Dann gilt:
\begin{thm}~
  \begin{enumerate}[label=(\alph*)]
   \item
Aus $X\sim B_{n,p}$ und $Y\sim B_{m,p}$ folgt $X+Y\sim B_{n+m,p}$  .
   \item
Aus $X\sim P_\lambda$ und $Y\sim P_\mu$ erh"alt man $X+Y\sim P_{\lambda+\mu}$ .
   \item
Aus $X\sim\mathcal N(\mu_1,\sigma_1^2)$ und $Y\sim\mathcal N(\mu_2,\sigma_2^2)$ folgt
$X+Y\sim \mathcal N(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$ .
  \end{enumerate}
\end{thm}
\subsection{Erwartungswert}
\subsubsection{Erwartungswert diskreter zuf"alliger Gr"o"sen}
Eine zuf"allige Gr"o"se $X$ nehme Werte $x_1,x_2,\ldots$ aus $[0,\infty)$ an. Dann
definiert man den \textbf{Erwartungswert} von $X$ durch
\begin{gather*}
  \E X :=\sum_{i=1}^\infty x_i\,\WKM(X=x_i)\;.
\end{gather*}
Es gilt dann $0\le \E X \le \infty$ .

Sind nunmehr die Werte von $X$ beliebige reelle Zahlen (nicht notwendig $\ge 0$)
, so sagt man, dass $X$ einen \textbf{Erwartungswert
besitzt}, wenn
\begin{gather*}
  \sum_{i=1}^\infty |x_i|\,\WKM(X=x_i)<\infty\;.
\end{gather*}
In diesem Fall ist der Erwartungswert von $X$ mit
\begin{gather*}
  \E X :=\sum_{i=1}^\infty x_i\,\WKM(X=x_i)
\end{gather*}
eine wohldefinierte reelle Zahl.

\subsubsection{Erwartungswert stetiger zuf"alliger Gr"o"sen}
Sei $p$ die Verteilungsdichte einer zuf"alligen Gr"o"se $X$. Dann \textbf{besitzt} $X$ einen
\textbf{Erwartungswert}, wenn
\begin{gather*}
  \int_{-\infty}^\infty |x|\,p(x)\,\mathrm   d x <\infty\;,
\end{gather*}
und man definiert den \textbf{Erwartungswert} von $X$ durch
\begin{gather*}
  \E X := \int_{-\infty}^\infty x \,p(x)\,\mathrm   d x\;.
\end{gather*}
\subsubsection{Beispiele zur Berechnung von Erwartungswerten}
\medskip

{\renewcommand{\arraystretch}{1.4}
\begin{center}
\begin{tabular}{|c|l|}\hline
\bf Verteilung von $X$& \bf Erwartungswert von $X$\\ \hline\hline
$X$ gleichverteilt auf $x_1,\ldots,x_N$& $\E X=\frac{1}{N}\sum_{i=1}^N x_i$\\ \hline
$X\sim B_{n,p}$& $\E X= n p$\\ \hline
$X\sim P_\lambda$&$\E X= \lambda$\\ \hline
$X$ geometrisch verteilt mit Parameter $p$& $\E X = \frac{1-p}{p}$\\ \hline
$X$ gleichverteilt auf $[a,b]$& $\E X= \frac{a+b}{2}$\\ \hline
$X\sim E_\lambda$& $\E X = \frac{1}{\lambda}$\\ \hline
$X\sim \mathcal N(\mu,\sigma^2)$&$ \E X = \mu$\\ \hline
\end{tabular}
\end{center}
}
\subsubsection{Eigenschaften des Erwartungswertes}
Der Erwartungswert einer zuf"alligen Gr"o"se hat folgende Eigenschaften:
\begin{thm}~
\begin{enumerate}
\item
Der Erwartungswert ist linear, d.\,h.~f"ur alle $a,b\in\R$ und zuf"allige Gr"o"sen $X$ und $Y$
gilt
  \begin{gather*}
    \E(a X+ b Y) = a\,\E X + b\, \E Y\;.
  \end{gather*}
\item
Sei  $X$ diskret mit m"oglichen Werten $x_1,x_2,\ldots$ aus $\R$ .
Dann existiert f"ur eine Funktion $f \colon\R\mapsto\R$
der Erwartungswert $\E f(X)$ genau dann, wenn
  \begin{gather*}
    \sum_{i=1}^\infty |f(x_i)|\,\WKM(X=x_i)<\infty\;,
  \end{gather*}
und es gilt
  \begin{gather*}
    \E f(X)=\sum_{i=1}^\infty f(x_i)\,\WKM(X=x_i)\;.
  \end{gather*}
\item
Ist $X$ stetig mit Verteilungsdichte $p$ , so existiert f"ur eine messbare
Abbildung $f \colon\R\mapsto\R$ genau dann der Erwartungswert von $f(X)$ , wenn
  \begin{gather*}
    \int_{-\infty}^\infty |f(x)|\,p(x)\,\mathrm d x<\infty\;,
  \end{gather*}
und man hat
  \begin{gather*}
    \E f(X)=\int_{-\infty}^\infty  f(x) \,p(x)\,\mathrm d x\;.
  \end{gather*}
\item
Sind $X$ und $Y$ unabh"angige zuf"allige Gr"o"sen deren Erwartungswert existiert,
so existiert auch der Erwartungswert von $X\cdot Y$, und es gilt
  \begin{gather*}
    \E(X\cdot Y)= \E X \cdot \E Y\;.
  \end{gather*}
\end{enumerate}
\end{thm}

\subsection{Varianz und Kovarianz}
\subsubsection{Momente}
Sei $n\in\N$. Eine zuf"allige Gr"o"se $X$ besitzt ein \textbf{$n$-tes Moment}, wenn
$\E|X|^n<\infty$ . Im diskreten Fall bedeutet dies
\begin{gather*}
  \sum_{i=1}^\infty |x_i|^n\WKM(X=x_i)<\infty
\end{gather*}
und im stetigen
\begin{gather*}
  \int_{-\infty}^\infty |x|^n\,p(x)\,\mathrm d x<\infty\;.
\end{gather*}
Insbesondere hat $X$ ein erstes Moment, genau dann, wenn $\E X$ existiert.
\begin{thm}
Sei $1\le m\le n$. Hat eine zuf"allige Gr"o"se $X$ ein $n$-tes Moment, so besitzt sie auch ein $m$-tes Moment.
Insbesondere hat jede zuf"allige Gr"o"se mit zweitem Moment einen Erwartungswert.
\end{thm}
\subsubsection{Varianz}
Es sei $X$ eine zuf"allige Gr"o"se mit zweitem Moment. Sei $a:=\E X$. Dann definiert man
die \textbf{Varianz} (oder \textbf{Streuung}) von $X$ durch
\begin{gather*}
  \V X:= \E(X-a)^2\;.
\end{gather*}
Die Varianz gibt den mittleren quadratischen Abstand einer zuf"alligen Gr"o"se $X$ von ihrem
Erwartungswert an. Sie ist ein Ma"s daf"ur, wie sehr die Werte von $X$ um $\E X$ schwanken.

\subsubsection{Eigenschaften der Varianz}
Im folgenden seien $X$ und $Y$ zuf"allige Gr"o"sen mit zweiten Momenten. Dann gelten die
folgenden Aussagen:
\begin{thm}~
\begin{enumerate}
\item
Mit $a:=\E X$ berechnet sich die Varianz f"ur diskrete zuf"allige Gr"o"sen in der Form
  \begin{gather*}
    \V X = \sum_{i=1}^\infty (x_i-a)^2\,\WKM(X=x_i)\;,
  \end{gather*}
und im stetigen Fall hat man
  \begin{gather*}
    \V X =\int_{-\infty}^\infty (x-a)^2\,p(x)\,\mathrm d x\;.
  \end{gather*}
\item
Es besteht die Identit"at
  \begin{gather*}
    \V X = \E X^2 -(\E X)^2\;.
  \end{gather*}
\item
F"ur eine konstante zuf"allige Gr"o"se $X$ folgt $\V X=0$ .
\item
F"ur $\alpha\in\R$ erh"alt man
  \begin{gather*}
    \V(\alpha\,X)=\alpha^2\,\V X\;.
  \end{gather*}
\item
Sind $X$ und $Y$ unabh"angig, dann gilt
  \begin{gather*}
    \V(X+Y)=\V X + \V Y\;.
  \end{gather*}
\end{enumerate}

\end{thm}
\subsubsection{Beispiele zur Berechnung von Varianzen}

\medskip

{\renewcommand{\arraystretch}{1.4}
\begin{center}
\begin{tabular}{|c|l|}\hline
\bf Verteilung von $X$& \bf Varianz von $X$\\ \hline\hline
$X$ gleichverteilt auf $x_1,\ldots,x_N$& $\V X=\frac{1}{N}\sum_{i=1}^N (x_i-\E X)^2$\\ \hline
$X\sim B_{n,p}$& $\V X= n\,p\,(1-p)$\\ \hline
$X\sim P_\lambda$&$\V X= \lambda$\\ \hline
$X$ geometrisch verteilt mit Parameter $p$& $\V X = \frac{1-p}{p^2}$\\ \hline
$X$ gleichverteilt auf $[a,b]$& $\V X= \frac{(b-a)^2}{12}$\\ \hline
$X\sim E_\lambda$& $\V X = \frac{1}{\lambda^2}$\\ \hline
$X\sim \mathcal N(\mu,\sigma^2)$&$ \V X = \sigma^2$\\ \hline
\end{tabular}
\end{center}
}
\subsubsection{Kovarianz}
Gegeben seien zwei zuf"allige Gr"o"sen $X$ und $Y$ mit zweiten Momenten.
Seien $a:=\E X$ und $b:=\E Y$ . Dann wird die \textbf{Kovarianz} von $X$ und $Y$ durch
\begin{gather*}
  {\rm  cov}(X,Y):= \E(X-a)(Y-b)
\end{gather*}
definiert.

\textbf{Eigenschaften:}

\begin{enumerate}
\item
Sind $X$ und $Y$ diskret mit m"oglichen Werten $x_1,x_2,\ldots$ bzw.~$y_1,y_2,\ldots$ aus $\R$,
so berechnet sich die Kovarianz aus der Formel
  \begin{gather*}
    {\rm  cov}(X,Y)=\sum_{i,j=1}^\infty (x_i-a)(y_j-b)\, p_{ij}
  \end{gather*}
wobei
  \begin{gather*}
    p_{ij}=\WKM(X=x_i, Y=y_j)\;.
  \end{gather*}
\item
Hat die Verteilung des zuf"alligen Vektors $(X,Y)$ eine Dichte $p \colon\R^2\mapsto\R$, so
ergibt sich die Kovarianz von $X$ und $Y$ aus
  \begin{gather*}
    {\rm  cov}(X,Y)=\int_{-\infty}^\infty \int_{-\infty}^\infty (x-a)(y-b)\,p(x,y)\,\mathrm d x\, \mathrm d y\;.
  \end{gather*}
\item
Sind $X$ und $Y$ unabh"angig, so impliziert dies ${\rm  cov}(X,Y)=0$, d.\,h.~$X$ und $Y$ sind
\textbf{unkorreliert}. Man beachte, dass aus der Unkorreliertheit i.a.~nicht die Unabh"angigkeit folgt.
\item
Man hat
\begin{equation}
\label{cov}
|{\rm  cov}(X,Y)|\le (\V X)^{1/2}(\V Y)^{1/2}\;.
\end{equation}
\subsubsection{Korrelationskoeffizient}
F"ur zwei zuf"allige Gr"o"sen $X$ und $Y$ mit zweiten Momenten definiert man ihren
\textbf{Korrelationskoeffizienten} durch
  \begin{gather*}
    \rho(X,Y):=\frac{{\rm  cov}(X,Y)}{(\V X)^{1/2}(\V Y)^{1/2}}\;.
  \end{gather*}
Aus (\ref{cov}) folgt
  \begin{gather*}
    -1\le \rho(X,Y)\le 1\;.
  \end{gather*}
F"ur unkorrelierte zuf"allige Gr"o"sen gilt $\rho(X,Y)=0$.
 Der Korrelationskoeffizient ist ein Ma"s f"ur den Grad der Abh"angigkeit von
$X$ und $Y$. Je n"aher $\rho(X,Y)$ an $1$ oder $-1$ liegt, desto gr"o"ser ist die
Abh"angigkeit zwischen $X$ und $Y$. Im st"arksten Fall
der Abh"angigkeit von $X$ und $Y$, n"amlich $Y=X$ bzw.~$Y=-X$, hat man $\rho(X,X)=1$ bzw.~$\rho(X,-X)=-1$ .
\end{enumerate}

\end{document}
