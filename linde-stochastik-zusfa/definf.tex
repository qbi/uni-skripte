% Einige zusätzliche Informationen für rubber
%  rubber erkennt nicht, dass die Datei weg kann, daher sagen wir es ihm
% rubber: clean $base.thm
%  rubber soll nach Änderungen an der Datei nochmal bauen
% rubber: watch $base.thm

\RequirePackage[l2tabu,orthodox]{nag}  % nag überprüft den Text auf veraltete
                   % Befehle oder solche, die man nicht in LaTeX verwenden
                   % soll -- l2tabu-Checker in LaTeX

\documentclass[ngerman,draft,parskip=half,twoside]{scrartcl}

\usepackage{xcolor}
\usepackage[draft=false,colorlinks,bookmarksnumbered,linkcolor=blue,breaklinks]{hyperref}

\usepackage[utf8]{inputenc}
\usepackage{babel}

\usepackage[T1]{fontenc}        % T1-Schriften notwendig für PDFs
\usepackage{lmodern}		% Latin Modern
\usepackage{textcomp}           % wird benötigt, damit der \textbullet
                                % für itemize in lmodern gefunden wird.

\usepackage[intlimits]{amsmath}
\usepackage[all,warning]{onlyamsmath}  % warnt bei Verwendung von nicht
                                       % amsmath-Umgebungen z.\,B. $$...$$
\usepackage{amssymb}     % wird für \R, \C,... gebraucht
\usepackage{fixmath}     % ISO-konforme griech. Buchstaben
\usepackage[euro]{isonums} % definiert Komma als Dezimaltrennzeichen

\usepackage[amsmath,thmmarks,hyperref]{ntheorem} % für die Theorem-Umgebungen
                                                 % (satz, defini, bemerk)
\usepackage{svn}         % Zum Auswerten und ordentlichen Darstellen der
                         % SVN-Schlüsselwörter (s. vor \begin{document})
                         % dafür muss in SVN noch das Flag svn:keywords
                         % auf "LastChangedRevision LastChangedDate"
                         % gesetzt werden
\usepackage{ellipsis}    % Korrektur für \dots
\usepackage{fixltx2e}
\usepackage[final,babel]{microtype} % Verbesserung der Typographie

% Damit auch die Zeichen im Mathemode in Überschriften fett sind
% <news:lzfyyvx3pt.fsf@tfkp12.physik.uni-erlangen.de>
\addtokomafont{sectioning}{\boldmath}

\newtheorem{thm}{Satz}[section]

% Hier die Definition, wie \autoref die Umgebungen nennen soll, die mit
% \newtheorem definiert wurden
\newcommand*{\thmautorefname}{Satz}
% Zwischen Unter- und Unterunterabschnitten sollte nicht unterschieden
% werden.
\renewcommand*{\subsectionautorefname}{Abschnitt}
\renewcommand*{\subsubsectionautorefname}{Abschnitt}

\pagestyle{headings}

\newcommand*{\R}{\mathbb{R}}      % reelle Zahlen
\newcommand*{\C}{\mathbb{C}}      % komplexe Zahlen
\newcommand*{\N}{\mathbb{N}}      % natürliche Zahlen
\newcommand*{\Q}{\mathbb{Q}}      % gebrochene Zahlen
\newcommand*{\Z}{\mathbb{Z}}      % ganze Zahlen

\SVN $LastChangedRevision$
\SVN $LastChangedDate$

\begin{document}
\renewcommand{\labelenumi}{\arabic{enumi}.}
\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}}
\renewcommand{\fr}{\mathcal}


%\thispagestyle{empty}
%\underline%
\title{Eine Auswahl wichtiger Definitionen und Aussagen
 zur Vorlesung
\glqq{}Stochastik f"ur Informatiker und Regelschullehrer\grqq{}, WS 08/09}

\author{Werner Linde}
\maketitle

\section{Wahrscheinlichkeiten}
\subsection{Wahrscheinlichkeitsr"aume}

%\item
\subsubsection{Grundraum}
Der \textbf{Grundraum} (meist mit $\Omega$ bezeichnet) ist eine Menge,
die mindestens alle bei einem stochastischen Versuch oder Vorgang
auftretenden Ergebnisse enth"alt. Die Teilmengen von $\Omega$ hei"sen \textbf{Ereignisse},
die einpunktigen Teilmengen nennt man \textbf{Elementarereignisse}.

\subsubsection{Eintreten eines Ereignisses}
Ein Ereignis $A\subseteq \Omega$ \textbf{tritt ein}, wenn das beim Versuch oder dem Vorgang
beobachtete zuf"allige Ergebnis in der Menge
$A$ liegt.

\subsubsection{$\sigma$--Algebra}
Auf dem Grundraum $\Omega$ wird ein System $\fr A\subseteq \fr P(\Omega)$  von Ereignissen
ausgezeichnet, denen man in sinnvoller Weise die
Wahrscheinlichkeit ihres Eintretens zuordnen kann. Aus naheliegenden Gr"unden fordert man,
dass $\fr A$ eine $\mathbf \sigma$--\textbf{Algebra} bildet, d.\,h.~$\fr A$ erf"ullt
folgende Eigenschaften$\colon$
\begin{eqnarray*}
&(i)&\quad\emptyset\in\fr A\;.\\
&(ii)&\quad\mbox{Aus}\quad A\in\fr A\quad\mbox{folgt}\quad A^c\in \fr A\;.\\
&(iii)&\quad
A_1,A_2,\ldots\in \fr A \quad \mbox{impliziert}\quad \bigcup_{j=1}^\infty A_j\in\fr A\;.
\end{eqnarray*}
Ist $\Omega$ h"ochstens abz"ahlbar unendlich, so kann man als $\sigma$--Algebra stets die
Potenzmenge $\fr P(\Omega)$ von $\Omega$ nehmen.
\subsubsection{Wahrscheinlichkeitsma"s}
Ein \textbf{Wahrscheinlichkeitsma"s} (oder eine \textbf{Wahrscheinlichkeitsverteilung})
 $\P$ ist eine Abbildung von $\fr
A$ nach $[0,1]$, die jedem Ereignis $A\in \fr A$ die Wahrscheinlichkeit seines
Eintretens zuordnet und folgende Eigenschaften besitzt$\colon$
\begin{eqnarray*}
&(i)&\quad\mbox{Es gilt}\quad \P(\emptyset)=0\quad\mbox{und}\quad \P(\Omega)=1\;.\\
&(ii)&\quad
\P\;\mbox{ist $\sigma$--additiv, d.\,h.~f"ur disjunkte}\;A_j\in\fr A\;\mbox{folgt}\;
\P\Big(\bigcup_{j=1}^\infty A_j\Big)=\sum_{j=1}^\infty\P(A_j)\;.
\end{eqnarray*}
\subsubsection{Wahrscheinlichkeitsraum}
Das Tripel $(\Omega,\fr A,\P)$ hei"st \textbf{Wahrscheinlichkeitsraum}. Zuf"allige
Experimente werden durch geeignete Wahrscheinlichkeitsr"aume  beschrieben.
\subsubsection{Eigenschaften von Wahrscheinlichkeitsma"sen}
%Wahrscheinlichkeitma"se besitzen
%folgende wichtigen Eigenschaften:
%\begin{thm}~
\begin{enumerate}
\item[\rm(i)]
Jedes Wahrscheinlichkeitsma"s ist auch \textbf{endlich additiv}, d.\,h.~sind
$A_1,\ldots,A_n$ aus $\fr A$ disjunkt, so folgt
$$
\P\Big(\bigcup_{j=1}^n A_j\Big)=\sum_{j=1}^n\P(A_j)\;.
$$
\item[\rm(ii)]
Wahrscheinlichkeitsma"se sind \textbf{monoton}, d.\,h.~gilt f"ur $A,B\in\fr A$ die Inklusion
$A\subseteq B$, so impliziert dies $\P(A)\le\P(B)$.
\item[\rm(iii)]
F"ur $A,B\in\fr A$ mit $A\subseteq B$ folgt $\P(B\setminus A)=\P(B)-\P(A)$. Insbesondere ergibt sich hieraus
$\P(A^c)=\P(\Omega\setminus A)=1-\P(A)$ f"ur $A\in\fr A$.
\item[\rm(iv)]
Wahrscheinlichkeitsma"se sind \textbf{stetig von oben}, d.\,h.~gilt f"ur $A_j\in\fr A$
die Aussage $A_1\supseteq A_2\supseteq\cdots$,
so folgt
$$
\P\Big(\bigcap_{j=1}^\infty A_j\Big)=\lim_{j\to\infty}\P(A_j)\;.
$$
\item[\rm(v)]
Wahrscheinlichkeitsma"se sind auch \textbf{stetig von unten}, d.\,h.~gilt f"ur $A_j\in\fr A$
die Aussage $A_1\subseteq A_2\subseteq\cdots$,
so folgt
$$
\P\Big(\bigcup_{j=1}^\infty A_j\Big)=\lim_{j\to\infty}\P(A_j)\;.
$$
\end{enumerate}
%\end{thm}
%\end{enumerate}
\subsection{Typen von Wahrscheinlichkeitsma"sen}
\subsubsection{Wahrscheinlichkeitsma"se auf h"ochstens abz"ahlbar unendlichen Grundr"aumen}
Bei einem Experiment seien h"ochstens abz"ahlbar unendlich viele Versuchsergebnisse
m"oglich. Dann kann man entweder $\Omega=\{\omega_1,\ldots,\omega_N\}$ oder
aber $\Omega=\{\omega_1,\omega_2,\ldots\}$ w"ahlen. Als $\sigma$--Algebra nimmt man in diesen
F"allen stets die Potenzmenge $\fr P(\Omega)$. Setzt man
\begin{equation}
\label{zu}
p_i :=\P(\{\omega_i\})\,,\quad 1\le i\le N\quad\mbox{bzw.}\quad i=1,2,\ldots,
\end{equation}
dann erh"alt man Zahlen mit den Eigenschaften
\begin{eqnarray}
\label{p1}
&&p_i\ge 0\quad \mbox{und}\\
\label{p2}
&&\sum_{i=1}^N p_i =1\quad \mbox{bzw.}\quad \sum_{i=1}^\infty p_i =1\;.
\end{eqnarray}
F"ur eine Menge $A\subseteq \Omega$ folgt dann
\begin{equation}
\label{diskret}
\P(A):= \sum_{\{i\, : \,\omega_i\in A\}} p_i\;.
\end{equation}
Umgekehrt, gibt man eine Folge $(p_i)_{i\ge 1}$ reeller Zahlen mit \eqref{p1} und \eqref{p2} vor,
so wird durch (\ref{diskret}) ein Wahrscheinlichkeitsma"s $\P$ auf $\fr P(\Omega)$ definiert. F"ur
endliche oder abz"ahlbar unendliche Grundr"aume $\Omega$ hat man also folgende "Aquivalenz$\colon$
$$
\{\P\,:\,\P\;\mbox{Wahrscheinlichkeitsma"s auf}\;\fr P(\Omega)\}
\Longleftrightarrow\{(p_i)_{i\ge 1}\,:\, (p_i)_{i\ge 1}\;\mbox{erf"ullen (\ref{p1}) und (\ref{p2})}\}
$$
Die Zuordnung erfolgt "uber (\ref{zu}) bzw.~(\ref{diskret}).

\subsubsection{Diskrete Wahrscheinlichkeitsma"se}
Sei nunmehr $\Omega$ ein beliebiger Grundraum (nicht notwendig endlich
oder abz"ahlbar unendlich). Ein Wahrscheinlichkeitsma"s $\P$ auf $(\Omega,\fr P(\Omega))$
hei"st \textbf{diskret}, wenn es eine h"ochstens abz"ahlbar unendliche Teilmenge $D\subseteq \Omega$
mit $\P(D)=1$ gibt. Mit $D=\{\omega_1,\omega_2,\ldots\}$ gilt dann f"ur $A\subseteq \Omega$ wie zuvor
$$
\P(A):= \sum_{\{i\, : \,\omega_i\in A\}} p_i\;,
$$
wobei $p_i:=\P(\{\omega_i\})$. Auf h"ochstens abz"ahlbar unendlichen Grundr"aumen ist somit \textbf{jedes}
Wahrscheinlichkeitsma"s diskret.
\subsubsection{Wahrscheinlichkeitsdichten}
Eine st"uckweise stetige Funktion $p\colon\R\mapsto\R$ hei"st
\textbf{Wahrscheinlichkeitsdichte}, wenn
\begin{eqnarray*}
&(i)&\quad p(x)\ge 0\quad\mbox{f"ur}\quad x\in\R\quad \mbox{und}\\
&(ii)&\quad\int_{-\infty}^\infty p(x)\,\mathrm d x=1
\end{eqnarray*}
gelten.
\subsubsection{Borel--$\sigma$--Algebra}
Mit $\fr B(\R)$ bezeichnet man die kleinste $\sigma$--Algebra von Mengen aus $\R$, die
die halboffenen Intervalle enth"alt. Man nennt $\fr B(\R)$ die $\sigma$--Algebra der
\textbf{Borelmengen}. Elemente von $\fr B(\R)$ sind z.\,B. alle offenen oder abgeschlossenen Mengen,
deren abz"ahlbaren Vereinigungen und Durchschnitte usw.
\subsubsection{Stetige Wahrscheinlichkeitsma"se}
Gegeben sei eine Wahrscheinlichkeitsdichte $p$. Dann existiert ein eindeutig bestimmtes
Wahrscheinlichkeitsma"s $\P \colon\fr B(\R)\mapsto [0,1]$ mit
$$
\P([\alpha,\beta])=\P((\alpha,\beta])=\int_\alpha^\beta\,p(x)\,\mathrm dx
$$
f"ur alle reelle Zahlen $\alpha<\beta$. Das so erzeugte Wahrscheinlichkeitsma"s $\P$ hei"st \textbf{stetig}
und $p$ nennt man die \textbf{Dichte} von $\P$. Stetige Wahrscheinlichkeitsma"se beschreiben Vorg"ange,
bei denen "uberabz"ahlbar viele reelle Zahlen als Ergebnis auftreten k"onnen (z.\,B. Lebenszeiten etc.).
\subsection{Die wichtigsten diskreten Wahrscheinlichkeitsverteilungen}

\subsubsection{Einpunktverteilung}
Gegeben sei ein $\omega_0\in \Omega$, fest aber beliebig. Dann wird
durch
$$
\delta_{\omega_0}(A):=\left\{
\begin{array}{rcl}
1 &:& \omega_0\in A\\
0 &:& \omega_0\notin A
\end{array}
\right.
$$
die \textbf{Einpunktverteilung} in $\omega_0$ (oder das \textbf{Diracsche $\mathbf \delta$--Ma"s} in
$\omega_0$) definiert. Der Wahrscheinlichkeitsraum $(\Omega,\fr P(\Omega),\delta_{\omega_0})$
beschreibt Vorg"ange, bei denen mit Wahrscheinlichkeit $1$ genau $\omega_0$ eintritt (deterministische
Vorg"ange).


\subsubsection{Gleichverteilung auf $N$ Punkten}
Gegeben seien $N$ Punkte $\omega_1,\ldots,\omega_N\in\Omega$.
Das Ma"s $\P$ auf $\fr P(\Omega)$ mit
$$
\P:=\frac{1}{N}\sum_{i=1}^N \delta_{\omega_i}
$$
hei"st \textbf{Gleichverteilung} auf $\{\omega_1,\ldots,\omega_N\}$. F"ur ein Ereignis $A$ gilt dann
$$
\P(A)=\frac{\mathrm{card}\{ i\le N : \omega_i\in A\}}{N}=
\frac{\mbox{Anzahl der g"unstigen F"alle f"ur A}}{\mbox{Anzahl der m"oglichen F"alle}}\;.
$$
\subsubsection{Binomialverteilung}
Sei $\Omega=\{0,\ldots,n\}$ und sei $p\in[0,1]$ vorgegeben. Dann wird durch
$$
B_{n,p}(\{k\}):={n\choose k} p^k(1-p)^{n-k},\quad k=0,\ldots,n\;,
$$
ein Wahrscheinlichkeitsma"s $B_{n,p}$ auf $\fr P(\Omega)$ definiert. Man nennt $B_{n,p}$
\textbf{Binomialverteilung} mit den Parametern $n$ und $p$. Die Zahl
$B_{n,p}(\{k\})$ gibt die Wahrscheinlichkeit an, dass man bei $n$ unabh"angigen Versuchen genau $k$--mal
Erfolg hat. Dabei ist die Erfolgswahrscheinlichkeit in jedem einzelnen Versuch $p$, die f"ur
Misserfolg $1-p$ .
\subsubsection{Hypergeometrische Verteilung}
Gegeben seien Zahlen $M,N,n\in\N_0$ mit $M,n\le N$. Dann wird durch
$$
H_{N, M ,n}(\{m\}) :=\frac{{M\choose m}{N-M\choose n-m}}{{N\choose n}}
$$
ein Wahrscheinlichkeitsma"s $H_{N,M,n}$ auf $\fr P(\{0,\ldots,n\})$
definiert. Man nennt $H_{N,M,n}$  \textbf{hypergeometrische Verteilung} mit den Parametern $N,M$ und $n$.
Sind in einer Lieferung von $N$ Ger"aten $M$ St"uck defekt, so beschreibt $H_{N,M,n}(\{m\})$
die Wahrscheinlichkeit, dass man in einer zuf"allig entnommenen Stichprobe vom Umfang $n$ genau
$m$ defekte Ger"ate beobachtet.
\subsubsection{Poissonverteilung}
Es sei $\Omega=\N_0=\{0,1,2,\ldots\}$. F"ur eine Zahl $\lambda>0$ definiert
man die \textbf{Poissonverteilung} mit Parameter $\lambda$ durch
$$
P_\lambda(\{k\}):= \frac{\lambda^k}{k !}\,\mathrm e^{-\lambda}\quad\mbox{f"ur}\quad k\in\N_0\;.
$$
Die Bedeutung der Poissonverteilung ergibt sich aus folgendem Satz$\colon$
\begin{thm}
Gegeben sei eine Zahl $\lambda>0$. F"ur $n\in\N$ setze man
$p_n:=\lambda/n$. Dann folgt f"ur alle $k\in\N_0$
stets
$$
\lim_{n\to\infty} B_{n,p_n}(\{k\})= P_\lambda(\{k\})\;.
$$
\end{thm}
Inhaltlich bedeutet dies$\colon$ F"uhrt man sehr viele unabh"angige Versuche durch ($n$ St"uck), bei denen jeweils
nur mit sehr kleiner
Wahrscheinlichkeit $p$ Erfolg eintreten kann, so ist die Anzahl der insgesamt beobachteten Erfolge
approximativ gem"a"s $P_\lambda$ verteilt, wobei $\lambda= n\cdot p$.
\subsubsection{Geometrische Verteilung}
Bei einem einzelnen Versuch trete Erfolg wieder mit Wahrscheinlichkeit $p$ und Misserfolg
mit Wahrscheinlichkeit $1-p$ auf. Man f"uhrt nun so lange unabh"angige Versuche durch, bis
man erstmals Erfolg beobachtet. Die Wahrscheinlichkeit, dass dies im $(k+1)$--ten Versuch mit
$k\in\N_0$ geschieht, wird durch die \textbf{geometrische Verteilung} mit Parameter $p\in(0,1]$
beschrieben$\colon$
$$
\P(\{k\}):= p\cdot(1-p)^k\,,\quad k\in\N_0\;.
$$
\subsection{Die wichtigsten stetigen Wahrscheinlichkeitsverteilungen}

\subsubsection{Gleichverteilung auf einem Intervall}
Es sei $[a,b]$ ein endliches Intervall. Durch
$$
p(x):=\left\{
\begin{array}{ccl}
\frac{1}{b-a} &:& x\in [a,b]\\
 0 &:& x\notin [a,b]
\end{array}
\right.
$$
wird eine Wahrscheinlichkeitsdichte auf $\R$ definiert. Damit berechnet sich die Wahrscheinlichkeit
eines Intervalls $[\alpha,\beta]$ durch
\begin{equation}
\label{gleich}
\P([\alpha,\beta])=\int_\alpha^\beta p(x)\,\mathrm dx= \frac{\mbox{L"ange von}\;([\alpha,\beta]\cap[a,b])}
{b-a}\;.
\end{equation}
Insbesondere ergibt sich im Fall $[\alpha,\beta]\subseteq [a,b]$ die Formel
$$
\P([\alpha,\beta])= \frac{\beta-\alpha}{b-a}\;,
$$
d.\,h.~die Wahrscheinlichkeit des Eintretens von $[\alpha,\beta]\subseteq[a,b]$
 h"angt nur von seiner L"ange, nicht aber von
seiner speziellen Lage innerhalb $[a,b]$ , ab.
Das durch (\ref{gleich}) erzeugte Wahrscheinlichkeitsma"s hei"st \textbf{Gleichverteilung} auf dem
Intervall $[a,b]$ .
\subsubsection{Exponentialverteilung}
Gegeben sei eine Zahl $\lambda>0$. Man definiert die
\textbf{Exponentialverteilung} $E_\lambda$ mit Parameter $\lambda>0$ durch
ihre Dichte
$$
p(x):=\left\{
\begin{array}{ccr}
\lambda\,\mathrm e^{-\lambda x} &:& x>0\\
0 &:& x\le 0
\end{array}
\right.\;.
$$
F"ur ein Intervall $[\alpha,\beta]\subseteq [0,\infty)$ berechnet sich damit
die Wahrscheinlichkeit seines Eintretens durch
$$
E_\lambda([\alpha,\beta])=\lambda \int_\alpha^\beta \mathrm e^{-\lambda x}\,\mathrm dx
= \mathrm e^{-\lambda\alpha}- \mathrm e^{-\lambda\beta}\;.
$$
\subsubsection{Normalverteilung}
Gegeben seien Zahlen $\mu\in\R$ und $\sigma>0$. Die
Funktion
$$
p_{\mu,\sigma^2}(x):= \frac{1}{\sqrt{2\pi}\sigma}\,\mathrm e^{-(x-\mu)^2/2\sigma^2}\;,
\quad x\in\R\;,
$$
erzeugt ein Wahrscheinlichkeitsma"s $\mathcal N(\mu,\sigma^2)$, das
man \textbf{Normalverteilung} mit Mittelwert $\mu$ und Varianz $\sigma^2$ nennt. Es gilt dann
$$
\mathcal N(\mu,\sigma^2)([\alpha,\beta])=\frac{1}{\sqrt{2\pi}\sigma}
\int_\alpha^\beta \mathrm e^{-(x-\mu)^2/2\sigma^2}\mathrm dx\;.
$$
Im Fall $\mu=0$ und $\sigma=1$ erh"alt man die \textbf{Standardnormalverteilung}
$\mathcal N(0,1)$ . Wahrscheinlichkeiten des Eintretens von Intervallen berechnen sich in diesem Fall durch
$$
\mathcal N(0,1)([\alpha,\beta])=\frac{1}{\sqrt{2\pi}}
\int_\alpha^\beta \mathrm e^{-x^2/2}\mathrm dx\;.
$$

\subsubsection{Gleichverteilung auf einer Menge im $\R^n$}
Es sei $E\subseteq \R^n$ eine beschr"ankte und abgeschlossene
Teilmenge, deren $n$--dimensionales Volumen $\mathrm{vol}_n(E)$ man
berechnen kann. Man definiert die \textbf{Gleichverteilung auf $E$} durch den Ansatz
$$
\P(A)=\frac{\mathrm{vol}_n(A\cap E)}{\mathrm{vol}_n(E)}\;.
$$
Insbesondere ergibt sich f"ur $A\subseteq E$ die Aussage
$$
\P(A)=\frac{\mathrm{vol}_n(A)}{\mathrm{vol}_n(E)}\;,
$$
d.\,h., wie im eindimensionalen Fall h"angt die Wahrscheinlichkeit des Eintretens einer Menge $A\subseteq E$
nur von deren Volumen ab, nicht aber von deren Lage innerhalb $E$ noch von ihrer Gestalt.
\subsection{Verteilungsfunktion}
\subsubsection{Definition}
F"ur ein Wahrscheinlichkeitsma"s $\P$ auf $(\R,\fr B(\R))$ wird die
\textbf{Verteilungsfunktion} $F\colon\R\mapsto\R$ durch
\begin{equation}
\label{F0}
F(t):=\P((-\infty,t])\,,\quad t\in\R  \;,
\end{equation}
definiert.\\
\textit{Hinweis:} Ist $\P$ ein diskretes Wahrscheinlichkeitsma"s auf $(\Omega,\fr P(\Omega))$ mit $\Omega\subseteq\R$,
so modifiziert sich die Definition zu
$$
F(t):=\P((-\infty,t]\cap\Omega)\,,\quad t\in\R  \;.
$$
\subsubsection{Eigenschaften der Verteilungsfunktion}
\begin{thm}
\label{VF}
Die Verteilungsfunktion $F$ eines Wahrscheinlichkeitsma"ses besitzt folgende Eigenschaften$\colon$
\begin{eqnarray*}
%\label{F1}
&(i)&\quad\lim_{t\to -\infty} F(t)=0\quad\mbox{\rm und}\quad\lim_{t\to\infty} F(t)=1\;,\\
%\label{F2}
&(ii)&\quad\mbox{\rm die Funktion}\; F\;\mbox{\rm ist nichtfallend und}\\
%\label{F3}
&(iii)&\quad\mbox{\rm die Funktion}\;
F\;\mbox{\rm ist rechtsseitig stetig}\;.
\end{eqnarray*}
\end{thm}
\subsubsection{Weitere Eigenschaften von Verteilungsfunktionen}
\begin{enumerate}
\item[(a)]
F"ur jedes halboffene Intervall $(\alpha,\beta]$ gilt
$$
\P((\alpha,\beta])= F(\alpha)-F(\beta)\;.
$$
\item[(b)]
Die Funktion $F$ besitzt in einem Punkt $t_0\in\R$ genau dann einen Sprung der H"ohe
$h>0$ (man hat $F(t_0)-F(t_0-0)=h$) , wenn $\P(\{t_0\})=h$ gilt. Insbesondere hat die
Verteilungsfunktion eines diskreten Ma"ses Spr"unge in den Punkten, wo die Masse
des Ma"ses konzentriert ist. Dazwischen ist sie konstant.
\item[(c)]
Ist $F$ Verteilungsfunktion eines stetigen Wahrscheinlichkeitsma"ses $\P$ mit Dichte $p$, so berechnet
sich $F$ aus
$$
F(t)=\int_{-\infty}^t\,p(x)\,\mathrm d x\,,\quad t\in\R\;.
$$
Insbesondere gilt f"ur alle $t\in\R$, in denen $p$ stetig ist, die Gleichung
$$
F'(t)=\left(\frac{\mathrm d F}{\mathrm d t} \right)(t)= p(t)\;.
$$
\end{enumerate}
\subsection{Bedingte Verteilungen}
\subsubsection{Definition}
Es sei $(\Omega,\fr A,\P)$ ein Wahrscheinlichkeitsraum. Dann wird
f"ur $B\in \fr A$ mit $\P(B)>0$ die \textbf{bedingte Wahrscheinlichkeit} $\P(\,\cdot\,|B)$
(oder die Wahrscheinlichkeit von $A$ unter der Bedingung $B$)
durch
\begin{equation}
\label{Bed}
 \P(A|B):=\frac{\P(A\cap B)}{\P(B)}\quad\mbox{f"ur}\;\;A\in\fr A
\end{equation}
definiert. Sie gibt die Wahrscheinlichkeit daf"ur an, dass $A$
eintritt, unter der Bedingung, dass $B$ bereits eingetreten ist. H"aufig verwendet man
Formel (\ref{Bed}) auch in der Form
$$
\P(A\cap B)= \P(B)\,\P(A|B)\;.
$$
\subsubsection{Eigenschaften}
\begin{thm}
Die Abbildung
$$
A\mapsto \P(A|B)
$$
von $\fr A$ nach $[0,1]$ ist ein Wahrscheinlichkeitsma"s mit den zus"atzlichen Eigenschaften
$$
\P(B|B)=1\quad\mbox{und}\quad\P(B^c|B)=0\;.
$$
\end{thm}
\subsubsection{Formel "uber die totale Wahrscheinlichkeit}
\begin{thm}
\label{total}
Gegeben seien disjunkte Mengen $B_1,\ldots,B_n$ in $\fr A$ mit
$\P(B_j)>0$. Dann gilt f"ur $A\in \fr A$ mit $A\subseteq\bigcup_{j=1}^n B_j$
die Aussage
$$
\P(A)=\sum_{j=1}^n\P(B_j)\cdot\P(A|B_j)\;.
$$
\end{thm}
\textit{Bemerkung:} Insbesondere gilt der Satz im Fall $\bigcup_{j=1}^n B_j=\Omega$
f"ur alle $A\in\fr A$.
\subsubsection{Formel von Bayes}
Zur Berechnung von a posteriori Wahrscheinlichkeiten ist die Formel von
Bayes wichtig. Sie besagt das folgende$\colon$
\begin{thm}
Unter den Voraussetzungen aus Satz $\ref{total}$ an $B_1,\ldots,B_n$ und $A$ folgt
f"ur
$\P(A)>0$ die Identit"at
\begin{equation}
\label{Bayes}
\P(B_k|A)=\frac{\P(B_k)\cdot\P(A|B_k)}{\sum_{j=1}^n
\P(B_j)\cdot\P(A|B_j)}\quad\mbox{f"ur}\;\;k=1,\ldots,n\;.
\end{equation}
\end{thm}
\textit{Bemerkung:} Den Nenner in Formel (\ref{Bayes}) kann man (falls bekannt) durch $\P(A)$
ersetzen.
\subsection{Unabh"angigkeit von Ereignissen}
\subsubsection{Unabh"angigkeit von zwei Ereignissen}
Gegeben seien zwei Ereignisse $A,B$ aus einem Wahrscheinlichkeitsraum $(\Omega,\fr A,\P)$.
Dann hei"sen $A$ und $B$ (stochastisch) \textbf{unabh"angig}, wenn
$$
\P(A\cap B)=\P(A)\,\P(B)
$$
gilt.
\subsubsection{Eigenschaften}
Die $\emptyset$ und $\Omega$ sind von jeder Menge $A\in\fr A$ unah"angig. Sind
$A$ und $B$ unah"angig, dann gilt dies auch f"ur die Paare $A$ und $B^c$ bzw.~$A^c$ und $B^c$.
\subsubsection{Unabh"angigkeit von $n$ Ereignissen}
Die Ereignisse $A_1,\ldots,A_n$ aus $\fr A$ hei"sen (stochastisch) \textbf{unabh"angig},
wenn f"ur alle Teilmengen $I\subseteq\{1,\ldots,n\}$ stets
\begin{equation}
\label{unab1}
\P\Bigl(\bigcap_{i\in I} A_i\Bigr)= \prod_{i\in I}\P(A_i)
\end{equation}
gilt. Man kann dies auch wie folgt formulieren$\colon$ F"ur alle $m\ge 2$ und alle
$1\le i_1<\cdots<i_m\le n$ hat man
\begin{equation}
\label{unab2}
\P(A_{i_1}\cap\cdots\cap A_{i_m})= \P(A_{i_1})\cdots\P(A_{i_m})\;.
\end{equation}
Die Ereignisse $A_1,\ldots,A_n$ aus $\fr A$ hei"sen \textbf{paarweise unabh"angig}, wenn
jeweils zwei Ereignisse aus $A_1,\ldots,A_n$ unabh"angig sind, d.\,h.~die Gleichungen
(\ref{unab1}) bzw.~(\ref{unab2}) m"ussen nur f"ur $\mathrm{card}(I)=2$ bzw.~f"ur $m=2$ erf"ullt sein.
\subsubsection{Eigenschaften}
Unabh"angige Mengen $A_1,\ldots,A_n$ sind auch paarweise unabh"angig. Die Umkehrung ist
i.a.~falsch. Ebenso falsch ist, dass aus
$$
\P(A_{1}\cap\cdots\cap A_{n})= \P(A_{1})\cdots\P(A_{n})
$$
stets die Unabh"angigkeit der $A_j$ folgt.

Sind $A_1,\ldots A_n$ unabh"angig, so gilt dies auch f"ur $(A_j)_{j\in J}$  mit
$J\subseteq \{1,\ldots,n\}$.

\section{Zufallsvariable}
\subsection{Definition und Verteilungsgesetz}

\subsubsection{Das vollst"andige Urbild}
F"ur eine Abbildung $X\colon\Omega\mapsto\R$ und eine Teilmenge $B\subseteq\R$ wird das
\textbf{vollst"andige Urbild} von $B$ unter $X$ durch
$$
X^{-1}(B):=\{\omega\in\Omega : X(\omega)\in B\}
$$
definiert. Verk"urzend schreibt man auch $X^{-1}(B)=\;\{X\in B\}$.
\subsubsection{Zuf"allige Gr"o"sen}
Sei $\Omega$ eine Menge, die mit einer $\sigma$--Algebra $\fr A$ versehen ist. Eine Abbildung
$X\colon\Omega\mapsto\R$ hei"st \textbf{zuf"allige Gr"o"se} oder \textbf{reellwertige Zufallsvariable}
oder \textbf{zuf"allige reelle Zahl}, wenn f"ur jedes $t\in\R$ die Menge $\{\omega\in\Omega: X(\omega)\le t\}$
zur $\sigma$--Algebra $\fr A$ geh"ort.\\
\textit{Bemerkung:} In diesem Fall gilt dann auch $X^{-1}(B)\in\fr A$ f"ur jede Borelmenge $B\subseteq \R$.

\subsubsection{Verteilungsgesetz einer zuf"alligen Gr"o"se}
Sei $(\Omega,\fr A,\P)$ ein Wahrscheinlichkeitsraum.
F"ur eine zuf"allige Gr"o"se $X\colon\Omega\mapsto\R$ ist die Abbildung
$\P_X \colon \fr B(\R)\mapsto [0,1]$ mit
$$
\P_X(B)=\P\big(X^{-1}(B)\big)=\P\{\omega\in\Omega : X(\omega)\in B\}=\P(\{X\in B\})= \P(X\in B)
$$
sinnvoll definiert.
\begin{thm}
Die Abbildung $\P_X$ ist ein Wahrscheinlichkeitsma"s auf $(\R,\fr B(\R))$.
\end{thm}
Man nennt $\P_X$ das \textbf{Verteilungsgesetz} von $X$ (bzgl.~$\P$).
\subsubsection{Typen von zuf"alligen Gr"o"sen}
Eine zuf"allige Gr"o"se $X$ hei"st \textbf{diskret}, wenn $\P_X$ ein diskretes
Wahrscheinlichkeitsma"s ist. Damit hat $\P_X$ die Gestalt
$$
\P_X(B)=\sum_{\{i\, : \,x_i\in  B\}} p_i
$$
mit geeigneten $x_i\in\R$ und $p_i\ge 0$. Die $x_i$ sind die m"oglichen Werte von $X$, d.\,h.~es
gilt\\ $\P(X\in\{x_1,x_2,\ldots\})=1$,
und
$$
p_i=\P\{\omega\in\Omega : X(\omega)=x_i\}\;.
$$
Eine zuf"allige Gr"o"se $X$ hei"st \textbf{stetig}, falls $\P_X$ ein stetiges Wahrscheinlichkeitsma"s
ist. Das gilt genau dann, wenn mit einer Wahrscheinlichkeitsdichte $p$ f"ur alle $\alpha<\beta$
die Gleichung
$$
\P_X([\alpha,\beta])=\P\{\omega\in\Omega : \alpha\le X(\omega)\le\beta\}=\int_\alpha^\beta\,p(x)\,
\mathrm d x
$$
erf"ullt ist. Die Funktion $p$ nennt man auch \textbf{Verteilungsdichte} (oder einfach
\textbf{Dichte}) von $X$.
\subsubsection{Speziell verteilte diskrete zuf"allige Gr"o"sen}
Eine zuf"allige Gr"o"se $X$ hei"st \textbf{gleichverteilt}
 auf einer endlichen Menge oder \textbf{binomialverteilt}
oder \textbf{Poissonverteilt} etc., wenn $\P_X$ von diesem Typ ist. In allen diesen F"allen
ist $X$ diskret. Zum Beispiel ist $X$ gem"a"s $B_{n,p}$ verteilt (man schreibt auch
$X\sim B_{n,p}$), falls f"ur $0\le k\le n$ stets
$$
\P_X(\{k\})=\P\{\omega\in\Omega : X(\omega)=k\} = {n \choose k} p^k(1-p)^{n-k}
$$
gilt. Analog ist $X$ gem"a"s $P_\lambda$ verteilt, sofern f"ur $k\in\N_0$
$$
\P_X(\{k\})=\P\{\omega\in\Omega : X(\omega)=k\}=\frac{\lambda^k}{k!}\mathrm e^{-\lambda}\;.
$$
\subsubsection{Speziell verteilte stetige zuf"allige Gr"o"sen}
Eine zuf"allige Gr"o"se $X$ hei"st \textbf{gleichverteilt} auf einem Intervall,
oder \textbf{exponentialverteilt} oder \textbf{normalverteilt} etc., wenn $\P_X$ von diesem Typ
ist.
Alle diese zuf"alligen Gr"o"sen sind stetig.
Zum Beispiel ist $X$ gleichverteilt auf $[a,b]$, falls f"ur alle $\alpha<\beta$
stets
$$
\P_X([\alpha,\beta])=\P\{\omega\in \Omega : \alpha\le X(\omega)\le \beta\}
=\frac{\mbox{L"ange von}\, ([\alpha,\beta]\cap[a,b])}{b-a}
$$
gilt. Oder $X$ ist $\mathcal N(\mu,\sigma^2)$--verteilt (man schreibt $X\sim \mathcal N(\mu,\sigma^2)$),
sofern
$$
\P_X([\alpha,\beta])=\P\{\omega\in \Omega : \alpha\le X(\omega)\le \beta\}=
\frac{1}{\sqrt{2\pi}\sigma}\int_\alpha^\beta \mathrm e^{-(x-\mu)^2/2\sigma^2}\mathrm d x\;.
$$
\subsubsection{Identisch verteilte zuf"allige Gr"o"sen}
Zwei zuf"allige Gr"o"sen $X$ und $Y$ sind \textbf{identisch verteilt}, wenn $\P_X=\P_Y$ gilt,
d.\,h.~f"ur alle $B\in\fr B(\R)$ hat man
$$
\P\{\omega\in \Omega : X(\omega)\in B\}= \P\{\omega\in \Omega : Y(\omega)\in B\}\;.
$$
Man schreibt dann $X\stackrel{d}{=}Y$.
\subsubsection{Verteilungsfunktion einer zuf"alligen Gr"o"se}
Die \textbf{Verteilungsfunktion} $F_X$ einer zuf"alligen Gr"o"se ist die Verteilungsfunktion
ihres Verteilungsgesetzes, d.\,h., es gilt
$$
F_X(t)=\P_X\big((-\infty,t]\big)=\P\{\omega\in\Omega : X(\omega)\le t\}\,,\quad t\in \R\;.
$$
F"ur zwei zuf"allige Gr"o"sen $X$ und $Y$ gilt genau dann $X\stackrel{d}{=}Y$, wenn man
$F_X=F_Y$ hat.

Die Funktion $F_X$ besitzt die Eigenschaften aus Satz \ref{VF}.

\subsection{Zuf"allige Vektoren und Unabh"angigkeit zuf"alliger Gr"o"sen}
\subsubsection{Zuf"allige Vektoren}
Sei $\Omega$ eine Menge mit einer $\sigma$--Algebra $\fr A$. Eine Abbildung $\vec X \colon \Omega\mapsto\R^n$
hei"st ($n$--dimensionaler) \textbf{zuf"alliger Vektor}, wenn seine Koordinatenabbildungen
$X_j \colon\Omega\mapsto \R$ alle zuf"allige Gr"o"sen sind. Dabei sind wie "ublich die $X_j$ durch
$$
\vec X(\omega)=(X_1(\omega),\ldots,X_n(\omega))\;,\quad \omega\in\Omega\,,
$$
definiert.

\subsubsection{Gemeinsames Verteilungsgesetz}
Sei $(\Omega,\fr A,\P)$ ein Wahrscheinlichkeitsraum. Dann definiert
man wie im eindimensionalen Fall das Verteilungsgesetz $\P_{\vec X}$ von $\vec X$
durch
$$
\P_{\vec X}(B):=\P\big(\vec X^{-1}(B)\big)=\P\{\omega\in\Omega : (X_1(\omega),\ldots,X_n(\omega))\in B\}\;.
$$
Im Spezialfall $B=B_1\times\cdots\times B_n$ f"ur Borelmengen $B_j\subseteq\R$ folgt
$$
\P_{\vec X}(B)=\P(X_1\in B_1,\ldots, X_n\in B_n)\;.
$$
Deshalb nennt man $\P_{\vec X}$ auch \textbf{gemeinsames Verteilungsgesetz} der zuf"alligen
Gr"o"sen $X_1,\ldots,X_n$.
\subsubsection{Randverteilungen}
F"ur einen zuf"alligen Vektor $\vec X$ nennt man die Verteilungsgesetze $\P_{X_j}$, $1\le j\le n$,
die \textbf{Randverteilungen} von $\vec X$. Hierbei sind wie zuvor die zuf"alligen Gr"o"sen $X_j$ die
zugeh"origen Koordinatenabbildungen.
\begin{thm}
Die Randverteilungen berechnen sich aus der gemeinsamen Verteilung durch
$$
\P_{X_j}(B)= \P_{\vec X}(\R\times\cdots\times \underbrace{B}_j\times\cdots\times\R)\;,\quad B\in\fr B(\R)\;.
$$
Damit bestimmt die gemeinsame Verteilung die zugeh"origen Randverteilungen.
\end{thm}
\textit{Bemerkung:} Die Umkehrung der obigen Aussage ist i.a.~falsch, d.\,h.~es existieren zuf"allige
Vektoren $\vec X=(X_1,\ldots,X_n)$ und $\vec Y=(Y_1,\ldots, Y_n)$ mit $\P_{X_j}=\P_{Y_j}$,
$1\le j\le n$, aber mit $\P_{\vec X}\not=\P_{\vec Y}$.
\subsubsection{Randverteilungen diskreter Vektoren}
\label{disk}
Wir betrachten hier nur den Fall $n=2$. Ein zuf"alliger $2$--dimensionaler Vektor hat die Gestalt
$(X,Y)$ mit vorgegebenen zuf"alligen Gr"o"sen $X$ und $Y$. Weiterhin seien $X$ und $Y$ diskret und die
Folgen
$(x_i)_{i\ge 1}$ bzw.~$(y_j)_{j\ge 1}$ von reellen Zahlen bezeichnen  die m"oglichen Werte von $X$ bzw.~$Y$.
Dann nimmt der Vektor $(X,Y)$ die Werte $(x_i,y_j)_{i,j\ge 1}$ an und f"ur das Verteilungsgesetz
von $\P_{(X,Y)}$, d.\,h.~die gemeinsame Verteilung von $X$ und $Y$, gilt
$$
\P_{(X,Y)}(B)=\sum_{\{(i,j)\,:\, (x_i,y_j)\in B\}} p_{ij}\,,\quad B\in\fr P(\R^2)\,,
$$
wobei
$$
p_{ij}= \P_{(X,Y)}(\{(x_i,y_j)\})=\P(X=x_i,Y=y_j)\;.
$$
F"ur die Randverteilungen ergibt sich dann
$$
\P_X(B)=\sum_{\{i\,:\,x_i\in B\}} q_i \qquad\mbox{und}\qquad
\P_Y(B)=\sum_{\{j\,:\,y_j\in B\}} r_j\,,\quad B\in\fr P(\R)\,,
$$
mit
$$
q_i=\sum_{j=1}^\infty p_{ij}\qquad\mbox{und}\qquad r_j=\sum_{i=1}^\infty p_{ij}\;\;\;.
$$
\subsubsection{Randverteilungen stetiger Vektoren}
\label{stet}
Zur besseren "Ubersichtlichkeit betrachten wir auch hier nur den Fall $n=2$. Der
$2$--dimensionale Vektor $(X,Y)$ sei wie oben definiert. Diesmal nehmen wir aber an, dass
$\P_{(X,Y)}$ eine Dichte hat, es also eine Funktion $p \colon\R^2\mapsto\R$ gibt, so
dass f"ur alle $\alpha<\beta$ und $\gamma<\delta$ stets
$$
\P_{(X,Y)}\big([\alpha,\beta]\times[\gamma,\delta]\big)
=\P\{\omega\in \Omega : \alpha\le X(\omega)\le\beta,\,\gamma\le Y(\omega)\le\delta\}
=\int_\alpha^\beta\int_\gamma^\delta p(x,y)\,\mathrm d y \,\mathrm d x
$$
gilt. Dann haben $X$ bzw.~$Y$ Verteilungsdichten $q$ und $r$ mit
$$
q(x):=\int_{-\infty}^\infty p(x,y)\,\mathrm d y\qquad\mbox{und}\qquad
r(y):=\int_{-\infty}^\infty p(x,y)\,\mathrm d x\;.
$$
\subsubsection{Unabh"angigkeit von zuf"alligen Gr"o"sen}
Gegeben seien $n$ zuf"allige Gr"o"sen $X_1,\ldots,X_n$ auf $(\Omega,\fr A,\P)$. Gilt f"ur beliebige
Borelmengen $B_1,\ldots,B_n\in\fr B(\R)$ stets
\begin{equation}
\label{unab}
\P(X_1\in B_1,\ldots,X_n\in B_n)=\P(X_1\in B_1)\cdots\P(X_n\in B_n)\;,
\end{equation}
so hei"sen $X_1,\ldots,X_n$ \textbf{unabh"angig}.\\
\textit{Bemerkung 1:} Die Unabh"angigkeit der $X_j$ ist "aquivalent zu folgender Aussage$\colon$ F"ur
beliebige Borelmengen $B_j\in\fr B(\R)$ sind die Ereignisse $\left(X_j^{-1}(B_j)\right)_{j=1}^n$
unabh"angig. Das folgt aus der Tatsache, dass man in Gleichung (\ref{unab}) f"ur gewisse vorgegebene $B_j$ auch die
reellen Zahlen $\R$ einsetzen kann.\\
\textit{Bemerkung 2:} Es reicht aus, wenn Gleichung (\ref{unab}) mit Intervallen $B_j$ der Form
$(-\infty,t_j]$ f"ur alle $t_j\in\R$ gilt. Die zuf"alligen Gr"o"sen $X_1,\ldots,X_n$ sind also
dann und nur dann unabh"angig, wenn f"ur alle $t_j\in\R$ stets
$$
\P(X_1\le t_1,\ldots,X_n\le t_n)=\P(X_1\le t_1)\cdots\P(X_n\le t_n)
$$
folgt.\\
\textit{Bemerkung 3:} Aufgrund von (\ref{unab}) ist die gemeinsame Verteilung von $X_1,\ldots,X_n$
im Fall der Unabh"angigkeit eindeutig durch
ihre Randverteilungen $P_{X_j},$ $1\le j\le n$, bestimmt.
\subsubsection{Spezialf"alle}
Besitzen $X$ und $Y$ die Eigenschaften aus \ref{disk}, so sind $X$ und $Y$ dann und nur  dann
unabh"angig, wenn
$$
p_{ij}=q_i\cdot r_j\,,\quad 1\le i,j<\infty\;.
$$
Im stetigen Fall \ref{stet} sind $X$ und $Y$ genau dann unabh"angig, wenn
$$
p(x,y)=q(x)\cdot r(y)\,,\quad x,y\in\R\;.
$$
\subsection{Rechnen mit zuf"alligen Gr"o"sen}
\subsubsection{Transformationen}
Eine Abbildung $f\colon\R\mapsto\R$ hei"st \textbf{messbar}, wenn f"ur jedes $t\in\R$ die
Menge $\{x\in \R : f(x)\le t\}$ eine Borelmenge ist. Stetige Funktionen, Grenzwerte stetiger
Funktionen oder auch monotone Funktionen besitzen diese Eigenschaft.
\begin{thm}
Sei $X$ eine zuf"allige Gr"o"se und sei $f\colon\R\mapsto\R$ messbar. Dann ist $Y:=f(X)$ ebenfalls
eine zuf"allige Gr"o"se.
\end{thm}
\textit{Allgemeine Aufgabe:} Man bestimme $\P_Y$ mit Hilfe von $\P_X$ und $f$.
Folgendes Beispiel illustriere die Situation$\colon$ Sei $U$ gleichverteilt auf $[0,1]$, so ist mit
$f(s):=1-s$ auch $Y:=f(U)=1-U$ gleichverteilt auf $[0,1]$.
\subsubsection{Simulation stetiger zuf"alliger Gr"o"sen}
Sei $X$ eine stetige zuf"allige Gr"o"se mit Verteilungsfunktion $F_X$. Wir nehmen an,
dass mit zwei Zahlen
$-\infty\le a<b\le\infty$ die Verteilungsfunktion $F_X(a)=0$, $F_X(b)=1$ erf"ulle  und
auf $(a,b)$ streng wachsend sei. Dann
existiert die inverse Funktion von $F_X$, die mit $F_X^{-1}$ bezeichnet wird, und es
gilt $F_X^{-1}\colon(0,1)\mapsto (a,b)$.
\begin{thm}
Sei $U$ eine auf $[0,1]$ gleichverteilte zuf"allige Gr"o"se. Unter den
obigen Voraussetzungen gilt dann f"ur $Y:=F_X^{-1}(U)$ die Aussage
$X\stackrel{d}{=} Y$.
\end{thm}
\textit{Anwendung:} Sind $u_1,\ldots,u_n$ unabh"angig erzeugte reelle Zahlen, die gem"a"s der
Gleichverteilung aus $[0,1]$ gew"ahlt wurden, so sind die Zahlen $x_j:= F_X^{-1}(u_j)$ ebenfalls unabh"angig
und gem"a"s $\P_X$ verteilt.
\subsubsection{Lineare Transformationen}
F"ur reelle Zahlen $a\not=0$ und $b\in\R$ betrachte man die lineare Transformation
$$
Y:=a\,X+ b
$$
einer zuf"alligen Gr"o"se $X$.
\begin{thm}
Im Fall $a>0$ folgt
$$
F_Y(t)=F_X\left(\frac{t-b}{a}\right)\;.
$$
Ist $a<0$, so ergibt sich
$$
F_Y(t)=1-\P\left(X<\frac{t-b}{a}\right)\;,
$$
also
$$
F_Y(t)=1-F_X\left(\frac{t-b}{a}\right)
$$
im Fall stetiger $X$.
\end{thm}
\textit{Folgerung:} Besitzt $X$ die Verteilungsdichte $p$, so hat $Y=a\,X+b$ eine Dichte $q$,
die sich aus $p$ durch
$$
q(t)=\frac{1}{|a|}\;p\left(\frac{t-b}{a}\right)\,,\quad t\in\R\,,
$$
ergibt.
\subsubsection{Addition zuf"alliger Gr"o"sen}
F"ur zwei zuf"allige Gr"o"sen $X$  und $Y$ wird ihre Summe $X+Y$ durch
$$
(X+Y)(\omega):=X(\omega)+Y(\omega)\;,\qquad \omega\in\Omega\,,
$$
definiert.
\begin{thm}
Sind $X$ und $Y$ zuf"allige Gr"o"sen, so gilt dies auch f"ur $X+Y$ .
\end{thm}
Das Verteilungsgesetz der Summe $X+Y$ kann man f"ur unabh"angige zuf"allige Gr"o"sen in einigen
F"allen in einfacher Form angeben.
\begin{thm}~
Es seien $X$ und $Y$ unabh"angige zuf"allige Gr"o"sen.
\begin{enumerate}
\item
Nehmen $X$ und $Y$ Werte in den ganzen Zahlen $\Z$ an, so folgt
$$
\P(X+Y=k)=\sum_{i=-\infty}^\infty\P(X=i)\,\P(Y=k-i)\;,\quad k\in \Z\;.
$$
\item
Besitzen $X$ und $Y$ Werte in $\N_0$, so ergibt sich
$$
\P(X+Y=k)=\sum_{i=0}^k\P(X=i)\,\P(Y=k-i)\;,\quad k\in \N_0\;.
$$
\end{enumerate}
\end{thm}
Im Fall stetiger zuf"alliger Gr"o"sen gilt folgender Satz$\colon$
\begin{thm}
Seien $X$ und $Y$ unabh"angig mit Verteilungsdichten $p$ und $q$. Dann besitzt $X+Y$
die Verteilungsdichte $r$  mit
$$
r(x)=\int_{-\infty}^\infty p(x-y)\,q(y)\,\mathrm d y = \int_{-\infty}^\infty p(y)\,q(x-y)\,\mathrm d y \;.
$$
\end{thm}
Man nennt $r$ die \textbf{Faltung} von $p$ und $q$ und schreibt $r=p*q$.
\subsubsection{Addition speziell verteilter zuf"alliger Gr"o"sen}
Im folgenden seien $X$ und $Y$ stets als unabh"angig vorausgesetzt. Dann gilt$\colon$
\begin{thm}~
\begin{enumerate}
\item[\rm (a)]
Aus $X\sim B_{n,p}$ und $Y\sim B_{m,p}$ folgt $X+Y\sim B_{n+m,p}$  .
\item[\rm (b)]
Aus $X\sim P_\lambda$ und $Y\sim P_\mu$ erh"alt man $X+Y\sim P_{\lambda+\mu}$ .
\item[\rm (c)]
Aus $X\sim\mathcal N(\mu_1,\sigma_1^2)$ und $Y\sim\mathcal N(\mu_2,\sigma_2^2)$ folgt
$X+Y\sim \mathcal N(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$ .
\end{enumerate}
\end{thm}
\subsection{Erwartungswert}
\subsubsection{Erwartungswert diskreter zuf"alliger Gr"o"sen}
Eine zuf"allige Gr"o"se $X$ nehme Werte $x_1,x_2,\ldots$ aus $[0,\infty)$ an. Dann
definiert man den \textbf{Erwartungswert} von $X$ durch
$$
\E X :=\sum_{i=1}^\infty x_i\,\P(X=x_i)\;.
$$
Es gilt dann $0\le \E X \le \infty$ .

Sind nunmehr die Werte von $X$ beliebige reelle Zahlen (nicht notwendig $\ge 0$)
, so sagt man, dass $X$ einen \textbf{Erwartungswert
besitzt}, wenn
$$
\sum_{i=1}^\infty |x_i|\,\P(X=x_i)<\infty\;.
$$
In diesem Fall ist der Erwartungswert von $X$ mit
$$
\E X :=\sum_{i=1}^\infty x_i\,\P(X=x_i)
$$
eine wohldefinierte reelle Zahl.

\subsubsection{Erwartungswert stetiger zuf"alliger Gr"o"sen}
Sei $p$ die Verteilungsdichte einer zuf"alligen Gr"o"se $X$. Dann \textbf{besitzt} $X$ einen
\textbf{Erwartungswert}, wenn
$$
\int_{-\infty}^\infty |x|\,p(x)\,\mathrm   d x <\infty\;,
$$
und man definiert den \textbf{Erwartungswert} von $X$ durch
$$
\E X := \int_{-\infty}^\infty x \,p(x)\,\mathrm   d x\;.
$$
\subsubsection{Beispiele zur Berechnung von Erwartungswerten}
\medskip

{\renewcommand{\arraystretch}{1.4}
\begin{center}
\begin{tabular}{|c|l|}\hline
\bf Verteilung von $X$& \bf Erwartungswert von $X$\\ \hline\hline
$X$ gleichverteilt auf $x_1,\ldots,x_N$& $\E X=\frac{1}{N}\sum_{i=1}^N x_i$\\ \hline
$X\sim B_{n,p}$& $\E X= n p$\\ \hline
$X\sim P_\lambda$&$\E X= \lambda$\\ \hline
$X$ geometrisch verteilt mit Parameter $p$& $\E X = \frac{1-p}{p}$\\ \hline
$X$ gleichverteilt auf $[a,b]$& $\E X= \frac{a+b}{2}$\\ \hline
$X\sim E_\lambda$& $\E X = \frac{1}{\lambda}$\\ \hline
$X\sim \mathcal N(\mu,\sigma^2)$&$ \E X = \mu$\\ \hline
\end{tabular}
\end{center}
}
\subsubsection{Eigenschaften des Erwartungswertes}
Der Erwartungswert einer zuf"alligen Gr"o"se hat folgende Eigenschaften$\colon$
\begin{thm}~
\begin{enumerate}
\item
Der Erwartungswert ist linear, d.\,h.~f"ur alle $a,b\in\R$ und zuf"allige Gr"o"sen $X$ und $Y$
gilt
$$
\E(a X+ b Y) = a\,\E X + b\, \E Y\;.
$$
\item
Sei  $X$ diskret mit m"oglichen Werten $x_1,x_2,\ldots$ aus $\R$ .
Dann existiert f"ur eine Funktion $f \colon\R\mapsto\R$
der Erwartungswert $\E f(X)$ genau dann, wenn
$$
\sum_{i=1}^\infty |f(x_i)|\,\P(X=x_i)<\infty\;,
$$
und es gilt
$$
\E f(X)=\sum_{i=1}^\infty f(x_i)\,\P(X=x_i)\;.
$$
\item
Ist $X$ stetig mit Verteilungsdichte $p$ , so existiert f"ur eine messbare
Abbildung $f \colon\R\mapsto\R$ genau dann der Erwartungswert von $f(X)$ , wenn
$$
\int_{-\infty}^\infty |f(x)|\,p(x)\,\mathrm d x<\infty\;,
$$
und man hat
$$
\E f(X)=\int_{-\infty}^\infty  f(x) \,p(x)\,\mathrm d x\;.
$$
\item
Sind $X$ und $Y$ unabh"angige zuf"allige Gr"o"sen deren Erwartungswert existiert,
so existiert auch der Erwartungswert von $X\cdot Y$, und es gilt
$$
\E(X\cdot Y)= \E X \cdot \E Y\;.
$$
\end{enumerate}
\end{thm}

\subsection{Varianz und Kovarianz}
\subsubsection{Momente}
Sei $n\in\N$. Eine zuf"allige Gr"o"se $X$ besitzt ein \textbf{$n$--tes Moment}, wenn
$\E|X|^n<\infty$ . Im diskreten Fall bedeutet dies
$$
\sum_{i=1}^\infty |x_i|^n\P(X=x_i)<\infty
$$
und im stetigen
$$
\int_{-\infty}^\infty |x|^n\,p(x)\,\mathrm d x<\infty\;.
$$
Insbesondere hat $X$ ein erstes Moment, genau dann, wenn $\E X$ existiert.
\begin{thm}
Sei $1\le m\le n$. Hat eine zuf"allige Gr"o"se $X$ ein $n$--tes Moment, so besitzt sie auch ein $m$--tes Moment.
Insbesondere hat jede zuf"allige Gr"o"se mit zweitem Moment einen Erwartungswert.
\end{thm}
\subsubsection{Varianz}
Es sei $X$ eine zuf"allige Gr"o"se mit zweitem Moment. Sei $a:=\E X$. Dann definiert man
die \textbf{Varianz} (oder \textbf{Streuung}) von $X$ durch
$$
\V X:= \E(X-a)^2\;.
$$
Die Varianz gibt den mittleren quadratischen Abstand einer zuf"alligen Gr"o"se $X$ von ihrem
Erwartungswert an. Sie ist ein Ma"s daf"ur, wie sehr die Werte von $X$ um $\E X$ schwanken.

\subsubsection{Eigenschaften der Varianz}
Im folgenden seien $X$ und $Y$ zuf"allige Gr"o"sen mit zweiten Momenten. Dann gelten die
folgenden Aussagen$\colon$
\begin{thm}~
\begin{enumerate}
\item
Mit $a:=\E X$ berechnet sich die Varianz f"ur diskrete zuf"allige Gr"o"sen in der Form
$$
\V X = \sum_{i=1}^\infty (x_i-a)^2\,\P(X=x_i)\;,
$$
und im stetigen Fall hat man
$$
\V X =\int_{-\infty}^\infty (x-a)^2\,p(x)\,\mathrm d x\;.
$$
\item
Es besteht die Identit"at
$$
\V X = \E X^2 -(\E X)^2\;.
$$
\item
F"ur eine konstante zuf"allige Gr"o"se $X$ folgt $\V X=0$ .
\item
F"ur $\alpha\in\R$ erh"alt man
$$
\V(\alpha\,X)=\alpha^2\,\V X\;.
$$
\item
Sind $X$ und $Y$ unabh"angig, dann gilt
$$
\V(X+Y)=\V X + \V Y\;.
$$
\end{enumerate}

\end{thm}
\subsubsection{Beispiele zur Berechnung von Varianzen}

\medskip

{\renewcommand{\arraystretch}{1.4}
\begin{center}
\begin{tabular}{|c|l|}\hline
\bf Verteilung von $X$& \bf Varianz von $X$\\ \hline\hline
$X$ gleichverteilt auf $x_1,\ldots,x_N$& $\V X=\frac{1}{N}\sum_{i=1}^N (x_i-\E X)^2$\\ \hline
$X\sim B_{n,p}$& $\V X= n\,p\,(1-p)$\\ \hline
$X\sim P_\lambda$&$\V X= \lambda$\\ \hline
$X$ geometrisch verteilt mit Parameter $p$& $\V X = \frac{1-p}{p^2}$\\ \hline
$X$ gleichverteilt auf $[a,b]$& $\V X= \frac{(b-a)^2}{12}$\\ \hline
$X\sim E_\lambda$& $\V X = \frac{1}{\lambda^2}$\\ \hline
$X\sim \mathcal N(\mu,\sigma^2)$&$ \V X = \sigma^2$\\ \hline
\end{tabular}
\end{center}
}
\subsubsection{Kovarianz}
Gegeben seien zwei zuf"allige Gr"o"sen $X$ und $Y$ mit zweiten Momenten.
Seien $a:=\E X$ und $b:=\E Y$ . Dann wird die \textbf{Kovarianz} von $X$ und $Y$ durch
$$
{\rm  cov}(X,Y):= \E(X-a)(Y-b)
$$
definiert.

\textbf{Eigenschaften:}

\begin{enumerate}
\item
Sind $X$ und $Y$ diskret mit m"oglichen Werten $x_1,x_2,\ldots$ bzw.~$y_1,y_2,\ldots$ aus $\R$,
so berechnet sich die Kovarianz aus der Formel
$$
{\rm  cov}(X,Y)=\sum_{i,j=1}^\infty (x_i-a)(y_j-b)\, p_{ij}
$$
wobei
$$
p_{ij}=\P(X=x_i, Y=y_j)\;.
$$
\item
Hat die Verteilung des zuf"alligen Vektors $(X,Y)$ eine Dichte $p \colon\R^2\mapsto\R$, so
ergibt sich die Kovarianz von $X$ und $Y$ aus
$$
{\rm  cov}(X,Y)=\int_{-\infty}^\infty \int_{-\infty}^\infty (x-a)(y-b)\,p(x,y)\,\mathrm d x\, \mathrm d y\;.
$$
\item
Sind $X$ und $Y$ unabh"angig, so impliziert dies ${\rm  cov}(X,Y)=0$, d.\,h.~$X$ und $Y$ sind
\textbf{unkorreliert}. Man beachte, dass aus der Unkorreliertheit i.a.~nicht die Unabh"angigkeit folgt.
\item
Man hat
\begin{equation}
\label{cov}
|{\rm  cov}(X,Y)|\le (\V X)^{1/2}(\V Y)^{1/2}\;.
\end{equation}
\subsubsection{Korrelationskoeffizient}
F"ur zwei zuf"allige Gr"o"sen $X$ und $Y$ mit zweiten Momenten definiert man ihren
\textbf{Korrelationskoeffizienten} durch
$$
\rho(X,Y):=\frac{{\rm  cov}(X,Y)}{(\V X)^{1/2}(\V Y)^{1/2}}\;.
$$
Aus (\ref{cov}) folgt
$$
-1\le \rho(X,Y)\le 1\;.
$$
F"ur unkorrelierte zuf"allige Gr"o"sen gilt $\rho(X,Y)=0$.
 Der Korrelationskoeffizient ist ein Ma"s f"ur den Grad der Abh"angigkeit von
$X$ und $Y$. Je n"aher $\rho(X,Y)$ an $1$ oder $-1$ liegt, desto gr"o"ser ist die
Abh"angigkeit zwischen $X$ und $Y$. Im st"arksten Fall
der Abh"angigkeit von $X$ und $Y$, n"amlich $Y=X$ bzw.~$Y=-X$, hat man $\rho(X,X)=1$ bzw.~$\rho(X,-X)=-1$ .
\end{enumerate}

\end{document}






\item
F"ur $B\in \fr A$ mit $\P(B)>0$ ist die {\it bedingte
Wahrscheinlichkeit} $\P(\,.\,|B)$ durch
 \[\P(A|B)=\frac{\P(A\cap B)}{\P(B)}\quad\mbox{f"ur}\;\;A\in\fr A\]
definiert. Sie gibt die Wahrscheinlichkeit daf"ur an, da"s $A$
eintritt, unter der Bedingung, da"s $B$ bereits eingetreten ist. Man
beachte, da"s $\P(\,.\,|B)$ ebenfalls ein Wahrscheinlichkeitsma"s
auf $(\Omega,\fr A)$ ist, allerdings auf $B$ konzentriert.
\item
Die Formel "uber die {\it totale Wahrscheinlichkeit} besagt, f"ur
beliebige disjunkte Mengen $B_1,\ldots,B_n$ in $\fr A$ mit
$\P(B_j)>0$ und f"ur $A\in \fr A$ mit $A\subseteq\bigcup_1^n B_j$
folgt stets
\[\P(A)=\sum_{j=1}^n\P(B_j)\cdot\P(A|B_j)\;.\]
\item
Unter obigen Voraussetzungen an $B_1,\ldots,B_n$ und $A$ mit
$\P(A)>0$ lautet die {\it Formel von Bayes} wie folgt:
\[\P(B_k|A)=\frac{\P(B_k)\cdot\P(A|B_k)}{\sum_{j=1}^n
\P(B_j)\cdot\P(A|B_j)}\quad\mbox{f"ur}\;\;k=1,\ldots,n\;.\]
\item
Mengen $A_1,\ldots,A_n\in\fr A$ hei"sen ({\it stochastisch}) {\it
unabh"angig}, wenn
\[\P(A_{i_1}\cap\cdots\cap A_{i_m})=\prod_{j=1}^m\P(A_{i_j})\quad
\mbox{f"ur alle}\;\; 1\le i_1<i_2<\cdots<i_m\le n\]
 gilt. Falls dies nur f"ur $m=2$ richtig ist, so hei"sen
$A_1,\ldots,A_n$ {\it paarweise unabh"angig}.
\item
Sei $\P$ ein Wahrscheinlichkeitsma"s auf $\Omega$ und existiert eine
h"ochstens abz"ahlbar unendliche Menge
$D=\{\omega_1,\omega_2,\ldots\} \subseteq\Omega$ mit $\P(D)=1$, so
nennt man $\P$ {\it diskret}. $\P$ wird dann eindeutig durch die
Folge $(p_i)_{i\ge 1}$ mit
$$
\P(\omega_i):=\P(\{\omega_i\})=p_i,\quad i=1,2,\ldots,
$$
bestimmt, denn man hat $\displaystyle\P(A)=\sum_{\omega_i\in A}
p_i$, und f"ur die Folge $(p_i)_{i\ge 1}$ gilt $p_i\ge 0$ sowie
$\sum_{i\ge 1} p_i=1$.
\item
Sei $\Omega\subseteq\R^n$ eine offene oder abgeschlossene Menge und
$p:\Omega\to[0,\infty)$ eine stetige Funktion mit $\int_\Omega
p(x)dx =1$, so wird durch
\[\P(A):=\int_A p(x)dx\]
ein Wahrscheinlichkeitsma"s $\P$ auf $\Omega$ (oder $\R^n$)
definiert. Ein so erzeugtes Wahrscheinlichkeitsma"s hei"st {\it
stetig} und $p$ nennt man die ({\it Wahrscheinlichkeits}--) {\it
Dichte} von $\P$.
\item
F"ur ein Wahrscheinlichkeitsma"s $\P$ auf $\R$ ist
\[\F(x):=\P((-\infty,x]),\quad x\in\R\]
die {\it Verteilungsfunktion} von $\P$. Sie bestimmt $\P$ eindeutig,
und es gilt
$$
p(x)=\F'(x)\;,
$$
sofern $\P$ die Dichte $p$ auf $\R$ besitzt.
\end{enumerate}
\item
{\large Zufallsvariable}
\begin{enumerate}
\item
Sei $(\Omega,\fr A,\P)$ ein Wahrscheinlichkeitsraum. Eine Abbildung
$X:\Omega\to\R$ hei"st {\it zuf"allige Gr"o"se} (ZG), wenn f"ur alle
$B\in\fr B(\R)$ stets das Ereignis "'$X\in B$"' zu $\fr A$ geh"ort,
d.h.~es gilt
\[(X\in B)=\{\omega\in\Omega:\,X(\omega)\in B\}=X^{-1}(B)\in\fr A\]
f"ur alle $B\in\fr B(\R)$. Hierbei bezeichnet $\fr B(\R)$ die
$\sigma$--Algebra der Borelmengen auf $\R$, d.h.~die kleinste, die
Intervalle auf $\R$ enthaltende, $\sigma$--Algebra.
\item
Eine Abbildung $X:\Omega\to\R$ ist genau dann eine ZG, wenn f"ur
alle $t\in \R$ die Ereignisse $(X\le t):=\{\omega\in\Omega:
X(\omega)\le t\}$ zu $\fr A$ geh"oren, ihnen also die
Wahrscheinlichkeit ihres Eintretens zugeordnet werden kann.
\item
Sei $X$ eine auf $(\Omega,\fr A,\P)$ definierte ZG. Dann wird durch
\[\P_X(B)=\P(X^{-1}(B))=\P(X\in B)\]
ein Wahrscheinlichkeitsma"s $\P_X$ auf $(\R,\fr B(\R))$ definiert,
das man {\it Verteilungsgesetz} der ZG $X$ nennt.
\item
Zwei ZG $X$ und $Y$ hei"sen {\it identisch verteilt} (Schreibweise:
$X\stackrel{d}{=}Y$), wenn $\P_X=\P_Y$ gilt. Dabei m"ussen $X$ und
$Y$ nicht notwendigerweise auf dem selben Wahrscheinlichkeitsraum
definiert sein.
\item
Eine Abbildung $\vec X =(X_1,\ldots,X_n)$ von $\Omega$ in $\R^n$
hei"st $n$--{\it dimensionaler zuf"alliger Vektor}, wenn alle
Abbildungen $X_1,\ldots,X_n:\Omega\to\R$ zuf"allige Gr"o"sen sind.
\item
Das Verteilungsgesetz $\P_{\vec X}$ eines zuf"alligen Vektors $\vec
X=(X_1,\ldots,X_n)$, definiert durch $\P_{\vec X}(B)=\P(\vec X\in
B)$ f"ur $B\in \fr B(\R^n),$ nennt man auch {\it gemeinsame
Verteilung} der ZG $X_1,\ldots,X_n$, und $\P_{X_1},\ldots,\P_{X_n}$
sind dann die {\it Randverteilungen} von $\P_{\vec X}$. Man beachte,
da"s die gemeinsame Verteilung durch
\[\P_{\vec X}(B_1\times\cdots\times B_n)=\P(X_1\in B_1,\ldots,X_n\in B_n)\]
f"ur $B_j\in\fr B(\R)$ eindeutig bestimmt wird. Kenntnis der
gemeinsamen Verteilung f"uhrt zur Kenntnis der Randverteilungen. Die
Umkehrung ist aber i.a.~falsch; nur im Fall unabh"angiger ZG kann
man aus den Randverteilungen auf die gemeinsame Verteilung
schlie"sen.
\item
Eine ZG $X$ hei"st {\it diskret}, wenn $\P_X$ diskret ist, und {\it
stetig} f"ur $\P_X$ mit Dichte $p$ ($p$ hei"st auch {\it
Verteilungsdichte} von $X$).
\item
F"ur eine ZG $X$ ist ihre {\it Verteilungsfunktion} $\F_X$ als
Abbildung von $\R$ nach $[0,1]$ durch
\[\F_X(x)=\P(X\le x)=\P(\omega\in\Omega:\;X(\omega)\le x)\]
definiert. $\F_X$ ist monoton wachsend und rechtsseitig stetig.
$\F_X$ ist genau dann stetig, wenn $\P(X=x)=0$ f"ur alle $x\in\R$
gilt. Ein Sprung von $\F_X$ der H"ohe $h$ in $x_0$ ist
gleichbedeutend mit $\P(X=x_0)=h$.
\item
Zuf"allige Gr"o"sen $X_1,\ldots,X_n:\Omega\to\R$ sind ({\it
stochastisch}) {\it unabh"angig}, wenn f"ur alle $B_j\in\fr B(\R)$
die Ereignisse $X_1\in B_1,\ldots,X_n\in B_n$ in $\Omega$
stochastisch unabh"angig sind. Das ist gleichbedeutend mit
\[ (\ast)\quad \P(X_1\in B_1,\ldots,X_n\in B_n)=\prod_{j=1}^n\P(X_j\in
B_j)\] f"ur alle $B_j\in\fr B(\R)\,,\;j=1,\ldots,n\;.$
\item
Sind $X_1,\ldots,X_n$ ZG und besitzt der zuf"allige Vektor $\vec
X=(X_1,\ldots,X_n)$ eine Dichte $p$, so sind $X_1,\ldots,X_n$ dann
und nur dann unabh"angig, wenn
\[p(x_1,\ldots,x_n)=p_1(x_1)\cdots p_n(x_n)\quad\mbox{f"ur}\quad
(x_1,\ldots,x_n)\in\R^n\] gilt, wobei $p_1,\ldots,p_n$ die
(existierenden) Dichten der Randverteilungen $\P_{X_j}$
sind.\newline Im Fall von $\P_{\vec X}$ diskret, gen"ugt es $(\ast)$
f"ur Elementarereignisse mit positiver Wahrscheinlichkeit
nachzuweisen.
\item
F"ur unabh"angige ZV $X_1$ und $X_2$ mit Werten in $\N_0$ berechnet
sich das Verteilungsgesetz $\P_{X_1+X_2}$ durch
\[\P_{X_1+X_2}(k)=\sum_{j=0}^k \P(X_1=j)\cdot\P(X_2=k-j)
\;.\] Sind $X_1$ und $X_2$ unabh"angig und bilden nach $\R$ ab, so
folgt aus der Existenz von Dichten $p_1$ und $p_2$, da"s auch
$X_1+X_2$ eine Dichte $p$ besitzt, und diese berechnet sich als
Faltung von $p_1$ und $p_2$, d.h.
\[p(x)=(p_1\ast p_2)(x)=\int_{\R}p_1(y) p_2(x-y)dy=
\int_{\R}p_1(x-y)p_2(y)dy\quad\mbox{f"ur}\;\;x\in\R\;.\]
\end{enumerate}
\item
{\large Erwartungswert und Varianz}
\begin{enumerate}
\item
Nimmt eine ZG $X$ die reellen Zahlen $x_1,x_2,\ldots$ mit
Wahrscheinlichkeit\\
$p_j\ge 0,\;\sum_{j=1}^\infty p_j=1,$ an, und gilt
$\sum_{j=1}^\infty p_j |x_j|<\infty$, so besitzt $X$ einen {\it
Erwartungswert} ${\E}\,X$, der durch
\[{\E}\,X=\sum_{j=1}^\infty p_j x_j =\sum_{j=1}^\infty x_j\,\P(X=x_j)\]
definiert ist.
\item
Besitzt $\P_X$ eine Verteilungsdichte $p$, so existiert f"ur
$\int_{-\infty}^\infty |x|p(x)dx<\infty$ der Erwartungswert von $X$,
und man hat
\[{\E}\,X=\int_{-\infty}^\infty x\, p(x)dx\,.\]
\item
F"ur eine ZG $X$ mit Werten in $[0,\infty)$ (diskret oder stetig)
berechnet sich ihr Erwartungswert ${\E}\,X$ durch
\[{\E}\,X=\int_0^\infty \P(X>t)dt= \int_0^\infty \P(X\ge t)dt\;.\]
$X$ hat einen endlichen Erwartungswert, genau dann, wenn das obige
Integral endlich ist.
\item
Die wichtigsten Eigenschaften des Erwartungswertes sind:
\begin{eqnarray*}
&&X\stackrel{d}{=} Y\quad \mbox{impliziert}\quad \E\, X= \E\, Y\;.\\
&&\mbox{F"ur}\quad f:\R\to\R\quad \mbox{hat man}\\
&&\E\, f(X)=\sum_{j=1}^\infty f(x_j)\P(X=x_j),\quad X\;\mbox{diskret, bzw.}\qquad\\
&&\E\, f(X)=\int_{-\infty}^\infty f(x) p(x) dx,\quad X\;\mbox{stetig
und}
\; f\;\mbox{integrabel}.\\
&& \E\,(aX+b)=a\E\,(X)+b,\quad a,b\in\R.\\
&&\E\,(X+Y)=\E\,(X)+\E\,(Y).\\
&& X,Y\quad\mbox{unabh"angig impliziert}\quad \E\,(X\cdot
Y)=\E\,(X)\cdot \E\,(Y).
\end{eqnarray*}
\item
Die ZG $X$ besitzt ein $n$--{\it tes Moment}, wenn
$\E\,|X|^n<\infty$ gilt. Aus der Existenz des $n$-ten Momentes folgt
die Existenz aller $k$--ten Momente mit $k\le n$. Das $n$--te Moment
${\E}\,X^n$ berechnet sich dann als
\[{\E}\,X^n=\sum_{j=1}^\infty x_j^n\; \P(X=x_j)\]
f"ur diskrete ZG und als
\[{\E}\,X^n=\int_{-\infty}^\infty x^n p(x)dx\]
f"ur ZG mit Dichte $p$.
\item
Sind $X$ und $Y$ zwei reellwertige ZG mit 2.~Moment, so ist ihre
{\it Kovarianz}
 ${\rm cov}(X,Y)$ durch ${\rm cov}(X,Y)=\E\,
[(X-\E\,X)(Y-\E\,Y)]$ definiert. F"ur unabh"angige ZG $X$ und $Y$
gilt ${\rm cov}(X,Y)=0$. Zwei ZG $X$ und $Y$ mit ${\rm cov}(X,Y)=0$
hei"sen {\it unkorelliert}. Die Unkorreliertheit ist eine
schw"achere Eigenschaft als die Unabh"angigkeit.
\item
F"ur eine ZG $X$ mit 2.~Moment wird die {\it Varianz} $\V(X)$ durch
\[\V(X)={\rm cov}(X,X)={\E}\big(X-{\E}\,X\big)^2
={\E}\,X^2-({\E}\,X)^2\] definiert. Wichtige  Eigenschaften der
Varianz sind $\V(a X+b)= a^2\V(X)$ f"ur $a,b\in\R$, und f"ur
unabh"angige ZG $X$ und $Y$ mit 2.~Moment besteht die Gleichheit
$\V(X+Y)= \V(X)+\V(Y)$.
\end{enumerate}
\item
{\large Wichtige diskrete Verteilungsgesetze}
\begin{enumerate}
\item
Sei $\Omega$ eine Menge von $N$ Elementen und gilt f"ur ein
Wahrscheinlichkeitsma"s $\P$ auf $\Omega$ die Aussage
$\P(\{\omega\})=1/N$ f"ur alle $\omega\in \Omega$, so hei"st $\P$
{\it Gleichverteilung}  oder  {\it klassische Wahrscheinlichkeit}
auf $\Omega$. F"ur eine Menge $A\subseteq \Omega$ folgt dann
$\P(A)=\#(A)/N$ (Anzahl der g"unstigen F"alle dividiert durch die
Anzahl aller F"alle).\newline Bemerkung: Ist eine ZG $X$
gleichverteilt auf $\{x_1,\ldots,x_N\}\subseteq\R$, d.h.~$\P_X$ ist
die Gleichverteilung auf dieser Menge, so gilt ${\E}\,X=\sum_{j=1}^N
x_j/N$ (arithmetisches Mittel) und $\V(X)=\left((N-1)\sum_{j=1}^N
x_j^2 -\sum_{i\not=j}^N x_ix_j\right)/N^2\;.$
\item
Ist eine ZG $X$ mit Wahrscheinlichkeit $1$ konstant, d.h.~man hat
f"ur ein $x_0\in \R$ die Aussage $\P(X=x_0)=1$, so ist $\P_X$ die
$\delta$--{\it Verteilung} $\delta_{x_0}$ im Punkt $x_0$,
d.h.~$\delta_{x_0}(B)=0$ f"ur $x_0\notin B$ und $\delta_{x_0}(B)=1$
f"ur $x_0\in B$. \newline Bemerkung: Es gilt ${\E}\,X =x_0$ und
$\V(X)=0$.
\item
F"ur $0<p<1$ und $n\in\N$ ist das Wahrscheinlichkeitsma"s $B_{n,p} $
durch
\[B_{n,p}(k)={n\choose k} p^k(1-p)^{n-k},\quad k\in\{0,\ldots,n\},\]
definiert. $B_{n,p}$ hei"st {\it Binomialverteilung} mit den
Parametern $n$ und $p$.
Speziell gilt $B_{1,p}(0)=1-p$ und $B_{1,p}(1)=p$, d.h.~%
$B_{1,p}=(1-p)\delta_0+p\delta_1$ ist eine Zweipunktverteilung in
den Punkten 0 und 1 . Ist eine ZG $X$ gem"a"s $B_{n,p}$ verteilt,
d.h.~man hat $\P_X=B_{n,p}$, so gilt ${\E}\,X =np$ und
$\V(X)=np(1-p)$.\newline Bemerkung 1: $B_{n,p}(k)$ ist die
Wahrscheinlichkeit daf"ur, da"s bei $n$ unabh"angigen Versuchen,
wobei jeder mit Wahrscheinlichkeit $p$ den Wert $1$ liefert, und mit
Wahrscheinlichkeit $1-p$ den Wert $0$, genau $k$--mal eine $1$
auftritt. Anders ausgedr"uckt, sind $X_1,\ldots,X_n$ unabh"angig und
nach  $B_{1,p}$ verteilt, so ist $X_1+\cdots+X_n$ gem"a"s $B_{n,p}$
verteilt. \newline Bemerkung 2: Allgemein gilt: Sind $X$ gem"a"s
$B_{n,p}$ und $Y$ gem"a"s $B_{m,p}$ verteilt und unabh"angig, so ist
$X+Y$ nach $B_{n+m,p}$ verteilt.
\item
Gilt f"ur eine ZG $X$ die Aussage
\[\P(X=k)=\frac{{M\choose k}{N-M\choose n-k}}{{N\choose n}}\]
f"ur $k\in\N_0$ mit $\max\{0,M-N+n\}\le k\le \min\{M,n\}$ und
$\P(X=k)=0$ sonst, so ist $X$ {\it hypergeometrisch} verteilt mit
den Parametern $N\in\N$, $M\in {\N}_0$ und $n\in\{0,\ldots,N\}$. Man
hat dann ${\E}\,X =n\cdot\frac{M}{N}$ und
$\V(X)=n\cdot\frac{M}{N}\cdot\left(1-\frac{M}{N}
\right)\cdot\frac{N-n}{N-1}$.\newline Bemerkung: Sind in einer Urne
mit $N$ Kugeln genau $M$ wei"se Kugeln, so ist die Anzahl der
wei"sen Kugeln in einer Stichprobe vom Umfang $n$ hypergeometrisch
mit den Parametern $N,M,n$ verteilt.
\item
Es erscheine bei einem stochastischem Versuch die Zahl "'1"' mit
Wahrscheinlichkeit $p$ und "'0"' mit Wahrscheinlichkeit $1-p$. Fragt
man danach, wie gro"s die Wahrscheinlichkeit daf"ur ist, da"s bei
unabh"angigen Versuchen dem ersten Auftreten einer $1$ genau $k$
Nullen vorausgegangen sind, so ist das gleichbedeutend mit der Frage
nach der Wahrscheinlichkeit daf"ur, da"s genau beim $(k+1)$-ten
Versuch erstmals eine $1$ erscheint. Dann gilt
\[\P\big(\,\mbox{"'1"' erscheint erstmals im $(k+1)$--ten Versuch}\,\big)
=p\, (1-p)^{k}\,.\] Eine ZG $X$ hei"st nun {\it geometrisch}
verteilt mit Parameter $p\in(0,1)$, wenn
\[\P(X=k)= p\,(1-p)^{k}\quad\mbox{f"ur}\;\;k\in\N_0\]
gilt.\newline Bemerkung: In manchen B"uchern wird die Rolle von 0
und 1 (und damit auch von $p$ und $1-p$) vertauscht, d.h.~man fragt
nach der Wahrscheinlichkeit des ersten Eintretens der Null im
$(k+1)$--ten Versuch ($=(1-p)\,p^{k}$).
\item
Das Verteilungsgesetz $P_\lambda$ mit
\[P_\lambda(k)=\frac{\lambda^k}{k!}e^{-\lambda}\quad
\mbox{f"ur} \;\;k\in{\N}_0\] hei"st {\it Poissonverteilung} mit
Parameter $\lambda>0$. Eine ZG $X$ mit Verteilungsgesetz $P_\lambda$
nennt man Poissonverteilt (mit Parameter $\lambda$). Es gilt dann
${\E}\,X=\lambda$ und $\V(X)=\lambda$.\newline Bemerkung:
Poissonverteilungen beschreiben u.a.~die Wahrscheinlichkeit des
Auftretens seltener Ereignisse bei einer gro"sen Zahl von Versuchen
(Poissonscher Grenzwertsatz) oder die Anzahl von Punkten in einer
zuf"allig ausgew"ahlten Menge, sofern die Punkte "'gleichm"a"sig"'
in der Gesamtmenge verteilt sind.
\end{enumerate}
\item
{\large Wichtige stetige Verteilungen}
\begin{enumerate}
\item
Eine ZG $X$ ist gem"a"s ${\mathcal N}(a,\sigma^2)$ verteilt ({\it
normalverteilt} mit Parametern $a\in\R$ und $\sigma^2>0$), wenn
\[\P(X<x)=\frac{1}{\sqrt{2\pi}\,\sigma}\int_{-\infty}^x
e^{-(t-a)^2/2\sigma^2}dt\] f"ur alle $x\in\R$ gilt. F"ur $a=0$ und
$\sigma=1$ hei"st $X$ {\it standard normalverteilt}.\newline
Bemerkung 1: Ist $X$ standard normalverteilt, so ist $\alpha
X+\beta$ gem"a"s ${\mathcal N}(\beta,\alpha^2)$ verteilt,
$\alpha\not=0$.\newline Bemerkung 2: F"ur $X$ nach ${\mathcal
N}(a,\sigma^2)$--verteilt gilt ${\E}\,X=a$ und
$\V(X)=\sigma^2$.\newline Bemerkung 3: Sind $X_1$ und $X_2$ gem"a"s
${\mathcal N}(a_1,\sigma_1^2)$ bzw.~gem"a"s ${\mathcal
N}(a_2,\sigma_2^2)$ verteilt und unabh"angig, so ist die Summe
$X_1+X_2$ nach ${\mathcal N}(a_1+a_2,\sigma_1^2+\sigma_2^2)$
verteilt.
\item
Eine ZG $X$ hei"st {\it exponential verteilt} mit Parameter
$\lambda>0$, wenn $\P_X$ die Dichte $p$ der Gestalt
\[p(x)=\lambda e^{-\lambda x}\quad\mbox{f"ur}\;\;x>0\quad\mbox{und}\;\;
p(x)=0\quad\mbox{f"ur}\;\;x\le 0\] besitzt. Es gilt dann
${\E}\,X=\lambda^{-1}$ und $\V(X)=\lambda^{-2}$.\newline Bemerkung:
Exponentialverteilungen beschreiben u.a.~die Lebenszeitverteilung
eines nichtalternden Systems, d.h. eines Systems mit konstanter
Ausfallrate.
\item
F"ur $k$ unabh"angige exponentialverteilte ZG mit Parameter
$\lambda>0$ ist die Summe {\it Erlangverteilt} mit Parameter
$(k,\lambda)$. Die Dichte $p$ der Erlangverteilung lautet
\[p(x)=\frac{\lambda^k}{(k-1)!}x^{k-1}e^{-\lambda x}\quad\mbox{f"ur}\;\;x>0
\quad\mbox{und}\;\;p(x)=0\quad\mbox{f"ur}\;\;x\le 0\;.\] Die
Verteilungsfunktion $\F$ der Erlangverteilung vom Parameter
$(k,\lambda)$ berechnet sich als
\[\F(x)=1-\sum_{j=0}^{k-1}\frac{\lambda^j x^j}{j!}\cdot
e^{-\lambda x}\;,\quad x\ge 0\;.\] Bemerkung: Wenn man $k$
gleichartige nichtalternde Systeme hat, und bei Ausfall eines
Systems dieses durch das n"achste ersetzt, so ist die Verteilung der
Gesamtlebenszeit (d.h.~der Zeit bis zum Ausfall des $k$--ten
Systems) eine Erlangverteilung mit Parameter $(k, \lambda)$. Dabei
ist $\lambda$ die Ausfallrate der einzelnen Systeme.
\item
Eine ZG $X$ mit Werten im Intervall $[a,b]$ hei"st {\it
gleichverteilt}, wenn $\P_X$ die Dichte $p$ mit
\[p(x)=\frac{1}{b-a}\quad\mbox{f"ur}\;\;a\le x\le b\quad\mbox{und}\quad p(x)=0
\quad\mbox{f"ur}\;\; x\notin [a,b]\] besitzt.\newline Bemerkung: Es
gilt dann ${\E}\,X=\frac{a+b}{2}$ und ${\V}(X)=\frac{1}{12}(b-a)^2$.
\end{enumerate}
\item
{\large Verteilungen auf ${\R}^n$}
\begin{enumerate}
\item
Sei $B$ eine beschr"ankte abgeschlossene Menge des $\R^n$ und sei
die Funktion $p$ durch $p(x)=0$ f"ur $x\notin B$ und $p(x)=({\rm
vol}_n(B))^{-1}$ f"ur $x\in B$ definiert. Ein zuf"alliger Vektor
$\vec X$ hei"st {\it gleichverteilt} auf $B$, wenn $p$ Dichte von
$\vec X$ ist. \newline Bemerkung: F"ur beliebige $B\subseteq\R^n$
sind die Komponenten $X_1,\ldots,X_n$ von $\vec X$ i.a.~nicht
unabh"angig. Ist allerdings $B$ ein Quader der Form
$B=[\alpha_1,\beta_1]\times\cdots\times[\alpha_n, \beta_n]$, so sind
die $X_j$ unabh"angig und die Verteilungsgesetze $\P_{X_j}$ sind
Gleichverteilungen auf $[\alpha_j,\beta_j]$.
\end{enumerate}
\item
{\large Grenzwerts"atze}
\begin{enumerate}
\item
F"ur $k\in\N_0$ und $\lambda>0$ gilt
\[\lim_{n\to\infty}B_{n,\lambda/n}(k)=P_{\lambda}(k)=\frac{\lambda^k}{k!}e^{
-\lambda}\;.\] ({\it Poissonscher Grenzwertsatz})\newline Bemerkung:
Werden $N$ ($N$ "'sehr gro"s"') unabh"angige Versuche ausgef"uhrt,
wobei die Wahrscheinlichkeit $p$ f"ur das Auftreten einer $1$ "'sehr
klein"' ist (mit Wahrscheinlichkeit $1-p$ erscheint die $0$), so ist
die Anzahl der Einsen bei diesen $N$ Versuchen "'ungef"ahr"'
Poissonverteilt mit Parameter $p\cdot N$.\newline
\item
Sei $X$ eine ZG mit Werten in $\R$ und mit zweitem Moment. Dann
besagt die {\it Tschebyschevsche Ungleichung}, da"s die Absch"atzung
\[\P\big\{\,|X-\E\,X|\ge c\big\}\le\frac{\V(X)}{c^2}\quad
\mbox{f"ur}\;\;c>0\] richtig ist. Die Varianz einer ZG ist also ein
Ma"s daf"ur, mit welcher Wahrscheinlichkeit sich die Werte der ZG
von ihrem Mittelwert entfernen k"onnen.
\item
Sei $X,X_1,X_2,\ldots$ eine Folge unabh"angiger identisch verteilter ZG, d.h~%
es gilt $X_j\stackrel{d}{=}X$ f"ur alle nat"urlichen Zahlen $j$.
Existiert der Erwartungswert $\E\,X:=a$ (da die ZG identisch
verteilt sind, haben alle den gleichen Erwartungswert), so folgt
\[\lim_{n\to\infty}\P\Big\{\Big|\frac{X_1+\cdots+X_n}{n}-a
\Big|>\varepsilon\Big\}=0\quad\mbox{f"ur alle}\;\;\varepsilon>0\;.\]
({\it Schwaches Gesetz der gro"sen Zahlen})\newline Bemerkung 1:
Besitzt $X$ sogar ein zweites Moment, so kann die obige Limesaussage
genauer gefa"st werden. Es gilt dann n"amlich
\[\P\Big\{\Big|\frac{X_1+\cdots+X_n}{n}-a
\Big|>\varepsilon\Big\}\le\frac{\V(X)}{n\varepsilon^2}\quad
\mbox{f"ur alle}\;\;\varepsilon>0\;.\] Bemerkung 2: F"ur
$A\subseteq\R$ offen oder abgeschlossen folgt aus dem obigen Satz
\[\lim_{n\to\infty}\P\Big\{\Big|\frac{\#\{j\le n;\,X_j\in A\}}
{n}-\P_{X}(A)\Big|>\varepsilon\Big\}=0\quad\mbox{f"ur alle}\;\;
\varepsilon>0\;,\] d.h. die Wahrscheinlichkeit, da"s die relative
H"aufigkeit einer Menge $A$ von ihrer Wahrscheinlichkeit um mehr als
um $\varepsilon$ abweicht, wird bei einer gen"ugend gro"sen Anzahl
von Versuchen beliebig klein.
\item
Im Fall des Bernoullischemas liefert das "'Schwache Gesetz der
gro"sen Zahlen"' folgenden auf J. Bernoulli zur"uckgehenden
Satz:\newline Sind $X_1,X_2,\ldots$ nach $B_{1,p}$ verteilt und
unabh"angig, so gilt f"ur $\mu_n:=\#\{j\le n;\,X_j=1\}$ die Aussage
\[\lim_{n\to\infty}\P\Big\{\Big|\frac{\mu_n}{n}-p\Big|<\varepsilon\Big\}=1\quad
\mbox{f"ur alle}\;\;\varepsilon>0\;.\]
\item
Das {\it starke Gesetz der gro"sen Zahlen} besagt, da"s nicht nur
die Wahrscheinlichkeit des Abweichens von $\sum_{j=1}^n X_j /n$ von
$\E\,X$ beliebig klein wird, sondern f"ur eine geeignete Teilmenge
$\Omega_0\subseteq\Omega$ mit $\P(\Omega_0)=1$  gilt sogar
\[\lim_{n\to\infty}\frac{X_1(\omega)+\cdots+X_n(\omega)}{n}=
\E\,X\quad\mbox{f"ur alle}\;\;\omega\in\Omega_0\;,\] d.h.~zu
$\varepsilon>0$ und $\omega\in\Omega_0$ existiert eine (zuf"allige,
also von $\omega$ abh"angende) Stelle  $n_0\in\N$, so da"s
\[\left|\frac{X_1(\omega)+\cdots X_n(\omega)}{n}-\E\,X\right|<\varepsilon\]
f"ur $n\ge n_0$ gilt. Insbesondere folgt f"ur $A\subseteq\R$ offen
oder abgeschlossen und f"ur $\omega\in\Omega_0$ die Aussage
\[\lim_{n\to\infty}\frac{\#\{j\le n;\,X_j(\omega)\in A\}}{n}=
\P_{X}(A)\;,\] d.h.~die relativen H"aufigkeiten einer Menge
konvergieren fast sicher gegen deren Wahrscheinlichkeit.
\item
Sei $X,X_1,X_2,\ldots$ wieder eine Folge unabh"angiger indentisch
verteilter ZG mit $\E\,X=a$ und $\V(X)=\sigma^2$. Der {\it Zentrale
Grenzwertsatz} besagt dann, da"s f"ur beliebige reelle Zahlen
$\alpha,\beta$ mit $-\infty<\alpha<\beta<\infty$ stets
\[\lim_{n\to\infty}\P\Big\{\alpha\le\sum_{j=1}^n\frac{X_j-a}{
\sqrt n\sigma}\le\beta\Big\}=\frac{1}{\sqrt{2\pi}}\int_\alpha^\beta
e^{-t^2/2}dt\] folgt. Die Konvergenz ist au"serdem gleichm"a"sig
bzgl.~$\alpha$ und $\beta$. Eine andere Formulierung lautet: Sind
die ZG $Y_j$ durch $Y_j=(X_j-\E\,X_j)/\V(X_j)^{1/2}$ definiert, so
hat man
\[\lim_{n\to\infty}\P\Big\{n^{-1/2}\sum_{j=1}^nY_j\le t\Big\}=\Phi(t)\]
gleichm"a"sig f"ur $t\in\R$, wobei $\Phi$ die Verteilungsfunktion
einer ${\mathcal N}(0,1)$--verteilten ZG ist, d.h.\newline
$\Phi(t)=(2\pi)^{-1/2}\int_{-\infty}^te^{-\tau^2/2}d\tau$.
\item
Im Spezialfall von binomialverteilten ZG erh"alt man aus dem
Zentralen Grenzwertsatzes den {\it Grenzwertsatz von
Moivre--Laplace}:

Sind die ZG $X_j$ unabh"angig und gem"a"s $B_{1,p}$ verteilt, so
folgt
\[\lim_{n\to\infty}\P\Big\{\alpha\le\sum_{j=1}^n\frac{X_j- p}
{\sqrt{np(1-p)}}\le
\beta\Big\}=\frac{1}{\sqrt{2\pi}}\int_\alpha^\beta
e^{-\tau^2/2}d\tau\quad\mbox{f"ur}\;\;\alpha<\beta\;.\] In anderen
Worten, es gilt
\[\lim_{n\to\infty}\sup_{t\in\R}\big|\P
\big\{\sum_{j=1}^nX_j\le t\big\}-\P(Y_n\le t)\big|=0\;,\] wobei die
$Y_n$ gem"a"s ${\mathcal N}\big(n p,n p\, (1-p)\big)$ verteilt sind.
\end{enumerate}
\end{enumerate}
\end{document}
